From 42acf1146a13d2fbdaeb2ff42b5680754b70db52 Mon Sep 17 00:00:00 2001
From: Hua Ma <hua.ma@linux.intel.com>
Date: Thu, 21 Jun 2018 17:37:48 +0800
Subject: [PATCH] Add support for lantiq datapath driver common

---
 drivers/net/ethernet/lantiq/datapath/Kconfig       |  127 +
 drivers/net/ethernet/lantiq/datapath/Makefile      |   12 +
 drivers/net/ethernet/lantiq/datapath/datapath.h    |  746 ++++++
 .../net/ethernet/lantiq/datapath/datapath_api.c    | 2711 ++++++++++++++++++++
 .../ethernet/lantiq/datapath/datapath_instance.c   |  654 +++++
 .../ethernet/lantiq/datapath/datapath_instance.h   |   86 +
 .../lantiq/datapath/datapath_logical_dev.c         |  207 ++
 .../lantiq/datapath/datapath_loopeth_dev.c         | 1671 ++++++++++++
 .../net/ethernet/lantiq/datapath/datapath_misc.c   | 1123 ++++++++
 .../ethernet/lantiq/datapath/datapath_notifier.c   |  197 ++
 .../lantiq/datapath/datapath_platform_dev.c        |   62 +
 .../net/ethernet/lantiq/datapath/datapath_proc.c   | 2241 ++++++++++++++++
 .../ethernet/lantiq/datapath/datapath_proc_api.c   |  317 +++
 .../ethernet/lantiq/datapath/datapath_proc_qos.c   | 1464 +++++++++++
 .../net/ethernet/lantiq/datapath/datapath_qos.c    |  229 ++
 .../net/ethernet/lantiq/datapath/datapath_soc.c    |   60 +
 .../net/ethernet/lantiq/datapath/datapath_swdev.c  | 1134 ++++++++
 .../net/ethernet/lantiq/datapath/datapath_swdev.h  |   69 +
 .../ethernet/lantiq/datapath/datapath_swdev_api.h  |   22 +
 .../datapath/gswip31/datapath_tc_asym_vlan.c       |  804 ++++++
 include/net/datapath_api.h                         | 1040 ++++++++
 include/net/datapath_api_qos.h                     | 1503 +++++++++++
 include/net/datapath_api_skb.h                     |  179 ++
 include/net/datapath_api_vlan.h                    |  172 ++
 include/net/datapath_inst.h                        |  155 ++
 include/net/datapath_proc_api.h                    |   84 +
 26 files changed, 17069 insertions(+)

diff --git a/drivers/net/ethernet/lantiq/datapath/Kconfig b/drivers/net/ethernet/lantiq/datapath/Kconfig
new file mode 100644
index 000000000000..9ed63cc2a1d4
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/Kconfig
@@ -0,0 +1,127 @@
+#
+# Datapath Lib
+#
+menuconfig LTQ_DATAPATH
+	bool "Datapath LIB"
+	default y
+	depends on LTQ_CBM
+	---help---
+	  Datapath Lib is to provide common rx/tx wrapper Lib without taking
+	  care of much HW knowledge and also provide common interface for legacy
+	  devices and different HW like to CBM or LRO.
+	  Take note: All devices need to register to datapath Lib first
+
+if LTQ_DATAPATH
+config LTQ_DATAPATH_ACA_CSUM_WORKAROUND
+	bool "ACA Checksum Workaround"
+	default n
+	depends on SOC_GRX500 && LTQ_DATAPATH
+	---help---
+	  It is to solve system bus hang hang issue in GRX500
+	  Once there is SW-UMT, there is no need to enable it
+	  in Facon-MX and later SOC, HW bug is fixed already
+	  So it should be disabled
+
+config LTQ_DATAPATH_MANUAL_PARSE
+	bool "Datapath manual parse network protocol"
+	depends on LTQ_DATAPATH
+	default y
+	---help---
+	  Manual parse network protocol for tcp offloading
+	  Only support limited tunnel yet
+	  later need to enhance to support other tunnels
+	  Also need to study to use network stack information
+
+config LTQ_DATAPATH_COPY_LINEAR_BUF_ONLY
+	bool "Datapath Copy linear buffer only for skb"
+	default n
+	depends on LTQ_DATAPATH
+	---help---
+	  Datapath Copy linear buffer only for skb if need to alloc new buffer.
+	  For TSO/GSO case, it will not consider
+	  Make sure TSO/GSO always with enough header room to insert pmac header
+	  need to enhance in the future
+
+config LTQ_DATAPATH_DBG
+	bool "Datapath Debug Tool"
+	default y
+	depends on LTQ_DATAPATH
+	---help---
+	  Datapath Debug Tool is used to provide simple debug tool
+	  All other debug tools is based on it
+	  Once it is disabled, all other datapath debug tool disabled.
+	  By default had better enable it
+
+config LTQ_DATAPATH_DBG_PROTOCOL_PARSE
+	bool "Datapath Debug Tool for hw checksum's protocol parsing"
+	default n
+	depends on LTQ_DATAPATH_DBG
+	---help---
+	  Datapath Debug Tool for hw checksum's protocol parsing
+	  Only for debugging purpose
+	  By default it should be disabled.
+config LTQ_DATAPATH_EXTRA_DEBUG
+	bool "extra debugging support"
+	default n
+	depends on  LTQ_DATAPATH_DBG
+	---help---
+	  This is to enable/disable extra strict debugging support.
+	  This is useful during initial system bring up
+	  It will affect performance
+	  By default it should be disabled.
+
+config LTQ_DATAPATH_SWDEV_TEST
+	bool "Test Switchdev Event"
+	default n
+	depends on  LTQ_DATAPATH_DBG && LTQ_DATAPATH_SWITCHDEV
+	---help---
+	  This is to force enable macro CONFIG_SOC_SWITCHDEV_TESTING
+	  in order to test switchdev event
+	  without real switchdev handling
+
+config LTQ_DATAPATH_SKB
+	bool "Datapath Skb Hack"
+	default n
+	depends on  LTQ_DATAPATH
+	---help---
+	  For Ethernet OAM and MPE FW purpose testing purpose,
+	  It needs to hack SKB
+
+config LTQ_DATAPATH_MPE_FASTHOOK_TEST
+	bool "MPE Fast Hook Test"
+	default n
+	depends on  LTQ_DATAPATH_SKB
+	---help---
+	  MPE FW Fast Hook is used to quick verify MPE FW Functionality without
+	  full PPA support. Once it is enabled, it will add some fields in skb structure
+	  in order to support MPE FAST HOOK. The reason is that some network driver is
+	  pre-build out of this build system.
+	  The testing code by default is not checked in.
+
+config LTQ_DATAPATH_ETH_OAM
+	bool "ETH OAM SUPPORT"
+	default n
+	depends on  LTQ_DATAPATH_SKB
+	---help---
+	  Datapath Ethernet OAM support. Once it is enabled, it will add some fields in skb structure
+	  in order to support MPE FAST HOOK. The reason is that some network driver is
+	  pre-build out of this build system.
+	  The testing code by default is not checked in.
+
+config LTQ_DATAPATH_SWITCHDEV
+	bool "Switchdev Support"
+	default n
+	depends on  LTQ_DATAPATH && NET_SWITCHDEV
+	---help---
+	  Switchdev support for different switch in datapath
+
+config LTQ_DATAPATH_DDR_SIMULATE_GSWIP31
+	bool "Force FALCON-MX SOC"
+	default n
+	depends on LTQ_DATAPATH
+	---help---
+	  test falcon-mx HAL in GRX350 boards
+source "drivers/net/ethernet/lantiq/datapath/gswip31/Kconfig"
+source "drivers/net/ethernet/lantiq/datapath/gswip30/Kconfig"
+endif
+
diff --git a/drivers/net/ethernet/lantiq/datapath/Makefile b/drivers/net/ethernet/lantiq/datapath/Makefile
new file mode 100644
index 000000000000..0367e49fade9
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/Makefile
@@ -0,0 +1,12 @@
+obj-$(CONFIG_LTQ_DATAPATH) = datapath_api.o datapath_proc_api.o datapath_proc.o  datapath_misc.o datapath_notifier.o datapath_logical_dev.o datapath_instance.o datapath_platform_dev.o datapath_soc.o datapath_qos.o datapath_proc_qos.o
+
+ifneq ($(CONFIG_LTQ_DATAPATH_HAL_GSWIP31),)
+obj-$(CONFIG_LTQ_DATAPATH) += gswip31/
+endif
+ifneq ($(CONFIG_LTQ_DATAPATH_HAL_GSWIP30),)
+obj-$(CONFIG_LTQ_DATAPATH) += gswip30/
+endif
+
+ifneq ($(CONFIG_LTQ_DATAPATH_SWITCHDEV),)
+obj-$(CONFIG_LTQ_DATAPATH) += datapath_swdev.o
+endif
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath.h b/drivers/net/ethernet/lantiq/datapath/datapath.h
new file mode 100644
index 000000000000..71e716d82a66
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/datapath.h
@@ -0,0 +1,746 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#ifndef DATAPATH_H
+#define DATAPATH_H
+
+#include <linux/klogging.h>
+#include <linux/skbuff.h>	/*skb */
+#include <linux/types.h>
+#include <linux/netdevice.h>
+#include <linux/platform_device.h>
+#include <net/lantiq_cbm_api.h>
+
+//#define CONFIG_LTQ_DATAPATH_DUMMY_QOS
+//#define DUMMY_PPV4_QOS_API_OLD
+
+#ifdef DUMMY_PPV4_QOS_API_OLD
+/*TODO:currently need to include both header file */
+#include <net/pp_qos_drv_slim.h>
+#include <net/pp_qos_drv.h>
+#else
+#include <net/pp_qos_drv.h>
+#endif
+#include <net/datapath_api_qos.h>
+#ifdef CONFIG_NET_SWITCHDEV
+#include <net/switchdev.h>
+#endif
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_SWITCHDEV)
+#include "datapath_swdev.h"
+#endif
+#include <net/datapath_inst.h>
+
+#define MAX_SUBIFS 256
+#define MAX_DP_PORTS  16
+#define PMAC_SIZE 8
+#define PMAC_CPU_ID  0
+#define DP_MAX_BP_NUM 128
+#define DP_MAX_QUEUE_NUM 256
+#define DP_MAX_SCHED_NUM 2048  /*In fact it should use max number of node*/
+#define DP_MAX_CQM_DEQ 128 /*CQM dequeue port*/
+
+#ifdef LOGF_KLOG_ERROR
+#define PR_ERR  LOGF_KLOG_ERROR
+#else
+#define PR_ERR printk
+#endif
+
+#ifdef LOGF_KLOG_INFO
+#undef PR_INFO
+#define PR_INFO LOGF_KLOG_ERROR
+#else
+#undef PR_INFO
+#define PR_INFO printk
+#endif
+
+#ifdef LOGF_KLOG_INFO_ONCE
+#define PR_INFO_ONCE    LOGF_KLOG_INFO_ONCE
+#else
+#define PR_INFO_ONCE printk_once
+#endif
+
+#ifdef LOGF_KLOG_RATELIMITED
+#define PR_RATELIMITED LOGF_KLOG_RATELIMITED
+#else
+#define PR_RATELIMITED printk_ratelimited
+#endif
+
+#define DP_PLATFORM_INIT    1
+#define DP_PLATFORM_DE_INIT 2
+
+#define UP_STATS(atomic) atomic_add(1, &(atomic))
+
+#define STATS_GET(atomic) atomic_read(&(atomic))
+#define STATS_SET(atomic, val) atomic_set(&(atomic), val)
+#define DP_CB(i, x) dp_port_prop[i].info.x
+
+#define PORT(inst, ep) &dp_port_info[inst][ep]
+#define PORT_INFO(inst, ep, x) dp_port_info[inst][ep].x
+#define PORT_SUBIF(inst, ep, ix, x) dp_port_info[inst][ep].subif_info[ix].x
+#define PORT_VAP_MIB(i, ep, vap, x) dp_port_info[i][ep].subif_info[vap].mib.x
+#define PORT_VAP(i, ep, vap, x) dp_port_info[i][ep].subif_info[vap].x
+
+#define dp_set_val(reg, val, mask, offset) do {\
+	(reg) &= ~(mask);\
+	(reg) |= (((val) << (offset)) & (mask));\
+} while (0)
+
+#define dp_get_val(val, mask, offset) (((val) & (mask)) >> (offset))
+
+#define DP_DEBUG_ASSERT(expr, fmt, arg...)  do { if (expr) \
+	PR_ERR(fmt, ##arg); \
+} while (0)
+
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_DBG)
+#define DP_DEBUG(flags, fmt, arg...)  do { \
+	if (unlikely((dp_dbg_flag & (flags)) && \
+		     (((dp_print_num_en) && \
+		      (dp_max_print_num)) || (!dp_print_num_en)))) {\
+	PR_INFO(fmt, ##arg); \
+	if ((dp_print_num_en) && \
+	    (dp_max_print_num)) \
+		dp_max_print_num--; \
+	} \
+} while (0)
+
+#define DP_ASSERT_SCOPE __func__
+
+#else
+#define DP_DEBUG(flags, fmt, arg...)
+#endif				/* end of CONFIG_LTQ_DATAPATH_DBG */
+
+#define IFNAMSIZ 16
+#define DP_MAX_HW_CAP 4
+
+/*#define DP_SPIN_LOCK */
+#ifdef DP_SPIN_LOCK
+#define DP_LIB_LOCK    spin_lock_bh
+#define DP_LIB_UNLOCK  spin_unlock_bh
+#else
+#define DP_LIB_LOCK    mutex_lock
+#define DP_LIB_UNLOCK  mutex_unlock
+#endif
+
+#define PARSER_FLAG_SIZE   40
+#define PARSER_OFFSET_SIZE 8
+
+#define PKT_PASER_FLAG_OFFSET   0
+#define PKT_PASER_OFFSET_OFFSET (PARSER_FLAG_SIZE)
+#define PKT_PMAC_OFFSET         ((PARSER_FLAG_SIZE) + (PARSER_OFFSET_SIZE))
+#define PKT_DATA_OFFSET         ((PKT_PMAC_OFFSET) + (PMAC_SIZE))
+
+#define CHECK_BIT(var, pos) (((var) & (1 << (pos))) ? 1 : 0)
+
+#define PASAR_OFFSETS_NUM 40	/*40 bytes offset */
+#define PASAR_FLAGS_NUM 8	/*8 bytes */
+
+#define GET_VAP(subif, bit_shift, mask) (((subif) >> (bit_shift)) & (mask))
+#define SET_VAP(vap, bit_shift, mask) ((((u32)vap) & (mask)) << (bit_shift))
+
+enum dp_xmit_errors {
+	DP_XMIT_ERR_DEFAULT = 0,
+	DP_XMIT_ERR_NOT_INIT,
+	DP_XMIT_ERR_IN_IRQ,
+	DP_XMIT_ERR_NULL_SUBIF,
+	DP_XMIT_ERR_PORT_TOO_BIG,
+	DP_XMIT_ERR_NULL_SKB,
+	DP_XMIT_ERR_NULL_IF,
+	DP_XMIT_ERR_REALLOC_SKB,
+	DP_XMIT_ERR_EP_ZERO,
+	DP_XMIT_ERR_GSO_NOHEADROOM,
+	DP_XMIT_ERR_CSM_NO_SUPPORT,
+};
+
+enum PARSER_FLAGS {
+	PASER_FLAGS_NO = 0,
+	PASER_FLAGS_END,
+	PASER_FLAGS_CAPWAP,
+	PASER_FLAGS_GRE,
+	PASER_FLAGS_LEN,
+	PASER_FLAGS_GREK,
+	PASER_FLAGS_NN1,
+	PASER_FLAGS_NN2,
+
+	PASER_FLAGS_ITAG,
+	PASER_FLAGS_1VLAN,
+	PASER_FLAGS_2VLAN,
+	PASER_FLAGS_3VLAN,
+	PASER_FLAGS_4VLAN,
+	PASER_FLAGS_SNAP,
+	PASER_FLAGS_PPPOES,
+	PASER_FLAGS_1IPV4,
+
+	PASER_FLAGS_1IPV6,
+	PASER_FLAGS_2IPV4,
+	PASER_FLAGS_2IPV6,
+	PASER_FLAGS_ROUTEXP,
+	PASER_FLAGS_TCP,
+	PASER_FLAGS_1UDP,
+	PASER_FLAGS_IGMP,
+	PASER_FLAGS_IPV4OPT,
+
+	PASER_FLAGS_IPV6EXT,
+	PASER_FLAGS_TCPACK,
+	PASER_FLAGS_IPFRAG,
+	PASER_FLAGS_EAPOL,
+	PASER_FLAGS_2IPV6EXT,
+	PASER_FLAGS_2UDP,
+	PASER_FLAGS_L2TPNEXP,
+	PASER_FLAGS_LROEXP,
+
+	PASER_FLAGS_L2TP,
+	PASER_FLAGS_GRE_VLAN1,
+	PASER_FLAGS_GRE_VLAN2,
+	PASER_FLAGS_GRE_PPPOE,
+	PASER_FLAGS_BYTE4_BIT4,
+	PASER_FLAGS_BYTE4_BIT5,
+	PASER_FLAGS_BYTE4_BIT6,
+	PASER_FLAGS_BYTE4_BIT7,
+
+	PASER_FLAGS_BYTE5_BIT0,
+	PASER_FLAGS_BYTE5_BIT1,
+	PASER_FLAGS_BYTE5_BIT2,
+	PASER_FLAGS_BYTE5_BIT3,
+	PASER_FLAGS_BYTE5_BIT4,
+	PASER_FLAGS_BYTE5_BIT5,
+	PASER_FLAGS_BYTE5_BIT6,
+	PASER_FLAGS_BYTE5_BIT7,
+
+	PASER_FLAGS_BYTE6_BIT0,
+	PASER_FLAGS_BYTE6_BIT1,
+	PASER_FLAGS_BYTE6_BIT2,
+	PASER_FLAGS_BYTE6_BIT3,
+	PASER_FLAGS_BYTE6_BIT4,
+	PASER_FLAGS_BYTE6_BIT5,
+	PASER_FLAGS_BYTE6_BIT6,
+	PASER_FLAGS_BYTE6_BIT7,
+
+	PASER_FLAGS_BYTE7_BIT0,
+	PASER_FLAGS_BYTE7_BIT1,
+	PASER_FLAGS_BYTE7_BIT2,
+	PASER_FLAGS_BYTE7_BIT3,
+	PASER_FLAGS_BYTE7_BIT4,
+	PASER_FLAGS_BYTE7_BIT5,
+	PASER_FLAGS_BYTE7_BIT6,
+	PASER_FLAGS_BYTE7_BIT7,
+
+	/*Must be put at the end of the enum */
+	PASER_FLAGS_MAX
+};
+
+/*! PMAC port flag */
+enum PORT_FLAG {
+	PORT_FREE = 0,		/*! The port is free */
+	PORT_ALLOCATED,		/*! the port is already allocated to others,
+				 * but not registered or no need to register.\n
+				 * eg, LRO/CAPWA, only need to allocate,
+				 * but no need to register
+				 */
+	PORT_DEV_REGISTERED,	/*! dev Registered already. */
+	PORT_SUBIF_REGISTERED,	/*! subif Registered already. */
+
+	PORT_FLAG_NO_VALID	/*! Not valid flag */
+};
+
+#define DP_DBG_ENUM_OR_STRING(name, value, short_name) {name = value}
+
+enum PMAC_TCP_TYPE {
+	TCP_OVER_IPV4 = 0,
+	UDP_OVER_IPV4,
+	TCP_OVER_IPV6,
+	UDP_OVER_IPV6,
+	TCP_OVER_IPV6_IPV4,
+	UDP_OVER_IPV6_IPV4,
+	TCP_OVER_IPV4_IPV6,
+	UDP_OVER_IPV4_IPV6
+};
+
+enum DP_DBG_FLAG {
+	DP_DBG_FLAG_DBG = BIT(0),
+	DP_DBG_FLAG_DUMP_RX_DATA = BIT(1),
+	DP_DBG_FLAG_DUMP_RX_DESCRIPTOR = BIT(2),
+	DP_DBG_FLAG_DUMP_RX_PASER = BIT(3),
+	DP_DBG_FLAG_DUMP_RX_PMAC = BIT(4),
+	DP_DBG_FLAG_DUMP_RX = (BIT(1) | BIT(2) | BIT(3) | BIT(4)),
+	DP_DBG_FLAG_DUMP_TX_DATA = BIT(5),
+	DP_DBG_FLAG_DUMP_TX_DESCRIPTOR = BIT(6),
+	DP_DBG_FLAG_DUMP_TX_PMAC = BIT(7),
+	DP_DBG_FLAG_DUMP_TX_SUM = BIT(8),
+	DP_DBG_FLAG_DUMP_TX = (BIT(5) | BIT(6) | BIT(7) | BIT(8)),
+	DP_DBG_FLAG_COC = BIT(9),
+	DP_DBG_FLAG_MIB = BIT(10),
+	DP_DBG_FLAG_MIB_ALGO = BIT(11),
+	DP_DBG_FLAG_CBM_BUF = BIT(12),
+	DP_DBG_FLAG_PAE = BIT(13),
+	DP_DBG_FLAG_INST = BIT(14),
+	DP_DBG_FLAG_SWDEV = BIT(15),
+	DP_DBG_FLAG_NOTIFY = BIT(16),
+	DP_DBG_FLAG_LOGIC = BIT(17),
+	DP_DBG_FLAG_GSWIP_API = BIT(18),
+	DP_DBG_FLAG_QOS = BIT(19),
+	DP_DBG_FLAG_QOS_DETAIL = BIT(20),
+	DP_DBG_FLAG_LOOKUP = BIT(21),
+	DP_DBG_FLAG_REG = BIT(22),
+
+	/*Note, once add a new entry here int the enum,
+	 *need to add new item in below macro DP_F_FLAG_LIST
+	 */
+	DP_DBG_FLAG_MAX = BIT(31)
+};
+
+/*Note: per bit one variable */
+#define DP_DBG_FLAG_LIST {\
+	DP_DBG_ENUM_OR_STRING(DP_DBG_FLAG_DBG, "dbg"), \
+	DP_DBG_ENUM_OR_STRING(DP_DBG_FLAG_DUMP_RX_DATA, "rx_data"), \
+	DP_DBG_ENUM_OR_STRING(DP_DBG_FLAG_DUMP_RX_DESCRIPTOR, "rx_desc"), \
+	DP_DBG_ENUM_OR_STRING(DP_DBG_FLAG_DUMP_RX_PASER, "rx_parse"), \
+	DP_DBG_ENUM_OR_STRING(DP_DBG_FLAG_DUMP_RX_PMAC, "rx_pmac"), \
+	DP_DBG_ENUM_OR_STRING(DP_DBG_FLAG_DUMP_RX, "rx"), \
+	DP_DBG_ENUM_OR_STRING(DP_DBG_FLAG_DUMP_TX_DATA, "tx_data"), \
+	DP_DBG_ENUM_OR_STRING(DP_DBG_FLAG_DUMP_TX_DESCRIPTOR, "tx_desc"), \
+	DP_DBG_ENUM_OR_STRING(DP_DBG_FLAG_DUMP_TX_PMAC, "tx_pmac"), \
+	DP_DBG_ENUM_OR_STRING(DP_DBG_FLAG_DUMP_TX_SUM, "tx_sum"), \
+	DP_DBG_ENUM_OR_STRING(DP_DBG_FLAG_DUMP_TX, "tx"), \
+	DP_DBG_ENUM_OR_STRING(DP_DBG_FLAG_COC, "coc"), \
+	DP_DBG_ENUM_OR_STRING(DP_DBG_FLAG_MIB, "mib"), \
+	DP_DBG_ENUM_OR_STRING(DP_DBG_FLAG_MIB_ALGO, "mib_algo"), \
+	DP_DBG_ENUM_OR_STRING(DP_DBG_FLAG_CBM_BUF, "cbm_buf"), \
+	DP_DBG_ENUM_OR_STRING(DP_DBG_FLAG_PAE, "pae"), \
+	DP_DBG_ENUM_OR_STRING(DP_DBG_FLAG_INST, "inst"), \
+	DP_DBG_ENUM_OR_STRING(DP_DBG_FLAG_SWDEV, "swdev"), \
+	DP_DBG_ENUM_OR_STRING(DP_DBG_FLAG_NOTIFY, "notify"), \
+	DP_DBG_ENUM_OR_STRING(DP_DBG_FLAG_LOGIC, "logic"), \
+	DP_DBG_ENUM_OR_STRING(DP_DBG_FLAG_GSWIP_API, "gswip"), \
+	DP_DBG_ENUM_OR_STRING(DP_DBG_FLAG_QOS, "qos"), \
+	DP_DBG_ENUM_OR_STRING(DP_DBG_FLAG_QOS_DETAIL, "qos2"), \
+	DP_DBG_ENUM_OR_STRING(DP_DBG_FLAG_LOOKUP, "lookup"), \
+	DP_DBG_ENUM_OR_STRING(DP_DBG_FLAG_REG, "register"), \
+	\
+	\
+	/*must be last one */\
+	DP_DBG_ENUM_OR_STRING(DP_DBG_FLAG_MAX, "")\
+}
+
+enum {
+	NODE_LINK_ADD = 0, /*add a link node */
+	NODE_LINK_GET,     /*get a link node */
+	NODE_LINK_EN_GET,  /*Get link status: enable/disable */
+	NODE_LINK_EN_SET,  /*Set link status: enable/disable */
+	NODE_UNLINK,       /*unlink a node: in fact, it is just flush now*/
+	LINK_ADD,          /*add a link with multiple link nodes */
+	LINK_GET,          /*get a link may with multiple link nodes */
+	LINK_PRIO_SET,     /*set arbitrate/priority */
+	LINK_PRIO_GET,     /*get arbitrate/priority */
+	QUEUE_CFG_SET,     /*set queue configuration */
+	QUEUE_CFG_GET,     /*get queue configuration */
+	SHAPER_SET,        /*set shaper/bandwidth*/
+	SHAPER_GET,        /*get shaper/bandwidth*/
+	NODE_ALLOC,        /*allocate a node */
+	NODE_FREE,         /*free a node */
+	NODE_CHILDREN_FREE,  /*free all children under one specified parent:
+			      *   scheduler/port
+			      */
+	DEQ_PORT_RES_GET,  /*get all full links under one specified port */
+	COUNTER_MODE_SET,  /*set counter mode: may only for TMU now so far*/
+	COUNTER_MODE_GET,  /*get counter mode: may only for TMU now so far*/
+	QUEUE_MAP_GET,     /*get lookup entries based on the specified qid*/
+	QUEUE_MAP_SET,     /*set lookup entries to the specified qid*/
+	NODE_CHILDREN_GET, /*get direct children list of node*/
+	QOS_LEVEL_GET,     /* get Max Scheduler level for Node */
+};
+
+struct dev_mib {
+	atomic_t rx_fn_rxif_pkt; /*! received packet counter */
+	atomic_t rx_fn_txif_pkt; /*! transmitted packet counter */
+	atomic_t rx_fn_dropped; /*! transmitted packet counter */
+	atomic_t tx_cbm_pkt; /*! transmitted packet counter */
+	atomic_t tx_clone_pkt; /*! duplicate unicast packet for cloned flag */
+	atomic_t tx_hdr_room_pkt; /*! duplicate pkt for no enough headerroom*/
+	atomic_t tx_tso_pkt;	/*! transmitted packet counter */
+	atomic_t tx_pkt_dropped;	/*! dropped packet counter */
+};
+
+struct logic_dev {
+	struct list_head list;
+	struct net_device *dev;
+	u16 bp; /*bridge port */
+	u16 ep;
+	u16 ctp;
+	u32 subif_flag; /*save the flag used during dp_register_subif*/
+};
+
+/*! Sub interface detail information */
+struct dp_subif_info {
+	s32 flags;
+	u32 subif:15;
+	struct net_device *netif; /*! pointer to  net_device */
+	char device_name[IFNAMSIZ]; /*! devide name, like wlan0, */
+	struct dev_mib mib; /*! mib */
+	struct net_device *ctp_dev; /*CTP dev for PON pmapper case*/
+	u16 bp; /*bridge port */
+	u16 fid; /* switch bridge id */
+	struct list_head logic_dev; /*unexplicit logical dev*/
+	struct net_device_ops *old_dev_ops;
+	struct net_device_ops new_dev_ops;
+#ifdef CONFIG_NET_SWITCHDEV
+	struct switchdev_ops *old_swdev_ops;
+	struct switchdev_ops new_swdev_ops;
+	void *swdev_priv; /*to store ext vlan info*/
+#endif
+	s16 qid;    /* physical queue id */
+	s16 sched_id; /* can be physical scheduler id or logical node id */
+	s16 q_node; /* logical queue node Id if applicable */
+	s16 qos_deq_port; /* qos port id */
+	s16 cqm_deq_port; /* CQM physical dequeue port ID (absolute) */
+	s16 cqm_port_idx; /* CQM relative dequeue port index, like tconf id */
+	u32 subif_flag; /* To store original flag from caller during
+			 * dp_register_subif
+			 */
+};
+
+struct vlan_info {
+	u16 out_proto;
+	u16 out_vid;
+	u16 in_proto;
+	u16 in_vid;
+	int cnt;
+};
+
+#define MAX_TEMPLATE 3
+#define TEMPL_NORMAL 0
+#define TEMPL_CHECKSUM 1
+#define TEMPL_OTHERS   2
+
+enum DP_PRIV_F {
+	DP_PRIV_PER_CTP_QUEUE = BIT(0), /*Manage Queue per CTP/subif */
+};
+
+struct pmac_port_info {
+	enum PORT_FLAG status;	/*! port status */
+	int alloc_flags;	/* the flags saved when calling dp_port_alloc */
+	struct dp_cb cb;	/*! Callback Pointer to DIRECTPATH_CB */
+	struct module *owner;
+	struct net_device *dev;
+	u32 dev_port;
+	u32 num_subif;
+	s32 port_id;
+	struct dp_subif_info subif_info[MAX_SUBIFS];
+	atomic_t tx_err_drop;
+	atomic_t rx_err_drop;
+	struct gsw_itf *itf_info;  /*point to switch interface configuration */
+	int ctp_max; /*maximum ctp */
+	u32 vap_offset; /*shift bits to get vap value */
+	u32 vap_mask; /*get final vap after bit shift */
+	u8  cqe_lu_mode; /*cqe lookup mode */
+
+	u32 flag_other; /*save flag from cbm_dp_port_alloc */
+	u32 deq_port_base; /*CQE Dequeue Port */
+	u32 deq_port_num;  /*for PON IP: 64 ports, others: 1 */
+	u32 dma_chan; /*associated dma tx CH,-1 means no DMA CH*/
+	u32 tx_pkt_credit;  /*PP port tx bytes credit */
+	u32 tx_b_credit;  /*PP port tx bytes credit */
+	u32 tx_ring_addr;  /*PP port ring address. should follow HW definition*/
+	u32 tx_ring_size; /*PP ring size */
+	u32 tx_ring_offset;  /*PP: next tx_ring_addr=
+			      *   current tx_ring_addr + tx_ring_offset
+			      */
+};
+
+struct pmac_port_info2 {
+	/*only valid for 1st dp instanace which need dp_xmit/dp_rx*/
+	/*[0] for non-checksum case,
+	 *[1] for checksum offload
+	 *[2] two cases:
+	 * a: only traffic directly to MPE DL FW
+	 * b: DSL bonding FCS case
+	 */
+	struct pmac_tx_hdr pmac_template[MAX_TEMPLATE];
+	struct dma_tx_desc_0 dma0_template[MAX_TEMPLATE];
+	struct dma_tx_desc_0 dma0_mask_template[MAX_TEMPLATE];
+	struct dma_tx_desc_1 dma1_template[MAX_TEMPLATE];
+	struct dma_tx_desc_1 dma1_mask_template[MAX_TEMPLATE];
+};
+
+/*bridge port with pmapper supported dev structure */
+struct bp_pmapper_dev {
+	int flag;/*0-FREE, 1-Used*/
+	struct net_device *dev; /*bridge port device pointer */
+	int pcp[DP_PMAP_PCP_NUM];  /*PCP table */
+	int dscp[DP_PMAP_DSCP_NUM]; /*DSCP table*/
+	int def_ctp; /*Untag & nonip*/
+	int mode; /*mode*/
+	int ref_cnt; /*reference counter */
+};
+
+/*queue struct */
+struct q_info {
+	int flag;  /*0-FREE, 1-Used*/
+	int need_free; /*if this queue is allocated by dp_register_subif,
+			*   it needs free during de-register.
+			*Otherwise, no free
+			*/
+	int q_node_id;
+	int ref_cnt; /*subif_counter*/
+	int cqm_dequeue_port; /*CQM dequeue port */
+};
+
+/*scheduler struct */
+struct sched_info {
+	int flag;  /*0-FREE, 1-Used*/
+	int ref_cnt; /*subif_counter*/
+	int cqm_dequeue_port; /*CQM dequeue port */
+};
+
+struct cqm_port_info {
+	int f_first_qid : 1; /*0 not valid */
+	u32 ref_cnt; /*reference counter: the number of CTP attached to it*/
+	u32 tx_pkt_credit;  /*PP port tx bytes credit */
+	u32 tx_ring_addr;  /*PP port ring address. should follow HW definition*/
+	u32 tx_ring_size; /*PP port ring size */
+	int qos_port; /*qos port id*/
+	int first_qid; /*in order to auto sharing queue, 1st queue allocated by
+			*dp_register_subif_ext for that cqm_dequeue_port will be
+			*stored here. later it will be shared by other subif via
+			*dp_register_subif_ext
+			*/
+	int q_node; /*first_qid's logical node id*/
+	int dp_port; /* dp_port info */
+};
+
+struct parser_info {
+	u8 v;
+	s8 size;
+};
+
+struct subif_platform_data {
+	struct net_device *dev;
+	struct dp_subif_data *subif_data;  /*from dp_register_subif_ex */
+#define TRIGGER_CQE_DP_ENABLE  1
+	int act; /*Set by HAL subif_platform_set and used by DP lib */
+};
+
+struct vlan_info1 {
+	/* Changed this TPID field to GSWIP API enum type.
+	 * We do not have flexible for any TPID, only following are supported:
+	 * 1. ignore (don't care)
+	 * 2. 0x8100
+	 * 3. Value configured int VTE Type register
+	 */
+	GSW_ExtendedVlanFilterTpid_t tpid;  /* TPID like 0x8100, 0x8800 */
+	u16 vid ;  /*VLAN ID*/
+	/*note: user priority/CFI both don't care */
+	/* DSCP to Priority value mapping is possible */
+};
+
+struct vlan1 {
+	int bp;  /*assigned bp for this single VLAN dev */
+	struct vlan_info1 outer_vlan; /*out vlan info */
+	/* Add Ethernet type with GSWIP API enum type.
+	 * Following types are supported:
+	 * 1. ignore	(don't care)
+	 * 2. IPoE	(0x0800)
+	 * 3. PPPoE	(0x8863 or 0x8864)
+	 * 4. ARP	(0x0806)
+	 * 5. IPv6 IPoE	(0x86DD)
+	 */
+	GSW_ExtendedVlanFilterEthertype_t ether_type;
+};
+
+struct vlan2 {
+	int bp;  /*assigned bp for this double VLAN dev */
+	struct vlan_info1 outer_vlan;  /*out vlan info */
+	struct vlan_info1 inner_vlan;   /*in vlan info */
+	/* Add Ethernet type with GSWIP API enum type.
+	 * Following types are supported:
+	 * 1. ignore	(don't care)
+	 * 2. IPoE	(0x0800)
+	 * 3. PPPoE	(0x8863 or 0x8864)
+	 * 4. ARP	(0x0806)
+	 * 5. IPv6 IPoE	(0x86DD)
+	 */
+	GSW_ExtendedVlanFilterEthertype_t ether_type;
+};
+
+struct ext_vlan_info {
+	int subif_grp, logic_port; /* base subif group and logical port.
+				    * In DP it is subif
+				    */
+	int bp; /*default bp for this ctp */
+	int n_vlan1, n_vlan2; /*size of vlan1/2_list*/
+	int n_vlan1_drop, n_vlan2_drop; /*size of vlan1/2_drop_list */
+	struct vlan1 *vlan1_list; /*allow single vlan dev info list auto
+				   * bp is for egress VLAN setting
+				   */
+	struct vlan2 *vlan2_list; /* allow double vlan dev info list auto
+				   * bp is for egress VLAN setting
+				   */
+	struct vlan1 *vlan1_drop_list; /* drop single vlan list - manual
+					*  bp no use
+					*/
+	struct vlan2 *vlan2_drop_list; /* drop double vlan list - manual
+					* bp no use
+					*/
+	/* Need add other input / output information for deletion. ?? */
+	/* private data stored by function set_gswip_ext_vlan */
+	void *priv;
+};
+
+struct dp_tc_vlan_info {
+	int dev_type; /* 1: apply VLAN to bp,
+		       * 0: apply VLAN to subix (subif group)
+		       */
+	int subix;  /*similar like GSWIP subif group*/
+	int bp;  /*bridge port id */
+	int dp_port; /*logical port */
+	int inst;  /*DP instance */
+};
+
+/*port 0 is reserved*/
+extern int dp_inst_num;
+extern struct inst_property dp_port_prop[DP_MAX_INST];
+extern struct pmac_port_info dp_port_info[DP_MAX_INST][MAX_DP_PORTS];
+extern struct pmac_port_info2 dp_port_info2[DP_MAX_INST][MAX_DP_PORTS];
+extern struct q_info dp_q_tbl[DP_MAX_INST][DP_MAX_QUEUE_NUM];
+extern struct sched_info dp_sched_tbl[DP_MAX_INST][DP_MAX_SCHED_NUM];
+extern struct cqm_port_info dp_deq_port_tbl[DP_MAX_INST][DP_MAX_CQM_DEQ];
+extern struct bp_pmapper_dev dp_bp_dev_tbl[DP_MAX_INST][DP_MAX_BP_NUM];
+
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_DBG)
+extern u32 dp_dbg_flag;
+extern unsigned int dp_dbg_err;
+extern unsigned int dp_max_print_num;
+extern unsigned int dp_print_num_en;
+#endif
+
+int dp_loop_eth_dev_init(struct dentry *parent);
+void dp_loop_eth_dev_exit(void);
+struct dentry *dp_proc_install(void);
+extern char *dp_dbg_flag_str[];
+extern unsigned int dp_dbg_flag_list[];
+extern u32 dp_port_flag[];
+extern char *dp_port_type_str[];
+extern char *dp_port_status_str[];
+extern struct parser_info pinfo[];
+//extern GSW_API_HANDLE gswr_r;
+enum TEST_MODE {
+	DP_RX_MODE_NORMAL = 0,
+	DP_RX_MODE_LAN_WAN_BRIDGE,
+	DPR_RX_MODE_MAX
+};
+
+extern u32 dp_rx_test_mode;
+extern struct dma_rx_desc_1 dma_rx_desc_mask1;
+extern struct dma_rx_desc_3 dma_rx_desc_mask3;
+extern struct dma_rx_desc_0 dma_tx_desc_mask0;
+extern struct dma_rx_desc_1 dma_tx_desc_mask1;
+ssize_t proc_print_mode_write(struct file *file, const char *buf,
+			      size_t count, loff_t *ppos);
+void proc_print_mode_read(struct seq_file *s);
+int parser_size_via_index(u8 index);
+struct pmac_port_info *get_port_info_via_dp_name(struct net_device *dev);
+void dp_clear_mib(dp_subif_t *subif, uint32_t flag);
+extern u32 dp_drop_all_tcp_err;
+extern u32 dp_pkt_size_check;
+void dp_mib_exit(void);
+void print_parser_status(struct seq_file *s);
+void proc_mib_timer_read(struct seq_file *s);
+int mpe_fh_netfiler_install(void);
+int dp_coc_cpufreq_exit(void);
+int dp_coc_cpufreq_init(void);
+int qos_dump_start(void);
+int qos_dump(struct seq_file *s, int pos);
+ssize_t proc_qos_write(struct file *file, const char *buf,
+		       size_t count, loff_t *ppos);
+int update_coc_up_sub_module(enum ltq_cpufreq_state new_state,
+			     enum ltq_cpufreq_state old_state, uint32_t flag);
+void proc_coc_read(struct seq_file *s);
+ssize_t proc_coc_write(struct file *file, const char *buf, size_t count,
+		       loff_t *ppos);
+void dump_parser_flag(char *buf);
+
+//int dp_reset_sys_mib(u32 flag);
+void dp_clear_all_mib_inside(uint32_t flag);
+
+struct sk_buff *dp_create_new_skb(struct sk_buff *skb);
+extern int ip_offset_hw_adjust;
+int register_notifier(u32 flag);
+int unregister_notifier(u32 flag);
+//int supported_logic_dev(int inst, struct net_device *dev, char *subif_name);
+struct net_device *get_base_dev(struct net_device *dev, int level);
+int add_logic_dev(int inst, int port_id, struct net_device *dev,
+		  dp_subif_t *subif_id, u32 flags);
+int del_logic_dev(int inst, struct list_head *head, struct net_device *dev,
+		  u32 flags);
+int dp_get_port_subitf_via_dev_private(struct net_device *dev,
+				       dp_subif_t *subif);
+int get_vlan_via_dev(struct net_device *dev, struct vlan_prop *vlan_prop);
+void dp_parser_info_refresh(u32 cpu, u32 mpe1, u32 mpe2, u32 mpe3, u32 verify);
+int dp_inst_init(u32 flag);
+int request_dp(u32 flag);
+/*static __init */ int dp_init_module(void);
+/*static __exit*/ void dp_cleanup_module(void);
+int dp_probe(struct platform_device *pdev);
+#define NS_INT16SZ	 2
+#define NS_INADDRSZ	 4
+#define NS_IN6ADDRSZ	16
+
+int inet_pton4(const char *src, u_char *dst);
+int pton(const char *src, void *dst);
+int inet_pton4(const char *src, u_char *dst);
+int inet_pton6(const char *src, u_char *dst);
+int low_10dec(u64 x);
+int high_10dec(u64 x);
+int dp_atoi(unsigned char *str);
+int mac_stob(const char *mac, u8 bytes[6]);
+int get_offset_clear_chksum(struct sk_buff *skb, u32 *ip_offset,
+			    u32 *tcp_h_offset,
+			    u32 *tcp_type);
+int get_vlan_info(struct net_device *dev, struct vlan_info *vinfo);
+int dp_basic_proc(void);
+
+struct pmac_port_info *get_port_info(int inst, int index);
+struct pmac_port_info *get_port_info_via_dp_port(int inst, int dp_port);
+
+void set_dp_dbg_flag(uint32_t flags);
+uint32_t get_dp_dbg_flag(void);
+void dp_dump_raw_data(char *buf, int len, char *prefix_str);
+
+#ifdef CONFIG_LTQ_TOE_DRIVER
+/*! @brief  ltq_tso_xmit
+ *@param[in] skb  pointer to packet buffer like sk_buff
+ *@param[in] hdr  point to packet header, like pmac header
+ *@param[in] len  packet header
+ *@param[in] flags :Reserved
+ *@return 0 if OK  / -1 if error
+ *@note
+ */
+int ltq_tso_xmit(struct sk_buff *skb, void *hdr, int len, int flags);
+#endif
+
+int dp_set_meter_rate(enum ltq_cpufreq_state stat, unsigned int rate);
+char *dp_skb_csum_str(struct sk_buff *skb);
+extern struct dentry *dp_proc_node;
+int get_dp_dbg_flag_str_size(void);
+int get_dp_port_status_str_size(void);
+int get_dp_port_type_str_size(void);
+char *get_dp_port_type_str(int k);
+
+u32 *get_port_flag(int inst, int index);
+int dp_request_inst(struct dp_inst_info *info, u32 flag);
+int register_dp_cap(u32 flag);
+int print_symbol_name(unsigned long addr);
+typedef GSW_return_t(*dp_gsw_cb)(void *, void *);
+void falcon_test(void); /*defined in Pp qos driver */
+int bp_pmapper_dev_get(int inst, struct net_device *dev);
+
+extern int32_t (*qos_mgr_hook_setup_tc)(struct net_device *dev, u32 handle,
+					__be16 protocol,
+					struct tc_to_netdev *tc);
+#endif /*DATAPATH_H */
+
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_api.c b/drivers/net/ethernet/lantiq/datapath/datapath_api.c
new file mode 100644
index 000000000000..70736e705bd5
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/datapath_api.c
@@ -0,0 +1,2711 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include<linux/init.h>
+#include<linux/module.h>
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/version.h>
+#include <linux/if_ether.h>
+#include <linux/ethtool.h>
+#include <linux/proc_fs.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/clk.h>
+#include <linux/if_ether.h>
+#include <linux/clk.h>
+#include <linux/ip.h>
+#include <net/ip.h>
+#include <lantiq_soc.h>
+#include <net/lantiq_cbm_api.h>
+#include <net/datapath_api.h>
+#include <net/datapath_api_skb.h>
+#include "datapath.h"
+#include "datapath_instance.h"
+#include "datapath_swdev_api.h"
+
+#if IS_ENABLED(CONFIG_PPA_API_SW_FASTPATH)
+#include <net/ppa/ppa_api.h>
+#endif
+
+#if defined(CONFIG_LTQ_DATAPATH_DBG) && CONFIG_LTQ_DATAPATH_DBG
+unsigned int dp_max_print_num = -1, dp_print_num_en = 0;
+#endif
+
+GSW_API_HANDLE gswr_r;
+u32    dp_rx_test_mode = DP_RX_MODE_NORMAL;
+struct dma_rx_desc_1 dma_rx_desc_mask1;
+struct dma_rx_desc_3 dma_rx_desc_mask3;
+struct dma_rx_desc_0 dma_tx_desc_mask0;
+struct dma_rx_desc_1 dma_tx_desc_mask1;
+u32 dp_drop_all_tcp_err;
+u32 dp_pkt_size_check;
+
+u32 dp_dbg_flag;
+EXPORT_SYMBOL(dp_dbg_flag);
+
+#ifdef CONFIG_LTQ_DATAPATH_MPE_FASTHOOK_TEST
+u32 ltq_mpe_eanble;
+EXPORT_SYMBOL(ltq_mpe_eanble);
+
+int (*ltq_mpe_fasthook_free_fn)(struct sk_buff *) = NULL;
+EXPORT_SYMBOL(ltq_mpe_fasthook_free_fn);
+
+int (*ltq_mpe_fasthook_tx_fn)(struct sk_buff *, u32, void *) = NULL;
+EXPORT_SYMBOL(ltq_mpe_fasthook_tx_fn);
+
+int (*ltq_mpe_fasthook_rx_fn)(struct sk_buff *, u32, void *) = NULL;
+EXPORT_SYMBOL(ltq_mpe_fasthook_rx_fn);
+#endif	/*CONFIG_LTQ_DATAPATH_MPE_FASTHOOK_TEST */
+
+#undef DP_DBG_ENUM_OR_STRING
+#define DP_DBG_ENUM_OR_STRING(name, short_name) short_name
+char *dp_dbg_flag_str[] = DP_DBG_FLAG_LIST;
+
+#undef DP_DBG_ENUM_OR_STRING
+#define DP_DBG_ENUM_OR_STRING(name, short_name) name
+u32 dp_dbg_flag_list[] = DP_DBG_FLAG_LIST;
+
+#undef DP_F_ENUM_OR_STRING
+#define DP_F_ENUM_OR_STRING(name, short_name) short_name
+char *dp_port_type_str[] = DP_F_FLAG_LIST;
+
+#undef DP_F_ENUM_OR_STRING
+#define DP_F_ENUM_OR_STRING(name, short_name) name
+u32 dp_port_flag[] = DP_F_FLAG_LIST;
+
+char *dp_port_status_str[] = {
+	"PORT_FREE",
+	"PORT_ALLOCATED",
+	"PORT_DEV_REGISTERED",
+	"PORT_SUBIF_REGISTERED",
+	"Invalid"
+};
+
+static int try_walkaround;
+static int dp_init_ok;
+#ifdef DP_SPIN_LOCK
+static DEFINE_SPINLOCK(dp_lock); /*datapath spinlock*/
+#else
+static DEFINE_MUTEX(dp_lock);
+#endif
+unsigned int dp_dbg_err = 1; /*print error */
+static int32_t dp_rx_one_skb(struct sk_buff *skb, uint32_t flags);
+/*port 0 is reserved and never assigned to any one */
+int dp_inst_num;
+/* Keep per DP instance information here */
+struct inst_property dp_port_prop[DP_MAX_INST];
+/* Keep all subif information per instance/LPID/subif */
+struct pmac_port_info dp_port_info[DP_MAX_INST][MAX_DP_PORTS];
+/* Keep all default DMA descriptor mask/bit per instance/LPID */
+struct pmac_port_info2 dp_port_info2[DP_MAX_INST][MAX_DP_PORTS];
+
+/* bp_mapper_dev[] is mainly for PON case
+ * Only if multiple gem port are attached to same bridge port,
+ * This bridge port device will be recorded into this bp_mapper_dev[].
+ * later other information, like pmapper ID/mapping table will be put here also
+ */
+struct bp_pmapper_dev dp_bp_dev_tbl[DP_MAX_INST][DP_MAX_BP_NUM];
+/* q_tbl[] is mainly for the queue created/used during dp_register_subif_ext
+ */
+struct q_info dp_q_tbl[DP_MAX_INST][DP_MAX_QUEUE_NUM];
+/* sched_tbl[] is mainly for the sched created/used during dp_register_subif_ext
+ */
+struct sched_info dp_sched_tbl[DP_MAX_INST][DP_MAX_SCHED_NUM];
+/* dp_deq_port_tbl[] is to record cqm dequeue port info
+ */
+struct cqm_port_info dp_deq_port_tbl[DP_MAX_INST][DP_MAX_CQM_DEQ];
+
+struct parser_info pinfo[4];
+static int print_len;
+#ifdef CONFIG_LTQ_DATAPATH_ACA_CSUM_WORKAROUND
+static struct module aca_owner;
+static struct net_device aca_dev;
+static int aca_portid = -1;
+#endif
+
+char *get_dp_port_type_str(int k)
+{
+	return dp_port_type_str[k];
+}
+
+u32 get_dp_port_flag(int k)
+{
+	return dp_port_flag[k];
+}
+
+int get_dp_port_type_str_size(void)
+{
+	return ARRAY_SIZE(dp_port_type_str);
+}
+
+int get_dp_dbg_flag_str_size(void)
+{
+	return ARRAY_SIZE(dp_dbg_flag_str);
+}
+
+int get_dp_port_status_str_size(void)
+{
+	return ARRAY_SIZE(dp_port_status_str);
+}
+
+int parser_size_via_index(u8 index)
+{
+	if (index >= ARRAY_SIZE(pinfo)) {
+		PR_ERR("Wrong index=%d, it should less than %d\n", index,
+		       ARRAY_SIZE(pinfo));
+		return 0;
+	}
+
+	return pinfo[index].size;
+}
+
+static inline int parser_enabled(int ep, struct dma_rx_desc_1 *desc_1)
+{
+#ifdef CONFIG_LTQ_DATAPATH_EXTRA_DEBUG
+	if (!desc_1) {
+		PR_ERR("NULL desc_1 is not allowed\n");
+		return 0;
+	}
+#endif
+	if (!ep)
+		return pinfo[(desc_1->field.mpe2 << 1) +
+			desc_1->field.mpe1].size;
+	return 0;
+}
+
+struct pmac_port_info *get_port_info(int inst, int index)
+{
+	if (index < dp_port_prop[inst].info.cap.max_num_dp_ports)
+		return &dp_port_info[inst][index];
+
+	return NULL;
+}
+
+u32 *get_port_flag(int inst, int index)
+{
+	if (index < dp_port_prop[inst].info.cap.max_num_dp_ports)
+		return &dp_port_info[inst][index].alloc_flags;
+
+	return NULL;
+}
+
+struct pmac_port_info *get_port_info_via_dp_port(int inst, int dp_port)
+{
+	int i;
+
+	for (i = 0; i < dp_port_prop[inst].info.cap.max_num_dp_ports; i++) {
+		if ((dp_port_info[inst][i].status & PORT_DEV_REGISTERED) &&
+		    (dp_port_info[inst][i].port_id == dp_port))
+			return &dp_port_info[inst][i];
+	}
+
+	return NULL;
+}
+
+struct pmac_port_info *get_port_info_via_dp_name(struct net_device *dev)
+{
+	int i;
+	int inst = dp_get_inst_via_dev(dev, NULL, 0);
+
+	for (i = 0; i < dp_port_prop[inst].info.cap.max_num_dp_ports; i++) {
+		if ((dp_port_info[inst][i].status & PORT_DEV_REGISTERED) &&
+		    (dp_port_info[inst][i].dev == dev))
+			return &dp_port_info[inst][i];
+	}
+	return NULL;
+}
+
+int8_t parser_size(int8_t v)
+{
+	if (v == DP_PARSER_F_DISABLE)
+		return 0;
+
+	if (v == DP_PARSER_F_HDR_ENABLE)
+		return PASAR_OFFSETS_NUM;
+
+	if (v == DP_PARSER_F_HDR_OFFSETS_ENABLE)
+		return PASAR_OFFSETS_NUM + PASAR_FLAGS_NUM;
+
+	PR_ERR("Wrong parser setting: %d\n", v);
+	/*error */
+	return -1;
+}
+
+/*Only for SOC side, not for peripheral device side */
+int dp_set_gsw_parser(u8 flag, u8 cpu, u8 mpe1, u8 mpe2, u8 mpe3)
+{
+	int inst = 0;
+
+	if (!dp_port_prop[inst].info.dp_set_gsw_parser)
+		return -1;
+
+	return dp_port_prop[inst].info.dp_set_gsw_parser(flag, cpu, mpe1,
+							 mpe2, mpe3);
+}
+EXPORT_SYMBOL(dp_set_gsw_parser);
+
+int dp_get_gsw_parser(u8 *cpu, u8 *mpe1, u8 *mpe2, u8 *mpe3)
+{
+	int inst = 0;
+
+	if (!dp_port_prop[inst].info.dp_get_gsw_parser)
+		return -1;
+
+	return dp_port_prop[inst].info.dp_get_gsw_parser(cpu, mpe1,
+							 mpe2, mpe3);
+}
+EXPORT_SYMBOL(dp_get_gsw_parser);
+
+char *parser_str(int index)
+{
+	if (index == 0)
+		return "cpu";
+
+	if (index == 1)
+		return "mpe1";
+
+	if (index == 2)
+		return "mpe2";
+
+	if (index == 3)
+		return "mpe3";
+
+	PR_ERR("Wrong index:%d\n", index);
+	return "Wrong index";
+}
+
+/* some module may have reconfigure parser configuration in FMDA_PASER.
+ * It is necessary to refresh the pinfo
+ */
+void dp_parser_info_refresh(u32 cpu, u32 mpe1, u32 mpe2,
+			    u32 mpe3, u32 verify)
+{
+	int i;
+
+	pinfo[0].v = cpu;
+	pinfo[1].v = mpe1;
+	pinfo[2].v = mpe2;
+	pinfo[3].v = mpe3;
+
+	for (i = 0; i < ARRAY_SIZE(pinfo); i++) {
+		if (verify &&
+		    (pinfo[i].size != parser_size(pinfo[i].v)))
+			PR_ERR
+			 ("Lcal parser pinfo[%d](%d) != register cfg(%d)??\n",
+			 i, pinfo[i].size,
+			 parser_size(pinfo[i].v));
+
+		/*force to update */
+		pinfo[i].size = parser_size(pinfo[i].v);
+
+		if ((pinfo[i].size < 0) || (pinfo[i].size > PKT_PMAC_OFFSET)) {
+			PR_ERR("Wrong parser setting for %s: %d\n",
+			       parser_str(i), pinfo[i].v);
+		}
+	}
+}
+EXPORT_SYMBOL(dp_parser_info_refresh);
+
+void print_parser_status(struct seq_file *s)
+{
+	if (!s)
+		return;
+
+	seq_printf(s, "REG.cpu  value=%u size=%u\n", pinfo[0].v,
+		   pinfo[0].size);
+	seq_printf(s, "REG.MPE1 value=%u size=%u\n", pinfo[1].v,
+		   pinfo[1].size);
+	seq_printf(s, "REG.MPE2 value=%u size=%u\n", pinfo[2].v,
+		   pinfo[2].size);
+	seq_printf(s, "REG.MPE3 value=%u size=%u\n", pinfo[3].v,
+		   pinfo[3].size);
+}
+
+/*note: dev can be NULL */
+static int32_t dp_alloc_port_private(int inst,
+				     struct module *owner,
+				     struct net_device *dev,
+				     u32 dev_port, s32 port_id,
+				     dp_pmac_cfg_t *pmac_cfg,
+				     struct dp_port_data *data,
+				     u32 flags)
+{
+	int i;
+	struct cbm_dp_alloc_data cbm_data = {0};
+
+	if (!owner) {
+		PR_ERR("Allocate port failed for owner NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (port_id >= MAX_DP_PORTS || port_id < 0) {
+		DP_DEBUG_ASSERT((port_id >= MAX_DP_PORTS),
+				"port_id(%d) >= MAX_DP_PORTS(%d)", port_id,
+				MAX_DP_PORTS);
+		DP_DEBUG_ASSERT((port_id < 0), "port_id(%d) < 0", port_id);
+		return DP_FAILURE;
+	}
+
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_DBG)
+	if (unlikely(dp_dbg_flag & DP_DBG_FLAG_REG)) {
+		DP_DEBUG(DP_DBG_FLAG_REG, "Flags=");
+		for (i = 0; i < ARRAY_SIZE(dp_port_type_str); i++)
+			if (flags & dp_port_flag[i])
+				DP_DEBUG(DP_DBG_FLAG_REG, "%s ",
+					 dp_port_type_str[i]);
+		DP_DEBUG(DP_DBG_FLAG_REG, "\n");
+	}
+#endif
+	cbm_data.dp_inst = inst;
+	cbm_data.cbm_inst = dp_port_prop[inst].cbm_inst;
+
+	if (flags & DP_F_DEREGISTER) {	/*De-register */
+		if (dp_port_info[inst][port_id].status != PORT_ALLOCATED) {
+			PR_ERR
+			    ("No Deallocate for module %s w/o deregistered\n",
+			     owner->name);
+			return DP_FAILURE;
+		}
+		cbm_data.deq_port = dp_port_info[inst][port_id].deq_port_base;
+		cbm_data.dma_chan = dp_port_info[inst][port_id].dma_chan;
+		cbm_dp_port_dealloc(owner, dev_port, port_id, &cbm_data, flags);
+		dp_inst_insert_mod(owner, port_id, inst, 0);
+		DP_DEBUG(DP_DBG_FLAG_REG, "de-alloc port %d\n", port_id);
+		DP_CB(inst, port_platform_set)(inst, port_id, data, flags);
+		memset(&dp_port_info[inst][port_id], 0,
+		       sizeof(dp_port_info[inst][port_id]));
+		return DP_SUCCESS;
+	}
+	if (port_id) { /*with specified port_id */
+		if (dp_port_info[inst][port_id].status != PORT_FREE) {
+			PR_ERR("%s %s(%s %d) fail: port %d used by %s %d\n",
+			       "module", owner->name,
+			       "dev_port", dev_port, port_id,
+			       dp_port_info[inst][port_id].owner->name,
+			       dp_port_info[inst][port_id].dev_port);
+			return DP_FAILURE;
+		}
+	}
+	if (cbm_dp_port_alloc(owner, dev, dev_port, port_id,
+			      &cbm_data, flags)) {
+		PR_ERR
+		    ("cbm_dp_port_alloc fail for %s/dev_port %d: %d\n",
+		     owner->name, dev_port, port_id);
+		return DP_FAILURE;
+	} else if (!(cbm_data.flags & CBM_PORT_DP_SET) ||
+		   !(cbm_data.flags & CBM_PORT_DQ_SET)) {
+		PR_ERR("%s NO DP_SET/DQ_SET(%x):%s/dev_port %d\n",
+		       "cbm_dp_port_alloc",
+		       cbm_data.flags,
+		       owner->name, dev_port);
+		return DP_FAILURE;
+	}
+	port_id = cbm_data.dp_port;
+	memset(&dp_port_info[inst][port_id], 0,
+	       sizeof(dp_port_info[inst][port_id]));
+	/*save info from caller */
+	dp_port_info[inst][port_id].owner = owner;
+	dp_port_info[inst][port_id].dev = dev;
+	dp_port_info[inst][port_id].dev_port = dev_port;
+	dp_port_info[inst][port_id].alloc_flags = flags;
+	dp_port_info[inst][port_id].status = PORT_ALLOCATED;
+	/*save info from cbm_dp_port_alloc*/
+	dp_port_info[inst][port_id].flag_other = cbm_data.flags;
+	dp_port_info[inst][port_id].port_id = cbm_data.dp_port;
+	dp_port_info[inst][port_id].deq_port_base = cbm_data.deq_port;
+	dp_port_info[inst][port_id].deq_port_num = cbm_data.deq_port_num;
+	DP_DEBUG(DP_DBG_FLAG_REG,
+		 "cbm alloc dp_port:%d deq:%d deq_num:%d\n",
+		 cbm_data.dp_port, cbm_data.deq_port, cbm_data.deq_port_num);
+	if (cbm_data.flags & CBM_PORT_DMA_CHAN_SET)
+		dp_port_info[inst][port_id].dma_chan = cbm_data.dma_chan;
+	if (cbm_data.flags & CBM_PORT_PKT_CRDT_SET)
+		dp_port_info[inst][port_id].tx_pkt_credit =
+				cbm_data.tx_pkt_credit;
+	if (cbm_data.flags & CBM_PORT_BYTE_CRDT_SET)
+	dp_port_info[inst][port_id].tx_b_credit = cbm_data.tx_b_credit;
+	if (cbm_data.flags & CBM_PORT_RING_ADDR_SET)
+	dp_port_info[inst][port_id].tx_ring_addr = cbm_data.tx_ring_addr;
+	if (cbm_data.flags & CBM_PORT_RING_SIZE_SET)
+	dp_port_info[inst][port_id].tx_ring_size = cbm_data.tx_ring_size;
+	if (cbm_data.flags & CBM_PORT_RING_OFFSET_SET)
+		dp_port_info[inst][port_id].tx_ring_offset =
+				cbm_data.tx_ring_offset;
+	if (dp_port_prop[inst].info.port_platform_set(inst, port_id,
+						      data, flags)) {
+		PR_ERR("Failed port_platform_set for port_id=%d(%s)\n",
+		       port_id, owner ? owner->name : "");
+		cbm_dp_port_dealloc(owner, dev_port, port_id, &cbm_data,
+				    flags | DP_F_DEREGISTER);
+		memset(&dp_port_info[inst][port_id], 0,
+		       sizeof(dp_port_info[inst][port_id]));
+		return DP_FAILURE;
+	}
+	if (pmac_cfg)
+		dp_pmac_set(inst, port_id, pmac_cfg);
+	/*only 1st dp instance support real CPU path traffic */
+	if (!inst && dp_port_prop[inst].info.init_dma_pmac_template)
+		dp_port_prop[inst].info.init_dma_pmac_template(port_id, flags);
+	for (i = 0; i < MAX_SUBIFS; i++)
+		INIT_LIST_HEAD(&dp_port_info[inst][port_id].
+			subif_info[i].logic_dev);
+	dp_inst_insert_mod(owner, port_id, inst, 0);
+
+	DP_DEBUG(DP_DBG_FLAG_REG,
+		 "Port %d allocation succeed for module %s with dev_port %d\n",
+		 port_id, owner->name, dev_port);
+	return port_id;
+}
+
+int32_t dp_register_subif_private(int inst, struct module *owner,
+				  struct net_device *dev,
+				  char *subif_name, dp_subif_t *subif_id,
+				  /*device related info*/
+				  struct dp_subif_data *data, u32 flags)
+{
+	int res = DP_FAILURE;
+
+	int i, port_id, start, end;
+	struct pmac_port_info *port_info;
+	struct cbm_dp_en_data cbm_data = {0};
+
+	struct subif_platform_data platfrm_data = {0};
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_SWITCHDEV)
+	struct net_device *br_dev;
+	int fid, vap;
+	struct dp_dev *dp_dev;
+	struct br_info *br_info;
+	u32 idx;
+	bool f_unlock = false;
+#endif
+
+	port_id = subif_id->port_id;
+	port_info = &dp_port_info[inst][port_id];
+	subif_id->inst = inst;
+	subif_id->subif_num = 1;
+	platfrm_data.subif_data = data;
+	platfrm_data.dev = dev;
+	/*Sanity Check*/
+	if (port_info->status < PORT_DEV_REGISTERED) {
+		DP_DEBUG(DP_DBG_FLAG_REG,
+			 "register subif failed:%s is not a registered dev!\n",
+			 subif_name);
+		return res;
+	}
+
+	if (subif_id->subif < 0) {/*dynamic mode */
+		if (flags & DP_F_SUBIF_LOGICAL) {
+			if (!(DP_CB(inst, supported_logic_dev)(inst,
+							       dev,
+							       subif_name))) {
+				DP_DEBUG(DP_DBG_FLAG_REG,
+					 "reg subif fail:%s not support dev\n",
+				 subif_name);
+				return res;
+			}
+			if (!(flags & DP_F_ALLOC_EXPLICIT_SUBIFID)) {
+				/*Share same subif with its base device
+				 *For GRX350: nothing need except save it
+				 *For Falcon_Mx: it need to allocate BP for it
+				 */
+				res = add_logic_dev(inst, port_id, dev,
+						    subif_id, flags);
+				return res;
+			}
+		}
+		start = 0;
+		end = port_info->ctp_max;
+	} else {
+		/*caller provided subif. Try to get its vap value as start */
+		start = GET_VAP(subif_id->subif, port_info->vap_offset,
+				port_info->vap_mask);
+		end = start + 1;
+	}
+
+	/*PR_INFO("search range: start=%d end=%d\n",start, end);*/
+	/*allocate a free subif */
+	for (i = start; i < end; i++) {
+		if (port_info->subif_info[i].flags) /*used already & not free*/
+			continue;
+
+		/*now find a free subif or valid subif
+		 *need to do configuration HW
+		 */
+		if (port_info->status) {
+			if (dp_port_prop[inst].info.
+					subif_platform_set(inst, port_id, i,
+							   &platfrm_data,
+							   flags)) {
+				PR_ERR("subif_platform_set fail\n");
+				goto EXIT;
+			} else {
+				DP_DEBUG(DP_DBG_FLAG_REG,
+					 "subif_platform_set succeed\n");
+			}
+		} else {
+			PR_ERR("port info status fail for 0\n");
+			return res;
+		}
+		port_info->subif_info[i].flags = 1;
+		port_info->subif_info[i].netif = dev;
+		port_info->port_id = port_id;
+
+		if (subif_id->subif < 0) /*dynamic:shift bits as HW defined*/
+			port_info->subif_info[i].subif =
+				SET_VAP(i, port_info->vap_offset,
+					port_info->vap_mask);
+		else /*provided by caller since it is alerady shifted properly*/
+			port_info->subif_info[i].subif =
+			    subif_id->subif;
+		strncpy(port_info->subif_info[i].device_name,
+			subif_name,
+		       sizeof(port_info->subif_info[i].device_name) - 1);
+		port_info->subif_info[i].flags = PORT_SUBIF_REGISTERED;
+		port_info->subif_info[i].subif_flag = flags;
+		port_info->status = PORT_SUBIF_REGISTERED;
+		subif_id->port_id = port_id;
+		subif_id->subif = port_info->subif_info[i].subif;
+		port_info->num_subif++;
+		if ((port_info->num_subif == 1) ||
+		    (platfrm_data.act & TRIGGER_CQE_DP_ENABLE)) {
+			cbm_data.dp_inst = inst;
+			cbm_data.cbm_inst = dp_port_prop[inst].cbm_inst;
+			cbm_data.deq_port = port_info->deq_port_base +
+				(data ? data->deq_port_idx : 0);
+			if ((cbm_data.deq_port == 0) ||
+			    (cbm_data.deq_port >= DP_MAX_CQM_DEQ)) {
+				PR_ERR("Wrong deq_port: %d\n",
+				       cbm_data.deq_port);
+				return res;
+			}
+			if (port_info->num_subif == 1)
+				cbm_data.dma_chnl_init = 1; /*to enable DMA*/
+			DP_DEBUG(DP_DBG_FLAG_REG, "%s:%s%d %s%d %s%d\n",
+				 "cbm_dp_enable",
+				 "dp_port=", port_id,
+				 "deq_port=", cbm_data.deq_port,
+				 "dma_chnl_init=", cbm_data.dma_chnl_init);
+			if (cbm_dp_enable(owner, port_id, &cbm_data, 0,
+					  port_info->alloc_flags)) {
+				DP_DEBUG(DP_DBG_FLAG_REG,
+					 "cbm_dp_enable fail\n");
+				return res;
+			}
+			DP_DEBUG(DP_DBG_FLAG_REG, "cbm_dp_enable ok\n");
+		} else {
+			DP_DEBUG(DP_DBG_FLAG_REG,
+				 "No need cbm_dp_enable:dp_port=%d subix=%d\n",
+				 port_id, i);
+		}
+		break;
+	}
+
+	if (i < end) {
+		res = DP_SUCCESS;
+		if (dp_bp_dev_tbl[inst][port_info->subif_info[i].bp].
+		    ref_cnt > 1)
+			return res;
+		dp_inst_add_dev(dev, subif_name,
+				subif_id->inst, subif_id->port_id,
+				port_info->subif_info[i].bp,
+				subif_id->subif, flags);
+	#if IS_ENABLED(CONFIG_LTQ_DATAPATH_SWITCHDEV)
+	/*Added as workaround to alloc FID & config MAC,when bridge
+	 * port registration happens after br addif
+	 */
+		if (dev) {
+			idx = dp_dev_hash(dev, NULL);
+			dp_dev = dp_dev_lookup(&dp_dev_list[idx], dev, NULL, 0);
+			if (!dp_dev) {
+				PR_ERR("DP dev not exists!!,No mac config\n");
+				return 0;
+			}
+			vap = GET_VAP(subif_id->subif, port_info->vap_offset,
+				      port_info->vap_mask);
+			if (!rtnl_is_locked()) {
+				rtnl_lock();
+				f_unlock = true;
+			}
+			br_dev = netdev_master_upper_dev_get(dev);
+			if (f_unlock)
+				rtnl_unlock();
+			if (br_dev) {
+				br_info = dp_swdev_bridge_entry_lookup(br_dev->
+								       name, 0);
+				if (br_info) {
+					dp_dev->fid = br_info->fid;
+					port_info->subif_info[vap].fid =
+								dp_dev->fid;
+				} else {
+					fid = dp_notif_br_alloc(br_dev);
+					if (fid > 0) {
+						dp_dev->fid = fid;
+						port_info->subif_info[vap].fid =
+								dp_dev->fid;
+					} else {
+						PR_ERR("FID alloc fail %s\r\n",
+						       __func__);
+						return 0;
+					}
+				}
+			}
+		}
+	#endif
+	} else {
+		DP_DEBUG(DP_DBG_FLAG_REG,
+			 "register subif failed for no matched vap\n");
+	}
+EXIT:
+	return res;
+}
+
+int32_t dp_deregister_subif_private(int inst, struct module *owner,
+				    struct net_device *dev,
+				    char *subif_name, dp_subif_t *subif_id,
+				    struct dp_subif_data *data,
+				    uint32_t flags)
+{
+	int res = DP_FAILURE;
+	int i, port_id, cqm_port, bp;
+	u8 find = 0;
+	struct pmac_port_info *port_info;
+	struct cbm_dp_en_data cbm_data = {0};
+	struct subif_platform_data platfrm_data = {0};
+
+	port_id = subif_id->port_id;
+	port_info = &dp_port_info[inst][port_id];
+	platfrm_data.subif_data = data;
+	platfrm_data.dev = dev;
+
+	DP_DEBUG(DP_DBG_FLAG_REG,
+		 "Try to unregister subif=%s with dp_port=%d subif=%d\n",
+		 subif_name, subif_id->port_id, subif_id->subif);
+
+	if (port_info->status != PORT_SUBIF_REGISTERED) {
+		DP_DEBUG(DP_DBG_FLAG_REG,
+			 "Unregister failed:%s not registered subif!\n",
+			 subif_name);
+		return res;
+	}
+
+	for (i = 0; i < port_info->ctp_max; i++) {
+		if (port_info->subif_info[i].subif ==
+		    subif_id->subif) {
+			find = 1;
+			break;
+		}
+	}
+	if (!find)
+		return res;
+
+	DP_DEBUG(DP_DBG_FLAG_REG,
+		 "Found matched subif: port_id=%d subif=%x vap=%d\n",
+		 subif_id->port_id, subif_id->subif, i);
+	if (port_info->subif_info[i].netif != dev) {
+		/* device not match. Maybe it is unexplicit logical dev */
+		res = del_logic_dev(inst, &port_info->subif_info[i].logic_dev,
+				    dev, flags);
+		return res;
+	}
+	if (!list_empty(&port_info->subif_info[i].logic_dev)) {
+		DP_DEBUG(DP_DBG_FLAG_REG,
+			 "Unregister fail: logic_dev of %s not empty yet!\n",
+			 subif_name);
+		return res;
+	}
+	cqm_port = port_info->subif_info[i].cqm_deq_port;
+	bp = port_info->subif_info[i].bp;
+	/* reset mib, flag, and others */
+	memset(&port_info->subif_info[i].mib, 0,
+	       sizeof(port_info->subif_info[i].mib));
+	port_info->subif_info[i].flags = 0;
+	port_info->num_subif--;
+	if (dp_port_prop[inst].info.subif_platform_set(inst,
+						       port_id, i,
+						       &platfrm_data, flags)) {
+		PR_ERR("subif_platform_set fail\n");
+		/*return res;*/
+	}
+
+	if (!dp_deq_port_tbl[inst][cqm_port].ref_cnt) {
+		/*delete all queues which may created by PPA or other apps*/
+		struct dp_node_alloc port_node;
+
+		port_node.inst = inst;
+		port_node.dp_port = port_id;
+		port_node.type = DP_NODE_PORT;
+		port_node.id.cqm_deq_port = cqm_port;
+		dp_node_children_free(&port_node, 0);
+		/*disable cqm port */
+		cbm_data.dp_inst = inst;
+		cbm_data.cbm_inst = dp_port_prop[inst].cbm_inst;
+		cbm_data.deq_port = cqm_port;
+		if (!port_info->num_subif) {
+			port_info->status = PORT_DEV_REGISTERED;
+			cbm_data.dma_chnl_init = 1; /*to disable DMA */
+		}
+		if (cbm_dp_enable(owner, port_id, &cbm_data,
+				  CBM_PORT_F_DISABLE, port_info->alloc_flags)) {
+			DP_DEBUG(DP_DBG_FLAG_REG,
+				 "cbm_dp_disable fail:port=%d subix=%d %s=%d\n",
+				 port_id, i,
+				 "dma_chnl_init", cbm_data.dma_chnl_init);
+
+			return res;
+		}
+		DP_DEBUG(DP_DBG_FLAG_REG,
+			 "cbm_dp_disable ok:port=%d subix=%d cqm_port=%d\n",
+			 port_id, i, cqm_port);
+	}
+	/* for pmapper and non-pmapper both
+	 *  1)for falcon_mx, dev is managed at its HAL level
+	 *  2)for GRX350, bp/dev should be always zero/NULL at present
+	 *        before adapting to new datapath framework
+	 */
+	if (!dp_bp_dev_tbl[inst][bp].dev) {
+		DP_DEBUG(DP_DBG_FLAG_REG,
+			 "dp_inst_del_dev for %s inst=%d bp=%d\n",
+			 dev->name, inst, bp);
+		dp_inst_del_dev(dev, subif_name, inst, port_id,
+				subif_id->subif, 0);
+	}
+
+	DP_DEBUG(DP_DBG_FLAG_REG, "  dp_port=%d subif=%d cqm_port=%d\n",
+		 subif_id->port_id, subif_id->subif, cqm_port);
+	res = DP_SUCCESS;
+
+	return res;
+}
+
+/*Note: For same owner, it should be in the same HW instance
+ *          since dp_register_dev/subif no dev_port information at all,
+ *          at the same time, dev is optional and can be NULL
+ */
+
+int32_t dp_alloc_port(struct module *owner, struct net_device *dev,
+		      u32 dev_port, int32_t port_id,
+		      dp_pmac_cfg_t *pmac_cfg, uint32_t flags)
+{
+	struct dp_port_data data = {0};
+
+	return dp_alloc_port_ext(0, owner, dev, dev_port, port_id, pmac_cfg,
+				 &data, flags);
+}
+EXPORT_SYMBOL(dp_alloc_port);
+
+int32_t dp_alloc_port_ext(int inst, struct module *owner,
+			  struct net_device *dev,
+			  u32 dev_port, int32_t port_id,
+			  dp_pmac_cfg_t *pmac_cfg,
+			  struct dp_port_data *data, uint32_t flags)
+{
+	int res;
+	struct dp_port_data tmp_data = {0};
+
+	if (unlikely(!dp_init_ok)) {
+		DP_LIB_LOCK(&dp_lock);
+		if (!try_walkaround) {
+			try_walkaround = 1;
+			dp_probe(NULL); /*workaround to re-init */
+		}
+		DP_LIB_UNLOCK(&dp_lock);
+		if (!dp_init_ok) {
+			PR_ERR("dp_alloc_port fail: datapath can't init\n");
+			return DP_FAILURE;
+		}
+	}
+	if (!dp_port_prop[0].valid) {
+		PR_ERR("No Valid datapath instance yet?\n");
+		return DP_FAILURE;
+	}
+	if (!data)
+		data = &tmp_data;
+	DP_LIB_LOCK(&dp_lock);
+	res = dp_alloc_port_private(inst, owner, dev, dev_port,
+				    port_id, pmac_cfg, data, flags);
+	DP_LIB_UNLOCK(&dp_lock);
+	if (!inst)
+		return res;
+
+#ifdef CONFIG_LTQ_DATAPATH_ACA_CSUM_WORKAROUND
+	/*For VRX518, it will always carry DP_F_FAST_WLAN flag for
+	 * ACA HW resource purpose in CBM
+	 */
+	if ((res > 0) &&
+	    (flags & DP_F_FAST_WLAN) &&
+	    (aca_portid < 0)) {
+		dp_subif_t subif_id;
+		#define ACA_CSUM_NAME "aca_csum"
+		strcpy(aca_owner.name, ACA_CSUM_NAME);
+		strcpy(aca_dev.name, ACA_CSUM_NAME);
+		aca_portid = dp_alloc_port(&aca_owner, &aca_dev,
+					   0, 0, NULL, DP_F_CHECKSUM);
+		if (aca_portid <= 0) {
+			PR_ERR("dp_alloc_port failed for %s\n", ACA_CSUM_NAME);
+			return res;
+		}
+		if (dp_register_dev(&aca_owner, aca_portid,
+				    NULL, DP_F_CHECKSUM)) {
+			PR_ERR("dp_register_dev fail for %s\n", ACA_CSUM_NAME);
+			return res;
+		}
+		subif_id.port_id = aca_portid;
+		subif_id.subif = -1;
+		if (dp_register_subif(&aca_owner, &aca_dev,
+				      ACA_CSUM_NAME, &subif_id,
+				      DP_F_CHECKSUM)) {
+			PR_ERR("dp_register_subif fail for %s\n",
+			       ACA_CSUM_NAME);
+			return res;
+		}
+	}
+#endif
+	return res;
+}
+EXPORT_SYMBOL(dp_alloc_port_ext);
+
+int32_t dp_register_dev(struct module *owner, uint32_t port_id,
+			dp_cb_t *dp_cb, uint32_t flags)
+{
+	int inst = dp_get_inst_via_module(owner, port_id, 0);
+	struct dp_dev_data data = {0};
+
+	if (inst < 0) {
+		PR_ERR("dp_register_dev not valid module %s\n", owner->name);
+		return -1;
+	}
+
+	return dp_register_dev_ext(inst, owner, port_id, dp_cb, &data, flags);
+}
+EXPORT_SYMBOL(dp_register_dev);
+
+int32_t dp_register_dev_ext(int inst, struct module *owner, uint32_t port_id,
+			    dp_cb_t *dp_cb, struct dp_dev_data *data,
+			    uint32_t flags)
+{
+	int res = DP_FAILURE;
+	struct pmac_port_info *port_info;
+
+	if (unlikely(!dp_init_ok)) {
+		PR_ERR("dp_register_dev failed for datapath not init yet\n");
+		return DP_FAILURE;
+	}
+
+	if (!port_id || !owner || (port_id >= MAX_DP_PORTS)) {
+		if ((inst < 0) || (inst >= DP_MAX_INST))
+			DP_DEBUG(DP_DBG_FLAG_REG, "wrong inst=%d\n", inst);
+		else if (!owner)
+			DP_DEBUG(DP_DBG_FLAG_REG, "owner NULL\n");
+		else
+			DP_DEBUG(DP_DBG_FLAG_REG, "Wrong port_id:%d\n",
+				 port_id);
+
+		return DP_FAILURE;
+	}
+	port_info = &dp_port_info[inst][port_id];
+
+	DP_LIB_LOCK(&dp_lock);
+	if (flags & DP_F_DEREGISTER) {	/*de-register */
+		if (port_info->status != PORT_DEV_REGISTERED) {
+			DP_DEBUG(DP_DBG_FLAG_REG,
+				 "No or %s to de-register for num_subif=%d\n",
+				 owner->name,
+				 port_info->num_subif);
+		} else if (port_info->status ==
+			   PORT_DEV_REGISTERED) {
+			port_info->status = PORT_ALLOCATED;
+			res = DP_SUCCESS;
+		} else {
+			DP_DEBUG(DP_DBG_FLAG_REG,
+				 "No for %s to de-register for unknown status:%d\n",
+				 owner->name, port_info->status);
+		}
+
+		DP_LIB_UNLOCK(&dp_lock);
+		return res;
+	}
+
+	/*register a device */
+	if (port_info->status != PORT_ALLOCATED) {
+		DP_DEBUG(DP_DBG_FLAG_REG,
+			 "No de-register for %s for unknown status:%d\n",
+			 owner->name, port_info->status);
+		return DP_FAILURE;
+	}
+
+	if (port_info->owner != owner) {
+		DP_DEBUG(DP_DBG_FLAG_REG, "No matched owner(%s):%p->%p\n",
+			 owner->name, owner, port_info->owner);
+		DP_LIB_UNLOCK(&dp_lock);
+		return res;
+	}
+	port_info->status = PORT_DEV_REGISTERED;
+	if (dp_cb)
+		port_info->cb = *dp_cb;
+
+	DP_LIB_UNLOCK(&dp_lock);
+	return DP_SUCCESS;
+}
+EXPORT_SYMBOL(dp_register_dev_ext);
+
+/* if subif_id->subif < 0: Dynamic mode
+ * else subif is provided by caller itself
+ * Note: 1) for register logical device, if DP_F_ALLOC_EXPLICIT_SUBIFID is not
+ *       specified, subif will take its base dev's subif.
+ *       2) for IPOA/PPPOA, dev is NULL and subif_name is dummy string.
+ *          in this case, dev->name may not be subif_name
+ */
+int32_t dp_register_subif_ext(int inst, struct module *owner,
+			      struct net_device *dev,
+			      char *subif_name, dp_subif_t *subif_id,
+			      /*device related info*/
+			      struct dp_subif_data *data, uint32_t flags)
+{
+	int res = DP_FAILURE;
+	int port_id;
+	struct pmac_port_info *port_info;
+	struct dp_subif_data tmp_data = {0};
+
+	if (unlikely(!dp_init_ok)) {
+		DP_DEBUG(DP_DBG_FLAG_REG,
+			 "dp_register_subif fail for datapath not init yet\n");
+		return DP_FAILURE;
+	}
+	DP_DEBUG(DP_DBG_FLAG_REG,
+		 "%s:owner=%s dev=%s %s=%s port_id=%d subif=%d(%s) %s%s\n",
+		 (flags & DP_F_DEREGISTER) ?
+			"unregister subif:" : "register subif",
+		 owner ? owner->name : "NULL",
+		 dev ? dev->name : "NULL",
+		 "subif_name",
+		 subif_name,
+		 subif_id->port_id,
+		 subif_id->subif,
+		 (subif_id->subif < 0) ? "dynamic" : "fixed",
+		 (flags & DP_F_SUBIF_LOGICAL) ? "Logical" : "",
+		 (flags & DP_F_ALLOC_EXPLICIT_SUBIFID) ?
+			"Explicit" : "Non-Explicit");
+
+	if ((!subif_id) || (!subif_id->port_id) || (!owner) ||
+	    (subif_id->port_id >= MAX_DP_PORTS) ||
+	    (subif_id->port_id <= 0) ||
+	    ((inst < 0) || (inst >= DP_MAX_INST))) {
+		if (!owner)
+			DP_DEBUG(DP_DBG_FLAG_REG,
+				 "register subif failed for owner NULL\n");
+		else if (!subif_id)
+			DP_DEBUG(DP_DBG_FLAG_REG,
+				 "register subif failed for NULL subif_id\n");
+		else if ((inst < 0) || (inst >= DP_MAX_INST))
+			DP_DEBUG(DP_DBG_FLAG_REG,
+				 "register subif failed for wrong inst=%d\n",
+				 inst);
+		else
+			DP_DEBUG(DP_DBG_FLAG_REG,
+				 "register subif failed port_id=%d or others\n",
+				 subif_id->port_id);
+
+		return DP_FAILURE;
+	}
+	port_id = subif_id->port_id;
+	port_info = &dp_port_info[inst][port_id];
+
+	if (((!dev) && !(port_info->alloc_flags & DP_F_FAST_DSL)) ||
+	    !subif_name) {
+		DP_DEBUG(DP_DBG_FLAG_REG, "Wrong dev=%p, subif_name=%p\n",
+			 dev, subif_name);
+		return DP_FAILURE;
+	}
+	if (!data)
+		data = &tmp_data;
+	DP_LIB_LOCK(&dp_lock);
+	if (port_info->owner != owner) {
+		DP_DEBUG(DP_DBG_FLAG_REG,
+			 "Unregister subif fail:Not matching:%p(%s)->%p(%s)\n",
+			 owner, owner->name, port_info->owner,
+			 port_info->owner->name);
+		DP_LIB_UNLOCK(&dp_lock);
+		return res;
+	}
+
+	if (flags & DP_F_DEREGISTER) /*de-register */
+		res =
+		dp_deregister_subif_private(inst, owner, dev,
+					    subif_name,
+					    subif_id, data, flags);
+	else /*register */
+		res =
+		dp_register_subif_private(inst, owner, dev,
+					  subif_name,
+					  subif_id, data, flags);
+	DP_LIB_UNLOCK(&dp_lock);
+	return res;
+}
+EXPORT_SYMBOL(dp_register_subif_ext);
+
+int32_t dp_register_subif(struct module *owner, struct net_device *dev,
+			  char *subif_name, dp_subif_t *subif_id,
+			  uint32_t flags)
+{
+	int inst;
+	struct dp_subif_data data = {0};
+
+	if ((!subif_id) || (!subif_id->port_id) || (!owner) ||
+	    (subif_id->port_id >= MAX_DP_PORTS) ||
+	    (subif_id->port_id <= 0)) {
+		if (!owner)
+			DP_DEBUG(DP_DBG_FLAG_REG,
+				 "register subif fail for owner NULL\n");
+		else if (!subif_id)
+			DP_DEBUG(DP_DBG_FLAG_REG,
+				 "register subif fail for NULL subif_id\n");
+		else
+			DP_DEBUG(DP_DBG_FLAG_REG,
+				 "register subif fail port_id=%d or others\n",
+				 subif_id->port_id);
+
+		return DP_FAILURE;
+	}
+	inst = dp_get_inst_via_module(owner, subif_id->port_id, 0);
+	if (inst < 0) {
+		PR_ERR("wrong inst for owner=%s with ep=%d\n", owner->name,
+		       subif_id->port_id);
+		return DP_FAILURE;
+	}
+	return dp_register_subif_ext(inst, owner, dev, subif_name,
+				     subif_id, &data, flags);
+}
+EXPORT_SYMBOL(dp_register_subif);
+
+/*Note:
+ * try to get subif according to netif, skb,vcc,dst_mac.
+ * For DLS nas interface, must provide valid subif_data, otherwise set to NULL.
+ * Note: subif_data is mainly used for DSL WAN mode, esp ATM.
+ * If subif->port_id valid, take it, otherwise search all to get the port_id
+ */
+int32_t dp_get_netif_subifid(struct net_device *netif, struct sk_buff *skb,
+			     void *subif_data, uint8_t dst_mac[DP_MAX_ETH_ALEN],
+			     dp_subif_t *subif, uint32_t flags)
+{
+	int res = -1;
+	int i, k;
+	int port_id = -1;
+	u16 bport = 0;
+	dp_get_netif_subifid_fn_t subifid_fn_t;
+	int inst, start, end;
+	u8 match = 0;
+	u8 num = 0;
+	u16 *subifs = NULL;
+	u32 *subif_flag = NULL;
+	struct logic_dev *tmp = NULL;
+
+	subifs = kmalloc(sizeof(*subifs) * DP_MAX_CTP_PER_DEV,
+			 GFP_ATOMIC);
+	if (!subifs) {
+		PR_ERR("Failed to alloc %d bytes\n",
+		       sizeof(*subifs) * DP_MAX_CTP_PER_DEV);
+		return res;
+	}
+	subif_flag = kmalloc(sizeof(*subif_flag) * DP_MAX_CTP_PER_DEV,
+			     GFP_ATOMIC);
+	if (!subif_flag) {
+		PR_ERR("Failed to alloc %d bytes\n",
+		       sizeof(*subif_flag) * DP_MAX_CTP_PER_DEV);
+		kfree(subifs);
+		return res;
+	}
+	if (!netif && !subif_data) {
+		DP_DEBUG(DP_DBG_FLAG_REG,
+			 "dp_get_netif_subifid failed: netif=%p subif_data=%p\n",
+			 netif, subif_data);
+		goto EXIT;
+	}
+	if (!subif) {
+		DP_DEBUG(DP_DBG_FLAG_REG,
+			 "dp_get_netif_subifid failed:subif=%p\n", subif);
+		goto EXIT;
+	}
+	if (!netif && subif_data)
+		inst = 0;
+	else
+		inst = dp_get_inst_via_dev(netif, NULL, 0);
+	start = 0;
+	end = dp_port_prop[inst].info.cap.max_num_dp_ports;
+#ifdef DP_FAST_SEARCH /* Don't enable since need to be back-compatible */
+	if ((subif->port_id < MAX_DP_PORTS) && (subif->port_id > 0)) {
+		start = subif->port_id;
+		end = start + 1;
+	}
+#endif
+	DP_LIB_LOCK(&dp_lock);
+	for (k = start; k < end; k++) {
+		if (dp_port_info[inst][k].status != PORT_SUBIF_REGISTERED)
+			continue;
+
+		/*Workaround for VRX318 */
+		if (subif_data &&
+		    (dp_port_info[inst][k].alloc_flags & DP_F_FAST_DSL)) {
+			/*VRX318 should overwritten them later if necessary */
+			port_id = k;
+			break;
+		}
+
+		/*search sub-interfaces/VAP */
+		for (i = 0; i < dp_port_info[inst][k].ctp_max; i++) {
+			if (!dp_port_info[inst][k].subif_info[i].flags)
+				continue;
+			if (dp_port_info[inst][k].subif_info[i].ctp_dev ==
+				netif) { /*for PON pmapper case*/
+				match = 1;
+				port_id = k;
+				if (num > 0) {
+					DP_LIB_UNLOCK(&dp_lock);
+					PR_ERR("Multiple same ctp_dev exist\n");
+					goto EXIT;
+				}
+				subifs[num] = PORT_SUBIF(inst, k, i, subif);
+				subif_flag[num] = PORT_SUBIF(inst, k, i,
+							subif_flag);
+				bport = PORT_SUBIF(inst, k, i, bp);
+				subif->flag_bp = 0;
+				num++;
+				break;
+			}
+
+			if (dp_port_info[inst][k].subif_info[i].netif ==
+			    netif) {
+				if ((subif->port_id > 0) &&
+				    (subif->port_id != k)) {
+					DP_DEBUG(DP_DBG_FLAG_REG,
+						 "dp_get_netif_subifid portid not match:%d expect %d\n",
+						 subif->port_id, k);
+				} else {
+					match = 1;
+					subif->flag_bp = 1;
+					port_id = k;
+					if (num >= DP_MAX_CTP_PER_DEV) {
+						DP_LIB_UNLOCK(&dp_lock);
+						PR_ERR("%s: Why CTP over %d\n",
+						       netif ? netif->name : "",
+						       DP_MAX_CTP_PER_DEV);
+						goto EXIT;
+					}
+					/* some dev may have multiple
+					 * subif,like pon
+					 */
+					subifs[num] = PORT_SUBIF(inst, k, i,
+								 subif);
+					subif_flag[num] = PORT_SUBIF(inst, k, i,
+								subif_flag);
+					bport = PORT_SUBIF(inst, k, i, bp);
+					if (num &&
+					    (bport != dp_port_info[inst][k].
+					     subif_info[i].bp)) {
+						PR_ERR("%s:Why many bp:%d %d\n",
+						       netif ? netif->name : "",
+						       dp_port_info[inst][k].
+							  subif_info[i].bp,
+						       bport);
+						DP_LIB_UNLOCK(&dp_lock);
+						goto EXIT;
+					}
+					num++;
+				}
+			}
+			/*continue search non-explicate logical device */
+			list_for_each_entry(tmp,
+					    &dp_port_info[inst][k].
+					    subif_info[i].logic_dev,
+					    list) {
+				if (tmp->dev == netif) {
+					subif->subif_num = 1;
+					subif->subif_list[0] = tmp->ctp;
+					subif->inst = inst;
+					subif->port_id = k;
+					subif->bport = tmp->bp;
+					DP_LIB_UNLOCK(&dp_lock);
+					res = 0;
+					/*note: logical device no callback */
+					goto EXIT;
+				}
+			}
+		}
+		if (match)
+			break;
+	}
+	DP_LIB_UNLOCK(&dp_lock);
+
+	if (port_id < 0) {
+		if (subif_data)
+			DP_DEBUG(DP_DBG_FLAG_DBG,
+				 "dp_get_netif_subifid failed with subif_data %p\n",
+				 subif_data);
+		else /*netif must should be valid */
+			DP_DEBUG(DP_DBG_FLAG_DBG,
+				 "dp_get_netif_subifid failed: %s\n",
+				 netif->name);
+
+		goto EXIT;
+	}
+	subif->inst = inst;
+	subif->port_id = port_id;
+	subif->bport = bport;
+	subif->alloc_flag = dp_port_info[inst][port_id].alloc_flags;
+	subifid_fn_t = dp_port_info[inst][port_id].cb.get_subifid_fn;
+
+	if (subifid_fn_t && !(flags & DP_F_SUBIF_LOGICAL)) {
+		/*subif->subif will be set by callback api itself */
+		res =
+		    subifid_fn_t(netif, skb, subif_data, dst_mac, subif,
+				 flags);
+		if (res != 0)
+			DP_DEBUG(DP_DBG_FLAG_DBG,
+				 "get_netif_subifid callback failed\n");
+		else if (!subif->subif_num)/*back-compatible */
+			subif->subif_num = 1;
+		goto EXIT;
+	}
+	subif->subif_num = num;
+	for (i = 0; i < num; i++) {
+		subif->subif_list[i] = subifs[i];
+		subif->subif_flag[i] = subif_flag[i];
+	}
+	res = 0;
+EXIT:
+	kfree(subifs);
+	kfree(subif_flag);
+	return res;
+}
+EXPORT_SYMBOL(dp_get_netif_subifid);
+
+#ifdef CONFIG_LTQ_DATAPATH_CPUFREQ
+int update_coc_up_sub_module(enum ltq_cpufreq_state new_state,
+			     enum ltq_cpufreq_state old_state, uint32_t flag)
+{
+	int i;
+	dp_coc_confirm_stat fn;
+	int inst = 0;
+
+	for (i = 0; i < dp_port_prop[inst].info.cap.max_num_dp_ports; i++) {
+		fn = dp_port_info[inst][i].cb.dp_coc_confirm_stat_fn;
+
+		if (fn)
+			fn(new_state, old_state, flag);
+	}
+
+	return 0;
+}
+#endif
+
+/* return DP_SUCCESS -- found
+ * return DP_FAILURE -- not found
+ */
+int dp_get_port_subitf_via_dev_private(struct net_device *dev,
+				       dp_subif_t *subif)
+{
+	int i, j;
+	int inst;
+
+	inst = dp_get_inst_via_dev(dev, NULL, 0);
+	for (i = 0; i < dp_port_prop[inst].info.cap.max_num_dp_ports; i++)
+		for (j = 0; j < dp_port_info[inst][i].ctp_max; j++) {
+			if (!dp_port_info[inst][i].subif_info[j].flags)
+				continue;
+			if (dp_port_info[inst][i].subif_info[j].netif != dev)
+				continue;
+			subif->port_id = i;
+			subif->subif =
+				SET_VAP(j,
+					PORT_INFO(inst, i, vap_offset),
+					PORT_INFO(inst, i, vap_mask));
+			subif->inst = inst;
+			subif->bport = dp_port_info[inst][i].
+				subif_info[j].bp;
+			return DP_SUCCESS;
+		}
+	return DP_FAILURE;
+}
+
+int dp_get_port_subitf_via_dev(struct net_device *dev, dp_subif_t *subif)
+{
+	int res;
+
+	DP_LIB_LOCK(&dp_lock);
+	res = dp_get_port_subitf_via_dev_private(dev, subif);
+	DP_LIB_UNLOCK(&dp_lock);
+	return res;
+}
+EXPORT_SYMBOL(dp_get_port_subitf_via_dev);
+
+int dp_get_port_subitf_via_ifname_private(char *ifname, dp_subif_t *subif)
+{
+	int i, j;
+	int inst;
+
+	inst = dp_get_inst_via_dev(NULL, ifname, 0);
+
+	for (i = 0; i < dp_port_prop[inst].info.cap.max_num_dp_ports; i++) {
+		for (j = 0; j < dp_port_info[inst][i].ctp_max; j++) {
+			if (strcmp
+			    (dp_port_info[inst][i].subif_info[j].device_name,
+			     ifname) == 0) {
+				subif->port_id = i;
+				subif->subif =
+					SET_VAP(j,
+						PORT_INFO(inst, i, vap_offset),
+						PORT_INFO(inst, i, vap_mask));
+				subif->inst = inst;
+				subif->bport = dp_port_info[inst][i].
+					subif_info[j].bp;
+				return DP_SUCCESS;
+			}
+		}
+	}
+
+	return DP_FAILURE;
+}
+
+int dp_get_port_subitf_via_ifname(char *ifname, dp_subif_t *subif)
+{
+	int res;
+	struct net_device *dev;
+
+	if (!ifname)
+		return -1;
+	dev = dev_get_by_name(&init_net, ifname);
+	if (!dev)
+		return -1;
+	res = dp_get_port_subitf_via_dev(dev, subif);
+	dev_put(dev);
+	return res;
+}
+EXPORT_SYMBOL(dp_get_port_subitf_via_ifname);
+
+int32_t dp_check_if_netif_fastpath_fn(struct net_device *netif,
+				      dp_subif_t *subif, char *ifname,
+				      uint32_t flags)
+{
+	int res = 1;
+	dp_subif_t tmp_subif = { 0 };
+
+	DP_LIB_LOCK(&dp_lock);
+	if (unlikely(!dp_init_ok)) {
+		PR_ERR("dp_check_if_netif_fastpath_fn fail: dp not ready\n");
+		return DP_FAILURE;
+	}
+	if (subif) {
+		tmp_subif = *subif;
+		tmp_subif.inst = 0;
+	} else if (netif) {
+		dp_get_port_subitf_via_dev_private(netif, &tmp_subif);
+	} else if (ifname) {
+		dp_get_port_subitf_via_ifname_private(ifname, &tmp_subif);
+	}
+
+	if (tmp_subif.port_id <= 0 && tmp_subif.port_id >=
+	    dp_port_prop[tmp_subif.inst].info.cap.max_num_dp_ports)
+		res = 0;
+	else if (!(dp_port_info[tmp_subif.inst][tmp_subif.port_id].alloc_flags &
+		 (DP_F_FAST_DSL || DP_F_FAST_ETH_LAN ||
+		 DP_F_FAST_ETH_WAN || DP_F_FAST_WLAN)))
+		res = 0;
+
+	DP_LIB_UNLOCK(&dp_lock);
+	return res;
+}
+EXPORT_SYMBOL(dp_check_if_netif_fastpath_fn);
+
+struct module *dp_get_module_owner(int ep)
+{
+	int inst = 0; /*here hardcode for PPA only */
+
+	if (unlikely(!dp_init_ok)) {
+		PR_ERR
+		    ("dp_get_module_owner failed for datapath not init yet\n");
+		return NULL;
+	}
+
+	if ((ep >= 0) && (ep < dp_port_prop[inst].info.cap.max_num_dp_ports))
+		return dp_port_info[inst][ep].owner;
+
+	return NULL;
+}
+EXPORT_SYMBOL(dp_get_module_owner);
+
+/*if subif->vap == -1, it means all vap */
+void dp_clear_mib(dp_subif_t *subif, uint32_t flag)
+{
+	int i, j, start_vap, end_vap;
+	dp_reset_mib_fn_t reset_mib_fn;
+	struct pmac_port_info *port_info;
+
+	if (!subif || (subif->port_id >= MAX_DP_PORTS) ||
+	    (subif->port_id < 0)) {
+		DP_DEBUG(DP_DBG_FLAG_DBG, "dp_clear_mib Wrong subif\n");
+		return;
+	}
+
+	i = subif->port_id;
+	port_info = &dp_port_info[subif->inst][i];
+
+	if (subif->subif == -1) {
+		start_vap = 0;
+		end_vap = port_info->ctp_max;
+	} else {
+		start_vap = GET_VAP(subif->subif, port_info->vap_offset,
+				    port_info->vap_mask);
+		end_vap = start_vap + 1;
+	}
+
+	for (j = start_vap; j < end_vap; j++) {
+		STATS_SET(port_info->tx_err_drop, 0);
+		STATS_SET(port_info->rx_err_drop, 0);
+		memset(&port_info->subif_info[j].mib, 0,
+		       sizeof(port_info->subif_info[j].mib));
+		reset_mib_fn = port_info->cb.reset_mib_fn;
+
+		if (reset_mib_fn)
+			reset_mib_fn(subif, 0);
+	}
+}
+
+void dp_clear_all_mib_inside(uint32_t flag)
+{
+	dp_subif_t subif;
+	int i;
+
+	memset(&subif, 0, sizeof(subif));
+	for (i = 0; i < MAX_DP_PORTS; i++) {
+		subif.port_id = i;
+		subif.subif = -1;
+		dp_clear_mib(&subif, flag);
+	}
+}
+
+int dp_get_drv_mib(dp_subif_t *subif, dp_drv_mib_t *mib, uint32_t flag)
+{
+	dp_get_mib_fn_t get_mib_fn;
+	dp_drv_mib_t tmp;
+	int i, vap;
+	struct pmac_port_info *port_info;
+
+	if (unlikely(!dp_init_ok)) {
+		DP_DEBUG(DP_DBG_FLAG_DBG,
+			 "dp_get_drv_mib failed for datapath not init yet\n");
+		return DP_FAILURE;
+	}
+
+	if (!subif || !mib)
+		return -1;
+	memset(mib, 0, sizeof(*mib));
+	port_info = &dp_port_info[subif->inst][subif->port_id];
+	vap = GET_VAP(subif->subif, port_info->vap_offset,
+		      port_info->vap_mask);
+	get_mib_fn = port_info->cb.get_mib_fn;
+
+	if (!get_mib_fn)
+		return -1;
+
+	if (!(flag & DP_F_STATS_SUBIF)) {
+		/*get all VAP's  mib counters if it is -1 */
+		for (i = 0; i < port_info->ctp_max; i++) {
+			if (!port_info->subif_info[i].flags)
+				continue;
+
+			subif->subif =
+			    port_info->subif_info[i].subif;
+			memset(&tmp, 0, sizeof(tmp));
+			get_mib_fn(subif, &tmp, flag);
+			mib->rx_drop_pkts += tmp.rx_drop_pkts;
+			mib->rx_error_pkts += tmp.rx_error_pkts;
+			mib->tx_drop_pkts += tmp.tx_drop_pkts;
+			mib->tx_error_pkts += tmp.tx_error_pkts;
+		}
+	} else {
+		if (port_info->subif_info[vap].flags)
+			get_mib_fn(subif, mib, flag);
+	}
+
+	return 0;
+}
+
+int dp_get_netif_stats(struct net_device *dev, dp_subif_t *subif_id,
+		       struct rtnl_link_stats64 *stats, uint32_t flags)
+{
+	dp_subif_t subif;
+	int res;
+	int (*get_mib)(dp_subif_t *subif_id, void *priv,
+		       struct rtnl_link_stats64 *stats,
+		       uint32_t flags);
+
+	if (subif_id) {
+		subif = *subif_id;
+	} else if (dev) {
+		res = dp_get_port_subitf_via_dev(dev, &subif);
+		if (res) {
+			DP_DEBUG(DP_DBG_FLAG_MIB,
+				 "dp_get_netif_stats fail:%s not registered yet to datapath\n",
+				 dev->name);
+			return DP_FAILURE;
+		}
+	} else {
+		DP_DEBUG(DP_DBG_FLAG_MIB,
+			 "dp_get_netif_stats: dev/subif_id both NULL\n");
+		return DP_FAILURE;
+	}
+	get_mib = dp_port_prop[subif.inst].info.dp_get_port_vap_mib;
+	if (!get_mib)
+		return DP_FAILURE;
+
+	return get_mib(&subif, NULL, stats, flags);
+}
+EXPORT_SYMBOL(dp_get_netif_stats);
+
+int dp_clear_netif_stats(struct net_device *dev, dp_subif_t *subif_id,
+			 uint32_t flag)
+{
+	dp_subif_t subif;
+	int (*clear_netif_mib_fn)(dp_subif_t *subif, void *priv, u32 flag);
+	int i;
+
+	if (subif_id) {
+		clear_netif_mib_fn =
+			dp_port_prop[subif_id->inst].info.dp_clear_netif_mib;
+		if (!clear_netif_mib_fn)
+			return -1;
+		return clear_netif_mib_fn(subif_id, NULL, flag);
+	}
+	if (dev) {
+		if (dp_get_port_subitf_via_dev(dev, &subif)) {
+			DP_DEBUG(DP_DBG_FLAG_MIB,
+				 "%s not register to dp_clear_netif_stats\n",
+				 dev->name);
+			return -1;
+		}
+		clear_netif_mib_fn =
+			dp_port_prop[subif.inst].info.dp_clear_netif_mib;
+		if (!clear_netif_mib_fn)
+			return -1;
+		return clear_netif_mib_fn(&subif, NULL, flag);
+	}
+	/*clear all */
+	for (i = 0; i < DP_MAX_INST; i++) {
+		clear_netif_mib_fn =
+			dp_port_prop[i].info.dp_clear_netif_mib;
+		if (!clear_netif_mib_fn)
+			continue;
+		clear_netif_mib_fn(NULL, NULL, flag);
+	}
+	return 0;
+}
+EXPORT_SYMBOL(dp_clear_netif_stats);
+
+int dp_pmac_set(int inst, u32 port, dp_pmac_cfg_t *pmac_cfg)
+{
+	int (*dp_pmac_set_fn)(int inst, u32 port, dp_pmac_cfg_t *pmac_cfg);
+
+	if (inst >= DP_MAX_INST) {
+		PR_ERR("Wrong inst(%d) id: should less than %d\n",
+		       inst, DP_MAX_INST);
+		return DP_FAILURE;
+	}
+	dp_pmac_set_fn = dp_port_prop[inst].info.dp_pmac_set;
+	if (!dp_pmac_set_fn)
+		return DP_FAILURE;
+	return dp_pmac_set_fn(inst, port, pmac_cfg);
+}
+EXPORT_SYMBOL(dp_pmac_set);
+
+/*\brief Datapath Manager Pmapper Configuration Set
+ *\param[in] dev: network device point to set pmapper
+ *\param[in] mapper: buffer to get pmapper configuration
+ *\param[in] flag: reserve for future
+ *\return Returns 0 on succeed and -1 on failure
+ *\note  for pcp mapper case, all 8 mapping must be configured properly
+ *       for dscp mapper case, all 64 mapping must be configured properly
+ *       def ctp will match non-vlan and non-ip case
+ *	For drop case, assign CTP value == DP_PMAPPER_DISCARD_CTP
+ */
+int dp_set_pmapper(struct net_device *dev, struct dp_pmapper *mapper, u32 flag)
+{
+	int inst, ret, bport, i;
+	dp_subif_t subif = {0};
+	struct dp_pmapper *map = NULL;
+	int res = DP_FAILURE;
+
+	if (!dev || !mapper) {
+		PR_ERR("dev or mapper is NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (unlikely(!dp_init_ok)) {
+		PR_ERR("Failed for datapath not init yet\n");
+		return DP_FAILURE;
+	}
+	if (mapper->mode >= DP_PMAP_MAX) {
+		PR_ERR("mapper->mode(%d) out of range %d\n",
+		       mapper->mode, DP_PMAP_MAX);
+		return DP_FAILURE;
+	}
+	/* get the subif from the dev */
+	ret = dp_get_netif_subifid(dev, NULL, NULL, NULL, &subif, 0);
+	if ((ret == DP_FAILURE) || (subif.flag_bp == 0)) {
+		PR_ERR("Fail to get the subif:dev=%s ret=%d flag_bp=%d bp=%d\n",
+		       dev->name, ret, subif.flag_bp, subif.bport);
+		return DP_FAILURE;
+	}
+	inst = subif.inst;
+	if (!dp_port_prop[inst].info.dp_set_gsw_pmapper) {
+		PR_ERR("Set pmapper is not supported\n");
+		return DP_FAILURE;
+	}
+
+	bport = subif.bport;
+	if (bport >= DP_MAX_BP_NUM) {
+		PR_ERR("BP port(%d) out of range %d\n", bport, DP_MAX_BP_NUM);
+		return DP_FAILURE;
+	}
+	map = kmalloc(sizeof(*map), GFP_ATOMIC);
+	if (!map) {
+		PR_ERR("Failed for kmalloc: %d bytes\n", sizeof(*map));
+		return DP_FAILURE;
+	}
+	memcpy(map, mapper, sizeof(*map));
+	if ((mapper->mode == DP_PMAP_PCP) ||
+	    (mapper->mode == DP_PMAP_DSCP)) {
+		map->mode = GSW_PMAPPER_MAPPING_PCP;
+	} else if (mapper->mode == DP_PMAP_DSCP_ONLY) {
+		map->mode = GSW_PMAPPER_MAPPING_DSCP;
+	} else {
+		PR_ERR("Unknown mapper mode: %d\n", map->mode);
+		goto EXIT;
+	}
+
+	/* workaround in case caller forget to set to default ctp */
+	if (mapper->mode == DP_PMAP_PCP)
+		for (i = 0; i < DP_PMAP_DSCP_NUM; i++)
+			map->dscp_map[i] = mapper->def_ctp;
+
+	ret = dp_port_prop[inst].info.dp_set_gsw_pmapper(inst, bport,
+							 subif.port_id, map,
+							 flag);
+	if (ret == DP_FAILURE) {
+		PR_ERR("Failed to set mapper\n");
+		goto EXIT;
+	}
+
+	/* update local table for pmapper */
+	dp_bp_dev_tbl[inst][bport].def_ctp = map->def_ctp;
+	dp_bp_dev_tbl[inst][bport].mode = mapper->mode; /* original mode */
+	for (i = 0; i < DP_PMAP_PCP_NUM; i++)
+		dp_bp_dev_tbl[inst][bport].pcp[i] = map->pcp_map[i];
+	for (i = 0; i < DP_PMAP_DSCP_NUM; i++)
+		dp_bp_dev_tbl[inst][bport].dscp[i] = map->dscp_map[i];
+	res = DP_SUCCESS;
+EXIT:
+	kfree(map);
+	return res;
+}
+EXPORT_SYMBOL(dp_set_pmapper);
+
+/*\brief Datapath Manager Pmapper Configuration Get
+ *\param[in] dev: network device point to set pmapper
+ *\param[out] mapper: buffer to get pmapper configuration
+ *\param[in] flag: reserve for future
+ *\return Returns 0 on succeed and -1 on failure
+ *\note  for pcp mapper case, all 8 mapping must be configured properly
+ *       for dscp mapper case, all 64 mapping must be configured properly
+ *       def ctp will match non-vlan and non-ip case
+ *	 For drop case, assign CTP value == DP_PMAPPER_DISCARD_CTP
+ */
+int dp_get_pmapper(struct net_device *dev, struct dp_pmapper *mapper, u32 flag)
+{
+	int inst, ret, bport;
+	dp_subif_t subif = {0};
+
+	if (!dev || !mapper) {
+		PR_ERR("The parameter dev or mapper can not be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (unlikely(!dp_init_ok)) {
+		PR_ERR("Failed for datapath not init yet\n");
+		return DP_FAILURE;
+	}
+
+	/*get the subif from the dev*/
+	ret = dp_get_netif_subifid(dev, NULL, NULL, NULL, &subif, 0);
+	if (ret == DP_FAILURE || subif.flag_bp == 0) {
+		PR_ERR("Can not get the subif from the dev\n");
+		return DP_FAILURE;
+	}
+	inst = subif.inst;
+	if (!dp_port_prop[inst].info.dp_get_gsw_pmapper) {
+		PR_ERR("Get pmapper is not supported\n");
+		return DP_FAILURE;
+	}
+
+	bport = subif.bport;
+	if (bport > DP_MAX_BP_NUM) {
+		PR_ERR("BP port(%d) out of range %d\n", bport, DP_MAX_BP_NUM);
+		return DP_FAILURE;
+	}
+	/* init the subif into the dp_port_info*/
+	/* call the switch api to get the HW*/
+	ret = dp_port_prop[inst].info.dp_get_gsw_pmapper(inst, bport,
+							 subif.port_id, mapper,
+							 flag);
+	if (ret == DP_FAILURE) {
+		PR_ERR("Failed to get mapper\n");
+		return DP_FAILURE;
+	}
+	return ret;
+}
+EXPORT_SYMBOL(dp_get_pmapper);
+
+int32_t dp_rx(struct sk_buff *skb, uint32_t flags)
+{
+	struct sk_buff *next;
+	int res = -1;
+
+	if (unlikely(!dp_init_ok)) {
+		while (skb) {
+			next = skb->next;
+			skb->next = 0;
+			dev_kfree_skb_any(skb);
+			skb = next;
+		}
+	}
+
+	while (skb) {
+		next = skb->next;
+		skb->next = 0;
+		res = dp_rx_one_skb(skb, flags);
+		skb = next;
+	}
+
+	return res;
+}
+EXPORT_SYMBOL(dp_rx);
+
+int dp_lan_wan_bridging(int port_id, struct sk_buff *skb)
+{
+	dp_subif_t subif;
+	struct net_device *dev;
+	static int lan_port = 4;
+	int inst = 0;
+
+	if (!skb)
+		return DP_FAILURE;
+
+	skb_pull(skb, 8);	/*remove pmac */
+
+	memset(&subif, 0, sizeof(subif));
+	if (port_id == 15) {
+		/*recv from WAN and forward to LAN via lan_port */
+		subif.port_id = lan_port;	/*send to last lan port */
+		subif.subif = 0;
+	} else if (port_id <= 6) { /*recv from LAN and forward to WAN */
+		subif.port_id = 15;
+		subif.subif = 0;
+		lan_port = port_id;	/*save lan port id */
+	} else {
+		dev_kfree_skb_any(skb);
+		return DP_FAILURE;
+	}
+
+	dev = dp_port_info[inst][subif.port_id].subif_info[0].netif;
+
+	if (!dp_port_info[inst][subif.port_id].subif_info[0].flags || !dev) {
+		dev_kfree_skb_any(skb);
+		return DP_FAILURE;
+	}
+
+	((struct dma_tx_desc_1 *)&skb->DW1)->field.ep = subif.port_id;
+	((struct dma_tx_desc_0 *)&skb->DW0)->field.dest_sub_if_id =
+	    subif.subif;
+
+	dp_xmit(dev, &subif, skb, skb->len, 0);
+	return DP_SUCCESS;
+}
+
+static void rx_dbg(u32 f, struct sk_buff *skb, struct dma_rx_desc_0 *desc0,
+		   struct dma_rx_desc_1 *desc1, struct dma_rx_desc_2 *desc2,
+		   struct dma_rx_desc_3 *desc3, unsigned char *parser,
+		   struct pmac_rx_hdr *pmac, int paser_exist)
+{
+	int inst = 0;
+
+	DP_DEBUG(DP_DBG_FLAG_DUMP_RX,
+		 "\ndp_rx:skb->data=%p Loc=%x offset=%d skb->len=%d\n",
+		 skb->data, desc2->field.data_ptr,
+		 desc3->field.byte_offset, skb->len);
+	if ((f) & DP_DBG_FLAG_DUMP_RX_DATA)
+		dp_dump_raw_data(skb->data,
+				 (skb->len >
+				  (print_len)) ? skb->len : (print_len),
+				 "Original Data");
+	DP_DEBUG(DP_DBG_FLAG_DUMP_RX, "parse hdr size = %d\n",
+		 paser_exist);
+	if ((f) & DP_DBG_FLAG_DUMP_RX_DESCRIPTOR)
+		dp_port_prop[inst].info.dump_rx_dma_desc(desc0, (desc1),
+			desc2, desc3);
+	if (paser_exist && (dp_dbg_flag & DP_DBG_FLAG_DUMP_RX_PASER))
+		dump_parser_flag(parser);
+	if ((f) & DP_DBG_FLAG_DUMP_RX_PMAC)
+		dp_port_prop[inst].info.dump_rx_pmac(pmac);
+}
+
+#define PRINT_INTERVAL  (5 * HZ) /* 5 seconds */
+unsigned long dp_err_interval = PRINT_INTERVAL;
+static void rx_dbg_zero_port(struct sk_buff *skb, struct dma_rx_desc_0 *desc0,
+			     struct dma_rx_desc_1 *desc1,
+			     struct dma_rx_desc_2 *desc2,
+			     struct dma_rx_desc_3 *desc3,
+			     unsigned char *parser,
+			     struct pmac_rx_hdr *pmac, int paser_exist,
+			     u32 ep, u32 port_id, int vap)
+{
+	int inst = 0;
+	static unsigned long last;
+
+	if (!dp_dbg_err) /*bypass dump */
+		return;
+	if (time_before(jiffies, last + dp_err_interval))
+		/* not print in order to keep console not busy */
+		return;
+	last = jiffies;
+	DP_DEBUG(-1, "%s=%d vap=%d\n",
+		 (ep) ? "ep" : "port_id", port_id, vap);
+	PR_ERR("\nDrop for ep and source port id both zero ??\n");
+	dp_port_prop[inst].info.dump_rx_dma_desc(desc0, desc1, desc2, desc3);
+
+	if (paser_exist)
+		dump_parser_flag(parser);
+	if (pmac)
+		dp_port_prop[inst].info.dump_rx_pmac(pmac);
+	dp_dump_raw_data((char *)(skb->data),
+			 (skb->len >
+			  print_len) ? skb->len : print_len,
+			 "Recv Data");
+}
+
+static inline int32_t dp_rx_one_skb(struct sk_buff *skb, uint32_t flags)
+{
+	int res = DP_SUCCESS;
+	struct dma_rx_desc_0 *desc_0 = (struct dma_rx_desc_0 *)&skb->DW0;
+	struct dma_rx_desc_1 *desc_1 = (struct dma_rx_desc_1 *)&skb->DW1;
+	struct dma_rx_desc_2 *desc_2 = (struct dma_rx_desc_2 *)&skb->DW2;
+	struct dma_rx_desc_3 *desc_3 = (struct dma_rx_desc_3 *)&skb->DW3;
+	struct pmac_rx_hdr *pmac;
+	unsigned char *parser = NULL;
+	int rx_tx_flag = 0;	/*0-rx, 1-tx */
+	u32 ep = desc_1->field.ep;	/* ep: 0 -15 */
+	int vap; /*vap: 0-15 */
+	int paser_exist;
+	u32 port_id = ep; /*same with ep now, later set to sspid if ep is 0 */
+	struct net_device *dev;
+	dp_rx_fn_t rx_fn;
+	char decryp = 0;
+	u8 inst = 0;
+	struct pmac_port_info *dp_port;
+
+	dp_port = &dp_port_info[inst][0];
+	if (!skb) {
+		PR_ERR("skb NULL\n");
+		return DP_FAILURE;
+	}
+	if (!skb->data) {
+		PR_ERR("skb->data NULL\n");
+		return DP_FAILURE;
+	}
+
+	paser_exist = parser_enabled(port_id, desc_1);
+	if (paser_exist)
+		parser = skb->data;
+	pmac = (struct pmac_rx_hdr *)(skb->data + paser_exist);
+
+	if (unlikely(dp_dbg_flag))
+		rx_dbg(dp_dbg_flag, skb, desc_0, desc_1, desc_2,
+		       desc_3, parser, pmac, paser_exist);
+	if (paser_exist) {
+		skb_pull(skb, paser_exist);	/*remove parser */
+#if IS_ENABLED(CONFIG_PPA_API_SW_FASTPATH)
+		skb->mark |= FLG_PPA_PROCESSED;
+#endif
+	}
+#ifdef CONFIG_LTQ_DATAPATH_EXTRA_DEBUG
+	/*Sanity check */
+	if (unlikely(dp_port_prop[inst].info.not_valid_rx_ep(ep))) {
+		DP_DEBUG(DP_DBG_FLAG_DUMP_RX, "Wrong: why ep=%d??\n", ep);
+		rx_dbg(-1, skb, desc_0, desc_1, desc_2, desc_3,
+		       parser, pmac, paser_exist);
+		goto RX_DROP;
+	}
+	if (unlikely(dp_drop_all_tcp_err && desc_1->field.tcp_err)) {
+		DP_DEBUG(DP_DBG_FLAG_DUMP_RX, "\n----dp_rx why tcp_err ???\n");
+		rx_dbg(-1, skb, desc_0, desc_1, desc_2, desc_3, parser,
+		       pmac, paser_exist);
+		goto RX_DROP;
+	}
+#endif
+
+	if (port_id == PMAC_CPU_ID) { /*To CPU and need check src pmac port */
+		dp_port_prop[inst].info.update_port_vap(inst, &port_id, &vap,
+			skb,
+			pmac, &decryp);
+	} else {		/*GSWIP-R already know the destination */
+		rx_tx_flag = 1;
+		vap = GET_VAP(desc_0->field.dest_sub_if_id,
+			      dp_port_info[inst][port_id].vap_offset,
+			      dp_port_info[inst][port_id].vap_mask);
+	}
+	if (unlikely(!port_id)) { /*Normally shouldnot go to here */
+		rx_dbg_zero_port(skb, desc_0, desc_1, desc_2, desc_3, parser,
+				 pmac, paser_exist, ep, port_id, vap);
+		goto RX_DROP;
+	}
+	dp_port = &dp_port_info[inst][port_id];
+	rx_fn = dp_port->cb.rx_fn;
+	if (likely(rx_fn && dp_port->status)) {
+		/*Clear some fields as SWAS V3.7 required */
+		//desc_1->all &= dma_rx_desc_mask1.all;
+		desc_3->all &= dma_rx_desc_mask3.all;
+		skb->priority = desc_1->field.classid;
+		skb->dev = dp_port->subif_info[vap].netif;
+		dev = dp_port->subif_info[vap].netif;
+		if (decryp) { /*workaround mark for bypass xfrm policy*/
+			desc_1->field.dec = 1;
+			desc_1->field.enc = 1;
+		}
+		if (!dev &&
+		    ((dp_port->alloc_flags & DP_F_FAST_DSL) == 0)) {
+			UP_STATS(dp_port->subif_info[vap].mib.rx_fn_dropped);
+			goto RX_DROP;
+		}
+
+		if (unlikely(dp_dbg_flag)) {
+			DP_DEBUG(DP_DBG_FLAG_DUMP_RX, "%s=%d vap=%d\n",
+				 (ep) ? "ep" : "port_id", port_id, vap);
+
+			if (dp_dbg_flag & DP_DBG_FLAG_DUMP_RX_DATA) {
+				dp_dump_raw_data(skb->data, PMAC_SIZE,
+						 "pmac to top drv");
+				dp_dump_raw_data(skb->data + PMAC_SIZE,
+						 ((skb->len - PMAC_SIZE) >
+							print_len) ?
+							skb->len - PMAC_SIZE :
+							print_len,
+						 "Data to top drv");
+			}
+			if (dp_dbg_flag & DP_DBG_FLAG_DUMP_RX_DESCRIPTOR)
+				dp_port_prop[inst].info.dump_rx_dma_desc(
+					desc_0, desc_1,
+					desc_2, desc_3);
+		}
+#ifdef CONFIG_LTQ_DATAPATH_MPE_FASTHOOK_TEST
+		if (unlikely(ltq_mpe_fasthook_rx_fn))
+			ltq_mpe_fasthook_rx_fn(skb, 1, NULL);	/*with pmac */
+#endif
+		if (unlikely((enum TEST_MODE)dp_rx_test_mode ==
+			DP_RX_MODE_LAN_WAN_BRIDGE)) {
+			/*for datapath performance test only */
+			dp_lan_wan_bridging(port_id, skb);
+			/*return DP_SUCCESS;*/
+		}
+		/*If switch h/w acceleration is enabled,setting of this bit
+		 *avoid forwarding duplicate packets from linux
+		 */
+		#if IS_ENABLED(CONFIG_LTQ_DATAPATH_SWITCHDEV)
+			if (dp_port->subif_info[vap].fid > 0)
+				skb->offload_fwd_mark = 1;
+		#endif
+		if (rx_tx_flag == 0) {
+			rx_fn(dev, NULL, skb, skb->len);
+			UP_STATS(dp_port->subif_info[vap].mib.rx_fn_rxif_pkt);
+		} else {
+			rx_fn(NULL, dev, skb, skb->len);
+			UP_STATS(dp_port->subif_info[vap].mib.rx_fn_txif_pkt);
+		}
+
+		return DP_SUCCESS;
+	}
+
+	if (unlikely(port_id >=
+	    dp_port_prop[inst].info.cap.max_num_dp_ports - 1)) {
+		PR_ERR("Drop for wrong ep or src port id=%u ??\n",
+		       port_id);
+		goto RX_DROP;
+	} else if (unlikely(dp_port->status == PORT_FREE)) {
+		DP_DEBUG(DP_DBG_FLAG_DUMP_RX, "Drop for port %u free\n",
+			 port_id);
+		goto RX_DROP;
+	} else if (unlikely(!rx_fn)) {
+		DP_DEBUG(DP_DBG_FLAG_DUMP_RX,
+			 "Drop for subif of port %u not registered yet\n",
+			 port_id);
+		UP_STATS(dp_port->subif_info[vap].mib.rx_fn_dropped);
+		goto RX_DROP2;
+	} else {
+		pr_info("Unknown issue\n");
+	}
+RX_DROP:
+	UP_STATS(dp_port->rx_err_drop);
+RX_DROP2:
+	if (skb)
+		dev_kfree_skb_any(skb);
+	return res;
+}
+
+void dp_xmit_dbg(
+	char *title,
+	struct sk_buff *skb,
+	s32 ep,
+	s32 len,
+	u32 flags,
+	struct pmac_tx_hdr *pmac,
+	dp_subif_t *rx_subif,
+	int need_pmac,
+	int gso,
+	int checksum)
+{
+	DP_DEBUG(DP_DBG_FLAG_DUMP_TX,
+		 "%s: dp_xmit:skb->data/len=0x%p/%d data_ptr=%x from port=%d and subitf=%d\n",
+		 title,
+		 skb->data, len,
+		 ((struct dma_tx_desc_2 *)&skb->DW2)->field.data_ptr,
+		 ep, rx_subif->subif);
+	if (dp_dbg_flag & DP_DBG_FLAG_DUMP_TX_DATA) {
+		if (pmac) {
+			dp_dump_raw_data((char *)pmac, PMAC_SIZE, "Tx Data");
+			dp_dump_raw_data(skb->data,
+					 (skb->len > print_len) ?
+						skb->len :
+						print_len,
+					 "Tx Data");
+		} else
+			dp_dump_raw_data(skb->data,
+					 (skb->len > print_len) ?
+						skb->len : print_len,
+					 "Tx Data");
+	}
+	DP_DEBUG(DP_DBG_FLAG_DUMP_TX_SUM,
+		 "ip_summed=%s(%d) encapsulation=%s\n",
+		 dp_skb_csum_str(skb), skb->ip_summed,
+		 skb->encapsulation ? "Yes" : "No");
+	if (skb->encapsulation)
+		DP_DEBUG(DP_DBG_FLAG_DUMP_TX_SUM,
+			 "inner ip start=0x%x(%d), transport=0x%x(%d)\n",
+			 (unsigned int)skb_inner_network_header(skb),
+			 (int)(skb_inner_network_header(skb) -
+			       skb->data),
+			 (unsigned int)
+			 skb_inner_transport_header(skb),
+			 (int)(skb_inner_transport_header(skb) -
+			       skb_inner_network_header(skb)));
+	else
+		DP_DEBUG(DP_DBG_FLAG_DUMP_TX_SUM,
+			 "ip start=0x%x(%d), transport=0x%x(%d)\n",
+			 (unsigned int)(unsigned int)
+			 skb_network_header(skb),
+			 (int)(skb_network_header(skb) - skb->data),
+			 (unsigned int)skb_transport_header(skb),
+			 (int)(skb_transport_header(skb) -
+			       skb_network_header(skb)));
+
+	if (dp_dbg_flag & DP_DBG_FLAG_DUMP_TX_DESCRIPTOR)
+		dp_port_prop[0].info.dump_tx_dma_desc(
+				 (struct dma_tx_desc_0 *)&skb->DW0,
+				 (struct dma_tx_desc_1 *)&skb->DW1,
+				 (struct dma_tx_desc_2 *)&skb->DW2,
+				 (struct dma_tx_desc_3 *)&skb->DW3);
+
+	DP_DEBUG(DP_DBG_FLAG_DUMP_TX, "flags=0x%x skb->len=%d\n",
+		 flags, skb->len);
+	DP_DEBUG(DP_DBG_FLAG_DUMP_TX,
+		 "skb->data=0x%p with pmac hdr size=%u\n", skb->data,
+		 sizeof(struct pmac_tx_hdr));
+	if (need_pmac) { /*insert one pmac header */
+		DP_DEBUG(DP_DBG_FLAG_DUMP_TX,
+			 "need pmac\n");
+		if (pmac && (dp_dbg_flag & DP_DBG_FLAG_DUMP_TX_DESCRIPTOR))
+			dp_port_prop[0].info.dump_tx_pmac(pmac);
+	} else {
+		DP_DEBUG(DP_DBG_FLAG_DUMP_TX, "no pmac\n");
+	}
+	if (gso)
+		DP_DEBUG(DP_DBG_FLAG_DUMP_TX, "GSO pkt\n");
+	else
+		DP_DEBUG(DP_DBG_FLAG_DUMP_TX, "Non-GSO pkt\n");
+	if (checksum)
+		DP_DEBUG(DP_DBG_FLAG_DUMP_TX, "Need checksum offload\n");
+	else
+		DP_DEBUG(DP_DBG_FLAG_DUMP_TX, "No need checksum offload pkt\n");
+
+	DP_DEBUG(DP_DBG_FLAG_DUMP_TX, "\n\n");
+}
+
+#define NO_NEED_PMAC(flags)  ((dp_info->alloc_flags & \
+		(DP_F_FAST_WLAN | DP_F_FAST_DSL)) && \
+		!((flags) & (DP_TX_CAL_CHKSUM | DP_TX_DSL_FCS)))
+
+static void set_chksum(struct pmac_tx_hdr *pmac, u32 tcp_type,
+		       u32 ip_offset, int ip_off_hw_adjust,
+		       u32 tcp_h_offset)
+{
+	pmac->tcp_type = tcp_type;
+	pmac->ip_offset = ip_offset + ip_off_hw_adjust;
+	pmac->tcp_h_offset = tcp_h_offset >> 2;
+}
+
+int32_t dp_xmit(struct net_device *rx_if, dp_subif_t *rx_subif,
+		struct sk_buff *skb, int32_t len, uint32_t flags)
+{
+	struct dma_tx_desc_0 *desc_0;
+	struct dma_tx_desc_1 *desc_1;
+	struct dma_tx_desc_2 *desc_2;
+	struct dma_tx_desc_3 *desc_3;
+	struct pmac_port_info *dp_info = NULL;
+	struct pmac_port_info2 *dp_info2 = NULL;
+	struct pmac_tx_hdr pmac = {0};
+	u32 ip_offset, tcp_h_offset, tcp_type;
+	char tx_chksum_flag = 0; /*check csum cal can be supported or not */
+	char insert_pmac_f = 1;	/*flag to insert one pmac */
+	int res = DP_SUCCESS;
+	int ep, vap;
+	enum dp_xmit_errors err_ret = 0;
+	int inst = 0;
+	struct cbm_tx_data data;
+
+#ifdef CONFIG_LTQ_DATAPATH_EXTRA_DEBUG
+	if (unlikely(!dp_init_ok)) {
+		err_ret = DP_XMIT_ERR_NOT_INIT;
+		goto lbl_err_ret;
+	}
+	if (unlikely(!rx_subif)) {
+		err_ret = DP_XMIT_ERR_NULL_SUBIF;
+		goto lbl_err_ret;
+	}
+	if (unlikely(!skb)) {
+		err_ret = DP_XMIT_ERR_NULL_SKB;
+		goto lbl_err_ret;
+	}
+#endif
+	ep = rx_subif->port_id;
+	if (unlikely(ep >= dp_port_prop[inst].info.cap.max_num_dp_ports)) {
+		err_ret = DP_XMIT_ERR_PORT_TOO_BIG;
+		goto lbl_err_ret;
+	}
+#ifdef CONFIG_LTQ_DATAPATH_EXTRA_DEBUG
+	if (unlikely(in_irq())) {
+		err_ret = DP_XMIT_ERR_IN_IRQ;
+		goto lbl_err_ret;
+	}
+#endif
+	dp_info = &dp_port_info[inst][ep];
+	dp_info2 = &dp_port_info2[inst][ep];
+	vap = GET_VAP(rx_subif->subif, dp_info->vap_offset, dp_info->vap_mask);
+	if (unlikely(!rx_if && /*For atm pppoa case, rx_if is NULL now */
+		     !(dp_info->alloc_flags & DP_F_FAST_DSL))) {
+		err_ret = DP_XMIT_ERR_NULL_IF;
+		goto lbl_err_ret;
+	}
+#ifdef CONFIG_LTQ_DATAPATH_MPE_FASTHOOK_TEST
+	if (unlikely(ltq_mpe_fasthook_tx_fn))
+		ltq_mpe_fasthook_tx_fn(skb, 0, NULL);
+#endif
+	if (unlikely(dp_dbg_flag))
+		dp_xmit_dbg("\nOrig", skb, ep, len, flags,
+			    NULL, rx_subif, 0, 0, flags & DP_TX_CAL_CHKSUM);
+
+	/*No PMAC for WAVE500 and DSL by default except bonding case */
+	if (unlikely(NO_NEED_PMAC(dp_info->alloc_flags)))
+		insert_pmac_f = 0;
+
+	/**********************************************
+	 *Must put these 4 lines after INSERT_PMAC
+	 *since INSERT_PMAC will change skb if needed
+	 *********************************************/
+	desc_0 = (struct dma_tx_desc_0 *)&skb->DW0;
+	desc_1 = (struct dma_tx_desc_1 *)&skb->DW1;
+	desc_2 = (struct dma_tx_desc_2 *)&skb->DW2;
+	desc_3 = (struct dma_tx_desc_3 *)&skb->DW3;
+
+	if (flags & DP_TX_CAL_CHKSUM) {
+		int ret_flg;
+
+		if (!dp_port_prop[inst].info.check_csum_cap()) {
+			err_ret = DP_XMIT_ERR_CSM_NO_SUPPORT;
+			goto lbl_err_ret;
+		}
+		ret_flg = get_offset_clear_chksum(skb, &ip_offset,
+						  &tcp_h_offset, &tcp_type);
+		if (likely(ret_flg == 0))
+			/*HW can support checksum offload*/
+			tx_chksum_flag = 1;
+#ifdef CONFIG_LTQ_DATAPATH_EXTRA_DEBUG
+		else if (ret_flg == -1)
+			pr_info_once("packet can't do hw checksum\n");
+#endif
+	}
+
+	/*reset all descriptors as SWAS required since SWAS 3.7 */
+	/*As new SWAS 3.7 required, MPE1/Color/FlowID is set by applications */
+	desc_0->all &= dma_tx_desc_mask0.all;
+	desc_1->all &= dma_tx_desc_mask1.all;
+	/*desc_2->all = 0;*/ /*remove since later it will be set properly */
+	if (desc_3->field.dic) {
+		desc_3->all = 0; /*keep DIC bit to support test tool*/
+		desc_3->field.dic = 1;
+	} else {
+		desc_3->all = 0;
+	}
+
+	if (flags & DP_TX_OAM) /* OAM */
+		desc_3->field.pdu_type = 1;
+	desc_1->field.classid = (skb->priority >= 15) ? 15 : skb->priority;
+	desc_2->field.data_ptr = (uint32_t)skb->data;
+
+	/*for ETH LAN/WAN */
+	if (dp_info->alloc_flags & (DP_F_FAST_ETH_LAN | DP_F_FAST_ETH_WAN |
+	    DP_F_GPON | DP_F_EPON)) {
+		/*always with pmac*/
+		if (likely(tx_chksum_flag)) {
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_CHECKSUM, &pmac,
+							desc_0, desc_1,
+							dp_info2);
+			set_chksum(&pmac, tcp_type, ip_offset,
+				   ip_offset_hw_adjust, tcp_h_offset);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
+		} else {
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_NORMAL, &pmac,
+							desc_0, desc_1,
+							dp_info2);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
+		}
+	} else if (dp_info->alloc_flags & DP_F_FAST_DSL) { /*some with pmac*/
+		if (unlikely(flags & DP_TX_CAL_CHKSUM)) { /* w/ pmac*/
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_CHECKSUM, &pmac,
+							desc_0, desc_1,
+							dp_info2);
+			set_chksum(&pmac, tcp_type, ip_offset,
+				   ip_offset_hw_adjust, tcp_h_offset);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
+#ifdef CONFIG_LTQ_DATAPATH_ACA_CSUM_WORKAROUND
+			if (aca_portid > 0)
+				desc_1->field.ep = aca_portid;
+#endif
+		} else if (flags & DP_TX_DSL_FCS) {/*must after checksum chk*/
+			/* w/ pmac for FCS purpose*/
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_OTHERS, &pmac,
+							desc_0, desc_1,
+							dp_info2);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
+		} else { /*no pmac */
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_NORMAL, NULL,
+							desc_0, desc_1,
+							dp_info2);
+		}
+	} else if (dp_info->alloc_flags & DP_F_FAST_WLAN) {/*some with pmac*/
+		/*normally no pmac. But if need checksum, need pmac*/
+		if (unlikely(tx_chksum_flag)) { /*with pmac*/
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_CHECKSUM, &pmac,
+							desc_0, desc_1,
+							dp_info2);
+			set_chksum(&pmac, tcp_type, ip_offset,
+				   ip_offset_hw_adjust, tcp_h_offset);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
+#ifdef CONFIG_LTQ_DATAPATH_ACA_CSUM_WORKAROUND
+			if (aca_portid > 0)
+				desc_1->field.ep = aca_portid;
+#endif
+		} else { /*no pmac*/
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_NORMAL, NULL,
+							desc_0, desc_1,
+							dp_info2);
+		}
+	} else if (dp_info->alloc_flags & DP_F_DIRECTLINK) { /*always w/ pmac*/
+		if (unlikely(flags & DP_TX_CAL_CHKSUM)) { /* w/ pmac*/
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_CHECKSUM, &pmac,
+							desc_0, desc_1,
+							dp_info2);
+			set_chksum(&pmac, tcp_type, ip_offset,
+				   ip_offset_hw_adjust, tcp_h_offset);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
+		} else if (flags & DP_TX_TO_DL_MPEFW) { /*w/ pmac*/
+			/*copy from checksum's pmac template setting,
+			 *but need to reset tcp_chksum in TCP header
+			 */
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_OTHERS, &pmac,
+							desc_0, desc_1,
+							dp_info2);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
+		} else { /*do like normal directpath with pmac */
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_NORMAL, &pmac,
+							desc_0, desc_1,
+							dp_info2);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
+		}
+	} else { /*normal directpath: always w/ pmac */
+		if (unlikely(tx_chksum_flag)) {
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_CHECKSUM,
+							&pmac,
+							desc_0,
+							desc_1,
+							dp_info2);
+			set_chksum(&pmac, tcp_type, ip_offset,
+				   ip_offset_hw_adjust, tcp_h_offset);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
+		} else { /*w/ pmac */
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_NORMAL, &pmac,
+							desc_0, desc_1,
+							dp_info2);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
+		}
+	}
+	desc_3->field.data_len = skb->len;
+
+	if (unlikely(dp_dbg_flag)) {
+		if (insert_pmac_f)
+			dp_xmit_dbg("After", skb, ep, len, flags, &pmac,
+				    rx_subif, insert_pmac_f, skb_is_gso(skb),
+				    tx_chksum_flag);
+		else
+			dp_xmit_dbg("After", skb, ep, len, flags, NULL,
+				    rx_subif, insert_pmac_f, skb_is_gso(skb),
+				    tx_chksum_flag);
+	}
+
+#if IS_ENABLED(CONFIG_LTQ_TOE_DRIVER)
+	if (skb_is_gso(skb)) {
+		res = ltq_tso_xmit(skb, &pmac, sizeof(pmac), 0);
+		UP_STATS(dp_info->subif_info[vap].mib.tx_tso_pkt);
+		return res;
+	}
+#endif /* CONFIG_LTQ_TOE_DRIVER */
+
+#ifdef CONFIG_LTQ_DATAPATH_EXTRA_DEBUG
+	if (unlikely(!desc_1->field.ep)) {
+		err_ret = DP_XMIT_ERR_EP_ZERO;
+		goto lbl_err_ret;
+	}
+#endif
+	if (insert_pmac_f) {
+		data.pmac = (u8 *)&pmac;
+		data.pmac_len = sizeof(pmac);
+		data.dp_inst = inst;
+		data.dp_inst = 0;
+	} else {
+		data.pmac = NULL;
+		data.pmac_len = 0;
+		data.dp_inst = inst;
+		data.dp_inst = 0;
+	}
+	res = cbm_cpu_pkt_tx(skb, &data, 0);
+	UP_STATS(dp_info->subif_info[vap].mib.tx_cbm_pkt);
+	return res;
+
+lbl_err_ret:
+	switch (err_ret) {
+	case DP_XMIT_ERR_NOT_INIT:
+		PR_RATELIMITED("dp_xmit failed for dp no init yet\n");
+		break;
+	case DP_XMIT_ERR_IN_IRQ:
+		PR_RATELIMITED("dp_xmit not allowed in interrupt context\n");
+		break;
+	case DP_XMIT_ERR_NULL_SUBIF:
+		PR_RATELIMITED("dp_xmit failed for rx_subif null\n");
+		UP_STATS(PORT_INFO(inst, 0, tx_err_drop));
+		break;
+	case DP_XMIT_ERR_PORT_TOO_BIG:
+		UP_STATS(PORT_INFO(inst, 0, tx_err_drop));
+		PR_RATELIMITED("rx_subif->port_id >= max_ports");
+		break;
+	case DP_XMIT_ERR_NULL_SKB:
+		PR_RATELIMITED("skb NULL");
+		UP_STATS(PORT_INFO(inst, rx_subif->port_id, tx_err_drop));
+		break;
+	case DP_XMIT_ERR_NULL_IF:
+		UP_STATS(PORT_VAP_MIB(inst, ep, vap, tx_pkt_dropped));
+		PR_RATELIMITED("rx_if NULL");
+		break;
+	case DP_XMIT_ERR_REALLOC_SKB:
+		PR_INFO_ONCE("dp_create_new_skb failed\n");
+		break;
+	case DP_XMIT_ERR_EP_ZERO:
+		PR_ERR("Why ep zero in dp_xmit for %s\n",
+		       skb->dev ? skb->dev->name : "NULL");
+		break;
+	case DP_XMIT_ERR_GSO_NOHEADROOM:
+		PR_ERR("No enough skb headerroom(GSO). Need tune SKB buffer\n");
+		break;
+	case DP_XMIT_ERR_CSM_NO_SUPPORT:
+		PR_RATELIMITED("dp_xmit not support checksum\n");
+		break;
+	default:
+		UP_STATS(dp_info->subif_info[vap].mib.tx_pkt_dropped);
+		PR_INFO_ONCE("Why come to here:%x\n",
+			     dp_port_info[inst][ep].status);
+	}
+	if (skb)
+		dev_kfree_skb_any(skb);
+	return DP_FAILURE;
+}
+EXPORT_SYMBOL(dp_xmit);
+
+void set_dp_dbg_flag(uint32_t flags)
+{
+	dp_dbg_flag = flags;
+}
+
+uint32_t get_dp_dbg_flag(void)
+{
+	return dp_dbg_flag;
+}
+
+/*!
+ *@brief  The API is for dp_get_cap
+ *@param[in,out] cap dp_cap pointer, caller must provide the buffer
+ *@param[in] flag for future
+ *@return 0 if OK / -1 if error
+ */
+int dp_get_cap(struct dp_cap *cap, int flag)
+{
+	if (!cap)
+		return DP_FAILURE;
+	if ((cap->inst < 0) || (cap->inst >= DP_MAX_INST))
+		return DP_FAILURE;
+	if (!hw_cap_list[cap->inst].valid)
+		return DP_FAILURE;
+	*cap = hw_cap_list[cap->inst].info.cap;
+
+	return DP_SUCCESS;
+}
+EXPORT_SYMBOL(dp_get_cap);
+
+int dp_set_min_frame_len(s32 dp_port,
+			 s32 min_frame_len,
+			 uint32_t flags)
+{
+	PR_INFO("Dummy dp_set_min_frame_len, need to implement later\n");
+	return DP_SUCCESS;
+}
+EXPORT_SYMBOL(dp_set_min_frame_len);
+
+int dp_rx_enable(struct net_device *netif, char *ifname, uint32_t flags)
+{
+	PR_INFO("Dummy dp_rx_enable, need to implement later\n");
+	return DP_SUCCESS;
+}
+EXPORT_SYMBOL(dp_rx_enable);
+
+int dp_vlan_set(struct dp_tc_vlan *vlan, int flags)
+{
+	dp_subif_t subif = {0};
+	struct dp_tc_vlan_info info = {0};
+	struct pmac_port_info *port_info;
+
+	if (dp_get_netif_subifid(vlan->dev, NULL, NULL, NULL, &subif, 0))
+		return DP_FAILURE;
+	port_info = PORT(subif.inst, subif.port_id);
+	info.subix = GET_VAP(subif.subif, port_info->vap_offset,
+			     port_info->vap_mask);
+	info.bp = subif.bport;
+	info.dp_port = subif.port_id;
+	info.inst = subif.inst;
+	info.dev_type = subif.flag_bp;
+	if (DP_CB(subif.inst, dp_tc_vlan_set))
+		return DP_CB(subif.inst, dp_tc_vlan_set)
+			    (dp_port_prop[subif.inst].ops[0],
+			     vlan, &info, flags);
+	return DP_FAILURE;
+}
+EXPORT_SYMBOL(dp_vlan_set);
+
+/*Return the table entry index based on dev:
+ *success: >=0
+ *fail: DP_FAILURE
+ */
+int bp_pmapper_dev_get(int inst, struct net_device *dev)
+{
+	int i;
+
+	if (!dev)
+		return -1;
+	for (i = 0; i < ARRAY_SIZE(dp_bp_dev_tbl[inst]); i++) {
+		if (!dp_bp_dev_tbl[inst][i].flag)
+			continue;
+		if (dp_bp_dev_tbl[inst][i].dev == dev) {
+			DP_DEBUG(DP_DBG_FLAG_PAE, "matched %s\n", dev->name);
+			return i;
+		}
+	}
+	return -1;
+}
+
+#ifdef DP_TEST_EXAMPLE
+void test(void)
+{
+	/* Base on below example data, it should print like below log
+	 *DMA Descripotr:D0=0x00004000 D1=0x00001000 D2=0xa0c02080 D3=0xb0000074
+	 *DW0:resv0=0 tunnel_id=00 flow_id=0 eth_type=0 dest_sub_if_id=0x4000
+	 *DW1:session_id=0x000 tcp_err=0 nat=0 dec=0 enc=0 mpe2=0 mpe1=0
+	 *color=01 ep=00 resv1=0 classid=00
+	 *DW2:data_ptr=0xa0c02080
+	 *DW3:own=1 c=0 sop=1 eop=1 dic=0 pdu_type=0
+	 *byte_offset=0 atm_qid=0 mpoa_pt=0 mpoa_mode=0 data_len= 116
+	 *paser flags: 00 00 00 00 80 18 80 00
+	 *paser flags: 00 80 18 80 00 00 00 00 (reverse)
+	 *flags 15 offset=14: PASER_FLAGS_1IPV4
+	 *flags 19 offset=22: PASER_FLAGS_ROUTEXP
+	 *flags 20 offset=34: PASER_FLAGS_TCP
+	 *flags 31 offset=46: PASER_FLAGS_LROEXP
+	 *pmac:0x4e 0x28 0xf0 0x00 0x00 0x00 0x00 0x01
+	 *byte 0:res=0 ver_done =1 ip_offset=14
+	 *byte 1:tcp_h_offset=5 tcp_type=0
+	 *byte 2:ppid=15 class=0
+	 *byte 3:res=0 pkt_type=0
+	 *byte 4:res=0 redirect=0 res2=0 src_sub_inf_id=0
+	 *byte 5:src_sub_inf_id2=0
+	 *byte 6:port_map=0
+	 *byte 7:port_map2=1
+	 */
+#ifdef CONFIG_LITTLE_ENDIAN
+	char example_data[] = {
+		0x00, 0x3a, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+		0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,
+		0x00, 0x00, 0x00, 0x16, 0x22, 0x00, 0x00, 0x00, 0x00, 0x00,
+		0x00, 0x00, 0x00, 0x00, 0x00, 0x2e,
+		0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+		0x00, 0x00, 0x80, 0x18, 0x80, 0x00,
+		0x00, 0xf0, 0x28, 0x4e, 0x01, 0x00, 0x00, 0x00, 0xaa, 0x00,
+		0x00, 0x00, 0x04, 0x03, 0xbb, 0x00,
+		0x00, 0x00, 0x04, 0x02, 0x08, 0x00, 0x45, 0x00, 0x00, 0x2e,
+		0x00, 0x00, 0x00, 0x00, 0x01, 0x06,
+		0xb9, 0xcb, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+		0x04, 0x00, 0xb2, 0x9a, 0x03, 0xde
+	};
+#else
+	char example_data[] = {
+		0x00, 0x3a, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+		0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,
+		0x00, 0x00, 0x00, 0x16, 0x22, 0x00, 0x00, 0x00, 0x00, 0x00,
+		0x00, 0x00, 0x00, 0x00, 0x00, 0x2e,
+		0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+		0x00, 0x00, 0x80, 0x18, 0x80, 0x00,
+		0x4e, 0x28, 0xf0, 0x00, 0x00, 0x00, 0x00, 0x01, 0xaa, 0x00,
+		0x00, 0x00, 0x04, 0x03, 0xbb, 0x00,
+		0x00, 0x00, 0x04, 0x02, 0x08, 0x00, 0x45, 0x00, 0x00, 0x2e,
+		0x00, 0x00, 0x00, 0x00, 0x01, 0x06,
+		0xb9, 0xcb, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+		0x04, 0x00, 0xb2, 0x9a, 0x03, 0xde
+	};
+#endif
+	struct sk_buff skb;
+
+	skb.DW0 = 0x4000;
+	skb.DW1 = 0x1000;
+	skb.DW2 = 0xa0c02080;
+	skb.DW3 = 0xb0000074;
+	skb.data = example_data;
+	skb.len = sizeof(example_data);
+	dp_rx(&skb, 0);
+}
+#endif				/* DP_TEST_EXAMPLE */
+
+int dp_basic_proc(void)
+{
+#ifdef CONFIG_LTQ_DATAPATH_LOOPETH
+	struct dentry *p_node;
+#endif
+
+	/*mask to reset some field as SWAS required  all others try to keep */
+	memset(dp_port_prop, 0, sizeof(dp_port_prop));
+	memset(dp_port_info, 0, sizeof(dp_port_info));
+#ifdef CONFIG_LTQ_DATAPATH_LOOPETH
+	p_node = dp_proc_install();
+	dp_loop_eth_dev_init(p_node);
+#else
+	dp_proc_install();
+#endif
+	dp_inst_init(0);
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_SWITCHDEV)
+	dp_switchdev_init();
+#endif
+	return 0;
+}
+
+/*static __init */ int dp_init_module(void)
+{
+	int res = 0;
+
+	if (dp_init_ok) /*alredy init */
+		return 0;
+	register_notifier(0);
+#ifdef CONFIG_LTQ_DATAPATH_DUMMY_QOS_VIA_FALCON_TEST
+	PR_INFO("\n\n--Falcon_test to simulate SLIM QOS drv---\n\n\n");
+	falcon_test();  /*Must put before register_dp_cap
+			 *since it needs to do CPU path cfg
+			 */
+#endif /*CONFIG_LTQ_DATAPATH_DUMMY_QOS_VIA_FALCON_TEST*/
+	register_dp_cap(0);
+	if (request_dp(0)) /*register 1st dp instance */ {
+		PR_ERR("register_dp instance fail\n");
+		return -1;
+	}
+#ifdef CONFIG_LTQ_DATAPATH_EXTRA_DEBUG
+	PR_INFO("preempt_count=%x\n", preempt_count());
+	if (preempt_count() & HARDIRQ_MASK)
+		PR_INFO("HARDIRQ_MASK\n");
+	if (preempt_count() & SOFTIRQ_MASK)
+		PR_INFO("SOFTIRQ_MASK\n");
+	if (preempt_count() & NMI_MASK)
+		PR_INFO("NMI_MASK\n");
+#endif
+	dp_init_ok = 1;
+	PR_INFO("datapath init done\n");
+	return res;
+}
+
+/*static __exit*/ void dp_cleanup_module(void)
+{
+	int i;
+
+	if (dp_init_ok) {
+		DP_LIB_LOCK(&dp_lock);
+		memset(dp_port_info, 0, sizeof(dp_port_info));
+#ifdef CONFIG_LTQ_DATAPATH_MIB
+		dp_mib_exit();
+#endif
+		for (i = 0; i < dp_inst_num; i++)
+			DP_CB(i, dp_platform_set)(i, DP_PLATFORM_DE_INIT);
+		dp_init_ok = 0;
+#ifdef CONFIG_LTQ_DATAPATH_LOOPETH
+		dp_loop_eth_dev_exit();
+#endif
+#ifdef CONFIG_LTQ_DATAPATH_CPUFREQ
+		dp_coc_cpufreq_exit();
+#endif
+		unregister_notifier(0);
+	}
+}
+
+MODULE_LICENSE("GPL");
+
+static int __init dp_dbg_lvl_set(char *str)
+{
+	PR_INFO("\n\ndp_dbg=%s\n\n", str);
+	dp_dbg_flag = dp_atoi(str);
+
+	return 0;
+}
+
+early_param("dp_dbg", dp_dbg_lvl_set);
+
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_instance.c b/drivers/net/ethernet/lantiq/datapath/datapath_instance.c
new file mode 100644
index 000000000000..3dfe79e41e90
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/datapath_instance.c
@@ -0,0 +1,654 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include<linux/init.h>
+#include<linux/module.h>
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/version.h>
+#include <linux/if_ether.h>
+#include <linux/ethtool.h>
+#include <linux/proc_fs.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/clk.h>
+#include <linux/if_ether.h>
+#include <linux/clk.h>
+#include <linux/ip.h>
+#include <net/ip.h>
+#include <lantiq_soc.h>
+#include <net/lantiq_cbm_api.h>
+#include <net/datapath_api.h>
+#include <net/datapath_api_skb.h>
+#include "datapath.h"
+#include "datapath_instance.h"
+#include "datapath_swdev_api.h"
+
+int dp_cap_num;
+struct dp_hw_cap hw_cap_list[DP_MAX_HW_CAP];
+
+/*Subif DEV hash list
+ *Note: in current implementation, its memory will not be really removed,
+ *Instead, it is just move to a free hash list dp_dev_list_free
+ *Reason: if we really free the memory, net_device's opt callback sometimes
+ *will be set to NULL?? Need further check
+ */
+struct hlist_head dp_dev_list[DP_DEV_HASH_SIZE];
+struct hlist_head dp_dev_list_free[DP_DEV_HASH_SIZE];
+
+/*Module hash list */
+struct hlist_head dp_mod_list[DP_MOD_HASH_SIZE];
+
+int register_dp_hw_cap(struct dp_hw_cap *info, u32 flag)
+{
+	int i;
+
+	if (!info) {
+		PR_ERR("register_dp_hw_cap: NULL info\n");
+		return -1;
+	}
+	for (i = 0; i < DP_MAX_HW_CAP; i++) {
+		if (hw_cap_list[i].valid)
+			continue;
+		hw_cap_list[i].valid = 1;
+		hw_cap_list[i].info = info->info;
+		dp_cap_num++;
+#ifdef CONFIG_LTQ_DATAPATH_EXTRA_DEBUG
+		PR_ERR("Succeed to %s HAL[%d]: type=%d ver=%d dp_cap_num=%d\n",
+		       "Register",
+		       i,
+		       info->info.type,
+		       info->info.ver,
+		       dp_cap_num);
+#endif
+		return 0;
+	}
+	PR_ERR("Failed to %s HAL: type=%d ver=%d\n",
+	       "Register",
+	       info->info.type,
+	       info->info.ver);
+	return -1;
+}
+
+/*return value:
+ *succeed: return 0 with info->inst updated
+ *fail: -1
+ */
+int dp_request_inst(struct dp_inst_info *info, u32 flag)
+{
+	int i, k;
+
+	if (!info)
+		return -1;
+
+	if (flag & DP_F_DEREGISTER) {
+		/*do de-register */
+
+		return 0;
+	}
+	/*register a dp instance */
+
+	/*to check whether any such matched HW cap */
+	for (k = 0; k < DP_MAX_HW_CAP; k++) {
+		if (!hw_cap_list[k].valid)
+			continue;
+		if ((hw_cap_list[k].info.type == info->type) &&
+		    (hw_cap_list[k].info.ver == info->ver)) {
+			break;
+		}
+	}
+	if (k == DP_MAX_HW_CAP) {
+		PR_ERR("dp_request_inst fail to math cap type=%d/ver=%d\n",
+		       info->type, info->ver);
+		return -1;
+	}
+
+	/* to find a free instance */
+	for (i = 0; i < DP_MAX_INST; i++) {
+		if (!dp_port_prop[i].valid)
+			break;
+	}
+	if (i == DP_MAX_INST) {
+		PR_ERR("dp_request_inst fail for dp inst full arealdy\n");
+		return -1;
+	}
+	dp_port_prop[i].ops[0] = info->ops[0];
+	dp_port_prop[i].ops[1] = info->ops[1];
+	dp_port_prop[i].info = hw_cap_list[k].info;
+	dp_port_prop[i].cbm_inst = info->cbm_inst;
+	dp_port_prop[i].qos_inst = info->qos_inst;
+	dp_port_prop[i].valid = 1;
+	if (dp_port_prop[i].info.dp_platform_set(i, DP_PLATFORM_INIT) < 0) {
+		dp_port_prop[i].valid = 0;
+		PR_ERR("dp_platform_init failed for inst=%d\n", i);
+		return -1;
+	}
+	info->inst = i;
+	dp_inst_num++;
+	DP_DEBUG(DP_DBG_FLAG_INST,
+		 "dp_request_inst ok: inst=%d, dp_inst_num=%d\n",
+		 i, dp_inst_num);
+	return 0;
+}
+EXPORT_SYMBOL(dp_request_inst);
+
+struct dp_hw_cap *match_hw_cap(struct dp_inst_info *info, u32 flag)
+{
+	int k;
+
+	for (k = 0; k < DP_MAX_HW_CAP; k++) {
+		if (!hw_cap_list[k].valid)
+			continue;
+		if ((hw_cap_list[k].info.type == info->type) &&
+		    (hw_cap_list[k].info.ver == info->ver)) {
+			return &hw_cap_list[k];
+		}
+	}
+	return NULL;
+}
+
+/*Note: like pon one device can have multiple ctp,
+ *ie, it may register multiple times
+ */
+u32 dp_dev_hash(struct net_device *dev, char *subif_name)
+{
+	unsigned long index;
+
+	if (!dev)
+		index = (unsigned long)subif_name;
+	else
+		index = (unsigned long)dev;
+	/*Note: it is 4K alignment. Need tune later */
+	return (u32)((index >> DP_DEV_HASH_SHIFT) % DP_DEV_HASH_SIZE);
+}
+
+struct dp_dev *dp_dev_lookup(struct hlist_head *head,
+			     struct net_device *dev, char *subif_name, u32 flag)
+{
+	struct dp_dev *item;
+
+	if (!dev) {
+		hlist_for_each_entry(item, head, hlist) {
+			if (strcmp(item->subif_name, subif_name) == 0)
+				return item;
+		}
+	} else {
+		hlist_for_each_entry(item, head, hlist) {
+			if (item->dev == dev)
+				return item;
+		}
+	}
+	return NULL;
+}
+
+static int dp_ndo_setup_tc(struct net_device *dev, u32 handle,
+		    __be16 protocol, struct tc_to_netdev *tc)
+{
+#if IS_ENABLED(CONFIG_PPA)
+	if (qos_mgr_hook_setup_tc)
+		return qos_mgr_hook_setup_tc(dev, handle, protocol, tc);
+#endif
+	if (dev->netdev_ops->ndo_setup_tc)
+		return dev->netdev_ops->ndo_setup_tc(dev, handle, protocol, tc);
+	PR_ERR("Cannot support ndo_setup_tc\n");
+	return -1;
+}
+
+/*Note:
+ *dev and subif_name: only one will be used for the hash index calculation.
+ *subif_name is only used for ATM IPOA/PPPOA case since its dev is NULL.
+ *otherwise it should use its dev.
+ *it will not work if just dev->name as subif_name
+ */
+int dp_inst_add_dev(struct net_device *dev, char *subif_name, int inst,
+		    int ep, int bp, int ctp, u32 flag)
+{
+	struct dp_dev *dp_dev;
+	u8 new_f = 0;
+	u32 idx;
+	struct subif_basic *subif;
+
+	if (!dev && !subif_name) {
+		PR_ERR("Why dev/subif_name both NULL?\n");
+		return -1;
+	}
+	idx = dp_dev_hash(dev, subif_name);
+	subif = kmalloc(sizeof(*subif), GFP_KERNEL);
+	if (!subif) {
+		PR_ERR("failed to alloc %d bytes\n", sizeof(*subif));
+		return -1;
+	}
+
+	subif->subif = ctp;
+	dp_dev = dp_dev_lookup(&dp_dev_list[idx], dev, subif_name, flag);
+
+	if (!dp_dev) { /*search free list */
+		dp_dev = dp_dev_lookup(&dp_dev_list_free[idx],
+				       dev, subif_name, flag);
+		if (dp_dev) {
+			hlist_del(&dp_dev->hlist); /*remove from free list */
+			dp_dev->count = 0;
+			new_f = 1;
+		}
+	}
+	if (!dp_dev) { /*alloc new */
+		dp_dev = kzalloc(sizeof(*dp_dev), GFP_KERNEL);
+		if (dp_dev) {
+			dp_dev->count = 0;
+			dp_dev->subif_name[0] = 0;
+			dp_dev->fid = 0;
+			//dp_port_info[inst][0].subif_info[0].fid = dp_dev->fid;
+			INIT_LIST_HEAD(&dp_dev->ctp_list);
+			new_f = 1;
+		}
+	}
+	if (!dp_dev) {
+		PR_ERR("Failed to kmalloc %d bytes\n", sizeof(*dp_dev));
+		kfree(subif);
+		return -1;
+	}
+	if (new_f) {
+		dp_dev->inst = inst;
+		dp_dev->dev = dev;
+		dp_dev->ep = ep;
+		dp_dev->bp = bp;
+		dp_dev->ctp = ctp;
+		if (subif_name) {
+			strncpy(dp_dev->subif_name, subif_name,
+				sizeof(dp_dev->subif_name) - 1);
+		}
+		hlist_add_head(&dp_dev->hlist, &dp_dev_list[idx]);
+#if IS_ENABLED(CONFIG_PPA)
+		/*backup ops*/
+		if (dev) {
+			dev->features |= NETIF_F_HW_TC;
+			if (!dev->netdev_ops) {
+				PR_ERR("netdev_ops not defined\n");
+				return -1;
+			}
+			if (!dp_dev->old_dev_ops) {
+				dp_dev->old_dev_ops = NULL;
+				dp_dev->old_dev_ops = dev->netdev_ops;
+				dp_dev->new_dev_ops = *dev->netdev_ops;
+				dp_dev->new_dev_ops.ndo_setup_tc =
+							dp_ndo_setup_tc;
+				dev->netdev_ops =
+					(const struct net_device_ops *)
+							&dp_dev->new_dev_ops;
+			} else if (dev->netdev_ops ==
+					(const struct net_device_ops *)
+					&dp_dev->new_dev_ops) {
+				dp_dev->new_dev_ops.ndo_setup_tc =
+							dp_ndo_setup_tc;
+			} else {
+				PR_ERR("error in old dev ops assignment\n");
+			}
+		}
+#endif
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_SWITCHDEV)
+		if (!(flag & DP_F_SUBIF_LOGICAL))
+			dp_port_register_switchdev(dp_dev, dev);
+#endif
+	}
+	dp_dev->count++;
+	list_add(&subif->list, &dp_dev->ctp_list);
+	DP_DEBUG(DP_DBG_FLAG_DBG, "dp_inst_add_dev:add new ctp=%d\n", ctp);
+	return 0;
+}
+
+int dp_inst_del_dev(struct net_device *dev, char *subif_name, int inst, int ep,
+		    u16 ctp, u32 flag)
+{
+	struct dp_dev *dp_dev;
+	u32 idx;
+	struct subif_basic *tmp;
+
+	idx = dp_dev_hash(dev, subif_name);
+	dp_dev = dp_dev_lookup(&dp_dev_list[idx], dev, subif_name, flag);
+	if (!dp_dev) {
+		PR_ERR("Failed to dp_dev_lookup: %s_%s\n",
+		       dev ? dev->name : "NULL", subif_name);
+		return -1;
+	}
+	if (dp_dev->count <= 0) {
+		PR_ERR("Count(%d) should > 0(%s_%s)\n", dp_dev->count,
+		       dev ? dev->name : "NULL", subif_name);
+		return -1;
+	}
+	if (dp_dev->inst != inst) {
+		PR_ERR("Why inst not same:%d_%d(%s_%s)\n", dp_dev->inst, inst,
+		       dev ? dev->name : "NULL", subif_name);
+		return -1;
+	}
+	if (dp_dev->ep != ep) {
+		PR_ERR("Why ep not same:%d_%d(%s_%s)\n", dp_dev->ep, ep,
+		       dev ? dev->name : "NULL", subif_name);
+		return -1;
+	}
+	list_for_each_entry(tmp, &dp_dev->ctp_list, list) {
+		if (tmp->subif == ctp) {
+			list_del(&tmp->list);
+			dp_dev->count--;
+
+			if (!dp_dev->count) { /*move to free list */
+				hlist_del(&dp_dev->hlist);
+#if IS_ENABLED(CONFIG_PPA)
+	if (dp_dev->old_dev_ops) {
+		if (dev->netdev_ops != dp_dev->old_dev_ops) {
+			dev->netdev_ops = dp_dev->old_dev_ops;
+			dp_dev->old_dev_ops = NULL;
+		}
+	}
+#endif
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_SWITCHDEV)
+				dp_port_deregister_switchdev(dp_dev, dev);
+#endif				/*do't really free now
+				 *in case network stack is holding the callback
+				 */
+				hlist_add_head(&dp_dev->hlist,
+					       &dp_dev_list_free[idx]);
+			}
+			return 0;
+		}
+	}
+	DP_DEBUG(DP_DBG_FLAG_INST, "dp_del_dev fail(%s_%s): not found ctp=%d\n",
+		 dev ? dev->name : "NULL",
+		 subif_name, ctp);
+	return -1;
+}
+
+u32 dp_mod_hash(struct module *owner, u16 ep)
+{
+	unsigned long index;
+
+	index = (unsigned long)owner;
+	return (u32)(((index >> DP_MOD_HASH_SHIFT) % DP_MOD_HASH_SIZE) | ep);
+}
+
+struct dp_mod *dp_mod_lookup(struct hlist_head *head, struct module *owner,
+			     u16 ep, u32 flag)
+{
+	struct dp_mod *item;
+
+	hlist_for_each_entry(item, head, hlist) {
+		if ((item->mod == owner) &&
+		    (item->ep == ep))
+		return item;
+	}
+	return NULL;
+}
+
+/* tuple: owner + ep
+ * act: inst
+ */
+int dp_inst_insert_mod(struct module *owner, u16 ep, u32 inst, u32 flag)
+{
+	struct dp_mod *dp_mod;
+	u8 new_f = 0;
+	u32 idx;
+
+	if (!owner) {
+		PR_ERR("owner NULL?\n");
+		return -1;
+	}
+	idx = dp_mod_hash(owner, ep);
+	DP_DEBUG(DP_DBG_FLAG_INST, "dp_inst_insert_mod:idx=%u\n", idx);
+	dp_mod = dp_mod_lookup(&dp_mod_list[idx], owner, ep, flag);
+	if (!dp_mod) { /*alloc new */
+		dp_mod = kmalloc(sizeof(*dp_mod), GFP_KERNEL);
+		if (dp_mod) {
+			dp_mod->mod = owner;
+			dp_mod->ep = ep;
+			dp_mod->inst = inst;
+			new_f = 1;
+		}
+	}
+	if (!dp_mod) {
+		PR_ERR("Failed to kmalloc %d bytes\n", sizeof(*dp_mod));
+		return -1;
+	}
+	if (new_f)
+		hlist_add_head(&dp_mod->hlist, &dp_mod_list[idx]);
+	DP_DEBUG(DP_DBG_FLAG_INST, "dp_inst_insert_mod:%s\n", owner->name);
+	return 0;
+}
+
+int dp_inst_del_mod(struct module *owner, u16 ep, u32 flag)
+{
+	struct dp_mod *dp_mod;
+	u32 idx;
+
+	if (!owner) {
+		PR_ERR("owner NULL?\n");
+		return -1;
+	}
+	idx = dp_mod_hash(owner, ep);
+	dp_mod = dp_mod_lookup(&dp_mod_list[idx], owner, ep, flag);
+	if (!dp_mod) {
+		PR_ERR("Failed to dp_mod_lookup: %s\n",
+		       owner->name);
+		return -1;
+	}
+	hlist_del(&dp_mod->hlist);
+	kfree(dp_mod);
+
+	DP_DEBUG(DP_DBG_FLAG_INST, "dp_inst_del_mod ok: %s:\n", owner->name);
+	return 0;
+}
+
+int dp_get_inst_via_module(struct module *owner,  u16 ep, u32 flag)
+{
+	struct dp_mod *dp_mod;
+	u32 idx;
+
+	if (!owner) {
+		PR_ERR("owner NULL?\n");
+		return -1;
+	}
+	idx = dp_mod_hash(owner, ep);
+	dp_mod = dp_mod_lookup(&dp_mod_list[idx], owner, ep, flag);
+	if (!dp_mod) {
+		PR_ERR("Failed to dp_mod_lookup: %s\n",
+		       owner->name);
+		return -1;
+	}
+
+	return dp_mod->inst;
+}
+
+/* if dev NULL, use subif_name, otherwise use dev to search */
+int dp_get_inst_via_dev(struct net_device *dev, char *subif_name, u32 flag)
+{
+	struct dp_dev *dp_dev;
+
+	if (!dev && !subif_name) /*for ATM IPOA/PPPOA */
+		return 0; /*workaround:otherwise caller need to check value */
+
+	dp_dev = dp_dev_lookup(dp_dev_list, dev, subif_name, flag);
+	if (!dp_dev)
+		return 0; /*workaround:otherwise caller need to check value */
+
+	return dp_dev->inst;
+}
+
+static u32 dev_hash_index;
+static struct dp_dev *dp_dev_proc;
+int proc_inst_dev_dump(struct seq_file *s, int pos)
+{
+	struct subif_basic *tmp;
+
+	while (!dp_dev_proc) {
+		dev_hash_index++;
+		pos = 0;
+		if (dev_hash_index == DP_DEV_HASH_SIZE)
+			return -1;
+
+		dp_dev_proc = hlist_entry_safe((
+			&dp_dev_list[dev_hash_index])->first,
+			struct dp_dev, hlist);
+	}
+	seq_printf(s, "Hash=%u pos=%d dev=%s(@%p) inst=%d ep=%d bp=%d ctp=%d count=%d @%p\n",
+		   dev_hash_index,
+		   pos,
+		   dp_dev_proc->dev ? dp_dev_proc->dev->name :
+		   dp_dev_proc->subif_name,
+		   dp_dev_proc->dev,
+		   dp_dev_proc->inst,
+		   dp_dev_proc->ep,
+		   dp_dev_proc->bp,
+		   dp_dev_proc->ctp,
+		   dp_dev_proc->count,
+		   dp_dev_proc);
+	seq_puts(s, "  ctp=");
+	list_for_each_entry(tmp, &dp_dev_proc->ctp_list, list) {
+		seq_printf(s, "%u ", tmp->subif);
+	}
+	seq_puts(s, "\n");
+	dp_dev_proc = hlist_entry_safe((dp_dev_proc)->hlist.next,
+				       struct dp_dev, hlist);
+
+	pos++;
+	return pos;
+}
+
+int proc_inst_dev_start(void)
+{
+	dev_hash_index = 0;
+	dp_dev_proc = hlist_entry_safe(
+		(&dp_dev_list[dev_hash_index])->first,
+		struct dp_dev, hlist);
+	return 0;
+}
+
+static void dump_cap(struct seq_file *s, struct dp_cap *cap)
+{
+	if (!s)
+		return;
+	seq_printf(s, "	HW TX checksum offloading: %s\n",
+		   cap->tx_hw_chksum ? "Yes" : "No");
+	seq_printf(s, "	HW RX checksum verification: %s\n",
+		   cap->rx_hw_chksum ? "Yes" : "No");
+	seq_printf(s, "	HW TSO: %s\n",
+		   cap->hw_tso ? "Yes" : "No");
+	seq_printf(s, "	HW GSO: %s\n",
+		   cap->hw_gso ? "Yes" : "No");
+	seq_printf(s, "	QOS Engine: %s\n",
+		   cap->qos_eng_name);
+	seq_printf(s, "	Pkt Engine: %s\n",
+		   cap->pkt_eng_name);
+	seq_printf(s, "	max_num_queues: %d\n",
+		   cap->max_num_queues);
+	seq_printf(s, "	max_num_scheds: %d\n",
+		   cap->max_num_scheds);
+	seq_printf(s, "	max_num_deq_ports: %d\n",
+		   cap->max_num_deq_ports);
+	seq_printf(s, "	max_num_qos_ports: %d\n",
+		   cap->max_num_qos_ports);
+	seq_printf(s, "	max_num_dp_ports: %d\n",
+		   cap->max_num_dp_ports);
+	seq_printf(s, "	max_num_subif_per_port: %d\n",
+		   cap->max_num_subif_per_port);
+	seq_printf(s, "	max_num_subif: %d\n",
+		   cap->max_num_subif);
+	seq_printf(s, "	max_num_bridge_port: %d\n",
+		   cap->max_num_bridge_port);
+}
+
+static u32 mod_hash_index;
+static struct dp_mod *dp_mod_proc;
+int proc_inst_mod_dump(struct seq_file *s, int pos)
+{
+	while (!dp_mod_proc) {
+		mod_hash_index++;
+		pos = 0;
+		if (mod_hash_index == DP_MOD_HASH_SIZE)
+			return -1;
+
+		dp_mod_proc = hlist_entry_safe((
+			&dp_mod_list[mod_hash_index])->first,
+			struct dp_mod, hlist);
+	}
+	seq_printf(s, "Hash=%u pos=%d owner=%s(@%p) ep=%d inst=%d\n",
+		   mod_hash_index,
+		   pos,
+		   dp_mod_proc->mod->name,
+		   dp_mod_proc->mod,
+		   dp_mod_proc->ep,
+		   dp_mod_proc->inst);
+
+	dp_mod_proc = hlist_entry_safe((dp_mod_proc)->hlist.next,
+				       struct dp_mod, hlist);
+	pos++;
+	return pos;
+}
+
+int proc_inst_dump(struct seq_file *s, int pos)
+{
+	struct dp_cap cap;
+
+	if (!dp_port_prop[pos].valid)
+		goto NEXT;
+	seq_printf(s, "Inst[%d] Type=%u ver=%d\n",
+		   pos,
+		   dp_port_prop[pos].info.type,
+		   dp_port_prop[pos].info.ver);
+	/*dump_cap(s, &dp_port_prop[pos].info.cap);*/
+	cap.inst = pos;
+	dp_get_cap(&cap, 0);
+	dump_cap(s, &cap);
+
+NEXT:
+	pos++;
+	if (pos == DP_MAX_INST)
+		return -1;
+	return pos;
+}
+
+int proc_inst_hal_dump(struct seq_file *s, int pos)
+{
+	if (!hw_cap_list[pos].valid)
+		goto NEXT;
+
+	seq_printf(s, "HAL[%d] Type=%u ver=%d dp_cap_num=%d\n",
+		   pos,
+		   hw_cap_list[pos].info.type,
+		   hw_cap_list[pos].info.ver,
+		   dp_cap_num);
+	dump_cap(s, &hw_cap_list[pos].info.cap);
+
+NEXT:
+	pos++;
+	if (pos == DP_MAX_HW_CAP)
+		return -1;
+	return pos;
+}
+
+int proc_inst_mod_start(void)
+{
+	mod_hash_index = 0;
+	dp_mod_proc = hlist_entry_safe(
+		(&dp_mod_list[mod_hash_index])->first,
+		struct dp_mod, hlist);
+	return 0;
+}
+
+int dp_inst_init(u32 flag)
+{
+	int i;
+
+	dp_cap_num = 0;
+	memset(hw_cap_list, 0, sizeof(hw_cap_list));
+	for (i = 0; i < DP_DEV_HASH_SIZE; i++) {
+		INIT_HLIST_HEAD(&dp_dev_list[i]);
+		INIT_HLIST_HEAD(&dp_dev_list_free[i]);
+	}
+
+	return 0;
+}
+
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_instance.h b/drivers/net/ethernet/lantiq/datapath/datapath_instance.h
new file mode 100644
index 000000000000..9b75e1e10b49
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/datapath_instance.h
@@ -0,0 +1,86 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#ifndef DATAPATH_INSTANCE_H
+#define DATAPATH_INSTANCE_H
+
+extern int dp_cap_num;
+extern struct dp_hw_cap hw_cap_list[DP_MAX_HW_CAP];
+
+#define DP_DEV_HASH_SHIFT 8
+#define DP_DEV_HASH_BIT_LENGTH 10
+#define DP_DEV_HASH_SIZE ((1 << DP_DEV_HASH_BIT_LENGTH) - 1)
+
+#define DP_MOD_HASH_SHIFT 8
+#define DP_MOD_HASH_BIT_LENGTH 10
+#define DP_MOD_HASH_SIZE ((1 << DP_MOD_HASH_BIT_LENGTH) - 1)
+
+extern struct hlist_head dp_dev_list[DP_DEV_HASH_SIZE];
+u32 dp_dev_hash(struct net_device *dev, char *subif_name);
+struct dp_dev *dp_dev_lookup(struct hlist_head *head,
+			     struct net_device *dev, char *subif_name,
+			     u32 flag);
+
+struct subif_basic {
+	struct list_head list;
+	int32_t subif:15;
+};
+
+struct ctp_list {
+	struct list_head list;
+	struct subif_basic subif;
+};
+
+struct dp_mod {
+	struct hlist_node hlist;
+	struct module *mod;
+	u16 ep;
+	int inst;
+};
+
+struct dp_dev {
+	struct hlist_node hlist;
+	struct net_device *dev;
+	char subif_name[IFNAMSIZ]; /*for ATM IPOA/PPPOA */
+	int inst;
+	int ep;
+	int bp;
+	int ctp;
+	int fid;
+	u32 count;
+	struct list_head ctp_list;
+	const struct net_device_ops *old_dev_ops;
+	struct net_device_ops new_dev_ops;
+#ifdef CONFIG_NET_SWITCHDEV
+	struct switchdev_ops *old_swdev_ops;
+	struct switchdev_ops new_swdev_ops;
+#endif
+};
+
+/*dp_inst_p: dp instance basic property */
+int dp_get_inst_via_dev(struct net_device *dev,
+			char *subif_name, u32 flag);
+int dp_get_inst_via_module(struct module *owner, u16 ep, u32 flag);
+struct dp_hw_cap *match_hw_cap(struct dp_inst_info *info, u32 flag);
+int dp_inst_add_dev(struct net_device *dev, char *subif_name, int inst,
+		    int ep, int bp, int ctp, u32 flag);
+int dp_inst_del_dev(struct net_device *dev, char *subif_name, int inst,
+		    int ep, u16 ctp, u32 flag);
+int dp_inst_insert_mod(struct module *owner, u16 ep, u32 inst, u32 flag);
+int dp_inst_del_mod(struct module *owner, u16 ep, u32 flag);
+
+int proc_inst_dev_dump(struct seq_file *s, int pos);
+int proc_inst_dev_start(void);
+int proc_inst_mod_dump(struct seq_file *s, int pos);
+int proc_inst_hal_dump(struct seq_file *s, int pos);
+int proc_inst_dump(struct seq_file *s, int pos);
+
+int proc_inst_mod_start(void);
+
+#endif
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_logical_dev.c b/drivers/net/ethernet/lantiq/datapath/datapath_logical_dev.c
new file mode 100644
index 000000000000..259d21b9a3be
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/datapath_logical_dev.c
@@ -0,0 +1,207 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+#include<linux/init.h>
+#include<linux/module.h>
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/version.h>
+#include <linux/if_ether.h>
+#include <linux/ethtool.h>
+#include <linux/proc_fs.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/clk.h>
+#include <linux/if_ether.h>
+#include <linux/clk.h>
+#include <linux/ip.h>
+#include <net/ip.h>
+#include <lantiq_soc.h>
+#include <net/datapath_api.h>
+#include <net/datapath_api_skb.h>
+#include <linux/if_vlan.h>
+#include "datapath.h"
+#include "datapath_instance.h"
+
+struct net_device *get_base_dev(struct net_device *dev, int level);
+
+/* return 0 -- succeed and supported by HW
+ * return -1  -- Not VLAN interface or not supported case
+ */
+int get_vlan_via_dev(struct net_device *dev, struct vlan_prop *vlan_prop)
+{
+	struct vlan_dev_priv *vlan;
+	struct net_device *base1, *base2;
+
+	if (!vlan_prop)
+		return -1;
+	vlan_prop->num = 0;
+	vlan_prop->base = NULL;
+	if (!is_vlan_dev(dev))
+		return 0;
+	base1 = get_base_dev(dev, 1);
+	vlan = vlan_dev_priv(dev);
+	if (!base1) { /*single vlan */
+		PR_ERR("Not 1st VLAN interface no base\n");
+		return -1;
+	}
+	if (is_vlan_dev(base1)) { /*double or more vlan*/
+		base2 = get_base_dev(base1, 1);
+		if (!base2) {
+			PR_ERR("Not 2nd VLAN interface no base\n");
+			return -1;
+		}
+		if (is_vlan_dev(base2)) {
+			PR_ERR("Too many VLAN tag, not supoprt\n");
+			return -1;
+		}
+		/*double vlan */
+		vlan_prop->num = 2;
+		vlan_prop->in_proto = vlan->vlan_proto;
+		vlan_prop->in_vid = vlan->vlan_id;
+
+		vlan = vlan_dev_priv(base1);
+		vlan_prop->out_proto = vlan->vlan_proto;
+		vlan_prop->out_vid = vlan->vlan_id;
+		vlan_prop->base = base2;
+		return 0;
+	}
+	/*single vlan */
+	vlan_prop->num = 1;
+	vlan_prop->out_proto = vlan->vlan_proto;
+	vlan_prop->out_vid = vlan->vlan_id;
+	vlan_prop->base = base1;
+	return 0;
+}
+
+struct logic_dev *logic_list_lookup(struct list_head *head,
+				    struct net_device *dev)
+{
+	struct logic_dev *pos;
+
+	list_for_each_entry(pos, head, list) {
+		if (pos->dev == dev)
+			return pos;
+	}
+	return NULL;
+}
+
+/*level >=1: only get the specified level if possible
+ *otherwise: -1 maximum
+ */
+struct net_device *get_base_dev(struct net_device *dev, int level)
+{
+	struct net_device *lower_dev, *tmp;
+	struct list_head *iter;
+
+	tmp = dev;
+	lower_dev = NULL;
+	do {
+		netdev_for_each_lower_dev(tmp, lower_dev, iter)
+			break;
+		if (lower_dev) {
+			tmp = lower_dev;
+			lower_dev = NULL;
+			level--;
+			if (level == 0)
+				break;
+		} else {
+			break;
+		}
+	} while (1);
+	if (tmp == dev)
+		return NULL;
+	return tmp;
+}
+
+/* add logic device into its base dev's logic dev list */
+int add_logic_dev(int inst, int port_id, struct net_device *dev,
+		  dp_subif_t *subif_id, u32 flags)
+{
+	struct logic_dev *logic_dev_tmp;
+	struct net_device *base_dev;
+	dp_subif_t subif;
+	int masked_subif;
+	struct pmac_port_info *port_info;
+
+	base_dev = get_base_dev(dev, -1);
+	if (!base_dev) {
+		DP_DEBUG(DP_DBG_FLAG_LOGIC,
+			 "Not found base dev of %s\n", dev->name);
+		return -1;
+	}
+	DP_DEBUG(DP_DBG_FLAG_LOGIC,
+		 "base_dev=%s for logic dev %s\n", base_dev->name, dev->name);
+	if (dp_get_port_subitf_via_dev_private(base_dev, &subif)) {
+		DP_DEBUG(DP_DBG_FLAG_LOGIC,
+			 "Not registered base dev %s in DP\n", dev->name);
+		return -1;
+	}
+	port_info = &dp_port_info[inst][port_id];
+	masked_subif = GET_VAP(subif.subif,
+			       port_info->vap_offset,
+			       port_info->vap_mask);
+	DP_DEBUG(DP_DBG_FLAG_LOGIC, "masked_subif=%x\n", masked_subif);
+	logic_dev_tmp = logic_list_lookup(
+		&port_info->subif_info[masked_subif].logic_dev,
+		dev);
+	if (logic_dev_tmp) {
+		DP_DEBUG(DP_DBG_FLAG_LOGIC, "Device already exist: %s\n",
+			 dev->name);
+		return -1;
+	}
+	logic_dev_tmp = kmalloc(sizeof(*logic_dev_tmp), GFP_KERNEL);
+	if (!logic_dev_tmp) {
+		DP_DEBUG(DP_DBG_FLAG_LOGIC, "kmalloc fail for %d bytes\n",
+			 sizeof(*logic_dev_tmp));
+		return -1;
+	}
+	logic_dev_tmp->dev = dev;
+	logic_dev_tmp->ep = port_id;
+	if (dp_port_prop[inst].info.subif_platform_set_unexplicit(inst,
+								  port_id,
+								  logic_dev_tmp,
+								  0)) {
+		DP_DEBUG(DP_DBG_FLAG_LOGIC, "dp_set_unexplicit fail\n");
+		return -1;
+	}
+	logic_dev_tmp->ctp = subif.subif;
+	DP_DEBUG(DP_DBG_FLAG_LOGIC, "add logic dev list\n");
+	list_add(&logic_dev_tmp->list,
+		 &port_info->subif_info[masked_subif].logic_dev);
+
+	subif_id->bport = logic_dev_tmp->bp;
+	subif_id->subif = subif.subif;
+	dp_inst_add_dev(dev, NULL,
+			inst, port_id,
+			logic_dev_tmp->bp,
+			subif.subif, flags);
+	return 0;
+}
+
+int del_logic_dev(int inst, struct list_head *head, struct net_device *dev,
+		  u32 flags)
+{
+	struct logic_dev *logic_dev;
+
+	logic_dev = logic_list_lookup(head, dev);
+	if (!logic_dev) {
+		DP_DEBUG(DP_DBG_FLAG_LOGIC, "Not find %s in logic dev list\n",
+			 dev->name);
+		return -1;
+	}
+	dp_port_prop[inst].info.subif_platform_set_unexplicit(inst,
+		logic_dev->ep,
+		logic_dev, flags);
+	dp_inst_del_dev(dev, NULL, inst, logic_dev->ep, logic_dev->ctp, 0);
+	list_del(&logic_dev->list);
+	kfree(logic_dev);
+
+	return 0;
+}
+
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_loopeth_dev.c b/drivers/net/ethernet/lantiq/datapath/datapath_loopeth_dev.c
new file mode 100644
index 000000000000..d6cfedbc341e
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/datapath_loopeth_dev.c
@@ -0,0 +1,1671 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+/*  Common Head File*/
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/spinlock.h>
+#include <linux/sched.h>
+#include <linux/kthread.h>
+#include <linux/version.h>
+#include <linux/types.h>
+#include <linux/ctype.h>
+#include <linux/fs.h>
+#include <linux/miscdevice.h>
+#include <linux/atmdev.h>
+#include <linux/init.h>
+#include <linux/etherdevice.h>	/*  eth_type_trans  */
+#include <linux/ethtool.h>	/*  ethtool_cmd     */
+#include <linux/if_ether.h>
+#include <linux/skbuff.h>
+#include <linux/inetdevice.h>
+#include <linux/ip.h>
+#include <linux/tcp.h>
+#include <linux/icmp.h>
+#include <net/tcp.h>
+#include <linux/uaccess.h>
+#include <asm/unistd.h>
+#include <asm/irq.h>
+#include <linux/delay.h>
+#include <linux/io.h>
+#include <asm/checksum.h>
+#include <linux/errno.h>
+#ifdef CONFIG_XFRM
+#include <net/xfrm.h>
+#endif
+
+#include <net/datapath_api.h>
+#include <net/datapath_proc_api.h>
+#include "datapath.h"
+
+/*####################################
+ *              Definition
+ * ####################################
+ */
+#define PROC_NAME	"loop"
+#define PROC_DBG	"dbg"
+#define PROC_DEV	"dev"
+#define PROC_MIB	"mib"
+#define PROC_DP		"directpath"
+#define PROC_CPUTX	"cputx"
+
+#define ENABLE_DEBUG                            1
+
+#define ENABLE_ASSERT                           1
+
+#define DEBUG_DUMP_SKB                          1
+#define PROC_DIR "/proc/dp/loop"
+#define PROC_DIRECTPATH PROC_DIR "/directpath"
+
+#if defined(ENABLE_DEBUG) && ENABLE_DEBUG
+#define ENABLE_DEBUG_PRINT                    1
+#define DISABLE_INLINE                        1
+#else
+#define ENABLE_DEBUG_PRINT                    0
+#define DISABLE_INLINE                        0
+#endif
+
+#if !defined(DISABLE_INLINE) || !DISABLE_INLINE
+#define INLINE                                inline
+#else
+#define INLINE
+#endif
+
+#if defined(ENABLE_DEBUG_PRINT) && ENABLE_DEBUG_PRINT
+#undef  dbg
+#define dbg(format, arg...) do { if ((g_dbg_enable &\
+	DBG_ENABLE_MASK_DEBUG_PRINT)) \
+	PR_INFO(KERN_WARNING ":%d:%s: " format "\n",\
+	__LINE__, __func__, ##arg); } \
+	while (0)
+#else
+#if !defined(dbg)
+#define dbg(format, arg...)
+#endif
+#endif
+
+#if defined(ENABLE_ASSERT) && ENABLE_ASSERT
+#define ASSERT(cond, format, arg...)      do {                \
+		if ((g_dbg_enable & DBG_ENABLE_MASK_ASSERT) && !(cond)) \
+			PR_INFO(":%d:%s: " format "\n",      \
+			       __LINE__, __func__, ##arg); } \
+	while (0)
+#else
+#define ASSERT(cond, format, arg...)
+#endif
+
+#if defined(DEBUG_DUMP_SKB) && DEBUG_DUMP_SKB
+#define DUMP_SKB_LEN                          ~0
+#endif
+
+#if (defined(DEBUG_DUMP_SKB) && DEBUG_DUMP_SKB) ||             \
+	(defined(ENABLE_DEBUG_PRINT) && ENABLE_DEBUG_PRINT) ||  \
+	(defined(ENABLE_ASSERT) && ENABLE_ASSERT)
+#define ENABLE_DBG_PROC                       1
+#else
+#define ENABLE_DBG_PROC                       0
+#endif
+
+/* Debug Print Mask*/
+#define DBG_ENABLE_MASK_ERR                     0x0001
+#define DBG_ENABLE_MASK_DEBUG_PRINT             0x0002
+#define DBG_ENABLE_MASK_ASSERT                  0x0004
+#define DBG_ENABLE_MASK_DUMP_SKB_RX             0x0008
+#define DBG_ENABLE_MASK_DUMP_SKB_TX             0x0010
+#define DBG_ENABLE_MASK_ALL     (DBG_ENABLE_MASK_ERR | \
+				 DBG_ENABLE_MASK_DEBUG_PRINT |\
+				 DBG_ENABLE_MASK_ASSERT |\
+				 DBG_ENABLE_MASK_DUMP_SKB_RX |\
+				 DBG_ENABLE_MASK_DUMP_SKB_TX)
+
+/* Constant Definition*/
+#define ETH_WATCHDOG_TIMEOUT                    (10 * HZ)
+#define MAX_RX_QUEUE_LENGTH                     100
+#define TASKLET_HANDLE_BUDGET                   25
+
+/* Ethernet Frame Definitions*/
+#define ETH_CRC_LENGTH                          4
+#define ETH_MAX_DATA_LENGTH                     ETH_DATA_LEN
+#define ETH_MIN_TX_PACKET_LENGTH                ETH_ZLEN
+
+/* ####################################
+ *              Data Type
+ * ####################################
+ */
+
+/* Internal Structure of Devices (ETH/ATM)*/
+#define LOOPETH_F_FREE           0
+#define LOOPETH_F_REGISTER_DEV   1
+#define LOOPETH_F_REGISTER_SUBIF 2
+
+struct loop_eth_priv_data {
+	int id;
+	struct net_device_stats stats;
+	unsigned int rx_preprocess_drop;
+	struct sk_buff_head rx_queue;
+	struct tasklet_struct rx_tasklet;
+	int f_tx_queue_stopped;
+	unsigned char dev_addr[MAX_ADDR_LEN];
+	unsigned int dp_pkts_to_ppe;
+	unsigned int dp_pkts_to_ppe_fail;
+	unsigned int dp_pkts_from_ppe;
+	unsigned int dp_pkts_tx;
+
+	struct module *owner;
+	dp_subif_t dp_subif;
+	s32 dev_port;	/*dev  instance */
+	s32 f_dp;		/* status for register to datapath */
+};
+
+static const char *const dbg_enable_mask_str[] = {
+	"err",			/*DBG_ENABLE_MASK_ERR */
+	"dbg",			/*DBG_ENABLE_MASK_DEBUG_PRINT */
+	"assert",		/*DBG_ENABLE_MASK_ASSERT */
+	"rx",			/*DBG_ENABLE_MASK_DUMP_SKB_RX */
+	"tx"			/*DBG_ENABLE_MASK_DUMP_SKB_TX */
+};
+
+/*####################################
+ *            Local Variable
+ * ####################################
+ */
+#define MAX_LOOPETH_NUM ((MAX_DP_PORTS * MAX_SUBIFS) + 2)
+static struct net_device *g_loop_eth_dev[MAX_LOOPETH_NUM] = { 0 };
+static u32 g_loop_eth_dev_flag[MAX_LOOPETH_NUM] = { 0 };
+static struct module g_loop_eth_module[MAX_LOOPETH_NUM];
+
+#if defined(ENABLE_DBG_PROC) && ENABLE_DBG_PROC
+static int g_dbg_enable = DBG_ENABLE_MASK_ERR | DBG_ENABLE_MASK_ASSERT;
+#endif
+
+/* Network Operations*/
+static void eth_setup(struct net_device *);
+static struct net_device_stats *eth_get_stats(struct net_device *);
+static int eth_open(struct net_device *);
+static int eth_stop(struct net_device *);
+static int eth_hard_start_xmit(struct sk_buff *, struct net_device *);
+static int eth_ioctl(struct net_device *, struct ifreq *, int);
+static void eth_tx_timeout(struct net_device *);
+
+/* RX path functions*/
+static INLINE int eth_rx_preprocess(struct sk_buff *, int);
+static INLINE void eth_rx_handler(struct sk_buff *, int);
+static void do_loop_eth_rx_tasklet(unsigned long);
+
+/* Datapath directpath functions*/
+static int32_t dp_fp_stop_tx(struct net_device *);
+static int32_t dp_fp_restart_tx(struct net_device *);
+static int32_t dp_fp_rx(struct net_device *, struct net_device *,
+			struct sk_buff *, int32_t);
+
+static const struct net_device_ops loop_netdev_ops = {
+	.ndo_open = eth_open,
+	.ndo_stop = eth_stop,
+	.ndo_start_xmit = eth_hard_start_xmit,
+	.ndo_do_ioctl = eth_ioctl,
+	.ndo_tx_timeout = eth_tx_timeout,
+	.ndo_get_stats = eth_get_stats,
+	.ndo_set_mac_address = eth_mac_addr,
+	.ndo_change_mtu = eth_change_mtu,
+};
+
+/*###################################
+ *           Global Variable
+ * ####################################
+ */
+
+/*####################################
+ *             Declaration
+ * ####################################
+ */
+
+/* Wrapper for Different Kernel Version
+ */
+static inline struct net_device *ltq_dev_get_by_name(const char *name)
+{
+	return dev_get_by_name(&init_net, name);
+}
+
+/*find the loopeth index via its device name
+ *return -1: not found
+ *>=0: index
+ */
+int find_loopeth_index_via_name(char *ifname)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(g_loop_eth_dev); i++) {
+		if (g_loop_eth_dev[i] &&
+		    dp_strncmpi(ifname, g_loop_eth_dev[i]->name, strlen(g_loop_eth_dev[i]->name)) == 0)
+			return i;
+	}
+
+	return -1;
+}
+
+static inline unsigned long ltq_get_xmit_fn(struct net_device *dev)
+{
+	return (unsigned long)dev->netdev_ops->ndo_start_xmit;
+}
+
+/*####################################
+ *            Local Function
+ * ####################################
+ */
+static void eth_setup(struct net_device *dev)
+{
+	struct loop_eth_priv_data *priv = netdev_priv(dev);
+
+	ether_setup(dev);	/*  assign some members */
+	dev->netdev_ops = &loop_netdev_ops;
+	dev->watchdog_timeo = ETH_WATCHDOG_TIMEOUT;
+	priv->id = -1;
+	priv->dp_subif.port_id = -1;
+	skb_queue_head_init(&priv->rx_queue);
+}
+
+struct net_device_stats *eth_get_stats(struct net_device *dev)
+{
+	struct loop_eth_priv_data *priv = netdev_priv(dev);
+
+	return &priv->stats;
+}
+
+int eth_open(struct net_device *dev)
+{
+	dbg("open %s", dev->name);
+	netif_start_queue(dev);
+	return 0;
+}
+
+int eth_stop(struct net_device *dev)
+{
+	netif_stop_queue(dev);
+	return 0;
+}
+
+int eth_hard_start_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+	unsigned long sysflag;
+	struct loop_eth_priv_data *priv = netdev_priv(dev);
+	int rx_queue_len;
+	struct sk_buff *old_skb = skb;
+
+	if (!skb)
+		return 0;
+	if (g_dbg_enable & DBG_ENABLE_MASK_DEBUG_PRINT)
+		dp_dump_raw_data(skb->data, skb->len,
+				 "loopeth_xmit original data");
+	if (skb_cloned(old_skb) && !skb_is_gso(old_skb)) {
+		/*sanity check before creating a new skb */
+		if (skb_shinfo(old_skb)->frag_list) {
+			PR_ERR("Not support frag_list here !!\n");
+			dev_kfree_skb_any(old_skb);
+			return 0;
+		}
+		if (old_skb->data_len) {
+			PR_ERR("Not support nr_frags for data_len not zero!\n");
+			dev_kfree_skb_any(old_skb);
+			return 0;
+		}
+		skb = alloc_skb(old_skb->len, GFP_KERNEL);
+		if (!skb) {
+			dev_kfree_skb_any(old_skb);
+			return 0;
+		}
+		memcpy(skb->data, old_skb->data, old_skb->len);
+		skb_put(skb, old_skb->len);
+		dev_kfree_skb_any(old_skb);
+		if (g_dbg_enable & DBG_ENABLE_MASK_DEBUG_PRINT)
+			dp_dump_raw_data(skb->data, skb->len,
+					 "loopeth_xmit original data after creating skb");
+	}
+
+	ASSERT((!skb->prev && !skb->next),
+	       "skb on list: prev = 0x%08x, next = 0x%08x",
+	       (unsigned int)skb->prev, (unsigned int)skb->next);
+
+	if (g_dbg_enable & DBG_ENABLE_MASK_DEBUG_PRINT)
+		dp_dump_raw_data(skb->data, skb->len,
+				 "loopeth_xmit for spoofing check:");
+
+	skb->dev = g_loop_eth_dev[priv->id];
+	spin_lock_irqsave(&priv->rx_queue.lock, sysflag);
+	rx_queue_len = skb_queue_len(&priv->rx_queue);
+
+	if (rx_queue_len < MAX_RX_QUEUE_LENGTH) {
+		__skb_queue_tail(&priv->rx_queue, skb);
+
+		if (rx_queue_len == 0)
+			tasklet_schedule(&priv->rx_tasklet);
+
+		if (skb_queue_len(&priv->rx_queue) >= MAX_RX_QUEUE_LENGTH)
+			netif_stop_queue(g_loop_eth_dev[priv->id]);
+
+		priv->stats.tx_packets++;
+		priv->stats.tx_bytes += skb->len;
+	} else {
+		dbg("drop packet for long queue\n");
+		dev_kfree_skb_any(skb);
+		priv->stats.tx_dropped++;
+	}
+
+	spin_unlock_irqrestore(&priv->rx_queue.lock, sysflag);
+	return 0;
+}
+
+int eth_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
+{
+	switch (cmd) {
+	default:
+		return -EOPNOTSUPP;
+	}
+
+	return 0;
+}
+
+void eth_tx_timeout(struct net_device *dev)
+{
+	struct loop_eth_priv_data *priv = netdev_priv(dev);
+
+	priv->stats.tx_errors++;
+	netif_wake_queue(dev);
+}
+
+/* Ethernet frame types according to RFC 2516 */
+#define ETH_PPPOE_DISCOVERY 0x8863
+#define ETH_PPPOE_SESSION   0x8864
+
+/* A PPPoE Packet, including Ethernet headers */
+struct pppoe_pkt {
+#ifdef PACK_BITFIELDS_REVERSED
+	unsigned int type:4;	/* PPPoE Type (must be 1) */
+	unsigned int ver:4;	/* PPPoE Version (must be 1) */
+#else
+	unsigned int ver:4;	/* PPPoE Version (must be 1) */
+	unsigned int type:4;	/* PPPoE Type (must be 1) */
+#endif
+	unsigned int code:8;	/* PPPoE code */
+	unsigned int session:16;	/* PPPoE session */
+	unsigned int length:16;	/* Payload length */
+	/* A bit of room to spare, here just for space holder only */
+	unsigned char payload[ETH_DATA_LEN];
+};
+
+/* PPPoE Tag */
+unsigned char ppp_ipv4_proto[2] = {
+	0x00, 0x21
+};
+
+unsigned char ppp_ipv6_proto[2] = {
+	0x00, 0x57
+};
+
+#define VLAN_HEAD_SIZE  4
+#define PPPOE_HEAD_SIZE  8
+
+/*return 1: send to directpath API dp_xmit to pae
+ * return 0: Should be dropped for not know how to spoof
+ */
+static INLINE int eth_rx_preprocess(struct sk_buff *skb, int id)
+{
+	unsigned char *p;
+	unsigned char mac[6];
+	unsigned char ip[4];
+	unsigned char port[2];
+	unsigned char ip_templ[4] = { 0 };
+	struct iphdr *iph;
+	struct icmphdr *icmph;
+	struct tcphdr *tcph;
+	u32 t, off_t, *opt;
+	int csum;
+	struct pppoe_pkt *pppoe;
+	int offset = 0;
+	int vlan_num = 0;
+	unsigned char *p_new_src_mac;
+	struct in_device __rcu *in_dev = NULL;
+	struct in_ifaddr *if_info = NULL;
+	__u8 *addr;
+
+	if (!skb)
+		return 0;
+	p = skb->data;
+
+	if (skb->data[6] & 0x1) {
+		/*source mac is broadcast or multicast. no spoof */
+		return 0;
+	}
+
+	if (g_dbg_enable & DBG_ENABLE_MASK_DEBUG_PRINT)
+		dp_dump_raw_data(skb->data, 20,
+				 "eth_rx_preprocess original skb:");
+
+	read_lock_bh(&dev_base_lock);
+	in_dev = (struct in_device *)skb->dev->ip_ptr;
+
+	if (!in_dev)
+		dbg("ip_ptr NULL. No IP\n");
+	else
+		if_info = in_dev->ifa_list;
+
+	if (if_info) {
+		memcpy(ip_templ, (char *)&if_info->ifa_address, 4);
+		addr = (char *)&if_info->ifa_local;
+
+		dbg("Device %s ifa_local: %u.%u.%u.%u\n", skb->dev->name,
+		    (__u32)addr[0], (__u32)addr[1], (__u32)addr[2],
+		    (__u32)addr[3]);
+
+		addr = (char *)&if_info->ifa_address;
+		dbg("Device %s ifa_address: %u.%u.%u.%u\n", skb->dev->name,
+		    (__u32)addr[0], (__u32)addr[1], (__u32)addr[2],
+		    (__u32)addr[3]);
+
+		addr = (char *)&if_info->ifa_mask;
+		dbg("Device %s ifa_mask: %u.%u.%u.%u\n", skb->dev->name,
+		    (__u32)addr[0], (__u32)addr[1], (__u32)addr[2],
+		    (__u32)addr[3]);
+
+		addr = (char *)&if_info->ifa_broadcast;
+		dbg("Device %s ifa_broadcast: %u.%u.%u.%u\n", skb->dev->name,
+		    (__u32)addr[0], (__u32)addr[1], (__u32)addr[2],
+		    (__u32)addr[3]);
+	}
+
+	read_unlock_bh(&dev_base_lock);
+
+	if (p[offset + 12] == 0x81 && p[offset + 13] == 0x00) {	/*VLAN header */
+		offset += VLAN_HEAD_SIZE;
+		vlan_num++;
+		dbg("Found VLAN%d\n", vlan_num);
+	}
+
+	if (p[offset + 12] == 0x88 && p[offset + 13] == 0x63) {
+		/*pppoe Discover(0x9)/Offer(0x7)/request(0x19)/Confirm(0x65) */
+		return 0;
+	}
+
+	if (p[offset + 12] == 0x88 && p[offset + 13] == 0x64) {	/*ppp */
+		pppoe = (struct pppoe_pkt *)(p + offset + 14);
+
+		if (((pppoe->payload[0] == ppp_ipv4_proto[0]) &&
+		     (pppoe->payload[1] == ppp_ipv4_proto[1])) /*PPP IPv4 */ ||
+		    ((pppoe->payload[0] == ppp_ipv6_proto[0]) &&
+		     (pppoe->payload[1] == ppp_ipv6_proto[1])) /* PPP IPv6 */) {
+			offset += PPPOE_HEAD_SIZE; /*skip 8 bytes ppp header*/
+			dbg("Found PPP IP packet\n");
+		} else {
+			return 0;
+		}
+	}
+
+	/*swap dst/src mac address */
+	memcpy(mac, p, 6);
+	memcpy(p, p + 6, 6);
+	memcpy(p + 6, mac, 6);
+	p_new_src_mac = p + 6;
+	p += offset;		/*Note, now p[12~13] points to protocol */
+
+	if (p[12] == 0x08 && p[13] == 0x06) {
+		/* arp request */
+		if (p[14] == 0x00 && p[15] == 0x01 && p[16] == 0x08 &&
+		    p[17] == 0x00 && p[20] == 0x00 && p[21] == 0x01) {
+			/*fill in spoof mac address */
+			p_new_src_mac[0] = 0;
+			p_new_src_mac[1] = 1;
+			p_new_src_mac[2] = id;
+			p_new_src_mac[3] = id;
+			p_new_src_mac[4] = id;
+			p_new_src_mac[5] = id;
+
+			if (((p[38] == ip_templ[0]) &&
+			     (p[39] == ip_templ[1])) ||
+			    (*(u32 *)ip_templ == 0)) {
+				dbg("Spoof arp:%d.%d.%d.%d\n",
+				    p[38], p[39], p[40], p[41]);
+				dbg(" %02x:%02x:%02x:%02x:%02x:%02x\n",
+				    p_new_src_mac[0], p_new_src_mac[1],
+				    p_new_src_mac[2], p_new_src_mac[3],
+				    p_new_src_mac[4], p_new_src_mac[5]);
+
+				/*  arp reply */
+				p[21] = 0x02;
+				/*  sender mac */
+				/*save orignal sender mac */
+				memcpy(mac, p + 22, 6);
+				/*set new sender mac */
+				memcpy(p + 22, p_new_src_mac, 6);
+
+				if (memcmp(p + 28, p + 38, 4) == 0) {
+					dbg("No reply arp for %d.%d.%d.%d\n",
+					    p[38], p[39], p[40], p[41]);
+					return 1;
+				}
+
+				/* sender IP */
+				/*save original sender ip address */
+				memcpy(ip, p + 28, 4);
+				/*set new sender ip address */
+				memcpy(p + 28, p + 38, 4);
+				/* target mac */
+				memcpy(p + 32, mac, 6);
+				/* target IP */
+				memcpy(p + 38, ip, 4);
+				if (g_dbg_enable & DBG_ENABLE_MASK_DEBUG_PRINT)
+					dp_dump_raw_data(skb->data, 20,
+							 "skb data after arp spoof");
+				return 1;
+			}
+
+			dbg("Not reply arp request for:%d.%d.%d.%d\n",
+			    p[38], p[39], p[40], p[41]);
+			return 1;
+		}
+
+		return 0;
+	} else if (((p[12] == 0x08) && (p[13] == 0x00)) || /*Normal IPV4 */
+		   ((p[12] == ppp_ipv4_proto[0]) &&
+		   (p[13] == ppp_ipv4_proto[1])) || /*PPP IPV4 */
+		   ((p[12] == ppp_ipv6_proto[0] &&
+		   p[13] == ppp_ipv6_proto[1]))) { /*PPP IPV6 */
+		/* IP */
+		switch ((int)p[23]) {
+		case 0x01:
+			/* ICMP - request */
+			if (p[34] == 0x08) {
+				/* src IP */
+				memcpy(ip, p + 26, 4);
+				memcpy(p + 26, p + 30, 4);
+				/* dest IP */
+				memcpy(p + 30, ip, 4);
+				/* ICMP reply */
+				p[34] = 0x00;
+				/* IP checksum */
+				iph = (struct iphdr *)(p + 14);
+				iph->check = 0;
+				iph->check =
+				    ip_fast_csum((unsigned char *)iph,
+						 iph->ihl);
+				/* ICMP checksum */
+				icmph = (struct icmphdr *)(p + 34);
+				icmph->checksum = 0;
+				csum = csum_partial((unsigned char *)
+						    icmph, skb->len - 34, 0);
+				icmph->checksum = csum_fold(csum);
+				dbg("spoof ping\n");
+				return 1;
+			}
+
+			break;
+
+		case 0x11:
+
+			/* UDP */
+		case 0x06:
+			/* TCP */
+			/*swap src/dst ip */
+			/* src IP */
+			dbg("spoof udp/tcp\n");
+			memcpy(ip, p + 26, 4);
+			memcpy(p + 26, p + 30, 4);
+			/* dest IP */
+			memcpy(p + 30, ip, 4);
+			/*save src port to port array and copy original dest
+			 *port to new src port
+			 */
+			memcpy(port, p + 34, 2);
+			memcpy(p + 34, p + 36, 2);
+			/* copy original src port to dest port */
+			memcpy(p + 36, port, 2);
+
+			/*return if UDP */
+			if ((int)p[23] == 0x11)
+				return 1;
+
+			iph = (struct iphdr *)(p + 14);
+			tcph = (struct tcphdr *)(p + 34);
+
+			if (tcph->syn == 1) {
+				/*set syn & ack, set seq NO same as the
+				 *incoming syn TCP packet, set ack seq
+				 *NO as seq NO + 1
+				 */
+				tcph->ack = 1;
+				tcph->ack_seq = tcph->seq + 1;
+			} else if (tcph->fin == 1) {	/*set fin & ack */
+				tcph->ack = 1;
+				t = tcph->ack_seq;
+				tcph->ack_seq = tcph->seq + 1;
+				tcph->seq = t;
+			} else if (tcph->rst == 1 || (tcph->psh == 0 &&
+					tcph->ack == 1))
+				return 0;/*rest or only ack, we ignore it. */
+			else if (tcph->psh == 1) {
+				t = tcph->ack_seq;
+				/*corrupt packet,ignore it.*/
+				if (iph->tot_len < 40)
+					return -1;
+
+				tcph->ack_seq =
+				    tcph->seq + iph->tot_len -
+				    (iph->ihl * 4) - (tcph->doff * 4);
+				tcph->seq = t;
+			}
+
+			/*check timestamp */
+			off_t = 14 + 20 + 20;	/*mac + ip + tcp */
+
+			while ((tcph->doff << 2) > (off_t - 34)) {
+				/*tcp option compare tcp header length */
+				switch (p[off_t]) {
+				case 0x0:	/*Option End */
+					break;
+
+				case 0x1:	/* NO Operation */
+					off_t += 1;
+					continue;
+
+				case 0x2:	/*Max Segment Size */
+					off_t += 4;
+					continue;
+
+				case 0x3:	/* Window Scale */
+					off_t += 3;
+					continue;
+
+				case 0x4:	/*TCP Sack permitted */
+					off_t += 2;
+					continue;
+
+				case 0x8:	/*TCP timestamp */
+#if 1
+					opt = (uint32_t *)(p + off_t + 2);
+					*(opt + 1) = htons(tcp_time_stamp);
+					t = *opt;
+					*opt = *(opt + 1);
+					*(opt + 1) = t;
+
+#else
+
+					for (t = 0; t < 10; t++)
+						*(p + off_t + t) = 1;
+
+#endif
+					off_t += 10;	/*option max is 64-20 */
+					continue;
+
+				default:
+					off_t += 64;
+					break;
+				}
+			}
+
+			/* IP checksum */
+			iph = (struct iphdr *)(p + 14);
+			iph->check = 0;
+			iph->check =
+			    ip_fast_csum((unsigned char *)iph, iph->ihl);
+			/* TCP checksum */
+			tcph->check = 0;
+			t = iph->tot_len - (iph->ihl * 4);
+			/*tcph->check = csum_partial((unsigned char *)tcph,
+			 *iph->tot_len - 20, 0);
+			 */
+			tcph->check =
+			    csum_tcpudp_magic(iph->saddr, iph->daddr, t,
+					      IPPROTO_TCP, csum_partial(tcph,
+									t,
+									0));
+			return 1;
+
+		default:
+			break;
+		}
+	}
+
+	dbg("Don't know how to spoof. Should be dropped\n");
+	return 0;
+}
+
+static INLINE void eth_rx_handler(struct sk_buff *skb, int id)
+{
+	struct loop_eth_priv_data *priv = netdev_priv(g_loop_eth_dev[id]);
+	int pktlen, i;
+	int flag = 0;
+	#define DUMP_LEN 14
+	char buf[(DUMP_LEN + 1) * 6];
+
+	pktlen = 0;
+
+	if (!netif_running(g_loop_eth_dev[id])) {
+		dev_kfree_skb_any(skb);
+		priv->stats.rx_dropped++;
+		return;
+	}
+
+	if (priv->dp_subif.port_id > 0) {
+		buf[0] = 0;
+		for (i = 0; i < DUMP_LEN; i++)
+			sprintf(buf + strlen(buf), "%02x ", skb->data[i]);
+		dbg("to xmit:%s\n", buf);
+		((struct dma_tx_desc_1 *)&skb->DW1)->field.ep =
+			priv->dp_subif.port_id;
+		((struct dma_tx_desc_0 *)&skb->DW0)->field.dest_sub_if_id =
+			priv->dp_subif.subif;
+		if (g_loop_eth_dev_flag[id] & DP_F_FAST_DSL)
+			flag = DP_TX_DSL_FCS;
+		if (dp_xmit(g_loop_eth_dev[id], &priv->dp_subif, skb, skb->len,
+			    flag) == DP_SUCCESS) {
+			priv->dp_pkts_to_ppe++;
+			return;
+		}
+		priv->dp_pkts_to_ppe_fail++;
+		return;
+	}
+
+	dbg("Drop packet since loopeth not registered yet to dp\n");
+	dev_kfree_skb_any(skb);
+	priv->stats.rx_packets++;
+	priv->stats.rx_bytes += pktlen;
+}
+
+static void do_loop_eth_rx_tasklet(unsigned long id)
+{
+	struct loop_eth_priv_data *priv = netdev_priv(g_loop_eth_dev[id]);
+	struct sk_buff *skb;
+	int i = 0;
+
+	if (id >= ARRAY_SIZE(g_loop_eth_dev)) {
+		PR_ERR("Wrong id(%ld) in do_loop_eth_rx_tasklet\n", id);
+		return;
+	}
+
+	while (1) {
+		if (i >= TASKLET_HANDLE_BUDGET) {
+			tasklet_schedule(&priv->rx_tasklet);
+			break;
+		}
+		skb = skb_dequeue(&priv->rx_queue);
+		if (!skb)
+			break;
+		netif_wake_queue(g_loop_eth_dev[id]);
+		dbg("dequeue one skb\n");
+		if (eth_rx_preprocess(skb, (int)id)) {
+			eth_rx_handler(skb, (int)id);
+		} else {
+			priv->rx_preprocess_drop++;
+			dev_kfree_skb_any(skb);
+			dbg("Drop for eth_rx_preprocess failed\n");
+		}
+
+		i++;
+	}
+}
+
+static int32_t dp_fp_stop_tx(struct net_device *netif)
+{
+	return 0;
+}
+
+static int32_t dp_fp_restart_tx(struct net_device *netif)
+{
+	return 0;
+}
+
+static int32_t dp_fp_rx(struct net_device *rxif, struct net_device *txif,
+			struct sk_buff *skb, int32_t len)
+{
+	struct loop_eth_priv_data *priv;
+	int pktlen;
+
+	skb_pull(skb, 8);	/*remove pmac header*/
+
+	if (g_dbg_enable & DBG_ENABLE_MASK_DEBUG_PRINT)
+		dp_dump_raw_data(skb->data, 20, "dp_fp_rx raw data");
+
+	if (rxif) {
+		dbg("dp_fp_rx to stack via %s\n", rxif->name);
+
+		if (netif_running(rxif)) {
+			priv = netdev_priv(rxif);
+			pktlen = skb->len;
+			skb->dev = rxif;
+			skb->protocol = eth_type_trans(skb, rxif);
+
+			if (netif_rx(skb) == NET_RX_DROP) {
+				priv->stats.rx_dropped++;
+			} else {
+				priv->stats.rx_packets++;
+				priv->stats.rx_bytes += pktlen;
+			}
+
+			priv->dp_pkts_from_ppe++;
+			return 0;
+		}
+	} else if (txif) {
+		dbg("dp_fp_rx to loopeth_xmit via %s for specified ep\n",
+		    rxif->name);
+		priv = netdev_priv(txif);
+		skb->dev = txif;
+		dev_queue_xmit(skb);
+		priv->dp_pkts_tx++;
+		return 0;
+	}
+
+	dev_kfree_skb_any(skb);
+	return 0;
+}
+
+#if defined(ENABLE_DBG_PROC) && ENABLE_DBG_PROC
+static void proc_read_dbg(struct seq_file *s)
+{
+	int i;
+
+	seq_printf(s, "g_dbg_enable=0x%08x\n. \tEnabled Flags:",
+		   g_dbg_enable);
+	for (i = 0; i < ARRAY_SIZE(dbg_enable_mask_str); i++)
+		if ((g_dbg_enable & (1 << i)))
+			seq_printf(s, "%s ", dbg_enable_mask_str[i]);
+	seq_puts(s, "\n");
+}
+
+static int proc_write_dbg(struct file *file, const char *buf, size_t count,
+			  loff_t *ppos)
+{
+	char str[100];
+	int len, rlen;
+	int f_enable = 0;
+	int i, j;
+	int num;
+	char *param_list[30];
+
+	len = count < sizeof(str) ? count : sizeof(str) - 1;
+	rlen = len - copy_from_user(str, buf, len);
+	str[rlen] = 0;
+
+	num = dp_split_buffer(str, param_list, ARRAY_SIZE(param_list));
+
+	if (num < 1)
+		goto help;
+
+	if (dp_strncmpi(param_list[0], "enable", strlen("enable")) == 0)
+		f_enable = 1;
+	else if (dp_strncmpi(param_list[0], "disable", strlen("disable")) == 0)
+		f_enable = -1;
+	else
+		goto help;
+
+	if (!param_list[1]) {
+		set_ltq_dbg_flag(g_dbg_enable, f_enable, -1);
+	} else {
+		for (i = 1; i < num; i++) {
+			for (j = 0; j < ARRAY_SIZE(dbg_enable_mask_str); j++) {
+				if (dp_strncmpi
+				    (param_list[i],
+				     dbg_enable_mask_str[j], strlen(dbg_enable_mask_str[j])) == 0) {
+					set_ltq_dbg_flag(g_dbg_enable,
+							 f_enable, (1 << j));
+
+					break;
+				}
+			}
+		}
+	}
+
+	return count;
+help:
+	PR_INFO("echo <enable/disable> [");
+
+	for (i = 0; i < ARRAY_SIZE(dbg_enable_mask_str); i++) {
+		if (i == 0)
+			PR_INFO("%s", dbg_enable_mask_str[i]);
+		else
+			PR_INFO("/%s", dbg_enable_mask_str[i]);
+	}
+
+	PR_INFO("] > %s/dbg\n", PROC_DIR);
+	return count;
+}
+#endif
+
+static int proc_read_dev(struct seq_file *s, int pos)
+{
+	if (g_loop_eth_dev[pos])
+		seq_printf(s, "  %s\n", g_loop_eth_dev[pos]->name);
+
+	pos++;
+
+	if (pos >= MAX_LOOPETH_NUM)
+		pos = -1;
+
+	return pos;
+}
+
+static int unregister_dev(int i)
+{
+	struct loop_eth_priv_data *priv;
+	struct pmac_port_info *port_info;
+	int res;
+
+	if (!g_loop_eth_dev[i])
+		return -1;
+
+	priv = netdev_priv(g_loop_eth_dev[i]);
+
+	if (priv->dp_subif.port_id == 0)
+		return 0;
+
+	if (priv->f_dp == LOOPETH_F_REGISTER_SUBIF) {
+		res =
+		    dp_register_subif(priv->owner, g_loop_eth_dev[i],
+				      g_loop_eth_dev[i]->name,
+				      &priv->dp_subif, DP_F_DEREGISTER);
+
+		if (res != DP_SUCCESS)
+			PR_ERR
+			    ("dp_register_subif failed for port %d subif %d\n",
+			     priv->dp_subif.port_id, priv->dp_subif.subif);
+
+		priv->f_dp = LOOPETH_F_REGISTER_DEV;
+		priv->dp_subif.subif = -1;
+	}
+
+	port_info = get_port_info_via_dp_port(0, priv->dp_subif.port_id);
+
+	if (!port_info) {
+		PR_ERR("get_port_info fail:%s specific port %d subif=%d\n",
+		       g_loop_eth_dev[i]->name, priv->dp_subif.port_id,
+		       priv->dp_subif.subif);
+		return -1;
+	}
+
+	if (port_info->num_subif == 0) {
+		if (priv->f_dp == LOOPETH_F_REGISTER_DEV) {
+			if (dp_register_dev(priv->owner,
+					    priv->dp_subif.port_id,
+					    NULL,
+					    DP_F_DEREGISTER) != DP_SUCCESS)
+				PR_ERR("unreg_dev fail:%s p_id/subif %d/%d\n",
+				       g_loop_eth_dev[i]->name,
+				       priv->dp_subif.port_id,
+				       priv->dp_subif.subif);
+
+			if (dp_alloc_port(priv->owner, g_loop_eth_dev[i], i,
+					  priv->dp_subif.port_id, NULL,
+					  DP_F_DEREGISTER) != DP_SUCCESS) {
+				PR_ERR("de-alloc fail: %s p_id/subif %d/%d\n",
+				       g_loop_eth_dev[i]->name,
+				       priv->dp_subif.port_id,
+				       priv->dp_subif.subif);
+			}
+
+			priv->dp_subif.port_id = -1;
+		}
+
+		priv->f_dp = LOOPETH_F_FREE;
+	}
+
+	return 0;
+}
+
+static int delete_loopeth_dev(int i)
+{
+	struct loop_eth_priv_data *priv;
+
+	if (!g_loop_eth_dev[i])
+		return -1;
+
+	priv = netdev_priv(g_loop_eth_dev[i]);
+
+	unregister_dev(i);
+
+	/*unregister loopeth dev itself */
+	unregister_netdev(g_loop_eth_dev[i]);
+	free_netdev(g_loop_eth_dev[i]);
+	g_loop_eth_dev[i] = NULL;
+	return 0;
+}
+
+int create_loopeth_dev(int i)
+{
+	char ifname[IFNAMSIZ];
+	struct loop_eth_priv_data *priv;
+
+	if (g_loop_eth_dev[i]) {
+		PR_ERR("g_loop_eth_dev[%d] already exist\n", i);
+		return 0;
+	}
+
+	snprintf(ifname, sizeof(ifname), "loopeth%d", i);
+
+	g_loop_eth_dev[i] = alloc_netdev(
+		sizeof(struct loop_eth_priv_data),
+		ifname, NET_NAME_ENUM, eth_setup);
+	if (!g_loop_eth_dev[i]) {
+		PR_ERR("alloc_netdev fail\n");
+		return -1;
+	}
+
+	g_loop_eth_dev[i]->dev_addr[0] = 0x00;
+	g_loop_eth_dev[i]->dev_addr[1] = 0x00;
+	g_loop_eth_dev[i]->dev_addr[2] = 0x00;
+	g_loop_eth_dev[i]->dev_addr[3] = 0x00;
+	g_loop_eth_dev[i]->dev_addr[4] = 0x00;
+	g_loop_eth_dev[i]->dev_addr[5] = i;
+	priv = netdev_priv(g_loop_eth_dev[i]);
+	priv->id = i;
+	tasklet_init(&priv->rx_tasklet, do_loop_eth_rx_tasklet, i);
+
+	if (register_netdev(g_loop_eth_dev[i])) {
+		free_netdev(g_loop_eth_dev[i]);
+		g_loop_eth_dev[i] = NULL;
+		PR_INFO("register device \"%s\" fail ??\n", ifname);
+	} else {
+		PR_INFO("add \"%s\" successfully\n", ifname);
+		priv = netdev_priv(g_loop_eth_dev[i]);
+		priv->f_dp = LOOPETH_F_FREE;
+	}
+
+	return 0;
+}
+
+static int proc_write_dev(struct file *file, const char *buf, size_t count,
+			  loff_t *ppos)
+{
+	char str[100];
+	int len, rlen;
+	char *param_list[10];
+	int num;
+	int i;
+
+	len = count < sizeof(str) ? count : sizeof(str) - 1;
+	rlen = len - copy_from_user(str, buf, len);
+	str[rlen] = 0;
+
+	num = dp_split_buffer(str, param_list, ARRAY_SIZE(param_list));
+
+	if (num < 1)
+		goto help;
+
+	if (dp_strncmpi(param_list[0], "add", strlen("add")) == 0) {
+		if (param_list[1]) {
+			i = dp_atoi(param_list[1]);
+
+			if ((i < 0) || (i >= ARRAY_SIZE(g_loop_eth_dev))) {
+				PR_ERR("Wrong index value: %d\n", i);
+				return count;
+			}
+
+			if (g_loop_eth_dev[i]) {
+				PR_ERR
+				    ("itf %d existed and no need to create\n",
+				     i);
+				return count;
+			}
+
+			/*create one dev */
+			create_loopeth_dev(i);
+		} else
+			for (i = 0; i < ARRAY_SIZE(g_loop_eth_dev); i++) {
+				/*create all dev if not created yet */
+				if (g_loop_eth_dev[i])
+					continue;
+				create_loopeth_dev(i);
+			}
+	} else if (dp_strncmpi(param_list[0], "del", strlen("del")) == 0) {
+		if (param_list[1]) {
+			for (i = 0; i < ARRAY_SIZE(g_loop_eth_dev); i++) {
+				if (g_loop_eth_dev[i] &&
+				    (dp_strncmpi
+				     (g_loop_eth_dev[i]->name,
+				      param_list[1], strlen(param_list[1])) == 0)) {
+					delete_loopeth_dev(i);
+					break;
+				}
+			}
+		} else {
+			for (i = 0; i < ARRAY_SIZE(g_loop_eth_dev); i++)
+				delete_loopeth_dev(i);
+		}
+	} else {
+		PR_ERR("Wrong command: %s\n", param_list[0]);
+		goto help;
+	}
+
+	return count;
+
+help:
+	PR_INFO("echo add [index] %s/dev\n", PROC_DIR);
+	PR_INFO("   example: echo add 1 > %s/dev\n", PROC_DIR);
+	PR_INFO("   example: echo add    > %s/dev\n", PROC_DIR);
+	PR_INFO("        Note, the maximum index is %d\n",
+		ARRAY_SIZE(g_loop_eth_dev));
+	PR_INFO("echo <del> [device name] > %s/dev\n", PROC_DIR);
+	PR_INFO("   example: echo del loopeth1 > %s/dev\n", PROC_DIR);
+	PR_INFO("   example: echo del          > %s/dev\n", PROC_DIR);
+
+	return count;
+}
+
+static int proc_read_mib(struct seq_file *s, int pos)
+{
+	struct loop_eth_priv_data *priv;
+
+	if (g_loop_eth_dev[pos]) {
+		priv = netdev_priv(g_loop_eth_dev[pos]);
+		seq_printf(s, "  %s:\n", g_loop_eth_dev[pos]->name);
+		seq_printf(s, "    rx_packets: %lu\n",
+			   priv->stats.rx_packets);
+		seq_printf(s, "    rx_bytes:   %lu\n", priv->stats.rx_bytes);
+		seq_printf(s, "    rx_errors:  %lu\n", priv->stats.rx_errors);
+		seq_printf(s, "    rx_dropped: %lu\n",
+			   priv->stats.rx_dropped);
+		seq_printf(s, "    tx_packets: %lu\n",
+			   priv->stats.tx_packets);
+		seq_printf(s, "    tx_bytes:   %lu\n", priv->stats.tx_bytes);
+		seq_printf(s, "    tx_errors:  %lu\n", priv->stats.tx_errors);
+		seq_printf(s, "    tx_dropped: %lu\n",
+			   priv->stats.tx_dropped);
+		seq_printf(s, "    rx_preprocess_drop:  %u\n",
+			   priv->rx_preprocess_drop);
+		seq_printf(s, "    dp_pkts_to_ppe:      %u\n",
+			   priv->dp_pkts_to_ppe);
+		seq_printf(s, "    dp_pkts_to_ppe_fail: %u\n",
+			   priv->dp_pkts_to_ppe_fail);
+		seq_printf(s, "    dp_pkts_from_ppe:    %u\n",
+			   priv->dp_pkts_from_ppe);
+		seq_printf(s, "    dp_pkts_tx:          %u\n",
+			   priv->dp_pkts_tx);
+	}
+
+	pos++;
+
+	if (pos >= MAX_LOOPETH_NUM)
+		pos = -1;
+
+	return pos;
+}
+
+void clear_mib(int i)
+{
+	struct loop_eth_priv_data *priv;
+
+	priv = netdev_priv(g_loop_eth_dev[i]);
+	memset(&priv->stats, 0, sizeof(priv->stats));
+	priv->rx_preprocess_drop = 0;
+	priv->dp_pkts_to_ppe = 0;
+	priv->dp_pkts_to_ppe_fail = 0;
+	priv->dp_pkts_from_ppe = 0;
+	priv->dp_pkts_tx = 0;
+}
+
+static int proc_write_mib(struct file *file, const char *buf, size_t count,
+			  loff_t *ppos)
+{
+	char str[100];
+	int len, rlen;
+	int i;
+	int num;
+	char *param_list[4];
+
+	len = count < sizeof(str) ? count : sizeof(str) - 1;
+	rlen = len - copy_from_user(str, buf, len);
+	str[rlen] = 0;
+	num = dp_split_buffer(str, param_list, ARRAY_SIZE(param_list));
+
+	if (num < 2)
+		goto help;
+
+	if (dp_strncmpi(param_list[0], "clear", strlen("clear")) != 0) {
+		PR_ERR("Wrong command:%s\n", param_list[0]);
+		goto help;
+	}
+
+	if ((dp_strncmpi(param_list[1], "all", strlen("all")) != 0)) {
+		for (i = 0; i < ARRAY_SIZE(g_loop_eth_dev); i++) {
+			if (g_loop_eth_dev[i] &&
+			    (dp_strncmpi
+			     (g_loop_eth_dev[i]->name, param_list[1], strlen(param_list[1])) == 0)) {
+				clear_mib(i);
+				break;
+			}
+		}
+
+		if (i >= ARRAY_SIZE(g_loop_eth_dev))
+			PR_ERR("not found device %s\n", param_list[1]);
+	} else {
+		for (i = 0; i < ARRAY_SIZE(g_loop_eth_dev); i++)
+			clear_mib(i);
+	}
+
+	return count;
+help:
+	PR_INFO("echo <clear> [all/device name] > %s/mib\n", PROC_DIR);
+	return count;
+}
+
+static int proc_read_dp(struct seq_file *s, int pos)
+{
+	struct loop_eth_priv_data *priv;
+
+	if (g_loop_eth_dev[pos]) {
+		priv = netdev_priv(g_loop_eth_dev[pos]);
+
+		if (priv->dp_subif.port_id >= 0) {
+			seq_printf(s,
+				   "%s - directpath on (ifid %d subif %d)\n",
+				   g_loop_eth_dev[pos]->name,
+				   priv->dp_subif.port_id,
+				   priv->dp_subif.subif);
+		} else
+			seq_printf(s, "%s - directpath off\n",
+				   g_loop_eth_dev[pos]->name);
+	}
+
+	pos++;
+
+	if (pos >= MAX_LOOPETH_NUM)
+		pos = -1;
+
+	return pos;
+}
+
+void unregister_from_dp(struct net_device *dev)
+{
+	struct loop_eth_priv_data *priv;
+	s32 dp_port_id;
+	int i;
+
+	priv = netdev_priv(dev);
+	dp_port_id = priv->dp_subif.port_id;
+
+	if (dp_port_id <= 0) {
+		PR_ERR
+		    ("Cannot undregister %s since it is not reigstered yet\n",
+		     dev->name);
+		return;
+	}
+
+	/*unregister all subif with same port_id first */
+	for (i = 0; i < ARRAY_SIZE(g_loop_eth_dev); i++) {
+		if (!g_loop_eth_dev[i])
+			continue;
+		priv = netdev_priv(g_loop_eth_dev[i]);
+		if (priv->dp_subif.port_id != dp_port_id)
+			continue;
+		if (priv->f_dp != LOOPETH_F_REGISTER_SUBIF)
+			continue;
+		if (dp_register_subif(priv->owner, g_loop_eth_dev[i],
+				      g_loop_eth_dev[i]->name,
+				      &priv->dp_subif, 0) != DP_SUCCESS)
+			PR_ERR("unreg_subif fail:%s port_id/subif %d/%d ?\n",
+			       dev->name, priv->dp_subif.port_id,
+			       priv->dp_subif.subif);
+		priv->f_dp = LOOPETH_F_REGISTER_DEV;
+		priv->dp_subif.subif = 0;
+	}
+
+	/*unregister/deallocate devices and reset all devices
+	 *with same port_id
+	 */
+	if (dp_register_dev(priv->owner, dp_port_id, NULL, DP_F_DEREGISTER)
+	    != DP_SUCCESS) {
+		PR_INFO("dp_unregister_dev failed for %s with port_id %d ?\n",
+			dev->name, dp_port_id);
+	}
+	if (dp_alloc_port
+	    (priv->owner, dev, priv->dev_port, dp_port_id, NULL,
+	     DP_F_DEREGISTER) != DP_SUCCESS) {
+		PR_INFO("dp_dealloc_port failed for %s with port_id %d\n",
+			dev->name, dp_port_id);
+	}
+
+	for (i = 0; i < ARRAY_SIZE(g_loop_eth_dev); i++) {
+		if (g_loop_eth_dev[i]) {
+			struct loop_eth_priv_data *priv =
+			    netdev_priv(g_loop_eth_dev[i]);
+
+			if (priv->dp_subif.port_id != dp_port_id)
+				continue;
+
+			priv->f_dp = LOOPETH_F_FREE;
+			priv->dp_subif.port_id = 0;
+		}
+	}
+}
+
+/*to find the device type index via flag name. for example eth_lan, eth_wan,
+ *return value:
+ *-1: not found
+ *>=0: type index
+ */
+int get_dev_type_index(char *flag_name)
+{
+	int i;
+
+	for (i = 1; i < get_dp_port_type_str_size(); i++) {	/*skip i = 0 */
+		if (dp_strncmpi(flag_name, dp_port_type_str[i], strlen(dp_port_type_str[i])) == 0) {
+			return i;
+			;
+		}
+	}
+
+	return -1;
+}
+
+#define OPT_PORT  "[-e <dp_port>]"
+#define OPT_F "[-f explicit]"
+#define OPT_Q "[-q <num_resv_queue>]"
+#define OPT_SCH "[-s <num_resv_schedule>]"
+#define OPT_NUM  "[-p <num_resv_port>]"
+#define OPT_NO "[ -b <start_port_no>]"
+#define OPT_ALLOC_PORT (OPT_PORT OPT_F OPT_Q OPT_SCH OPT_NUM OPT_NO)
+
+static int proc_write_directpath(struct file *file, const char *buf,
+				 size_t count, loff_t *ppos)
+{
+	char str[150];
+	int len, rlen;
+	char *ifname = NULL;
+	dp_cb_t cb = {
+		0
+	};
+	char *param_list[10] = {
+		NULL
+	};
+	int param_list_num = 0;
+	struct loop_eth_priv_data *priv = NULL;
+	int i, k;
+	s32 dp_port_id = 0;
+	u32 dev_port = 0;
+	int flag_index = 0;
+	char *flag_str = NULL;
+	char *dev_port_str = NULL;
+	char *dp_port_str = NULL;
+	struct pmac_port_info *port_info = NULL;
+
+	len = count < sizeof(str) ? count : sizeof(str) - 1;
+	rlen = len - copy_from_user(str, buf, len);
+	str[rlen] = 0;
+	param_list_num =
+	    dp_split_buffer(str, param_list, ARRAY_SIZE(param_list));
+
+	if (param_list_num < 3)
+		goto help;
+
+	ifname = param_list[2];
+	/*device must ready before register to Datapath */
+	i = find_loopeth_index_via_name(ifname);
+	if (i < 0) {
+		PR_ERR("Not found device %s in loopeth\n", ifname);
+		goto exit;
+	}
+
+	priv = netdev_priv(g_loop_eth_dev[i]);
+
+	if (((dp_strncmpi(param_list[0], "register", strlen("register")) == 0) ||
+	     (dp_strncmpi(param_list[0], "reg", strlen("reg")) == 0)) &&
+	    (dp_strncmpi(param_list[1], "dev", strlen("dev")) == 0)) {
+		PR_INFO("Not finished yet, need to continue here\n");
+		return count;
+#if 0
+		if ((c = dp_getopt(param_list, num, &start, &opt_arg, optstring)) > 0)
+				printf("c=%c opt_arg=%s next_offset=%d\n", c, opt_arg, start);
+			else if (c == 0)
+				break;
+			else {
+				printf("wrong format %s\n", opt[start]);
+				break;
+			}
+		PR_INFO("Try to register dev %s to datapath\n", ifname);
+		dev_port_str = param_list[3];
+		flag_str = param_list[4];
+		dp_port_str = param_list[5];
+		PR_INFO("dev_port_str=%s\n",
+			dev_port_str ? dev_port_str : "NULL");
+		PR_INFO("flag_str=%s\n", flag_str ? flag_str : "NULL");
+		PR_INFO("dp_port_str=%s\n",
+			dp_port_str ? dp_port_str : "NULL");
+#endif
+		dev_port = dp_atoi(dev_port_str);
+		flag_index = get_dev_type_index(flag_str);
+		dp_port_id = dp_atoi(dp_port_str);
+
+		if (flag_index <= 0) {
+			PR_INFO("Not valid device type:%s(%d)\n", flag_str,
+				flag_index);
+			goto help;
+		}
+
+		priv->owner = &g_loop_eth_module[i];
+		sprintf(priv->owner->name, "module%02d", i);
+		dp_port_id =
+		    dp_alloc_port(priv->owner, g_loop_eth_dev[i], dev_port,
+				  dp_port_id, NULL, 1 << flag_index);
+		g_loop_eth_dev_flag[i] = 1 << flag_index;
+
+		if (dp_port_id <= 0) {
+			PR_INFO("failed in register directpath for %s\n",
+				ifname);
+			goto exit;
+		}
+
+		PR_INFO("dp_alloc_port get port %d for %s\n", dp_port_id,
+			ifname);
+		cb.stop_fn = (dp_stop_tx_fn_t)dp_fp_stop_tx;
+		cb.restart_fn = (dp_restart_tx_fn_t)dp_fp_restart_tx;
+		cb.rx_fn = (dp_rx_fn_t)dp_fp_rx;
+		priv->dp_subif.port_id = dp_port_id;
+		priv->dev_port = dev_port;
+
+		if (dp_register_dev(priv->owner, dp_port_id, &cb, 0) !=
+		    DP_SUCCESS) {
+			PR_INFO
+			    ("dp_register_dev failed for %s\n and port_id %d",
+			     ifname, dp_port_id);
+			dp_alloc_port(priv->owner, g_loop_eth_dev[i],
+				      dev_port, 0, NULL, DP_F_DEREGISTER);
+			goto exit;
+		}
+
+		priv->f_dp = LOOPETH_F_REGISTER_DEV;
+		PR_INFO("Ok register dev %s dev_port %d with dp_port %d\n",
+			ifname, dev_port, dp_port_id);
+	} else if (((dp_strncmpi(param_list[0], "register", strlen("register")) == 0) ||
+		   (dp_strncmpi(param_list[0], "reg", strlen("reg")) == 0)) &&
+		   (dp_strncmpi(param_list[1], "subif", strlen("subif")) == 0)) {
+		if (param_list_num < 3)
+			goto help;
+
+		PR_INFO("Try to register subif %s to datapath\n", ifname);
+		if (priv->f_dp == LOOPETH_F_REGISTER_DEV) {
+			/*already alloc a port and registered dev */
+			if (param_list[3])
+				priv->dp_subif.subif = dp_atoi(param_list[3]);
+			else
+				priv->dp_subif.subif = -1;/*dynamic */
+
+			if (dp_register_subif
+			    (priv->owner, g_loop_eth_dev[i],
+			     g_loop_eth_dev[i]->name, &priv->dp_subif,
+			     0) != DP_SUCCESS)
+				goto exit;
+
+			priv->f_dp = LOOPETH_F_REGISTER_SUBIF;
+			PR_INFO("%s:%s=%s %s=%d: %s=%d %s=%d flag=%d\n",
+				"Register OK",
+				"dev", ifname,
+				"dev_port", priv->dev_port,
+				"ep", priv->dp_subif.port_id,
+				"subif", priv->dp_subif.subif, priv->f_dp);
+		} else if (priv->f_dp == LOOPETH_F_FREE) {
+			char *parent_dev_name = param_list[3];
+			struct net_device *parent_dev;
+
+			parent_dev =
+			    dev_get_by_name(&init_net, parent_dev_name);
+
+			if (!parent_dev) {
+				PR_ERR("Not found device %s\n",
+				       parent_dev_name);
+				goto exit;
+			}
+
+			port_info = get_port_info_via_dp_name(parent_dev);
+			if (!port_info) {
+				PR_INFO("No such registered device %s yet\n",
+					parent_dev_name);
+				goto exit;
+			}
+			PR_ERR("Parent portid=%d\n", port_info->port_id);
+
+			priv->dp_subif.subif = -1; /*dynamic */
+			priv->dp_subif.port_id = port_info->port_id;
+			priv->dev_port = port_info->dev_port;
+			priv->owner = port_info->owner;
+
+			if (dp_register_subif(priv->owner,
+					      g_loop_eth_dev[i],
+					      g_loop_eth_dev[i]->name,
+					      &priv->dp_subif,
+					      0) != DP_SUCCESS) {
+				PR_INFO("%s:%s to register under %s\n",
+					"dp_register_subif fail",
+					g_loop_eth_dev[i]->name,
+					parent_dev_name);
+				goto exit;
+			}
+
+			priv->f_dp = LOOPETH_F_REGISTER_SUBIF;
+			PR_INFO("%s: %s=%s dev port %d:dp_port %d subif %d\n",
+				"Ok to register",
+				"dev", ifname,
+				priv->dev_port, priv->dp_subif.port_id,
+				priv->dp_subif.subif);
+		} else if (priv->f_dp == LOOPETH_F_REGISTER_SUBIF) {
+			PR_INFO("Subif %s already registered\n", ifname);
+			goto exit;
+		} else {
+			PR_INFO("Failed for uknown reason:%d\n", priv->f_dp);
+			goto exit;
+		}
+	} else if (((dp_strncmpi(param_list[0], "unregister", strlen("unregister")) == 0) ||
+		 (dp_strncmpi(param_list[0], "unreg", strlen("unreg")) == 0)) &&
+		(dp_strncmpi(param_list[1], "dev", strlen("dev")) == 0)) {
+		PR_INFO("Try to register dev %s from datapath\n", ifname);
+		priv = netdev_priv(g_loop_eth_dev[i]);
+		dp_port_id = priv->dp_subif.port_id;
+
+		for (k = 0; k < ARRAY_SIZE(g_loop_eth_dev); k++) {
+			if (g_loop_eth_dev[k]) {
+				/*unregister all devices with same port_id */
+				priv = netdev_priv(g_loop_eth_dev[k]);
+
+				if (priv->dp_subif.port_id != dp_port_id)
+					continue;
+
+				unregister_dev(k);
+			}
+		}
+	} else {
+		PR_INFO("Wrong command: %s %s\n", param_list[0],
+			param_list[1]);
+		goto help;
+	}
+
+exit:
+	return count;
+help:
+	/*   param_list[0]    [1]    [2]         [3]      [4]     [5]   */
+	PR_INFO("echo register dev <name> <dev_port> <type>  %s > %s\n",
+		OPT_ALLOC_PORT, PROC_DIRECTPATH);
+	PR_INFO("echo unregister dev  <dev_name>  > %s\n",
+		PROC_DIRECTPATH);
+	PR_INFO("echo register subif <dev> -p [parent_dev_name] -i <idx> %s\n",
+		PROC_DIRECTPATH);
+	PR_INFO("Note: parent_dev_name is for register different subif\n");
+	PR_INFO("Device Type:\n");
+
+	for (i = 1; i < get_dp_port_type_str_size(); i++) /*skip i 0 */
+		PR_INFO("\t%s\n", dp_port_type_str[i]);
+
+	PR_INFO("----16 subifs test script-----\n");
+	PR_INFO("\n");
+	PR_INFO(" echo add 0 > %s/dev\n", PROC_DIR);
+	PR_INFO("echo register dev   loopeth0 1 DIRECTPATH > %s\n",
+		PROC_DIRECTPATH);
+	PR_INFO("echo register subif loopeth0 > %s\n", PROC_DIRECTPATH);
+	PR_INFO("\n");
+	PR_INFO("for i in 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\n");
+	PR_INFO("  do\n");
+	PR_INFO("    echo add $i > %s/dev\n", PROC_DIR);
+	PR_INFO("    echo register subif loopeth${i} loopeth0 >%s\n",
+		PROC_DIRECTPATH);
+	PR_INFO("  done\n");
+
+	return count;
+}
+
+static struct dp_proc_entry proc_entries[] = {
+	/*name single_callback_t multi_callback_t multi_callback_start
+	 *write_callback_t
+	 */
+#if defined(ENABLE_DBG_PROC) && ENABLE_DBG_PROC
+	{PROC_DBG, proc_read_dbg, NULL, NULL, proc_write_dbg},
+#endif
+	{PROC_DEV, NULL, proc_read_dev, NULL, proc_write_dev},
+	{PROC_MIB, NULL, proc_read_mib, NULL, proc_write_mib},
+	{PROC_DP, NULL, proc_read_dp, NULL, proc_write_directpath},
+
+	/*last one for place holder */
+	{NULL, NULL, NULL, NULL, NULL}
+};
+
+static struct dentry *proc_node;
+
+static struct dentry *proc_file_create(struct dentry
+					       *parent)
+{
+	proc_node = debugfs_create_dir(PROC_NAME, parent);
+	if (proc_node) {
+		int i;
+
+		for (i = 0; i < ARRAY_SIZE(proc_entries); i++)
+			dp_proc_entry_create(proc_node, &proc_entries[i]);
+	} else {
+		PR_ERR("datapath loopeth cannot create proc entry");
+		return NULL;
+	}
+
+	return proc_node;
+}
+
+/*####################################
+ *           Init/Cleanup API
+ * ####################################
+ */
+int dp_loop_eth_dev_init(struct dentry *parent)
+{
+	memset(g_loop_eth_dev, 0, sizeof(g_loop_eth_dev));
+	memset(g_loop_eth_module, 0, sizeof(g_loop_eth_module));
+	proc_file_create(parent);
+	PR_INFO("dp_loop_eth_dev_init Succeeded!\n");
+	return 0;
+}
+
+void dp_loop_eth_dev_exit(void)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(g_loop_eth_dev); i++)
+		if (g_loop_eth_dev[i]) {
+			delete_loopeth_dev(i);
+			free_netdev(g_loop_eth_dev[i]);
+		}
+}
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_misc.c b/drivers/net/ethernet/lantiq/datapath/datapath_misc.c
new file mode 100644
index 000000000000..d5a328c9a305
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/datapath_misc.c
@@ -0,0 +1,1123 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include<linux/init.h>
+#include<linux/module.h>
+#include <linux/kernel.h>
+#include <linux/kallsyms.h>
+#include <linux/types.h>
+#include <linux/version.h>
+#include <linux/if_ether.h>
+#include <linux/ethtool.h>
+#include <linux/proc_fs.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/clk.h>
+#include <linux/if_ether.h>
+#include <linux/if_vlan.h>
+#include <linux/clk.h>
+#include <linux/ip.h>
+#include <net/ip.h>
+#include <lantiq_soc.h>
+#include <net/datapath_api.h>
+#include "datapath.h"
+#include <net/lantiq_cbm_api.h>
+#if IS_ENABLED(CONFIG_LTQ_PPA_API_SW_FASTPATH)
+#include <net/ppa_api.h>
+#endif
+
+#if defined(CONFIG_LTQ_HWMCPY) && CONFIG_LTQ_HWMCPY
+/*#define dp_memcpy(x, y, z)
+ * ltq_hwmemcpy(x, y, z, 0, MCPY_IOCU_TO_IOCU, HWMCPY_F_PRIO_HIGH)
+ */
+#define dp_memcpy(x, y, z)   memcpy(x, y, z)
+#else
+#define dp_memcpy(x, y, z)   memcpy(x, y, z)
+#endif
+
+char *parser_flags_str[] = {
+	"PARSER_FLAGS_NO",
+	"PARSER_FLAGS_END",
+	"PARSER_FLAGS_CAPWAP",
+	"PARSER_FLAGS_GRE",
+	"PARSER_FLAGS_LEN",
+	"PARSER_FLAGS_GREK",
+	"PARSER_FLAGS_NN1",
+	"PARSER_FLAGS_NN2",
+
+	"PARSER_FLAGS_ITAG",
+	"PARSER_FLAGS_1VLAN",
+	"PARSER_FLAGS_2VLAN",
+	"PARSER_FLAGS_3VLAN",
+	"PARSER_FLAGS_4VLAN",
+	"PARSER_FLAGS_SNAP",
+	"PARSER_FLAGS_PPPOES",
+	"PARSER_FLAGS_1IPV4",
+
+	"PARSER_FLAGS_1IPV6",
+	"PARSER_FLAGS_2IPV4",
+	"PARSER_FLAGS_2IPV6",
+	"PARSER_FLAGS_ROUTEXP",
+	"PARSER_FLAGS_TCP",
+	"PARSER_FLAGS_1UDP",
+	"PARSER_FLAGS_IGMP",
+	"PARSER_FLAGS_IPV4OPT",
+
+	"PARSER_FLAGS_IPV6EXT",
+	"PARSER_FLAGS_TCPACK",
+	"PARSER_FLAGS_IPFRAG",
+	"PARSER_FLAGS_EAPOL",
+	"PARSER_FLAGS_2IPV6EXT",
+	"PARSER_FLAGS_2UDP",
+	"PARSER_FLAGS_L2TPNEXP",
+	"PARSER_FLAGS_LROEXP",
+
+	"PARSER_FLAGS_L2TP",
+	"PARSER_FLAGS_GRE_VLAN1",
+	"PARSER_FLAGS_GRE_VLAN2",
+	"PARSER_FLAGS_GRE_PPPOE",
+	"PARSER_FLAGS_BYTE4_BIT4_UNDEF",
+	"PARSER_FLAGS_BYTE4_BIT5_UNDEF",
+	"PARSER_FLAGS_BYTE4_BIT6_UNDEF",
+	"PARSER_FLAGS_BYTE4_BIT7_UNDEF",
+
+	"PARSER_FLAGS_BYTE5_BIT0_UNDEF",
+	"PARSER_FLAGS_BYTE5_BIT1_UNDEF",
+	"PARSER_FLAGS_BYTE5_BIT2_UNDEF",
+	"PARSER_FLAGS_BYTE5_BIT3_UNDEF",
+	"PARSER_FLAGS_BYTE5_BIT4_UNDEF",
+	"PARSER_FLAGS_BYTE5_BIT5_UNDEF",
+	"PARSER_FLAGS_BYTE5_BIT6_UNDEF",
+	"PARSER_FLAGS_BYTE5_BIT7_UNDEF",
+
+	"PARSER_FLAGS_BYTE6_BIT0_UNDEF",
+	"PARSER_FLAGS_BYTE6_BIT1_UNDEF",
+	"PARSER_FLAGS_BYTE6_BIT2_UNDEF",
+	"PARSER_FLAGS_BYTE6_BIT3_UNDEF",
+	"PARSER_FLAGS_BYTE6_BIT4_UNDEF",
+	"PARSER_FLAGS_BYTE6_BIT5_UNDEF",
+	"PARSER_FLAGS_BYTE6_BIT6_UNDEF",
+	"PARSER_FLAGS_BYTE6_BIT7_UNDEF",
+
+	"PARSER_FLAGS_BYTE7_BIT0_UNDEF",
+	"PARSER_FLAGS_BYTE7_BIT1_UNDEF",
+	"PARSER_FLAGS_BYTE7_BIT2_UNDEF",
+	"PARSER_FLAGS_BYTE7_BIT3_UNDEF",
+	"PARSER_FLAGS_BYTE7_BIT4_UNDEF",
+	"PARSER_FLAGS_BYTE7_BIT5_UNDEF",
+	"PARSER_FLAGS_BYTE7_BIT6_UNDEF",
+	"PARSER_FLAGS_BYTE7_BIT7_UNDEF",
+
+	/*Must be put at the end of the enum */
+	"PARSER_FLAGS_MAX"
+};
+
+void dump_parser_flag(char *buf)
+{
+	int i, j;
+	unsigned char *pflags = buf + PKT_PMAC_OFFSET - 1;
+	unsigned char *poffset = buf;
+	char *p;
+	int len;
+
+	if (!buf) {
+		PR_ERR("dump_parser_flag buf NULL\n");
+		return;
+	}
+	p = kmalloc(2000, GFP_KERNEL);
+	if (!p) {
+		PR_ERR("kmalloc NULL\n");
+		return;
+	}
+
+	/* one TCP example: offset
+	 * offset 0
+	 *  00 3a 00 00 00 00 00 00 00 00 00 00 00 00 00 0e
+	 * 00 00 00 16 22 00 00 00 00 00 00 00 00 00 00 2e
+	 * 00 00 00 00 00 00 00 00
+	 * flags: FLAG_L2TPFLAG_NO
+	 * 00 00 00 00 80 18 80 00
+	 */
+	PR_INFO("paser flag at 0x%p: ", buf);
+	len = 0;
+	for (i = 0; i < 8; i++)
+		len += sprintf(p + len, "%02x ", *(pflags - 7 + i));
+	PR_INFO("%s\n", p);
+#if 1
+	PR_INFO("paser flag: ");
+	len = 0;
+	for (i = 0; i < 8; i++)
+		len += sprintf(p + len, "%02x ", *(pflags - i));
+	PR_INFO("%s(reverse)\n", p);
+#endif
+
+	for (i = 0; i < PASAR_FLAGS_NUM; i++) {	/*8 flags per byte */
+		for (j = 0; j < 8; j++) {	/*8 bits per byte */
+			if ((i * 8 + j) >= PASER_FLAGS_MAX)
+				break;
+
+			if ((*(pflags - i)) & (1 << j)) {	/*flag is set */
+				if ((i * 8 + j) < PASAR_OFFSETS_NUM)
+					PR_INFO("  Flag %02d offset=%02d: %s\n",
+						i * 8 + j,
+						*(poffset + i * 8 + j),
+						parser_flags_str[i * 8 + j]);
+				else
+					PR_INFO("  Flag %02d %s (No offset)\n",
+						i * 8 + j,
+						parser_flags_str[i * 8 + j]);
+			}
+		}
+	}
+	kfree(p);
+}
+
+/*will be used at any context */
+void dp_dump_raw_data(char *buf, int len, char *prefix_str)
+{
+	int i, j, l;
+	int line_num = 32;
+	unsigned char *p = (unsigned char *)buf;
+	int bytes = line_num * 8 + 100;
+	char *s;
+
+	if (!p) {
+		PR_ERR("dp_dump_raw_data: p NULL ?\n");
+		return;
+	}
+	s = kmalloc(bytes, GFP_KERNEL);
+	if (!s) {
+		PR_ERR("kmalloc failed: %d\n", bytes);
+		return;
+	}
+	sprintf(s, "%s in hex at 0x%p\n",
+		prefix_str ? (char *)prefix_str : "Data", p);
+	PR_INFO("%s", s);
+
+	for (i = 0; i < len; i += line_num) {
+		l = sprintf(s, " %06d: ", i);
+		for (j = 0; (j < line_num) && (i + j < len); j++)
+			l += sprintf(s + l, "%02x ", p[i + j]);
+		sprintf(s + l, "\n");
+		PR_INFO("%s", s);
+	}
+	kfree(s);
+}
+EXPORT_SYMBOL(dp_dump_raw_data);
+
+#define PROTOCOL_IPIP 4
+#define PROTOCOL_TCP 6
+#define PROTOCOL_UDP 17
+#define PROTOCOL_ENCAPSULATED_IPV6 41
+#define PROTOCOL_ROUTING 43
+#define PROTOCOL_NONE 59
+#define PROTOCOL_IPV6_FRAGMENT 44
+
+#define TWO_MAC_SIZE 12
+#define VLAN_HDR_SIZE  4
+#define PPPOE_HDR_SIZE  8
+#define IPV6_HDR_SIZE  40
+#define IPV6_EXTENSION_SIZE 8
+
+#define IP_CHKSUM_OFFSET_IPV4 10
+#define UDP_CHKSUM_OFFSET 6
+#define TCP_CHKSUM_OFFSET 16
+/*Workaround: Currently need to includes PMAC
+ *although spec said it starts from mac address. ?
+ */
+struct ip_hdr_info {
+	u8 ip_ver;
+	u8 proto;		/*udp/tcp */
+	u16 ip_offset;		/*this offset is based on L2 MAC header */
+	u16 udp_tcp_offset;	/*this offset is based on ip header */
+	u16 next_ip_hdr_offset;	/*0 - means no next valid ip header.*/
+				/* Based on current IP header */
+	u8 is_fragment;		/*0 means non fragmented packet */
+};
+
+/*input p: pointers to ip header
+ * output info:
+ * return: 0:  it is UDP/TCP packet
+ * -1: not UDP/TCP
+ */
+#define DP_IP_VER4 4
+#define DP_IP_VER6 6
+int get_ip_hdr_info(u8 *pdata, int len, struct ip_hdr_info *info)
+{
+	int ip_hdr_size;
+	u8 *p = pdata;
+	struct iphdr *iphdr = (struct iphdr *)pdata;
+
+	memset((void *)info, 0, sizeof(*info));
+	info->ip_ver = p[0] >> 4;
+
+	if (info->ip_ver == DP_IP_VER4) {	/*ipv4 */
+		ip_hdr_size = (p[0] & 0xf) << 2;
+#ifdef CONFIG_LTQ_DATAPATH_DBG_PROTOCOL_PARSE
+		DP_DEBUG(DP_DBG_FLAG_DUMP_TX,
+			 "IPV4 pkt with protocol 0x%x with ip hdr size %d\n",
+			 p[9], ip_hdr_size);
+#endif
+		info->proto = p[9];
+
+		if ((info->proto == PROTOCOL_UDP) ||
+		    (info->proto == PROTOCOL_TCP)) {
+			if ((iphdr->frag_off & IP_MF) ||
+			    (iphdr->frag_off & IP_OFFSET)) {
+#ifdef CONFIG_LTQ_DATAPATH_DBG_PROTOCOL_PARSE
+				DP_DEBUG(DP_DBG_FLAG_DUMP_TX,
+					 "frag pkt:off=%x,IP_MF=%x,IP_OFFSET=%x\n",
+					 iphdr->frag_off, IP_MF, IP_OFFSET);
+#endif
+				info->udp_tcp_offset = (p[0] & 0x0f) << 2;
+				info->is_fragment = 1;
+				return -1;
+			}
+#ifdef CONFIG_LTQ_DATAPATH_DBG_PROTOCOL_PARSE
+			DP_DEBUG(DP_DBG_FLAG_DUMP_TX,
+				 "%s packet with src/dst port:%u/%u\n",
+				 (p[9] ==
+				  PROTOCOL_UDP) ? "UDP" : "TCP",
+				 *(unsigned short *)(pdata +
+						     ip_hdr_size),
+				 *(unsigned short *)(pdata +
+						     ip_hdr_size +
+						     2));
+#endif
+			info->udp_tcp_offset = (p[0] & 0x0f) << 2;
+			return 0;
+		} else if (p[9] == PROTOCOL_ENCAPSULATED_IPV6) {
+			/*6RD */
+			info->next_ip_hdr_offset = (p[0] & 0x0f) << 2;
+			return 0;
+		}
+#ifdef CONFIG_LTQ_DATAPATH_DBG_PROTOCOL_PARSE
+		DP_DEBUG(DP_DBG_FLAG_DUMP_TX,
+			 "Not supported extension hdr:0x%x\n", p[9]);
+#endif
+		return -1;
+	} else if (info->ip_ver == DP_IP_VER6) {	/*ipv6 */
+		int ip_hdr_size;
+		u8 next_hdr;
+		u8 udp_tcp_h_offset;
+		u8 first_extension = 1;
+
+		ip_hdr_size = IPV6_HDR_SIZE;
+		udp_tcp_h_offset = IPV6_HDR_SIZE;
+		next_hdr = p[6];
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_DBG)
+		if (dp_dbg_flag & DP_DBG_FLAG_DUMP_TX) {
+			int i;
+
+			PR_INFO("IPV6 packet with next hdr:0x%x\n", next_hdr);
+			PR_INFO(" src IP: ");
+			for (i = 0; i < 16; i++)
+				PR_INFO("%02x%s", pdata[8 + i],
+					(i != 15) ? ":" : " ");
+
+			PR_INFO("\n");
+
+			PR_INFO(" Dst IP: ");
+
+			for (i = 0; i < 16; i++)
+				PR_INFO("%02x%s", pdata[24 + i],
+					(i != 15) ? ":" : " ");
+
+			PR_INFO("\n");
+		}
+#endif
+		while (1) {
+			/*Next Header: UDP/TCP */
+			if ((next_hdr == PROTOCOL_UDP) ||
+			    (next_hdr == PROTOCOL_TCP)) {
+				info->proto = next_hdr;
+
+				if (!first_extension)
+					udp_tcp_h_offset +=
+					    IPV6_EXTENSION_SIZE + p[1];
+
+				info->udp_tcp_offset = udp_tcp_h_offset;
+#ifdef CONFIG_LTQ_DATAPATH_DBG_PROTOCOL_PARSE
+				DP_DEBUG(DP_DBG_FLAG_DUMP_TX,
+					 "IP6 UDP:src/dst port:%u/%u udp_tcp_off=%d\n",
+					 *(unsigned short *)(pdata +
+							     udp_tcp_h_offset),
+					 *(unsigned short *)(pdata +
+							     udp_tcp_h_offset
+							     + 2),
+					 udp_tcp_h_offset);
+#endif
+				return 0;
+			} else if (next_hdr == PROTOCOL_IPIP) {	/*dslite */
+				if (!first_extension)
+					udp_tcp_h_offset +=
+					    IPV6_EXTENSION_SIZE + p[1];
+
+				info->next_ip_hdr_offset = udp_tcp_h_offset;
+				return 0;
+			} else if (next_hdr == PROTOCOL_IPV6_FRAGMENT) {
+				pr_info_once("fragmented IPV6 packet !\n");
+				info->is_fragment = 1;
+				return -1;
+			}
+#ifdef CONFIG_LTQ_DATAPATH_DBG_PROTOCOL_PARSE
+			DP_DEBUG(DP_DBG_FLAG_DUMP_TX,
+				 "Skip extension hdr:0x%x\n", next_hdr);
+#endif
+			if ((next_hdr == PROTOCOL_NONE) ||
+			    (next_hdr == PROTOCOL_ENCAPSULATED_IPV6))
+				break;
+
+			if (first_extension) {
+				/*skip ip header */
+				p += IPV6_HDR_SIZE;
+				first_extension = 0;
+			} else {
+				/*TO NEXT */
+				udp_tcp_h_offset +=
+				    IPV6_EXTENSION_SIZE + p[1];
+				p += IPV6_EXTENSION_SIZE + p[1];
+			}
+			next_hdr = p[0];
+			if (udp_tcp_h_offset > len) {
+#ifdef CONFIG_LTQ_DATAPATH_DBG_PROTOCOL_PARSE
+				DP_DEBUG(DP_DBG_FLAG_DUMP_TX,
+					 "\n- Wrong IPV6 packet header ?\n");
+#endif
+				break;
+			}
+		}
+	}
+
+	/*not supported protocol */
+	return -1;
+}
+
+#ifdef CONFIG_LTQ_DATAPATH_MANUAL_PARSE
+int ip_offset_hw_adjust = 8;
+
+/*parse protol and get the ip_offset/tcp_h_offset and its type:
+ * return: 0-found udp/tcp header, -1 - not found  udp/tcp header
+ * Note: skb->data points to pmac header, not L2 MAC header;
+ */
+int get_offset_clear_chksum(struct sk_buff *skb, u32 *ip_offset,
+			    u32 *tcp_h_offset, u32 *tcp_type)
+{
+	u8 *p_l2_mac = skb->data;
+	u8 *p = p_l2_mac + TWO_MAC_SIZE;
+	struct ip_hdr_info pkt_info[2];
+	u8 ip_num = 0;
+#ifdef CONFIG_LTQ_DATAPATH_DBG
+	int i;
+#endif
+	int len;
+
+	if (skb->ip_summed != CHECKSUM_PARTIAL)
+		return -1;
+
+	*ip_offset = 0;
+	*tcp_h_offset = 0;
+
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_DBG)
+	if (dp_dbg_flag)
+		DP_DEBUG(DP_DBG_FLAG_DUMP_TX,
+			 "flags DP_TX_CAL_CHKSUM is set\n");
+#endif
+
+	while ((p[0] == 0x81) && (p[1] == 0x00))	/*skip vlan header */
+		p += VLAN_HDR_SIZE;
+
+	if ((p[0] == 0x88) && (p[1] == 0x64))	/*skip pppoe header */
+		p += PPPOE_HDR_SIZE;
+
+#ifdef CONFIG_LTQ_DATAPATH_DBG_PROTOCOL_PARSE
+	DP_DEBUG(DP_DBG_FLAG_DUMP_TX,
+		 "To find ip header:%02x %02x %02x %02x %02x %02x %02x %02x\n",
+		 p[0], p[1], p[2], p[3], p[4], p[5], p[6], p[7]);
+#endif
+	if (((p[0] != 0x08) || (p[1] != 0x00)) &&
+	    ((p[0] != 0x86) && (p[1] != 0xdd))) {
+#ifdef CONFIG_LTQ_DATAPATH_DBG_PROTOCOL_PARSE
+		DP_DEBUG(DP_DBG_FLAG_DUMP_TX, "None IP type:%02x%02x\n", p[0],
+			 p[1]);
+#endif
+	}
+
+	p += 2; /* jump to ip header */
+	len = skb->len - TWO_MAC_SIZE - 2;
+
+	while (1) {
+		if (get_ip_hdr_info(p, len, &pkt_info[ip_num]) == 0) {
+			pkt_info[ip_num].ip_offset = (u32)p - (u32)p_l2_mac;
+
+			if (pkt_info[ip_num].next_ip_hdr_offset) {
+				p += pkt_info[ip_num].next_ip_hdr_offset;
+				ip_num++;
+
+				if (ip_num >= ARRAY_SIZE(pkt_info))
+					return -1;
+
+				len -= pkt_info[ip_num].next_ip_hdr_offset;
+				continue;
+
+			} else {
+				ip_num++;
+
+				if (ip_num >= ARRAY_SIZE(pkt_info))
+					return -1;
+
+				break;
+			}
+		} else {
+			/*Not UDP/TCP and cannot do checksum calculation */
+			pr_info_once
+			    ("Not UDP/TCP and cannot do checksum cal!\n");
+			return -1;
+		}
+	}
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_DBG)
+	if (dp_dbg_flag & DP_DBG_FLAG_DUMP_TX) {
+		for (i = 0; i < ip_num; i++) {
+			DP_DEBUG(DP_DBG_FLAG_DUMP_TX,
+				 "Parse:ip[%d]:v=%d prot=%d ip_off=%d udp_tcp_off=%d, n_hdr_off=%d\n",
+				 i, pkt_info[i].ip_ver, pkt_info[i].proto,
+				 pkt_info[i].ip_offset,
+				 pkt_info[i].udp_tcp_offset,
+				 pkt_info[i].next_ip_hdr_offset);
+		}
+	}
+#endif
+	if (ip_num == 1) {
+		if (pkt_info[0].ip_ver == DP_IP_VER4) {
+			*ip_offset = pkt_info[0].ip_offset;
+			*tcp_h_offset = pkt_info[0].udp_tcp_offset;
+
+			if (pkt_info[0].proto == PROTOCOL_UDP) {
+				*tcp_type = UDP_OVER_IPV4;
+				/*clear original udp checksum */
+				if (!pkt_info[0].is_fragment)
+					*(uint16_t *)(p_l2_mac + *ip_offset +
+						       *tcp_h_offset +
+						       UDP_CHKSUM_OFFSET) = 0;
+			} else {
+				*tcp_type = TCP_OVER_IPV4;
+				/*clear original TCP checksum */
+				*(uint16_t *)(p_l2_mac + *ip_offset +
+					       *tcp_h_offset +
+					       TCP_CHKSUM_OFFSET) = 0;
+			}
+
+			if (!pkt_info[0].is_fragment) {
+				/*clear original ip4 checksum */
+				*(uint16_t *)(p_l2_mac + *ip_offset +
+					       IP_CHKSUM_OFFSET_IPV4) = 0;
+			} else {
+				return 1;
+			}
+
+			return 0;
+		} else if (pkt_info[0].ip_ver == DP_IP_VER6) {
+			*ip_offset = pkt_info[0].ip_offset;
+			*tcp_h_offset = pkt_info[0].udp_tcp_offset;
+
+			if (pkt_info[0].proto == PROTOCOL_UDP) {
+				*tcp_type = UDP_OVER_IPV6;
+				if (!pkt_info[0].is_fragment)
+					/*clear original udp checksum */
+					*(uint16_t *)(p_l2_mac + *ip_offset +
+						       *tcp_h_offset +
+						       UDP_CHKSUM_OFFSET) = 0;
+			} else {
+				*tcp_type = TCP_OVER_IPV6;
+				/*clear original TCP checksum */
+				if (!pkt_info[0].is_fragment) {
+					*(uint16_t *)(p_l2_mac + *ip_offset +
+						       *tcp_h_offset +
+						       TCP_CHKSUM_OFFSET) = 0;
+				} else {
+					return 1;
+				}
+			}
+
+			return 0;
+		}
+	} else if (ip_num == 2) {
+		/*for tunnels:current for 6rd/dslite only */
+		if ((pkt_info[0].ip_ver == DP_IP_VER4) &&
+		    (pkt_info[1].ip_ver == DP_IP_VER6)) {
+			/*6rd */
+			*ip_offset = pkt_info[0].ip_offset;
+			*tcp_h_offset =
+			    (pkt_info[0].next_ip_hdr_offset +
+			     pkt_info[1].udp_tcp_offset);
+
+			if (pkt_info[1].proto == PROTOCOL_UDP) {
+				*tcp_type = UDP_OVER_IPV6_IPV4;
+				/*clear original udp checksum */
+				if (!pkt_info[0].is_fragment)
+					*(uint16_t *)(p_l2_mac + *ip_offset +
+						       *tcp_h_offset +
+						       UDP_CHKSUM_OFFSET) = 0;
+			} else {
+				*tcp_type = TCP_OVER_IPV6_IPV4;
+				/*clear original udp checksum */
+				*(uint16_t *)(p_l2_mac + *ip_offset +
+					       *tcp_h_offset +
+					       TCP_CHKSUM_OFFSET) = 0;
+			}
+
+			if (!pkt_info[0].is_fragment) {
+				/*clear original ip4 checksum */
+				*(uint16_t *)(p_l2_mac + *ip_offset +
+					       IP_CHKSUM_OFFSET_IPV4) = 0;
+			} else {
+				return 1;
+			}
+
+			return 0;
+
+		} else if ((pkt_info[0].ip_ver == DP_IP_VER6) &&
+			(pkt_info[1].ip_ver == DP_IP_VER4)) {	/*dslite */
+			*ip_offset = pkt_info[0].ip_offset;
+			*tcp_h_offset =
+			    (pkt_info[0].next_ip_hdr_offset +
+			     pkt_info[1].udp_tcp_offset);
+
+			if (pkt_info[1].proto == PROTOCOL_UDP) {
+				*tcp_type = UDP_OVER_IPV4_IPV6;
+				if (!pkt_info[0].is_fragment)
+					/*clear original udp checksum */
+					*(uint16_t *)(p_l2_mac +
+						       pkt_info[1].ip_offset +
+						       *tcp_h_offset +
+						       UDP_CHKSUM_OFFSET) = 0;
+			} else {
+				*tcp_type = TCP_OVER_IPV4_IPV6;
+				/*clear original udp checksum */
+				*(uint16_t *)(p_l2_mac +
+					       pkt_info[1].ip_offset +
+					       pkt_info[1].udp_tcp_offset +
+					       TCP_CHKSUM_OFFSET) = 0;
+			}
+
+			if (!pkt_info[0].is_fragment) {
+				/*clear original ip4 checksum */
+				*(uint16_t *)(p_l2_mac +
+					       pkt_info[1].ip_offset +
+					       IP_CHKSUM_OFFSET_IPV4) = 0;
+			} else {
+				return 1;
+			}
+
+			return 0;
+		}
+	}
+
+	return -1;
+}
+#else	/* CONFIG_LTQ_DATAPATH_MANUAL_PARSE */
+/*parse protol and get the ip_offset/tcp_h_offset and its type
+ * based on skb_inner_network_header/skb_network_header/
+ *           skb_inner_transport_header/skb_transport_header
+ * return: 0-found udp/tcp header, -1 - not found  udp/tcp header
+ *  Note: skb->data points to pmac header, not L2 MAC header;
+ */
+int ip_offset_hw_adjust;
+
+int get_offset_clear_chksum(struct sk_buff *skb, u32 *ip_offset,
+			    u32 *tcp_h_offset, u32 *tcp_type)
+{
+	struct iphdr *iph;
+	struct tcphdr *tcph;
+	struct udphdr *udph;
+	unsigned char *l4_p;
+
+	if (skb->ip_summed != CHECKSUM_PARTIAL) {
+#ifdef CONFIG_LTQ_DATAPATH_DBG_PROTOCOL_PARSE
+		DP_DEBUG(DP_DBG_FLAG_DUMP_TX_SUM,
+			 "No need HW checksum Support\n");
+#endif
+		return -1;
+	}
+
+	if (skb->encapsulation) {
+		iph = (struct iphdr *)skb_inner_network_header(skb);
+		*ip_offset =
+		    (uint32_t)(skb_inner_network_header(skb) - skb->data);
+		*tcp_h_offset =
+		    (uint32_t)(skb_inner_transport_header(skb) -
+				skb_inner_network_header(skb));
+		l4_p = skb_inner_transport_header(skb);
+	} else {
+		iph = (struct iphdr *)skb_network_header(skb);
+		*ip_offset = (uint32_t)(skb_network_header(skb) - skb->data);
+		*tcp_h_offset =
+		    (uint32_t)(skb_transport_header(skb) -
+				skb_network_header(skb));
+		l4_p = skb_transport_header(skb);
+	}
+	if (((int)(ip_offset) <= 0) || ((int)(tcp_h_offset) <= 0)) {
+#ifdef CONFIG_LTQ_DATAPATH_DBG_PROTOCOL_PARSE
+		DP_DEBUG(DP_DBG_FLAG_DUMP_TX_SUM,
+			 "Wrong IP offset(%d) or TCP/UDP offset(%d)\n",
+			 ((int)(ip_offset) <= 0), ((int)(tcp_h_offset) <= 0));
+#endif
+		return -1;
+	}
+
+	if (iph->protocol == IPPROTO_UDP) {
+		if (iph->version == DP_IP_VER4) {
+			*tcp_type = UDP_OVER_IPV4;
+			iph->check = 0;	/*clear original ip checksum */
+		} else if (iph->version == DP_IP_VER6) {
+			*tcp_type = UDP_OVER_IPV6;
+		} else { /*wrong ver*/
+			return -1;
+		}
+		udph = (struct udphdr *)l4_p;
+		udph->check = 0; /*clear original UDP checksum */
+	} else if (iph->protocol == IPPROTO_TCP) {
+		if (iph->version == DP_IP_VER4) {
+			*tcp_type = TCP_OVER_IPV4;
+			iph->check = 0;	/*clear original ip checksum */
+		} else if (iph->version == DP_IP_VER6) {
+			*tcp_type = TCP_OVER_IPV6;
+		} else {
+			return -1;
+		}
+		tcph = (struct tcphdr *)l4_p;
+		tcph->check = 0;	/*clear original UDP checksum */
+	}
+#ifdef CONFIG_LTQ_DATAPATH_DBG_PROTOCOL_PARSE
+	DP_DEBUG(DP_DBG_FLAG_DUMP_TX_SUM, "Found tcp_type=%u ip_offset=%u\n",
+		 *tcp_type, *ip_offset);
+#endif
+	return 0;
+}
+#endif				/* CONFIG_LTQ_DATAPATH_MANUAL_PARSE */
+
+/*  Make a copy of both an &sk_buff and part of its data, located
+ * in header. Fragmented data remain shared. This is used since
+ * datapath need to modify only header of &sk_buff and needs
+ * private copy of the header to alter.
+ *  Returns NULL on failure, or the pointer to the buffer on success
+ *  Note, this API will used in dp_xmit when there is no enough room
+ *        to insert pmac header or the packet is cloned but we need
+ *        to insert pmac header or reset udp/tcp checksum
+ *  This logic is mainly copied from API __pskb_copy(...)
+ */
+struct sk_buff *dp_create_new_skb(struct sk_buff *skb)
+{
+	struct sk_buff *new_skb;
+#ifndef CONFIG_LTQ_DATAPATH_COPY_LINEAR_BUF_ONLY
+	/* seems CBM driver does not support it yet */
+	void *p;
+	const skb_frag_t *frag;
+	int len;
+#else
+	int linear_len;
+#endif
+	int i;
+
+	if (unlikely(skb->data_len >= skb->len)) {
+		PR_ERR("why skb->data_len(%d) >= skb->len(%d)\n",
+		       skb->data_len, skb->len);
+		dev_kfree_skb_any(skb);
+		return NULL;
+	}
+
+	if (skb_shinfo(skb)->frag_list) {
+		PR_ERR("DP Not support skb_shinfo(skb)->frag_list yet !!\n");
+		dev_kfree_skb_any(skb);
+		return NULL;
+	}
+#ifndef CONFIG_LTQ_DATAPATH_COPY_LINEAR_BUF_ONLY
+	new_skb = cbm_alloc_skb(skb->len + 8, GFP_ATOMIC);
+#else
+	linear_len = skb->len - skb->data_len;
+	/*cbm_alloc_skb will reserve enough header room */
+	new_skb = cbm_alloc_skb(linear_len, GFP_ATOMIC);
+#endif
+
+	if (unlikely(!new_skb)) {
+		DP_DEBUG(DP_DBG_FLAG_DUMP_TX, "allocate cbm buffer fail\n");
+		dev_kfree_skb_any(skb);
+		return NULL;
+	}
+#ifndef CONFIG_LTQ_DATAPATH_COPY_LINEAR_BUF_ONLY
+	p = new_skb->data;
+	dp_memcpy(p, skb->data, skb->len - skb->data_len);
+	p += skb->len - skb->data_len;
+
+	if (skb->data_len) {
+		for (i = 0; i < (skb_shinfo(skb)->nr_frags); i++) {
+			frag = &skb_shinfo(skb)->frags[i];
+			len = skb_frag_size(frag);
+			dp_memcpy(p, skb_frag_address(frag), len);
+			p += len;
+		}
+	}
+	skb_put(new_skb, skb->len);
+#else
+	/* Copy the linear data part only */
+	memcpy(new_skb->data, skb->data, linear_len);
+	skb_put(new_skb, linear_len);
+
+	/*Share the Fragmented data */
+	if (skb_shinfo(skb)->nr_frags) {
+		for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {
+			skb_shinfo(new_skb)->frags[i] =
+			    skb_shinfo(skb)->frags[i];
+			skb_frag_ref(skb, i);	/*increase counter */
+		}
+		skb_shinfo(new_skb)->nr_frags = i;
+		skb_shinfo(new_skb)->gso_size = skb_shinfo(skb)->gso_size;
+		skb_shinfo(new_skb)->gso_segs = skb_shinfo(skb)->gso_segs;
+		skb_shinfo(new_skb)->gso_type = skb_shinfo(skb)->gso_type;
+		new_skb->data_len = skb->data_len;
+		new_skb->len += skb->data_len;
+	}
+#endif
+	new_skb->dev = skb->dev;
+	new_skb->priority = skb->priority;
+	new_skb->truesize += skb->data_len;
+	new_skb->DW0 = skb->DW0;
+	new_skb->DW1 = skb->DW1;
+	new_skb->DW2 = skb->DW2;
+	new_skb->DW3 = skb->DW3;
+
+	/*copy other necessary fields for checksum calculation case */
+	new_skb->ip_summed = skb->ip_summed;
+	new_skb->encapsulation = skb->encapsulation;
+	new_skb->inner_mac_header = skb->inner_mac_header;
+	new_skb->protocol = skb->protocol;
+
+	if (skb->encapsulation) {
+		skb_reset_inner_network_header(new_skb);
+		skb_set_inner_network_header(new_skb,
+					     skb_inner_network_offset(skb));
+		skb_reset_transport_header(new_skb);
+		skb_set_inner_transport_header(new_skb,
+					       skb_inner_transport_offset
+					       (skb));
+	} else {
+		skb_reset_network_header(new_skb);
+		skb_set_network_header(new_skb, skb_network_offset(skb));
+		skb_reset_transport_header(new_skb);
+		skb_set_transport_header(new_skb, skb_transport_offset(skb));
+	}
+
+	/*DP_DEBUG(DP_DBG_FLAG_DUMP_TX, "alloc new buffer succeed\n");*/
+	/*free old skb */
+	dev_kfree_skb_any(skb);
+	return new_skb;
+}
+
+char *dp_skb_csum_str(struct sk_buff *skb)
+{
+	if (!skb)
+		return "NULL";
+	if (skb->ip_summed == CHECKSUM_PARTIAL)
+		return "HW Checksum";
+	if (skb->ip_summed == CHECKSUM_NONE)
+		return "SW Checksum";
+	return "Unknown";
+}
+
+/* int
+ * inet_pton(af, src, dst)
+ *	convert from presentation format (which usually means ASCII printable)
+ *	to network format (which is usually some kind of binary format).
+ * return:
+ *	4 if the address was valid and it is IPV4 format
+ *  6 if the address was valid and it is IPV6 format
+ *	0 if some other error occurred (`dst' is untouched in this case, too)
+ * author:
+ *	Paul Vixie, 1996.
+ */
+int pton(const char *src, void *dst)
+{
+	int ip_v = 0;
+
+	if (strstr(src, ":")) {	/* IPV6 */
+		if (inet_pton6(src, dst) == 1) {
+			ip_v = 6;
+			return ip_v;
+		}
+
+	} else {
+		if (inet_pton4(src, dst) == 1) {
+			ip_v = 4;
+			return ip_v;
+		}
+	}
+
+	return ip_v;
+}
+
+/* int
+ * inet_pton4(src, dst)
+ *	like inet_aton() but without all the hexadecimal and shorthand.
+ * return:
+ *	1 if `src' is a valid dotted quad, else 0.
+ * notice:
+ *	does not touch `dst' unless it's returning 1.
+ * author:
+ *	Paul Vixie, 1996.
+ */
+int inet_pton4(const char *src, u_char *dst)
+{
+	const char digits[] = "0123456789";
+	int saw_digit, octets, ch;
+	u_char tmp[NS_INADDRSZ], *tp;
+
+	saw_digit = 0;
+	octets = 0;
+	*(tp = tmp) = 0;
+	while ((ch = *src++) != '\0') {
+		const char *pch;
+
+		pch = strchr(digits, ch);
+		if (pch) {
+			u_int new = *tp * 10 + (u_int)(pch - digits);
+
+			if (new > 255)
+				return 0;
+			*tp = (u_char)new;
+			if (!saw_digit) {
+				if (++octets > 4)
+					return 0;
+				saw_digit = 1;
+			}
+		} else if (ch == '.' && saw_digit) {
+			if (octets == 4)
+				return 0;
+			*++tp = 0;
+			saw_digit = 0;
+		} else {
+			return 0;
+		}
+	}
+	if (octets < 4)
+		return 0;
+
+	memcpy(dst, tmp, NS_INADDRSZ);
+	return 1;
+}
+
+/* int
+ * inet_pton6(src, dst)
+ *	convert presentation level address to network order binary form.
+ * return:
+ *	1 if `src' is a valid [RFC1884 2.2] address, else 0.
+ * notice:
+ *	(1) does not touch `dst' unless it's returning 1.
+ *	(2) :: in a full address is silently ignored.
+ * credit:
+ *	inspired by Mark Andrews.
+ * author:
+ *	Paul Vixie, 1996.
+ */
+int inet_pton6(const char *src, u_char *dst)
+{
+	const char xdigits_l[] = "0123456789abcdef", xdigits_u[] =
+	    "0123456789ABCDEF";
+	u_char tmp[NS_IN6ADDRSZ], *tp, *endp, *colonp;
+	const char *xdigits, *curtok;
+	int ch, saw_xdigit;
+	u_int val;
+
+	memset((tp = tmp), '\0', NS_IN6ADDRSZ);
+	endp = tp + NS_IN6ADDRSZ;
+	colonp = NULL;
+	/* Leading :: requires some special handling. */
+	if (*src == ':')
+		if (*++src != ':')
+			return 0;
+	curtok = src;
+	saw_xdigit = 0;
+	val = 0;
+	while ((ch = *src++) != '\0') {
+		const char *pch;
+
+		pch = strchr((xdigits = xdigits_l), ch);
+		if (!pch)
+			pch = strchr((xdigits = xdigits_u), ch);
+		if (pch) {
+			val <<= 4;
+			val |= (pch - xdigits);
+			if (val > 0xffff)
+				return 0;
+			saw_xdigit = 1;
+			continue;
+		}
+		if (ch == ':') {
+			curtok = src;
+			if (!saw_xdigit) {
+				if (colonp)
+					return 0;
+				colonp = tp;
+				continue;
+			}
+			if (tp + NS_INT16SZ > endp)
+				return 0;
+			*tp++ = (u_char)(val >> 8) & 0xff;
+			*tp++ = (u_char)val & 0xff;
+			saw_xdigit = 0;
+			val = 0;
+			continue;
+		}
+		if (ch == '.' && ((tp + NS_INADDRSZ) <= endp) &&
+		    inet_pton4(curtok, tp) > 0) {
+			tp += NS_INADDRSZ;
+			saw_xdigit = 0;
+			break;	/* '\0' was seen by inet_pton4(). */
+		}
+		return 0;
+	}
+	if (saw_xdigit) {
+		if (tp + NS_INT16SZ > endp)
+			return 0;
+		*tp++ = (u_char)(val >> 8) & 0xff;
+		*tp++ = (u_char)val & 0xff;
+	}
+	if (colonp) {
+		/* Since some memmove()'s erroneously fail to handle
+		 * overlapping regions, we'll do the shift by hand.
+		 */
+		const int n = (int)(tp - colonp);
+		int i;
+
+		for (i = 1; i <= n; i++) {
+			endp[-i] = colonp[n - i];
+			colonp[n - i] = 0;
+		}
+		tp = endp;
+	}
+	if (tp != endp)
+		return 0;
+	memcpy(dst, tmp, NS_IN6ADDRSZ);
+	return 1;
+}
+
+int mac_stob(const char *mac, u8 bytes[6])
+{
+	unsigned int values[6];
+	int i;
+	int f = 0;
+
+	if (!mac || !bytes)
+		return -1;
+
+	if (6 ==
+	    sscanf(mac, "%x:%x:%x:%x:%x:%x", &values[0], &values[1],
+		   &values[2], &values[3], &values[4], &values[5]))
+		f = 1;
+	else if (6 ==
+		 sscanf(mac, "%x-%x-%x-%x-%x-%x", &values[0], &values[1],
+			&values[2], &values[3], &values[4], &values[5]))
+		f = 1;
+
+	if (f) {		/* convert to uint8_t */
+		for (i = 0; i < 6; ++i)
+			bytes[i] = (u8)values[i];
+		return 0;
+	}
+	return -1;
+}
+
+int low_10dec(u64 x)
+{
+	char buf[26];
+	char *p;
+	int len;
+
+	sprintf(buf, "%llu", x);
+	len = strlen(buf);
+	if (len >= 10)
+		p = buf + len - 10;
+	else
+		p = buf;
+
+	return dp_atoi(p);
+}
+
+int high_10dec(u64 x)
+{
+	char buf[26];
+	int len;
+
+	sprintf(buf, "%llu", x);
+	len = strlen(buf);
+	if (len >= 10)
+		buf[len - 10] = 0;
+	else
+		buf[0] = 0;
+
+	return dp_atoi(buf);
+}
+
+int get_vlan_info(struct net_device *dev, struct vlan_info *vinfo)
+{
+	struct vlan_dev_priv *vlan;
+	struct net_device *lower_dev;
+	struct list_head *iter;
+	int num = 0;
+
+	if (is_vlan_dev(dev)) {
+		num++;
+		vlan = vlan_dev_priv(dev);
+		PR_INFO("vlan proto:%x VID:%d real devname:%s\n",
+			vlan->vlan_proto, vlan->vlan_id,
+			vlan->real_dev ? vlan->real_dev->name : "NULL");
+		netdev_for_each_lower_dev(dev, lower_dev, iter) {
+			if (is_vlan_dev(lower_dev)) {
+				num++;
+				vinfo->in_proto = vlan->vlan_proto;
+				vinfo->in_vid = vlan->vlan_id;
+				vlan = vlan_dev_priv(lower_dev);
+				PR_INFO("%s:%x VID:%d %s:%s\n",
+					"Outer vlan proto",
+					vlan->vlan_proto, vlan->vlan_id,
+					"real devname",
+					vlan->real_dev ?
+					vlan->real_dev->name : "NULL");
+				vinfo->out_proto = vlan->vlan_proto;
+				vinfo->out_vid = vlan->vlan_id;
+				vinfo->cnt = num;
+				return 0;
+			}
+		}
+		vinfo->cnt = num;
+		vinfo->out_proto = vlan->vlan_proto;
+		vinfo->out_vid = vlan->vlan_id;
+	} else {
+		PR_ERR("Not a VLAN device\n");
+		return -1;
+	}
+	return 0;
+}
+
+/*Print call callback orginal function name */
+int print_symbol_name(unsigned long addr)
+{
+	unsigned long size;
+	unsigned long offset;
+	char *modname;
+	char namebuf[KSYM_NAME_LEN];
+	char *sym_name;
+
+	namebuf[0] = 0;
+	sym_name = (char *)kallsyms_lookup(addr, &size, &offset, &modname,
+		namebuf);
+	PR_ERR("call API:%s @0x%lx\n", sym_name ? sym_name : "Unknown",
+	       (unsigned long)addr);
+	return 0;
+}
+
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_notifier.c b/drivers/net/ethernet/lantiq/datapath/datapath_notifier.c
new file mode 100644
index 000000000000..fc3b083d66e9
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/datapath_notifier.c
@@ -0,0 +1,197 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include <linux/module.h>
+#include <linux/types.h>	/* size_t */
+#include <linux/inetdevice.h>
+#include <net/datapath_api.h>
+#include <net/datapath_proc_api.h>
+#include "datapath.h"
+#include "datapath_instance.h"
+#include "datapath_swdev_api.h"
+#include "datapath_swdev.h"
+
+static int dp_event(struct notifier_block *this,
+		    unsigned long event, void *ptr);
+static struct notifier_block dp_dev_notifier = {
+	dp_event, /*handler*/
+	NULL,
+	0
+};
+
+int register_notifier(u32 flag)
+{
+	return register_netdevice_notifier(&dp_dev_notifier);
+}
+
+int unregister_notifier(u32 flag)
+{
+	return unregister_netdevice_notifier(&dp_dev_notifier);
+}
+
+int dp_event(struct notifier_block *this, unsigned long event, void *ptr)
+{
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_SWITCHDEV)
+	struct net_device *dev;
+	u8 *addr;
+	struct net_device *br_dev;
+	struct dp_dev *dp_dev;
+	struct br_info *br_info;
+	int fid, inst, vap;
+	u32 idx;
+	struct pmac_port_info *port;
+	struct inst_property *prop;
+
+	dev = netdev_notifier_info_to_dev(ptr);
+	if(!dev)
+		return 0;
+	addr = (u8 *)dev->dev_addr;
+	if (!addr || dev->addr_len != ETH_ALEN) /*only support ethernet */
+		return 0;
+	if (is_zero_ether_addr(addr)) {
+		DP_DEBUG(DP_DBG_FLAG_NOTIFY,
+			 "Skip zero mac address for %s\n", dev->name);
+		return 0;
+	}
+	idx = dp_dev_hash(dev, NULL);
+	dp_dev = dp_dev_lookup(&dp_dev_list[idx], dev, NULL, 0);
+	if (!dp_dev) {
+		DP_DEBUG(DP_DBG_FLAG_NOTIFY,
+			 "datapath dev(%s) not exists!!,No mac config\n",
+			 dev ? dev->name : "NULL");
+		return 0;
+	}
+	inst = dp_dev->inst;
+	vap = GET_VAP(dp_dev->ctp,
+		      dp_port_info[inst][dp_dev->ep].vap_offset,
+		      dp_port_info[inst][dp_dev->ep].vap_mask);
+	/* CPU Path MAC address handling via LINUX dev notifier
+	 * incase of re-direct bit set
+	 */
+	switch (event) {
+	case NETDEV_GOING_DOWN:
+		port = &dp_port_info[inst][dp_dev->ep];
+		DP_DEBUG(DP_DBG_FLAG_NOTIFY,
+			 "%s%d %s%d %s%s %s%02x%02x%02x%02x%02x%02x\n",
+			 "Rem MAC with BP:",
+			 port->subif_info[vap].bp, "FID:", dp_dev->fid,
+			 "dev:", dev ? dev->name : "NULL",
+			 "MAC:", addr[0], addr[1], addr[2],
+			 addr[3], addr[4], addr[5]);
+		prop = &dp_port_prop[inst];
+		prop->info.dp_mac_reset(0,
+					dp_dev->fid,
+					dp_dev->inst,
+					addr);
+	break;
+	case NETDEV_CHANGEUPPER:
+		port = &dp_port_info[inst][dp_dev->ep];
+		dp_port_prop[inst].info.dp_mac_reset(0,
+						     dp_dev->fid,
+						     dp_dev->inst,
+						     addr);
+		DP_DEBUG(DP_DBG_FLAG_NOTIFY,
+			 "%s%d %s%d %s%s %s%02x%02x%02x%02x%02x%02x\n",
+			 "Rem MAC with BP:",
+			 port->subif_info[vap].bp, "FID:", dp_dev->fid,
+			 "dev:", dev ? dev->name : "NULL",
+			 "MAC:", addr[0], addr[1], addr[2],
+			 addr[3], addr[4], addr[5]);
+		if (!netif_is_bridge_port(dev)) {
+			DP_DEBUG(DP_DBG_FLAG_NOTIFY,
+				 "chg upper:dev not a Bport\n");
+		}
+		br_dev = netdev_master_upper_dev_get(dev);
+		if (!br_dev) {
+			DP_DEBUG(DP_DBG_FLAG_NOTIFY,
+				 "chg upper:without br name\n");
+				/* Set FID to Zero when br not attached to
+				 * bport
+				 */
+			dp_dev->fid = 0;
+			port->subif_info[vap].fid = dp_dev->fid;
+			goto dev_status;
+		}
+		/* Get respective FID when bport attached to bridge
+		 */
+		DP_DEBUG(DP_DBG_FLAG_NOTIFY, "Bridge name:%s\n",
+			 br_dev ? br_dev->name : "NULL");
+		br_info = dp_swdev_bridge_entry_lookup(br_dev->name, 0);
+		if (br_info) {
+			dp_dev->fid = br_info->fid;
+			port->subif_info[vap].fid = dp_dev->fid;
+		} else {
+			fid = dp_notif_br_alloc(br_dev);
+			if (fid > 0) {
+				dp_dev->fid = fid;
+				port->subif_info[vap].fid = dp_dev->fid;
+			} else {
+				PR_ERR("FID alloc failed in %s\r\n", __func__);
+				return 0;
+			}
+		}
+ dev_status:
+		if (dev->flags & IFF_UP) {
+			DP_DEBUG(DP_DBG_FLAG_NOTIFY,
+				 "%s%s%d%s%d %s%s %s%02x%02x%02x%02x%02x%02x\n",
+				 "link UP,",
+				 "ADD MAC with BP:",
+				 port->subif_info[vap].bp, " FID:", dp_dev->fid,
+				 "dev:", dev ? dev->name : "NULL",
+				 "MAC:", addr[0], addr[1], addr[2],
+				 addr[3], addr[4], addr[5]);
+			prop = &dp_port_prop[inst];
+			/* Note: For Linux network device's mac address,
+			 *       its bridge port should be CPU port, not its
+			 *       mapped bridge port in GSWIP
+			 */
+			prop->info.dp_mac_set(0,
+					      dp_dev->fid,
+					      dp_dev->inst,
+					      addr);
+		} else {
+			DP_DEBUG(DP_DBG_FLAG_NOTIFY, "DEV:%s %s %s\n",
+				 dev ? dev->name : "NULL",
+				 "link down", "No MAC configuration");
+		}
+		break;
+	case NETDEV_UP:
+		port = &dp_port_info[inst][dp_dev->ep];
+		DP_DEBUG(DP_DBG_FLAG_NOTIFY,
+			 "%s%d %s%d %s%s %s%02x%02x%02x%02x%02x%02x\n",
+			 "ADD MAC with BP:",
+			 port->subif_info[vap].bp, "FID:", dp_dev->fid,
+			 "dev:", dev ? dev->name : "NULL",
+			 "MAC:", addr[0], addr[1], addr[2],
+			 addr[3], addr[4], addr[5]);
+		prop = &dp_port_prop[inst];
+		prop->info.dp_mac_set(0,
+				      dp_dev->fid,
+				      dp_dev->inst, addr);
+	break;
+	case NETDEV_UNREGISTER:
+		DP_DEBUG(DP_DBG_FLAG_NOTIFY,
+			 "DevUnreg %s:%02x:%02x:%02x:%02x:%02x:%02x\n",
+			 dev->name,
+			 addr[0], addr[1], addr[2],
+			 addr[3], addr[4], addr[5]);
+		break;
+	case NETDEV_CHANGEADDR:
+		DP_DEBUG(DP_DBG_FLAG_NOTIFY,
+			 "DevChg %s:%02x:%02x:%02x:%02x:%02x:%02x\n",
+			 dev->name,
+			 addr[0], addr[1], addr[2],
+			 addr[3], addr[4], addr[5]);
+		break;
+	default:
+		break;
+	}
+#endif
+	return 0;
+}
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_platform_dev.c b/drivers/net/ethernet/lantiq/datapath/datapath_platform_dev.c
new file mode 100644
index 000000000000..1ef23301f5ec
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/datapath_platform_dev.c
@@ -0,0 +1,62 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include<linux/init.h>
+#include<linux/module.h>
+
+#include <net/datapath_api.h>
+#include "datapath.h"
+
+int dp_probe(struct platform_device *pdev)
+{
+	if (pdev) {
+		/*Note: if pdev NULL, it means no device tree support and
+		 *datapath lib use non-standard way to call it.
+		 *Datapath Device tree example:
+		 * datapath {
+		 *			 compatible = "lantiq,datapath-lib";
+		 *			 a-cell-property = <1 2 3 4>;
+		 *		 };
+		 *struct device_node *node = pdev->dev.of_node;
+		 *platform_get_resource(pdev, IORESOURCE_MEM, i)
+		 *of_irq_to_resource_table(node, irqres, CBM_NUM_INTERRUPTS);
+		 */
+	}
+	return dp_init_module();
+}
+
+static int dp_release(struct platform_device *pdev)
+{
+	dp_cleanup_module();
+	return 0;
+}
+
+static const struct of_device_id dp_match[] = {
+	{.compatible  = "lantiq,datapath-lib"},
+	{},
+};
+
+struct platform_driver dp_driver = {
+	.probe = dp_probe,
+	.remove = dp_release,
+	.driver = {
+			.name = "dp-lib",
+			.owner = THIS_MODULE,
+			.of_match_table = dp_match,
+		},
+};
+
+int __init dp_init(void)
+{
+	dp_basic_proc();
+	return platform_driver_register(&dp_driver);
+}
+
+module_init(dp_init);
+
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_proc.c b/drivers/net/ethernet/lantiq/datapath/datapath_proc.c
new file mode 100644
index 000000000000..81c159b25270
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/datapath_proc.c
@@ -0,0 +1,2241 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include <linux/module.h>
+#include <net/datapath_proc_api.h>	/*for proc api */
+#include <net/datapath_api.h>
+#include <net/datapath_api_vlan.h>
+
+#ifdef CONFIG_LTQ_VMB
+#include <asm/ltq_vmb.h>	/*vmb */
+#include <asm/ltq_itc.h>	/*mips itc */
+#endif
+#include <linux/list.h>
+#include <linux/of_device.h>
+#include <linux/of_platform.h>
+#include "datapath.h"
+#include "datapath_instance.h"
+
+#define DP_PROC_NAME       "dp"
+#define DP_PROC_BASE       "/proc/" DP_PROC_NAME "/"
+#define DP_PROC_PARENT     ""
+
+#define PROC_DBG   "dbg"
+#define PROC_PORT   "port"
+#define PROC_MEM "mem"
+#define DP_PROC_PRINT_MODE "print_mode"
+#define PROC_DT  "dt"
+#define PROC_LOGICAL_DEV "logic"
+#define PROC_INST_DEV "inst_dev"
+#define PROC_INST_MOD "inst_mod"
+#define PROC_INST_HAL "inst_hal"
+#define PROC_INST "inst"
+#define PROC_TX_PKT "tx"
+#define PROC_QOS  "qos"
+#define PROC_ASYM_VLAN  "asym_vlan"
+
+static int tmp_inst;
+static ssize_t proc_port_write(struct file *file, const char *buf,
+			       size_t count, loff_t *ppos);
+#if defined(CONFIG_LTQ_DATAPATH_DBG) && CONFIG_LTQ_DATAPATH_DBG
+static void proc_dbg_read(struct seq_file *s);
+static ssize_t proc_dbg_write(struct file *, const char *, size_t, loff_t *);
+#endif
+static int proc_port_dump(struct seq_file *s, int pos);
+static int proc_write_mem(struct file *, const char *, size_t, loff_t *);
+static int proc_asym_vlan(struct file *, const char *, size_t, loff_t *);
+static void print_device_tree_node(struct device_node *node, int depth);
+static ssize_t proc_dt_write(struct file *file, const char *buf,
+			     size_t count, loff_t *ppos);
+static ssize_t proc_logical_dev_write(struct file *file, const char *buf,
+				      size_t count, loff_t *ppos);
+static int proc_port_init(void);
+
+int proc_port_init(void)
+{
+	tmp_inst = 0;
+	return 0;
+}
+
+int proc_port_dump(struct seq_file *s, int pos)
+{
+	int i, j;
+	int cqm_p;
+	int (*print_ctp_bp)(struct seq_file *s, int inst,
+			    struct pmac_port_info *port,
+			    int subif_index, u32 flag);
+	struct pmac_port_info *port = get_port_info(tmp_inst, pos);
+	u16 start = 0;
+
+	if (!port) {
+		PR_ERR("Why port is NULL\n");
+		return -1;
+	}
+	print_ctp_bp = DP_CB(tmp_inst, proc_print_ctp_bp_info);
+	DP_CB(tmp_inst, get_itf_start_end)(port->itf_info, &start, NULL);
+
+	if (port->status == PORT_FREE) {
+		if (pos == 0) {
+			seq_printf(s,
+				   "Reserved Port: rx_err_drop=0x%08x  tx_err_drop=0x%08x\n",
+				   STATS_GET(port->rx_err_drop),
+				   STATS_GET(port->tx_err_drop));
+			if (print_ctp_bp) /*just to print bridge
+					   *port zero's member
+					   */
+				print_ctp_bp(s, tmp_inst, port, 0, 0);
+			i = 0;
+			seq_printf(s, "          : qid/node:    %d/%d\n",
+				   port->subif_info[i].qid,
+				   port->subif_info[i].q_node);
+			seq_printf(s, "          : port/node:    %d/%d\n",
+				   port->subif_info[i].cqm_deq_port,
+				   port->subif_info[i].qos_deq_port);
+
+		} else
+			seq_printf(s,
+				   "%02d: rx_err_drop=0x%08x  tx_err_drop=0x%08x\n",
+				   pos,
+				   STATS_GET(port->rx_err_drop),
+				   STATS_GET(port->tx_err_drop));
+
+		goto EXIT;
+	}
+
+	seq_printf(s,
+		   "%02d:%s=0x0x%0x(%s:%8s) %s=%02d %s=%02d %s=%d(%s) %s=%d\n",
+		   pos,
+		   "module",
+		   (u32)port->owner,
+		   "name",
+		   port->owner->name,
+		   "dev_port",
+		   port->dev_port,
+		   "dp_port",
+		   port->port_id,
+		   "itf_base",
+		   start,
+		   port->itf_info ? "Enabled" : "Not Enabled",
+		   "ctp_max",
+		   port->ctp_max);
+	seq_printf(s, "    status:            %s\n",
+		   dp_port_status_str[port->status]);
+	seq_puts(s, "    allocate_flags:    ");
+	for (i = 0; i < get_dp_port_type_str_size(); i++) {
+		if (port->alloc_flags & dp_port_flag[i])
+			seq_printf(s, "%s ", dp_port_type_str[i]);
+	}
+	seq_puts(s, "\n");
+	seq_printf(s, "    cb->rx_fn:         0x%0x\n", (u32)port->cb.rx_fn);
+	seq_printf(s, "    cb->restart_fn:    0x%0x\n",
+		   (u32)port->cb.restart_fn);
+	seq_printf(s, "    cb->stop_fn:       0x%0x\n",
+		   (u32)port->cb.stop_fn);
+	seq_printf(s, "    cb->get_subifid_fn:0x%0x\n",
+		   (u32)port->cb.get_subifid_fn);
+	seq_printf(s, "    num_subif:         %d\n", port->num_subif);
+	seq_printf(s, "    vap_offset/mask:   %d/0x%x\n", port->vap_offset,
+		   port->vap_mask);
+	seq_printf(s, "    flag_other:        0x%x\n", port->flag_other);
+	seq_printf(s, "    deq_port_base:     %d\n", port->deq_port_base);
+	seq_printf(s, "    deq_port_num:      %d\n", port->deq_port_num);
+	seq_printf(s, "    dma_chan:          0x%x\n", port->dma_chan);
+	seq_printf(s, "    tx_pkt_credit:     %d\n", port->tx_pkt_credit);
+	seq_printf(s, "    tx_b_credit:       %02d\n", port->tx_b_credit);
+	seq_printf(s, "    tx_ring_addr:      0x%x\n", port->tx_ring_addr);
+	seq_printf(s, "    tx_ring_size:      %d\n", port->tx_ring_size);
+	seq_printf(s, "    tx_ring_offset:    %d(to next dequeue port)\n",
+		   port->tx_ring_offset);
+	for (i = 0; i < port->ctp_max; i++) {
+		if (!port->subif_info[i].flags)
+			continue;
+		seq_printf(s,
+			   "      [%02d]:%s=0x%04x %s=0x%0x(%s=%s),%s=%s\n",
+			i,
+			"subif",
+			port->subif_info[i].subif,
+			"netif",
+			(u32)port->subif_info[i].netif,
+			"netif",
+			port->subif_info[i].netif ?
+			port->subif_info[i].netif->name : "NULL/DSL",
+			"device_name",
+			port->subif_info[i].device_name);
+		seq_puts(s, "          : subif_flag = ");
+		for (j = 0; j < get_dp_port_type_str_size(); j++) {
+			if (!port->subif_info[i].subif_flag) {
+				seq_printf(s, "%s ", "NULL");
+				break;
+			}
+			if (port->subif_info[i].subif_flag & dp_port_flag[j])
+				seq_printf(s, "%s ", dp_port_type_str[j]);
+		}
+		seq_puts(s, "\n");
+		seq_printf(s, "          : rx_fn_rxif_pkt =0x%08x\n",
+			   STATS_GET(port->subif_info[i].mib.rx_fn_rxif_pkt));
+		seq_printf(s, "          : rx_fn_txif_pkt =0x%08x\n",
+			   STATS_GET(port->subif_info[i].mib.rx_fn_txif_pkt));
+		seq_printf(s, "          : rx_fn_dropped  =0x%08x\n",
+			   STATS_GET(port->subif_info[i].mib.rx_fn_dropped));
+		seq_printf(s, "          : tx_cbm_pkt     =0x%08x\n",
+			   STATS_GET(port->subif_info[i].mib.tx_cbm_pkt));
+		seq_printf(s, "          : tx_tso_pkt     =0x%08x\n",
+			   STATS_GET(port->subif_info[i].mib.tx_tso_pkt));
+		seq_printf(s, "          : tx_pkt_dropped =0x%08x\n",
+			   STATS_GET(port->subif_info[i].mib.tx_pkt_dropped));
+		seq_printf(s, "          : tx_clone_pkt   =0x%08x\n",
+			   STATS_GET(port->subif_info[i].mib.tx_clone_pkt));
+		seq_printf(s, "          : tx_hdr_room_pkt=0x%08x\n",
+			   STATS_GET(port->subif_info[i].mib.tx_hdr_room_pkt));
+		if (print_ctp_bp)
+			print_ctp_bp(s, tmp_inst, port, i, 0);
+		seq_printf(s, "          : qid/node:    %d/%d\n",
+			   port->subif_info[i].qid,
+			   port->subif_info[i].q_node);
+		cqm_p = port->subif_info[i].cqm_deq_port;
+		seq_printf(s, "          : port/node:    %d/%d(ref=%d)\n",
+			   cqm_p,
+			   port->subif_info[i].qos_deq_port,
+			   dp_deq_port_tbl[tmp_inst][cqm_p].ref_cnt);
+		if (port->subif_info[i].ctp_dev &&
+		    port->subif_info[i].ctp_dev->name)
+			seq_printf(s, "          : ctp_dev = %s\n",
+				   port->subif_info[i].ctp_dev->name);
+		else
+			seq_puts(s, "          : ctp_dev = NULL\n");
+	}
+	seq_printf(s, "    rx_err_drop=0x%08x  tx_err_drop=0x%08x\n",
+		   STATS_GET(port->rx_err_drop),
+		   STATS_GET(port->tx_err_drop));
+EXIT:
+	if (!seq_has_overflowed(s))
+		pos++;
+	if (pos >= MAX_DP_PORTS) {
+		tmp_inst++;
+		pos = 0;
+	}
+	if (tmp_inst >= dp_inst_num)
+		pos = -1;	/*end of the loop */
+	return pos;
+}
+
+int display_port_info(int inst, u8 pos, int start_vap, int end_vap, u32 flag)
+{
+	int i;
+	int ret;
+	struct pmac_port_info *port = get_port_info(inst, pos);
+	u16 start = 0;
+
+	if (!port) {
+		PR_ERR("Why port is NULL\n");
+		return -1;
+	}
+
+	if (port->status == PORT_FREE) {
+		if (pos == 0) {
+			PR_INFO("%s:rx_err_drop=0x%08x  tx_err_drop=0x%08x\n",
+				"Reserved Port",
+				STATS_GET(port->rx_err_drop),
+				STATS_GET(port->tx_err_drop));
+
+		} else
+			PR_INFO("%02d:rx_err_drop=0x%08x  tx_err_drop=0x%08x\n",
+				pos,
+				STATS_GET(port->rx_err_drop),
+				STATS_GET(port->tx_err_drop));
+
+		goto EXIT;
+	}
+
+	DP_CB(tmp_inst, get_itf_start_end)(port->itf_info, &start, NULL);
+
+	PR_INFO("%02d: %s=0x0x%0x(name:%8s) %s=%02d %s=%02d itf_base=%d(%s)\n",
+		pos,
+		"module",
+		(u32)port->owner, port->owner->name,
+		"dev_port",
+		port->dev_port,
+		"dp_port",
+		port->port_id,
+		start,
+		port->itf_info ? "Enabled" : "Not Enabled");
+	PR_INFO("    status:            %s\n",
+		dp_port_status_str[port->status]);
+	PR_INFO("    allocate_flags:    ");
+
+	for (i = 0; i < get_dp_port_type_str_size(); i++) {
+		if (port->alloc_flags & dp_port_flag[i])
+			PR_INFO("%s ", dp_port_type_str[i]);
+	}
+
+	PR_INFO("\n");
+
+	if (!flag) {
+		PR_INFO("    cb->rx_fn:         0x%0x\n",
+			(u32)port->cb.rx_fn);
+		PR_INFO("    cb->restart_fn:    0x%0x\n",
+			(u32)port->cb.restart_fn);
+		PR_INFO("    cb->stop_fn:       0x%0x\n",
+			(u32)port->cb.stop_fn);
+		PR_INFO("    cb->get_subifid_fn:0x%0x\n",
+			(u32)port->cb.get_subifid_fn);
+		PR_INFO("    num_subif:         %02d\n", port->num_subif);
+	}
+
+	for (i = start_vap; i < end_vap; i++) {
+		if (port->subif_info[i].flags) {
+			PR_INFO
+			    ("      [%02d]:%s=0x%04x %s=0x%0x(%s=%s),%s=%s\n",
+			     i,
+			     "subif",
+			     port->subif_info[i].subif,
+			     "netif",
+			     (u32)port->subif_info[i].netif,
+			     "device_name",
+			     port->subif_info[i].netif ? port->subif_info[i].
+			     netif->name : "NULL/DSL",
+			     "name",
+			     port->subif_info[i].device_name);
+			PR_INFO("          : rx_fn_rxif_pkt =0x%08x\n",
+				STATS_GET(port->subif_info[i].mib.
+				rx_fn_rxif_pkt));
+			PR_INFO("          : rx_fn_txif_pkt =0x%08x\n",
+				STATS_GET(port->subif_info[i].mib.
+				rx_fn_txif_pkt));
+			PR_INFO("          : rx_fn_dropped  =0x%08x\n",
+				STATS_GET(port->subif_info[i].mib.
+				rx_fn_dropped));
+			PR_INFO("          : tx_cbm_pkt     =0x%08x\n",
+				STATS_GET(port->subif_info[i].mib.tx_cbm_pkt));
+			PR_INFO("          : tx_tso_pkt     =0x%08x\n",
+				STATS_GET(port->subif_info[i].mib.tx_tso_pkt));
+			PR_INFO("          : tx_pkt_dropped =0x%08x\n",
+				STATS_GET(port->subif_info[i].mib.
+				tx_pkt_dropped));
+			PR_INFO("          : tx_clone_pkt   =0x%08x\n",
+				STATS_GET(port->subif_info[i].mib.
+				tx_clone_pkt));
+			PR_INFO("          : tx_hdr_room_pkt=0x%08x\n",
+				STATS_GET(port->subif_info[i].mib.
+				tx_hdr_room_pkt));
+		}
+	}
+
+	ret = PR_INFO("    rx_err_drop=0x%08x  tx_err_drop=0x%08x\n",
+		      STATS_GET(port->rx_err_drop),
+		      STATS_GET(port->tx_err_drop));
+EXIT:
+	return 0;
+}
+
+ssize_t proc_port_write(struct file *file, const char *buf, size_t count,
+			loff_t *ppos)
+{
+	int len;
+	char str[64];
+	int num, i;
+	u8 index_start = 0;
+	u8 index_end = MAX_DP_PORTS;
+	int vap_start = 0;
+	int vap_end = MAX_SUBIFS;
+	char *param_list[10];
+	int inst;
+
+	len = (sizeof(str) > count) ? count : sizeof(str) - 1;
+	len -= copy_from_user(str, buf, len);
+	str[len] = 0;
+	num = dp_split_buffer(str, param_list, ARRAY_SIZE(param_list));
+
+	if (num <= 1)
+		goto help;
+	if (param_list[1]) {
+		index_start = dp_atoi(param_list[1]);
+		index_end = index_start + 1;
+	}
+
+	if (param_list[2]) {
+		vap_start = dp_atoi(param_list[2]);
+		vap_end = vap_start + 1;
+	}
+
+	if (index_start >= MAX_DP_PORTS) {
+		PR_ERR("wrong index: 0 ~ 15\n");
+		return count;
+	}
+
+	if (vap_start >= MAX_SUBIFS) {
+		PR_ERR("wrong VAP: 0 ~ 15\n");
+		return count;
+	}
+
+	if (dp_strncmpi(param_list[0], "mib", strlen("mib")) == 0) {
+		for (inst = 0; inst < dp_inst_num; inst++)
+		for (i = index_start; i < index_end; i++)
+			display_port_info(inst, i, vap_start, vap_end, 1);
+
+	} else if (dp_strncmpi(param_list[0], "port", strlen("port")) == 0) {
+		for (inst = 0; inst < dp_inst_num; inst++)
+		for (i = index_start; i < index_end; i++)
+			display_port_info(inst, i, vap_start, vap_end, 0);
+
+	} else {
+		goto help;
+	}
+
+	return count;
+ help:
+	PR_INFO("usage:\n");
+	PR_INFO("  echo mib  [ep][vap] > /prooc/dp/port\n");
+	PR_INFO("  echo port [ep][vap] > /prooc/dp/port\n");
+	return count;
+}
+
+#if defined(CONFIG_LTQ_DATAPATH_DBG) && CONFIG_LTQ_DATAPATH_DBG
+void proc_dbg_read(struct seq_file *s)
+{
+	int i;
+
+	seq_printf(s, "dp_dbg_flag=0x%08x\n", dp_dbg_flag);
+
+	seq_printf(s, "Supported Flags =%d\n", get_dp_dbg_flag_str_size());
+	seq_printf(s, "Enabled Flags(0x%0x):", dp_dbg_flag);
+
+	for (i = 0; i < get_dp_dbg_flag_str_size(); i++)
+		if ((dp_dbg_flag & dp_dbg_flag_list[i]) ==
+		    dp_dbg_flag_list[i])
+			seq_printf(s, "%s ", dp_dbg_flag_str[i]);
+
+	seq_puts(s, "\n\n");
+
+	seq_printf(s, "dp_drop_all_tcp_err=%d @ 0x%p\n", dp_drop_all_tcp_err,
+		   &dp_drop_all_tcp_err);
+	seq_printf(s, "dp_pkt_size_check=%d @ 0x%p\n", dp_pkt_size_check,
+		   &dp_pkt_size_check);
+
+	seq_printf(s, "dp_rx_test_mode=%d @ 0x%p\n", dp_rx_test_mode,
+		   &dp_rx_test_mode);
+	seq_printf(s, "dp_dbg_err(flat to print error or not)=%d @ 0x%p\n",
+		   dp_dbg_err,
+		   &dp_dbg_err);
+	print_parser_status(s);
+}
+
+ssize_t proc_dbg_write(struct file *file, const char *buf, size_t count,
+		       loff_t *ppos)
+{
+	int len, i, j;
+	char str[64];
+	int num;
+	char *param_list[20];
+	int f_enable;
+	char *tmp_buf;
+	#define BUF_SIZE_TMP 2000
+
+	len = (sizeof(str) > count) ? count : sizeof(str) - 1;
+	len -= copy_from_user(str, buf, len);
+	str[len] = 0;
+	num = dp_split_buffer(str, param_list, ARRAY_SIZE(param_list));
+
+	if (dp_strncmpi(param_list[0], "enable", strlen("enable")) == 0)
+		f_enable = 1;
+	else if (dp_strncmpi(param_list[0], "disable", strlen("disable")) == 0)
+		f_enable = -1;
+	else
+		goto help;
+
+	if (!param_list[1]) {	/*no parameter after enable or disable */
+		set_ltq_dbg_flag(dp_dbg_flag, f_enable, -1);
+		goto EXIT;
+	}
+
+	for (i = 1; i < num; i++) {
+		for (j = 0; j < get_dp_dbg_flag_str_size(); j++)
+			if (dp_strncmpi(param_list[i], dp_dbg_flag_str[j], strlen(dp_dbg_flag_str[j])) ==
+			    0) {
+				set_ltq_dbg_flag(dp_dbg_flag, f_enable,
+						 dp_dbg_flag_list[j]);
+				break;
+			}
+	}
+
+EXIT:
+	return count;
+help:
+	tmp_buf = kmalloc(BUF_SIZE_TMP, GFP_KERNEL);
+	if (!tmp_buf)
+		return count;
+	num = 0;
+	num = snprintf(tmp_buf + num, BUF_SIZE_TMP - num - 1,
+		       "echo <enable/disable> ");
+	for (i = 0; i < get_dp_dbg_flag_str_size(); i++)
+		num += snprintf(tmp_buf + num, BUF_SIZE_TMP - num - 1,
+				"%s ", dp_dbg_flag_str[i]);
+
+	num += snprintf(tmp_buf + num, BUF_SIZE_TMP - num - 1,
+			" > /proc/dp/dbg\n");
+	num += snprintf(tmp_buf + num, BUF_SIZE_TMP - num - 1,
+			" display command: cat /proc/dp/cmd\n");
+	PR_INFO("%s", tmp_buf);
+	kfree(tmp_buf);
+	return count;
+}
+#endif
+
+/**
+ * \brief directly read memory address with 4 bytes alignment.
+ * \param  reg_addr memory address (it must be 4 bytes alignment)
+ * \param  shift to the expected bits (its value is from 0 ~ 31)
+ * \param  size the bits number (its value is from 1 ~ 32).
+ *  Note: shift + size <= 32
+ * \param  buffer destionation
+ * \return on Success return 0
+ */
+int32_t dp_mem_read(u32 reg_addr, u32 shift, u32 size,
+		    u32 *buffer, u32 base)
+{
+	u32 v;
+	u32 mask = 0;
+	int i;
+
+	/*generate mask */
+	for (i = 0; i < size; i++)
+		mask |= 1 << i;
+
+	mask = mask << shift;
+
+	/*read data from specified address */
+	if (base == 4)
+		v = *(u32 *)reg_addr;
+	else if (base == 2)
+		v = *(u16 *)reg_addr;
+	else
+		v = *(u8 *)reg_addr;
+
+	v = dp_get_val(v, mask, shift);
+
+	*buffer = v;
+	return 0;
+}
+
+/**
+ * \brief directly write memory address with
+ * \param  reg_addr memory address (it must be 4 bytes alignment)
+ * \param  shift to the expected bits (its value is from 0 ~ 31)
+ * \param  size the bits number (its value is from 1 ~ 32)
+ * \param  value value writen to
+ * \return on Success return 0
+ */
+int32_t dp_mem_write(u32 reg_addr, u32 shift, u32 size,
+		     u32 value, u32 base)
+{
+	u32 tmp = 0;
+	u32 mask = 0;
+	int i;
+
+	/*generate mask */
+	for (i = 0; i < size; i++)
+		mask |= 1 << i;
+
+	mask = mask << shift;
+
+	/*read data from specified address */
+	if (base == 4) {
+		tmp = *(u32 *)reg_addr;
+	} else if (base == 2) {
+		tmp = *(u16 *)reg_addr;
+	} else if (base == 1) {
+		tmp = *(u8 *)reg_addr;
+	} else {
+		PR_ERR("wrong base in dp_mem_write\n");
+		return 0;
+	}
+
+	dp_set_val(tmp, value, mask, shift);
+
+	if (base == 4)
+		*(u32 *)reg_addr = tmp;
+	else if (base == 2)
+		*(u16 *)reg_addr = tmp;
+	else
+		*(u8 *)reg_addr = tmp;
+
+	return 0;
+}
+
+#define MODE_ACCESS_BYTE  1
+#define MODE_ACCESS_SHORT 2
+#define MODE_ACCESS_DWORD 4
+
+#define ACT_READ   1
+#define ACT_WRITE  2
+static int proc_write_mem(struct file *file, const char *buf, size_t count,
+			  loff_t *ppos)
+{
+	char str[100];
+	int num;
+	char *param_list[20] = { NULL };
+	int i, k, len;
+	u32 line_max_num = 32;	/* per line number printed */
+	u32 addr = 0;
+	u32 v = 0;
+	u32 act = ACT_READ;
+	u32 bit_offset = 0;
+	u32 bit_num = 32;
+	u32 repeat = 1;
+	u32 mode = MODE_ACCESS_DWORD;
+	int v_flag = 0;
+	char *tmp_buf;
+
+	len = sizeof(str) < count ? sizeof(str) - 1 : count;
+	len = len - copy_from_user(str, buf, len);
+	str[len] = 0;
+	num = dp_split_buffer(str, param_list, ARRAY_SIZE(param_list));
+
+	if (num < 2)
+		goto proc_help;
+
+	if (dp_strncmpi(param_list[0], "w", 1) == 0)
+		act = ACT_WRITE;
+	else if (dp_strncmpi(param_list[0], "r", 1) == 0)
+		act = ACT_READ;
+	else
+		goto proc_help;
+
+	if (num < 3)
+		goto proc_help;
+
+	k = 1;
+
+	while (k < num) {
+		if (dp_strncmpi(param_list[k], "-s", strlen("-s")) == 0) {
+			addr = dp_atoi(param_list[k + 1]);
+			k += 2;
+		} else if (dp_strncmpi(param_list[k], "-o", strlen("-o")) == 0) {
+			bit_offset = dp_atoi(param_list[k + 1]);
+			k += 2;
+		} else if (dp_strncmpi(param_list[k], "-n", strlen("-n")) == 0) {
+			bit_num = dp_atoi(param_list[k + 1]);
+			k += 2;
+		} else if (dp_strncmpi(param_list[k], "-r", strlen("-r")) == 0) {
+			repeat = dp_atoi(param_list[k + 1]);
+			k += 2;
+		} else if (dp_strncmpi(param_list[k], "-v", strlen("-v")) == 0) {
+			v = dp_atoi(param_list[k + 1]);
+			k += 2;
+			v_flag = 1;
+		} else if (dp_strncmpi(param_list[k], "-b", strlen("-b")) == 0) {
+			mode = MODE_ACCESS_BYTE;
+			k += 1;
+		} else if (dp_strncmpi(param_list[k], "-w", strlen("-w")) == 0) {
+			mode = MODE_ACCESS_SHORT;
+			k += 1;
+		} else if (dp_strncmpi(param_list[k], "-d", strlen("-d")) == 0) {
+			mode = MODE_ACCESS_DWORD;
+			k += 1;
+		} else {
+			PR_INFO("unknown command option: %s\n",
+				param_list[k]);
+			break;
+		}
+	}
+
+	if (bit_num > mode * 8)
+		bit_num = mode * 8;
+
+	if (repeat == 0)
+		repeat = 1;
+
+	if (!addr) {
+		PR_ERR("addr cannot be zero\n");
+		goto EXIT;
+	}
+
+	if ((mode != MODE_ACCESS_DWORD) && (mode != MODE_ACCESS_SHORT) &&
+	    (mode != MODE_ACCESS_BYTE)) {
+		PR_ERR("wrong access mode: %d bytes\n", mode);
+		goto EXIT;
+	}
+
+	if ((act == ACT_WRITE) && !v_flag) {
+		PR_ERR("For write command it needs to provide -v\n");
+		goto EXIT;
+	}
+
+	if (bit_offset > mode * 8 - 1) {
+		PR_ERR("valid bit_offset range:0 ~ %d\n", mode * 8 - 1);
+		goto EXIT;
+	}
+
+	if ((bit_num > mode * 8) || (bit_num < 1)) {
+		PR_ERR("valid bit_num range:0 ~ %d. Current bit_num=%d\n",
+		       mode * 8, bit_num);
+		goto EXIT;
+	}
+
+	if ((bit_offset + bit_num) > mode * 8) {
+		PR_ERR("valid bit_offset+bit_num range:0 ~ %d\n", mode * 8);
+		goto EXIT;
+	}
+
+	if ((addr % mode) != 0) {	/*access alignment */
+		PR_ERR("Cannot access 0x%08x in %d bytes\n", addr, mode);
+		goto EXIT;
+	}
+
+	line_max_num /= mode;
+
+	if (act == ACT_WRITE)
+		PR_INFO
+		    ("act=%s addr=0x%08x mode=%s %s=%d %s=%d v=0x%08x\n",
+		     "write", addr,
+		     (mode ==
+		      MODE_ACCESS_DWORD) ? "dword" : ((mode ==
+						       MODE_ACCESS_SHORT) ?
+						      "short" : "DWORD"),
+		     "bit_offset",
+		     bit_offset,
+		     "bit_num",
+		     bit_num,
+		     v);
+	else if (act == ACT_READ)
+		PR_INFO
+		    ("act=%s addr=0x%08x mode=%s bit_offset=%d bit_num=%d\n",
+		     "Read", addr,
+		     (mode ==
+		      MODE_ACCESS_DWORD) ? "dword" : ((mode ==
+						       MODE_ACCESS_SHORT) ?
+						      "short" : "DWORD"),
+		     bit_offset, bit_num);
+
+	if (act == ACT_WRITE)
+		for (i = 0; i < repeat; i++)
+			dp_mem_write(addr + mode * i, bit_offset, bit_num, v,
+				     mode);
+	else {
+		#define CH_PER_NUM  11 /*like 0X12345678*/
+		int offset = 0;
+
+		tmp_buf = kmalloc((line_max_num + 1) * CH_PER_NUM, GFP_KERNEL);
+		if (!tmp_buf)
+			return count;
+		for (i = 0; i < repeat; i++) {
+			v = 0;
+			dp_mem_read(addr + mode * i, bit_offset, bit_num, &v,
+				    mode);
+
+			/*print format control */
+			if ((i % line_max_num) == 0)
+				offset += sprintf(tmp_buf + offset,
+						  "0x%08x: ", addr + mode * i);
+
+			if (mode == MODE_ACCESS_DWORD)
+				offset += sprintf(tmp_buf + offset, "0x%08x ",
+						  v);
+			else if (mode == MODE_ACCESS_SHORT)
+				offset += sprintf(tmp_buf + offset, "0x%04x ",
+						  v);
+			else
+				offset += sprintf(tmp_buf + offset, "0x%02x ",
+						  v);
+
+			if ((i % line_max_num) == (line_max_num - 1)) {
+				offset += sprintf(tmp_buf + offset, "\n");
+				PR_INFO("%s", tmp_buf);
+				offset = 0;
+				}
+		}
+		if (offset) {
+			offset += sprintf(tmp_buf + offset, "\n");
+			PR_INFO("%s", tmp_buf);
+		}
+		kfree(tmp_buf);
+	}
+	PR_INFO("\n");
+EXIT:
+	return count;
+
+proc_help:
+	PR_INFO("echo <write/w> %s -s %s [-r %s]-v <value> [-o %s] [-n %s]\n",
+		"[-d/w/b]",
+		"<start_v_addr>",
+		"<repeat_times>",
+		"<bit_offset>",
+		"<bit_num>");
+	PR_INFO("echo <read/r>  %s -s %s [-r %s] [-o %s] [-n %s]\n",
+		"[-d/w/b]",
+		"<start_v_addr>",
+		"<repeat_times>",
+		"<bit_offset>",
+		"<bit_num>");
+	PR_INFO("\t -d: default read/write in dwords, ie 4 bytes\n");
+	PR_INFO("\t -w: read/write in short, ie 2 bytes\n");
+	PR_INFO("\t -b: read/write in bytes, ie 1 bytes\n");
+
+	return count;
+}
+
+static void set_dev(struct dp_tc_vlan *vlan, struct net_device *dev, int
+		    def_apply, int dir, int n_vlan0, int n_vlan1, int n_vlan2)
+{
+	vlan->dev = dev;
+	vlan->def_apply = def_apply;
+	vlan->dir = dir;
+	vlan->n_vlan0 = n_vlan0;
+	vlan->n_vlan1 = n_vlan1;
+	vlan->n_vlan2 = n_vlan2;
+}
+
+static void set_pattern(struct dp_pattern_vlan *patt,
+			int prio, int vid, int tpid,
+			int dei, int proto)
+{
+	patt->prio = prio;
+	patt->vid = vid;
+	patt->tpid = tpid;
+	patt->dei = dei;
+	patt->proto = proto;
+}
+
+static void set_action(struct dp_act_vlan *act, int action, int pop_n, int
+		       push_n)
+{
+	act->act = action;
+	act->pop_n = pop_n;
+	act->push_n = push_n;
+}
+
+static void set_tag(struct dp_act_vlan *act, int copy,  int index, int prio,
+		    int vid, int tpid, int dei)
+{
+	if ((copy == CP_FROM_INNER) || (copy == CP_FROM_OUTER)) {
+		act->prio[index] = copy;
+		act->vid[index] = copy;
+		act->tpid[index] = copy;
+		act->dei[index] = copy;
+	} else {
+		act->prio[index] = prio;
+		act->vid[index] = vid;
+		act->tpid[index] = tpid;
+		act->dei[index] = dei;
+	}
+}
+
+static int proc_asym_vlan(struct file *file, const char *buf, size_t count,
+			  loff_t *ppos)
+{
+	char str[256];
+	int num;
+	char *param_list[20] = { NULL };
+	int  k, len;
+	char dev_name[16];
+	int test_num = 0;
+	struct dp_tc_vlan vlan = {0};
+	struct dp_vlan0 vlan0_list = {0};
+	struct dp_vlan1 vlan1_list = {0};
+	struct dp_vlan2 vlan2_list = {0};
+	struct net_device *dev;
+
+#define TEST_VID 10
+	len = sizeof(str) < count ? sizeof(str) - 1 : count;
+	len = len - copy_from_user(str, buf, len);
+	str[len] = 0;
+	num = dp_split_buffer(str, param_list, ARRAY_SIZE(param_list));
+
+	if (num < 2)
+		goto proc_help;
+
+	k = 0;
+
+	while (k < num) {
+		if (dp_strncmpi(param_list[k], "dev", strlen("dev")) == 0) {
+			dev_name[0] = '\0';
+			strlcat(dev_name,  param_list[k + 1], 16);
+			k += 2;
+		} else if (dp_strncmpi(param_list[k], "-tnum", strlen("-tnum")) == 0) {
+			test_num = dp_atoi(param_list[k + 1]);
+			k += 2;
+		} else {
+			PR_INFO("unknown command option: %s\n",
+				param_list[k]);
+			break;
+		}
+	}
+	dev = dev_get_by_name(&init_net, dev_name);
+	if (!dev) {
+		PR_INFO("unknown device");
+		return -1;
+	}
+
+	switch (test_num) {
+	case 1:
+		PR_INFO("Input:IP packet with VID 10\n");
+		PR_INFO("Desc:pattern match = FALSE\n");
+		PR_INFO("Output:Enqueued packet is received without change\n");
+		set_dev(&vlan, dev, 0, 0, 0, 1, 0);
+		vlan.vlan1_list = &vlan1_list;
+		/*random proto for failing the pattern match*/
+		set_pattern(&vlan1_list.outer, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, 0x100);
+		vlan1_list.act.act = DP_VLAN_ACT_DROP;
+		dp_vlan_set(&vlan, 0);
+
+	break;
+	case 2:
+		PR_INFO("Input:IP packet without VLAN tag\n");
+		PR_INFO("Desc:pattern match = FALSE\n");
+		PR_INFO("Output:Enqueued packet is received without change\n");
+		set_dev(&vlan, dev, 0, 0, 1, 0, 0);
+		vlan.vlan0_list = &vlan0_list;
+		/*random proto for failing the pattern match*/
+		set_pattern(&vlan0_list.outer, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, 0x100);
+		vlan0_list.act.act = DP_VLAN_ACT_DROP;
+		dp_vlan_set(&vlan, 0);
+
+	break;
+	case 3:
+		PR_INFO("Input:IP packet without VLAN tag\n");
+		PR_INFO("Desc:pattern match = TRUE\n");
+		PR_INFO("Output:Enqueued packet is not received\n");
+		set_dev(&vlan, dev, 0, 0, 1, 0, 0);
+		vlan.vlan0_list = &vlan0_list;
+		/*random proto for failing the pattern match*/
+		set_pattern(&vlan0_list.outer, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_PROTO_IP4);
+		vlan0_list.act.act = DP_VLAN_ACT_DROP;
+		dp_vlan_set(&vlan, 0);
+
+	break;
+	case 4:
+		PR_INFO("This test is same as testnum 1, so skipping\n");
+	break;
+	case 5:
+		PR_INFO("Input:IP packet with VID 10\n");
+		PR_INFO("Desc:pattern match = TRUE and POP action\n");
+		PR_INFO("Output:Enqueued packet is received without vlantag\n");
+		set_dev(&vlan, dev, 0, 0, 0, 1, 0);
+		vlan.vlan1_list = &vlan1_list;
+		set_pattern(&vlan1_list.outer, DP_VLAN_PATTERN_NOT_CARE,
+			    TEST_VID, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_VLAN_PATTERN_NOT_CARE);
+		vlan1_list.act.act = DP_VLAN_ACT_POP;
+		vlan1_list.act.pop_n = 1;
+		dp_vlan_set(&vlan, 0);
+
+	break;
+	case 6:
+		PR_INFO("Input:IP packet with double tag VID 10 and vid	100\n");
+		PR_INFO("Desc:pattern match = TRUE and POP action\n");
+		PR_INFO("Output:Enqueued packet is received without vlantag\n");
+		set_dev(&vlan, dev, 0, 0, 0, 0, 1);
+		vlan.vlan2_list = &vlan2_list;
+		set_pattern(&vlan2_list.outer, DP_VLAN_PATTERN_NOT_CARE,
+			    TEST_VID, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_VLAN_PATTERN_NOT_CARE);
+		set_pattern(&vlan2_list.inner, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_VLAN_PATTERN_NOT_CARE);
+		vlan2_list.act.act = DP_VLAN_ACT_POP;
+		vlan2_list.act.pop_n = 2;
+		dp_vlan_set(&vlan, 0);
+
+	break;
+	case 7:
+		PR_INFO("Input:IP packet without vlan tag\n");
+		PR_INFO("Desc:pattern match = TRUE and PUSH action\n");
+		PR_INFO("Output:Enqueued packet is received with vlan tag");
+		PR_INFO("that is pushed\n");
+		set_dev(&vlan, dev, 0, 0, 1, 0, 0);
+		vlan.vlan0_list = &vlan0_list;
+		set_pattern(&vlan0_list.outer, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_PROTO_IP4);
+		set_action(&vlan0_list.act, DP_VLAN_ACT_PUSH, 0, 1);
+		set_tag(&vlan0_list.act, 0, 0, 0, 10, 0x8100, 0);
+		dp_vlan_set(&vlan, 0);
+
+	break;
+	case 8:
+		PR_INFO("Input:IP packet without vlan tag\n");
+		PR_INFO("Desc:pattern match = TRUE and PUSH action\n");
+		PR_INFO("Output:Enqueued packet is received with 2 vlan tags");
+		PR_INFO("that are pushed\n");
+		set_dev(&vlan, dev, 0, 0, 1, 0, 0);
+		vlan.vlan0_list = &vlan0_list;
+		set_pattern(&vlan0_list.outer, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_PROTO_IP4);
+		set_action(&vlan0_list.act, DP_VLAN_ACT_PUSH, 0, 2);
+		set_tag(&vlan0_list.act, 0, 0, 0, 10, 0x8100, 0);
+		set_tag(&vlan0_list.act, 0, 1, 1, 100, 0x8100, 0);
+		dp_vlan_set(&vlan, 0);
+
+	break;
+	case 9:
+		PR_INFO("Input:IP packet without vlan tag\n");
+		PR_INFO("Desc:pattern match = TRUE and PUSH action\n");
+		PR_INFO("Output:Enqueued packet is received with vlan tag");
+		PR_INFO("that is pushed\n");
+		set_dev(&vlan, dev, 0, 1, 1, 0, 0);
+		vlan.vlan0_list = &vlan0_list;
+		set_pattern(&vlan0_list.outer, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_PROTO_IP4);
+		set_action(&vlan0_list.act, DP_VLAN_ACT_PUSH, 0, 1);
+		set_tag(&vlan0_list.act, 0, 0, 0, 10, 0x8100, 0);
+		dp_vlan_set(&vlan, 0);
+
+	break;
+	case 10:
+		PR_INFO("Input:IP packet without vlan tag\n");
+		PR_INFO("Desc:pattern match = TRUE and PUSH action\n");
+		PR_INFO("Output:Enqueued packet is received with 2 vlan tags");
+		PR_INFO("that are pushed\n");
+		set_dev(&vlan, dev, 0, 1, 1, 0, 0);
+		vlan.vlan0_list = &vlan0_list;
+		set_pattern(&vlan0_list.outer, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_PROTO_IP4);
+		set_action(&vlan0_list.act, DP_VLAN_ACT_PUSH, 0, 2);
+		set_tag(&vlan0_list.act, 0, 0, 0, 10, 0x8100, 0);
+		set_tag(&vlan0_list.act, 0, 1, 1, 100, 0x8100, 0);
+		dp_vlan_set(&vlan, 0);
+	break;
+	case 11:
+		PR_INFO("Input:IP packet single vlan tag\n");
+		PR_INFO("Desc:pattern match = TRUE and PUSH action\n");
+		PR_INFO("Output:Enqueued packet is received with 2 vlan tags,");
+		PR_INFO("the original and the pushed one\n");
+		set_dev(&vlan, dev, 0, 0, 0, 1, 0);
+		vlan.vlan1_list = &vlan1_list;
+		set_pattern(&vlan1_list.outer, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_PROTO_IP4);
+		set_action(&vlan1_list.act, DP_VLAN_ACT_PUSH, 0, 1);
+		set_tag(&vlan1_list.act, CP_FROM_OUTER, 0, 0, 0, 0, 0);
+		dp_vlan_set(&vlan, 0);
+
+	break;
+	case 12:
+		PR_INFO("Input:IP packet single vlan tag\n");
+		PR_INFO("Desc:pattern match = TRUE and PUSH action\n");
+		PR_INFO("Output:Enqueued packet is received with 3 vlan tags,");
+		PR_INFO("the original and 2 pushed ones\n");
+		set_dev(&vlan, dev, 0, 0, 0, 1, 0);
+		vlan.vlan1_list = &vlan1_list;
+		set_pattern(&vlan1_list.outer, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_PROTO_IP4);
+		set_action(&vlan1_list.act, DP_VLAN_ACT_PUSH, 0, 2);
+		set_tag(&vlan1_list.act, CP_FROM_OUTER, 0, 0, 0, 0, 0);
+		set_tag(&vlan1_list.act, CP_FROM_OUTER, 1, 0, 0, 0, 0);
+		dp_vlan_set(&vlan, 0);
+
+	break;
+	case 13:
+		PR_INFO("Input:IP packet single vlan tag\n");
+		PR_INFO("Desc:pattern match = TRUE and PUSH action\n");
+		PR_INFO("Output:Enqueued packet is received with 2 vlan tags,");
+		PR_INFO("the original and the pushed one\n");
+		set_dev(&vlan, dev, 0, 1, 0, 1, 0);
+		vlan.vlan1_list = &vlan1_list;
+		set_pattern(&vlan1_list.outer, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_PROTO_IP4);
+		set_action(&vlan1_list.act, DP_VLAN_ACT_PUSH, 0, 1);
+		set_tag(&vlan1_list.act, CP_FROM_OUTER, 0, 0, 0, 0, 0);
+		dp_vlan_set(&vlan, 0);
+
+	break;
+	case 14:
+		PR_INFO("Input:IP packet single vlan tag\n");
+		PR_INFO("Desc:pattern match = TRUE and PUSH action\n");
+		PR_INFO("Output:Enqueued packet is received with 3 vlan tags");
+		PR_INFO("the original and 2 pushed ones\n");
+		set_dev(&vlan, dev, 0, 1, 0, 1, 0);
+		vlan.vlan1_list = &vlan1_list;
+		set_pattern(&vlan1_list.outer, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_PROTO_IP4);
+		set_action(&vlan1_list.act, DP_VLAN_ACT_PUSH, 0, 2);
+		set_tag(&vlan1_list.act, CP_FROM_OUTER, 0, 0, 0, 0, 0);
+		set_tag(&vlan1_list.act, CP_FROM_OUTER, 1, 0, 0, 0, 0);
+		dp_vlan_set(&vlan, 0);
+	break;
+	case 15:
+		PR_INFO("Input:IP packet single vlan tag and vid 10\n");
+		PR_INFO("Desc:pattern match = FALSE and DROP action\n");
+		PR_INFO("Output:Enqueued packet is received unaltered\n");
+		set_dev(&vlan, dev, 0, 0, 0, 1, 0);
+		vlan.vlan1_list = &vlan1_list;
+		set_pattern(&vlan1_list.outer, DP_VLAN_PATTERN_NOT_CARE,
+			    100, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_PROTO_IP4);
+		set_action(&vlan1_list.act, DP_VLAN_ACT_DROP, 0, 0);
+		dp_vlan_set(&vlan, 0);
+
+	break;
+	case 16:
+		PR_INFO("Input:IP packet single vlan tag and vid 10\n");
+		PR_INFO("Desc:pattern match = TRUE and DROP action\n");
+		PR_INFO("Output:Enqueued packet is dropped\n");
+		set_dev(&vlan, dev, 0, 0, 0, 1, 0);
+		vlan.vlan1_list = &vlan1_list;
+		set_pattern(&vlan1_list.outer, DP_VLAN_PATTERN_NOT_CARE,
+			    10, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_PROTO_IP4);
+		set_action(&vlan1_list.act, DP_VLAN_ACT_DROP, 0, 0);
+		dp_vlan_set(&vlan, 0);
+
+	break;
+	case 17:
+		PR_INFO("Input:IP packet double vlan tag, vid 10 and vid 20\n");
+		PR_INFO("Desc:pattern match = FALSE and DROP action\n");
+		PR_INFO("Output:Enqueued packet is received unaltered\n");
+		set_dev(&vlan, dev, 0, 0, 0, 0, 1);
+		vlan.vlan2_list = &vlan2_list;
+		set_pattern(&vlan2_list.outer, DP_VLAN_PATTERN_NOT_CARE,
+			    100, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_PROTO_IP4);
+		set_pattern(&vlan2_list.inner, DP_VLAN_PATTERN_NOT_CARE,
+			    200, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_PROTO_IP4);
+		set_action(&vlan2_list.act, DP_VLAN_ACT_DROP, 0, 0);
+		dp_vlan_set(&vlan, 0);
+
+	break;
+	case 18:
+		PR_INFO("Input:IP packet double vlan tag, vid 10 and vid 20\n");
+		PR_INFO("Desc:pattern match = TRUE and DROP action\n");
+		PR_INFO("Output:Enqueued packet is not received\n");
+		set_dev(&vlan, dev, 0, 0, 0, 0, 1);
+		vlan.vlan2_list = &vlan2_list;
+		set_pattern(&vlan2_list.outer, DP_VLAN_PATTERN_NOT_CARE,
+			    10, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_PROTO_IP4);
+		set_pattern(&vlan2_list.inner, DP_VLAN_PATTERN_NOT_CARE,
+			    20, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_PROTO_IP4);
+		set_action(&vlan2_list.act, DP_VLAN_ACT_DROP, 0, 0);
+		dp_vlan_set(&vlan, 0);
+
+	break;
+
+	case 19:
+	{
+		struct dp_vlan1 vlan1_list[4] = {0};
+
+		PR_INFO("Input:IP packet multiple streams single vlan tag,");
+		PR_INFO("with vid 5,6,7,8\n");
+		PR_INFO("Desc:pattern match = TRUE (vid match) , with action");
+		PR_INFO("PUSH for vid 5,6 POP for vid 7 and forward vid 8\n");
+		PR_INFO("Output:Enqueued packet received\n");
+		set_dev(&vlan, dev, 0, 0, 0, 4, 0);
+		vlan1_list[0].def = DP_VLAN_DEF_ACCEPT;
+		vlan1_list[1].def = DP_VLAN_DEF_ACCEPT;
+		vlan1_list[2].def = DP_VLAN_DEF_ACCEPT;
+		vlan1_list[3].def = DP_VLAN_DEF_DROP;
+		vlan.vlan1_list = vlan1_list;
+		set_pattern(&vlan1_list[0].outer, DP_VLAN_PATTERN_NOT_CARE,
+			    5, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_PROTO_IP4);
+		set_pattern(&vlan1_list[1].outer, DP_VLAN_PATTERN_NOT_CARE,
+			    6, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_PROTO_IP4);
+		set_pattern(&vlan1_list[2].outer, DP_VLAN_PATTERN_NOT_CARE,
+			    7, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_PROTO_IP4);
+		set_pattern(&vlan1_list[3].outer, DP_VLAN_PATTERN_NOT_CARE,
+			    8, DP_VLAN_PATTERN_NOT_CARE,
+			    DP_VLAN_PATTERN_NOT_CARE, DP_PROTO_IP4);
+		set_action(&vlan1_list[0].act, DP_VLAN_ACT_PUSH, 0, 1);
+		set_tag(&vlan1_list[0].act, 0, 0, 0, 0, 0x8100, 0);
+
+		set_action(&vlan1_list[1].act, DP_VLAN_ACT_PUSH, 0, 2);
+		set_tag(&vlan1_list[1].act, CP_FROM_OUTER, 0, 0, 0, 0x8100, 0);
+		set_tag(&vlan1_list[1].act, CP_FROM_OUTER, 1, 0, 0, 0x8100, 0);
+
+		set_action(&vlan1_list[2].act, DP_VLAN_ACT_POP, 1, 0);
+		set_action(&vlan1_list[3].act, DP_VLAN_ACT_FWD, 0, 0);
+
+		dp_vlan_set(&vlan, 0);
+	}
+	break;
+
+	default:
+		PR_INFO("unknown test case\n");
+	break;
+	}
+
+	PR_INFO("\n");
+	return count;
+
+proc_help:
+	PR_INFO("echo <dev> %s [-tnum %s]", "<device name>",
+		"test_number");
+	return count;
+}
+
+ssize_t proc_dt_write(struct file *file, const char *buf,
+		      size_t count, loff_t *ppos)
+{
+	u16 len;
+	char str[64];
+	char *param_list[20 * 2];
+	unsigned int num;
+	struct device_node *node;
+
+	len = (sizeof(str) > count) ? count : sizeof(str) - 1;
+	len -= copy_from_user(str, buf, len);
+	str[len] = 0;
+	num = dp_split_buffer(str, param_list, ARRAY_SIZE(param_list));
+	if (!param_list[0] || (strlen(param_list[0]) <= 1))
+		param_list[0] = "/";
+	node = of_find_node_by_path(param_list[0]);
+	if (!node)
+		node = of_find_node_by_name(NULL, param_list[0]);
+	if (!node) {
+		PR_INFO("Cannot find node for %s\n", param_list[0]);
+		return count;
+	}
+	print_device_tree_node(node, 0);
+	return count;
+}
+
+struct proc_vap_test {
+	struct net_device *dev;
+};
+
+struct vdev_priv {
+	int vap;
+	struct rtnl_link_stats64 stats;
+};
+
+static int vdev_open(struct net_device *dev)
+{
+	netif_start_queue(dev);
+	return 0;
+}
+
+static int vdev_close(struct net_device *dev)
+{
+	netif_stop_queue(dev);
+	return 0;
+}
+
+static int vdev_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+	dev_kfree_skb(skb);
+	return 0;
+}
+
+static int vdev_ioctl(struct net_device *dev,
+		      struct ifreq *ifr, int cmd)
+{
+	return -EOPNOTSUPP;
+}
+
+static int vdev_dev_init(struct net_device *dev)
+{
+	return 0;
+}
+
+static int vdev_set_mac_address(struct net_device *dev,
+				void *p)
+{
+	struct sockaddr *addr = p;
+
+	if (netif_running(dev))
+		return -EBUSY;
+
+	if (!is_valid_ether_addr(addr->sa_data))
+		return -EINVAL;
+
+	memcpy(dev->dev_addr, addr->sa_data, dev->addr_len);
+
+	return 0;
+}
+
+static int vdev_change_mtu(struct net_device *dev,
+			   int new_mtu)
+{
+	dev->mtu = new_mtu;
+	return 0;
+}
+
+static struct rtnl_link_stats64 *vdev_get_stats(
+		struct net_device *dev,
+		struct rtnl_link_stats64 *storage)
+{
+	struct vdev_priv *priv = netdev_priv(dev);
+	*storage = priv->stats;
+	return storage;
+}
+
+static struct net_device_ops vdev_ops = {
+	.ndo_init         = vdev_dev_init,
+	.ndo_open         = vdev_open,
+	.ndo_stop         = vdev_close,
+	.ndo_start_xmit   = vdev_xmit,
+	.ndo_do_ioctl     = vdev_ioctl,
+	.ndo_set_mac_address = vdev_set_mac_address,
+	.ndo_change_mtu   = vdev_change_mtu,
+	.ndo_get_stats64	  = vdev_get_stats,
+};
+
+static void vdev_setup(struct net_device *dev)
+{
+	ether_setup(dev);
+	dev->netdev_ops = &vdev_ops;
+}
+
+#define PROC_DP_MAX_VAP 256
+static struct proc_vap_test vaps[PROC_DP_MAX_VAP];
+int free_new_vap_dev(struct net_device *dev)
+{
+	int i;
+	dp_subif_t subif = {0};
+
+	if (!dev)
+		return 0;
+
+	/* Don't free it and DP may still using it for pmapper case */
+	if (dp_get_port_subitf_via_dev(dev, &subif) == DP_SUCCESS) {
+		PR_INFO("Don't free dev %s yet\n", dev->name);
+		return 0;
+	}
+	PR_INFO("Free dev %s\n", dev->name);
+	for (i = 0; i < ARRAY_SIZE(vaps); i++) {
+		if (vaps[i].dev != dev)
+			continue;
+		unregister_netdev(vaps[i].dev);
+		free_netdev(vaps[i].dev);
+		vaps[i].dev = NULL;
+		return i;
+	}
+	return -1;
+}
+
+struct net_device *create_new_vap_dev(char *name)
+{
+	int i;
+
+	if (!name)
+		return NULL;
+	for (i = 0; i < ARRAY_SIZE(vaps); i++) {
+		if (vaps[i].dev)
+			continue;
+		vaps[i].dev = alloc_netdev(sizeof(struct vdev_priv),
+					   name,
+					   NET_NAME_ENUM,
+					   vdev_setup);
+		strncpy(vaps[i].dev->name, name, sizeof(vaps[i].dev->name) - 1);
+		memset(vaps[i].dev->dev_addr, 0, sizeof(vaps[i].dev->dev_addr));
+		vaps[i].dev->dev_addr[5] = (u8)(i + 1);
+		if (register_netdev(vaps[i].dev)) {
+			free_netdev(vaps[i].dev);
+			vaps[i].dev = NULL;
+			PR_ERR("register device fail:%s?\n", name);
+		}
+		return vaps[i].dev;
+	}
+
+	return NULL;
+}
+
+int del_vap(char *param_list[], int num)
+{
+	u8 vap;
+	int dp_port;
+	dp_subif_t subif = {0};
+	struct pmac_port_info *port_info;
+	struct net_device *dev = NULL, *ctp_dev = NULL;
+	int inst = 0;
+	struct dp_subif_data data = {0};
+
+	if (num < 3) {
+		PR_ERR("Not enough parameters\n");
+		return -1;
+	}
+	/*dp_port */
+	dp_port = dp_atoi(param_list[1]);
+	if ((dp_port < 0) && (dp_port >= MAX_DP_PORTS)) {
+		PR_ERR("Wrong dp_port=%d\n", dp_port);
+		return -1;
+	}
+	if (!dp_port_info[inst][dp_port].status) {
+		PR_ERR("dp_port status=%d: expect non-zero\n",
+		       dp_port_info[0][dp_port].status);
+		return -1;
+	}
+	if (!dp_port_info[inst][dp_port].num_subif) {
+		PR_ERR("dp_port num_subif=%d: expect non-zero\n",
+		       dp_port_info[0][dp_port].num_subif);
+		return -1;
+	}
+	port_info = get_port_info_via_dp_port(inst, dp_port);
+	if (!port_info) {
+		PR_ERR("port_info NULL\n");
+		return -1;
+	}
+	/*vap */
+	vap = dp_atoi(param_list[2]);
+	if ((vap < 0) ||
+	    (vap >= port_info->ctp_max)) {
+		PR_ERR("Wrong vap=%d: expect 0 ~ %d\n",
+		       vap, port_info->ctp_max - 1);
+		return -1;
+	}
+	if (port_info->subif_info[vap].flags == 0) {
+		PR_ERR("VAP=%d not registered yet\n", vap);
+		return -1;
+	}
+	subif.subif = SET_VAP(vap,
+			      port_info->vap_offset,
+			      port_info->vap_mask);
+	dev = port_info->subif_info[vap].netif;
+	ctp_dev = port_info->subif_info[vap].ctp_dev;
+	/*de-register */
+	subif.inst = inst;
+	subif.port_id = dp_port;
+	if (dp_register_subif_ext(inst, port_info->owner, dev,
+				  dev->name, &subif, &data,
+				  DP_F_DEREGISTER)) {
+		PR_INFO("de-register_subif fail: %s\n", dev->name);
+		return -1;
+	}
+	if (ctp_dev && ctp_dev->name)
+		PR_INFO("\nDe-Register subif for %s ok: subif=0x%x(%d) %s:%s\n",
+			dev->name, subif.subif, vap,
+			"ctp_dev:", ctp_dev->name);
+	else
+		PR_INFO("\nDe-Register subif for %s ok: subif=0x%x(%d)\n",
+			dev->name, subif.subif, vap);
+
+	free_new_vap_dev(dev);
+	free_new_vap_dev(ctp_dev);
+	return 0;
+}
+
+#define DP_PROC_Q_AUTO_SHARE 0
+#define DP_PROC_Q_NEW_QUEUE  -1
+/* param_list[]: parameter list
+ * num: parameter list size
+ */
+int add_vap(char *param_list[], int num)
+{
+	struct dp_subif_data data = {0};
+	u8 vap;
+	int dp_port;
+	dp_subif_t subif = {0};
+	struct pmac_port_info *port_info;
+	char name[IFNAMSIZ] = {0};
+	struct net_device *dev = NULL;
+	int inst = 0;
+
+	if (num < 5) {
+		PR_ERR("Not enough parameters\n");
+		return -1;
+	}
+	/*dp_port */
+	dp_port = dp_atoi(param_list[1]);
+	if ((dp_port < 0) && (dp_port >= MAX_DP_PORTS)) {
+		PR_ERR("Wrong dp_port=%d\n", dp_port);
+		return -1;
+	}
+	if (!dp_port_info[inst][dp_port].status) {
+		PR_ERR("dp_port status=%d: expect non-zero\n",
+		       dp_port_info[0][dp_port].status);
+		return -1;
+	}
+	if (!dp_port_info[inst][dp_port].num_subif) {
+		PR_ERR("dp_port num_subif=%d: expect non-zero\n",
+		       dp_port_info[0][dp_port].num_subif);
+		return -1;
+	}
+	port_info = get_port_info_via_dp_port(inst, dp_port);
+	if (!port_info) {
+		PR_ERR("port_info NULL\n");
+		return -1;
+	}
+
+	/*vap */
+	vap = dp_atoi(param_list[2]);
+	if ((vap < 0) ||
+	    (vap >= port_info->ctp_max)) {
+		PR_ERR("Wrong vap=%d: expect 0 ~ %d\n",
+		       vap, port_info->ctp_max - 1);
+		return -1;
+	}
+	if (port_info->subif_info[vap].flags != 0) {
+		PR_ERR("VAP=%d already exist\n", vap);
+		return -1;
+	}
+	subif.subif = SET_VAP(vap,
+			      port_info->vap_offset,
+			      port_info->vap_mask);
+
+	/*tcont*/
+	data.deq_port_idx = dp_atoi(param_list[3]);
+	if ((data.deq_port_idx < 0) ||
+	    (data.deq_port_idx >= port_info->deq_port_num)) {
+		PR_ERR("Wrong tcont=%d: expect 0 ~ %d\n",
+		       data.deq_port_idx, port_info->deq_port_num - 1);
+		return -1;
+	}
+
+	/*qid */
+	if (dp_strncmpi(param_list[4], "def", 3) == 0) {/*sharing queue*/
+		data.flag_ops = 0;
+	} else if (dp_strncmpi(param_list[4], "new", 3) == 0) {
+		data.flag_ops = DP_SUBIF_AUTO_NEW_Q; /*alloc new queue */
+	} else {
+		data.q_id = dp_atoi(param_list[4]);
+		if (data.q_id > 0)
+			data.flag_ops = DP_SUBIF_SPECIFIC_Q; /*specify queue*/
+		else {
+			PR_ERR("Wrong Queue ID:%d\n", data.q_id);
+			return -1;
+		}
+	}
+	/*ctp-dev*/
+	if (param_list[5]) {
+		char ctp_name[IFNAMSIZ] = {0};
+		/* create ctp dev */
+		strncpy(name, param_list[5], sizeof(name) - 1);
+		snprintf(ctp_name, sizeof(ctp_name), "ctp%d_%d", dp_port, vap);
+		data.ctp_dev = create_new_vap_dev(ctp_name);
+		if (!data.ctp_dev) {
+			PR_ERR("failed to create dev %s\n", ctp_name);
+			goto ERR_EXIT;
+		}
+		/* parent dev */
+		rtnl_lock();
+		dev = dev_get_by_name(&init_net, param_list[5]);
+		if (!dev) {
+			rtnl_unlock();
+			snprintf(name, sizeof(name), "%s", param_list[5]);
+			/* need to create ctp parent later */
+			goto CREATE_DEV;
+		}
+		rtnl_unlock();
+		dev_put(dev);
+	}
+CREATE_DEV:
+	if (!dev) {
+		if (!name[0])
+			snprintf(name, sizeof(name), "vap%d_%x", dp_port, vap);
+		dev = create_new_vap_dev(name);
+		if (!dev) {
+			PR_ERR("failed to create dev %s\n", name);
+			goto ERR_EXIT;
+		}
+	}
+	/*register */
+	subif.inst = inst;
+	subif.port_id = dp_port;
+	if (dp_register_subif_ext(inst, port_info->owner, dev,
+				  dev->name, &subif, &data, 0)) {
+		PR_INFO("register_subif fail: %s 0x%x(vap=%d)\n",
+			dev->name, subif.subif, vap);
+		goto ERR_EXIT;
+	}
+	PR_INFO("\nRegister subif for %s ok: subif=0x%x(%d) %s\n",
+		dev->name,
+		subif.subif, vap,
+		data.ctp_dev ? data.ctp_dev->name : "");
+	return 0;
+ERR_EXIT:
+	if (dev) {
+		free_new_vap_dev(dev);
+		dev = NULL;
+	}
+	if (data.ctp_dev) {
+		free_new_vap_dev(data.ctp_dev);
+		data.ctp_dev = NULL;
+	}
+	return -1;
+}
+
+ssize_t proc_logical_dev_write(struct file *file, const char *buf,
+			       size_t count, loff_t *ppos)
+{
+	u16 len;
+	char str[64];
+	static char *param_list[20 * 4];
+	unsigned int num;
+	struct net_device *dev = NULL, *base = NULL;
+	dp_subif_t *subif = NULL;
+
+	struct pmac_port_info *port_info;
+	static struct vlan_prop vlan_prop;
+
+	memset(param_list, 0, sizeof(*param_list));
+	memset(&vlan_prop, 0, sizeof(vlan_prop));
+	len = (sizeof(str) > count) ? count : sizeof(str) - 1;
+	len -= copy_from_user(str, buf, len);
+	str[len] = 0;
+	num = dp_split_buffer(str, param_list, ARRAY_SIZE(param_list));
+	if (num <= 1)
+		goto HELP;
+	subif = kmalloc(sizeof(*subif), GFP_KERNEL);
+	if (!subif) {
+		PR_INFO("kmalloc failed for %d bytes\n", sizeof(*subif));
+		goto EXIT1;
+	}
+	if (dp_strncmpi(param_list[0], "check", strlen("check")) == 0) {
+		if (num != 2)
+			goto HELP;
+		dev = dev_get_by_name(&init_net, param_list[1]);
+		if (!dev) {
+			PR_INFO("No such device:%s\n", param_list[1]);
+			goto EXIT1;
+		}
+		get_vlan_via_dev(dev, &vlan_prop);
+		if (!vlan_prop.num) {
+			PR_INFO("No VLAN interface %s\n", param_list[1]);
+			goto EXIT1;
+		}
+		if (!vlan_prop.base) {
+			PR_INFO("No base found for %s\n", param_list[1]);
+			goto EXIT1;
+		}
+		if (vlan_prop.num > 0)
+			PR_INFO("outer VLAN proto=%x, vid=%d\n",
+				vlan_prop.out_proto, vlan_prop.out_vid);
+		if (vlan_prop.num == 2)
+			PR_INFO("Inner VLAN proto=%x, vid=%d\n",
+				vlan_prop.in_proto, vlan_prop.in_vid);
+		PR_INFO("base of %s: %s\n",
+			param_list[0], vlan_prop.base->name);
+		goto EXIT1;
+	} else if (dp_strncmpi(param_list[0], "set", strlen("set")) == 0) {
+		u32 flag = DP_F_SUBIF_LOGICAL;
+
+		if ((num != 2) && (num != 3))
+			goto HELP;
+		dev = dev_get_by_name(&init_net, param_list[1]);
+		if (!dev) {
+			PR_INFO("No such device:%s\n", param_list[1]);
+			goto EXIT1;
+		}
+		if (dp_strncmpi(param_list[2], "explicit", strlen("explicit")) == 0)
+			flag |= DP_F_ALLOC_EXPLICIT_SUBIFID;
+		base = get_base_dev(dev, -1);
+		if (!base) { /*not logical dev */
+			base = dev;
+			flag = 0;
+		}
+		if (dp_get_netif_subifid(base, NULL, NULL, NULL, subif, 0)) {
+			PR_INFO("dp_get_netif_subifid fail:%s\n", base->name);
+			goto EXIT1;
+		}
+		port_info = get_port_info_via_dp_port(0, subif->port_id);
+		if (!port_info) {
+			PR_INFO("get_port_info_via_dp_port fail: port_id:%d\n",
+				subif->port_id);
+			goto EXIT1;
+		}
+		subif->subif = -1;
+		if (dp_register_subif(port_info->owner, dev,
+				      dev->name, subif, flag)) {
+			PR_INFO("dp_register_subif fail: %s\n",
+				dev->name);
+			goto EXIT1;
+		}
+		PR_INFO("\nRegister subif for %s ok with %s\n",
+			param_list[1],
+			flag & DP_F_ALLOC_EXPLICIT_SUBIFID ?
+			"explicit" : "its base's subif/ctp");
+	} else if (dp_strncmpi(param_list[0], "unset", strlen("unset")) == 0) {
+		u32 flag = DP_F_DEREGISTER;
+
+		if (num != 2)
+			goto HELP;
+		dev = dev_get_by_name(&init_net, param_list[1]);
+		if (!dev) {
+			PR_INFO("No such device:%s\n", param_list[1]);
+			goto EXIT1;
+		}
+		if (dp_get_netif_subifid(dev, NULL, NULL, NULL, subif, 0)) {
+			PR_INFO("dp_get_netif_subifid fail:%s\n", dev->name);
+			goto EXIT1;
+		}
+		port_info = get_port_info_via_dp_port(0, subif->port_id);
+		if (!port_info) {
+			PR_INFO("get_port_info_via_dp_port fail: port_id:%d\n",
+				subif->port_id);
+			goto EXIT1;
+		}
+		if (dp_register_subif(port_info->owner, dev,
+				      dev->name, subif, flag)) {
+			PR_INFO("dp_deregister_subif fail: %s\n",
+				dev->name);
+			goto EXIT1;
+		}
+		PR_INFO("Deregister subif for %s ok with subif=%d\n",
+			param_list[1],
+			subif->subif);
+	} else if (dp_strncmpi(param_list[0], "get", strlen("get")) == 0) {
+		u32 flag = 0;
+		int i;
+
+		if (num < 2)
+			goto HELP;
+		dev = dev_get_by_name(&init_net, param_list[1]);
+		if (!dev) {
+			PR_INFO("No such device:%s\n", param_list[1]);
+			goto EXIT1;
+		}
+		subif->port_id = -1;
+		if (dp_get_netif_subifid(dev, NULL, NULL, NULL, subif,
+					 flag)) {
+			PR_INFO("dp_get_netif_subifid fail: %s\n",
+				dev->name);
+			goto EXIT1;
+		}
+		PR_INFO("dev %s subif info:\n", dev->name);
+		PR_INFO("  inst=%d ep=%d bp=%d flag_bp=%u\n",
+			subif->inst, subif->port_id, subif->bport,
+			subif->flag_bp);
+		for (i = 0; i < subif->subif_num; i++)
+			PR_INFO("  subif[%d]=%d\n",
+				i, subif->subif_list[i]);
+		PR_INFO("\n");
+		goto EXIT1;
+	} else if (dp_strncmpi(param_list[0], "add_v", strlen("add_v")) == 0) {
+		add_vap(param_list, num);
+		goto EXIT1;
+	} else if (dp_strncmpi(param_list[0], "del_v", strlen("del_v")) == 0) {
+		del_vap(param_list, num);
+		goto EXIT1;
+	} else {
+		PR_ERR("Wrong cmd:%s\n", param_list[0]);
+		goto EXIT1;
+	}
+EXIT1:
+	if (dev)
+		dev_put(dev);
+
+	kfree(subif);
+	return count;
+
+HELP:
+	PR_INFO("Check Base Dev:echo check logic_dev_name > /proc/dp/%s\n",
+		PROC_LOGICAL_DEV);
+	PR_INFO("Register Logical dev:echo set logic_dev [explicit] > %s/%s\n",
+		DP_PROC_BASE, PROC_LOGICAL_DEV);
+	PR_INFO("DeRegister Logical dev:echo unset logic_dev_name > %s/%s\n",
+		DP_PROC_BASE, PROC_LOGICAL_DEV);
+	PR_INFO("Get dp_subif info:echo get dev_name > /proc/dp/%s\n",
+		PROC_LOGICAL_DEV);
+	PR_INFO("Add vap:echo add_v <dp_port> <vap> <tcont> <qid> %s\n",
+		"<ctp_parent_dev> > " DP_PROC_BASE PROC_LOGICAL_DEV);
+	PR_INFO("   Note of qid:\n");
+	PR_INFO("     default  : auto share queue(default handling)\n");
+	PR_INFO("     new_queue: auto alloc new queue\n");
+	PR_INFO("     value(>0): specified queue id by caller.\n");
+	PR_INFO("   Note of vap: 0, 1, 2, 3,....\n");
+	PR_INFO("   Note of ctp_dev: for PON pmapper case\n");
+	PR_INFO("Del vap:echo del_v <dp_port> <vap> %s/%s\n",
+		DP_PROC_BASE, PROC_LOGICAL_DEV);
+	if (dev)
+		dev_put(dev);
+
+	kfree(subif);
+	return count;
+}
+
+struct property_info {
+	char *name;
+	int type;
+};
+
+enum PROPERTY_TYPE {
+	PROP_UNKNOWN = 0,
+	PROP_STRING,
+	PROP_REG,
+	PROP_RANGER,
+	PROP_U32_OCT,
+	PROP_U32_HEX,
+	PROP_HANDLE,
+	PROP_REFERENCE
+};
+
+struct property_info prop_info[] = {
+	{"compatible", PROP_STRING},
+	{"status", PROP_STRING},
+	{"name", PROP_STRING},
+	{"label", PROP_STRING},
+	{"model", PROP_STRING},
+	{"reg-names", PROP_STRING},
+	{"reg", PROP_REG},
+	{"interrupts", PROP_U32_OCT},
+	{"ranges", PROP_RANGER},
+	{"dma-ranges", PROP_RANGER},
+	{"phandle", PROP_HANDLE},
+	{"interrupt-parent", PROP_HANDLE}
+
+};
+
+int get_property_info(char *name)
+{
+	int i;
+
+	if (!name)
+		return PROP_UNKNOWN;
+	for (i = 0; i < ARRAY_SIZE(prop_info); i++) {
+		if (dp_strncmpi(name, prop_info[i].name, strlen(prop_info[i].name)) == 0)
+			return prop_info[i].type;
+	}
+
+	return PROP_UNKNOWN;
+}
+
+/*0--not string
+ *1--is string
+ */
+/*#define LOCAL_STRING_PARSE*/
+int is_print_string(char *p, int len)
+{
+	int i;
+
+	if (!p || !len)
+		return 0;
+	if (p[len - 1] != 0)
+		return 0;
+	for (i = 0; i < len - 1; i++) {
+#ifdef LOCAL_STRING_PARSE
+		if (!(((p[i] >= 'a') && (p[i] <= 'z')) ||
+		      ((p[i] >= 'A') && (p[i] <= 'Z')) ||
+		      ((p[i] >= '0') && (p[i] <= '9')) ||
+		      (p[i] == '.') ||
+		      (p[i] == '/')))
+#else
+		if (!isprint(p[i]) && p[i] != 0) /*string list */
+#endif
+			return 0;
+	}
+	if (p[0] == 0)
+		return 0;
+
+	return 1;
+}
+
+#define INDENT_BASE 3 /*3 Space */
+void print_property(struct device_node *node, struct property *p, char *indent)
+{
+	int type;
+	int k, times, i;
+
+	if (!p || !node)
+		return;
+	type = get_property_info(p->name);
+	if (type == PROP_UNKNOWN) {
+		if (is_print_string(p->value, p->length))
+			type = PROP_STRING;
+		else if ((p->length % 4) == 0)
+			type = PROP_U32_OCT;
+	}
+	if (type == PROP_STRING) {
+		char *s = (char *)p->value;
+		int k = 0;
+
+		PR_INFO("%s  %s=", indent, p->name);
+		do {
+			PR_INFO("\"%s\"", s);
+			k += strlen(s) + 1;
+			if (k < p->length) {
+				s += strlen(s) + 1;
+				PR_INFO(",");
+				continue;
+			}
+			PR_INFO("\n");
+			break;
+		} while (1);
+	} else if (type == PROP_U32_OCT) { /*each item is 4 bytes*/
+		PR_INFO("%s  %s=<", indent, p->name);
+		times = p->length / 4;
+		if (times) {
+			for (k = 0; k < times; k++)
+				PR_INFO("%d ", *(int *)(p->value + k * 4));
+		}
+		PR_INFO(">\n");
+	} else if (type == PROP_U32_HEX) { /*each item is 4 bytes*/
+		PR_INFO("%s  %s=<", indent, p->name);
+		times = p->length / 4;
+		if (times) {
+			for (k = 0; k < times; k++)
+				PR_INFO("0x%x ", *(int *)(p->value + k * 4));
+		}
+		PR_INFO(">\n");
+	} else if (type == PROP_REG) {/*two tuple: address and size */
+		int n = (of_n_addr_cells(node) + of_n_size_cells(node));
+		int j;
+
+		PR_INFO("%s  %s=<", indent, p->name);
+		times = p->length / (4 * n);
+		if (times) {
+			for (k = 0; k < times; k++) {
+				if (k)
+					PR_INFO("%s    ", indent);
+				for (j = 0; j < n; j++)
+					PR_INFO("0x%x ",
+						*(int *)(p->value + k * 8 +
+						4 * j));
+				if (k != (times - 1))
+					PR_INFO("\n");
+			}
+		}
+		PR_INFO(">\n");
+	} else if (type == PROP_RANGER) {
+		/*triple: child-bus-address, parent-bus-address, length */
+		PR_INFO("%s  %s=<", indent, p->name);
+		times = p->length / (4 * 3);
+		if (times) {
+			for (k = 0; k < times; k++) {
+				if (!k)
+					PR_INFO("0x%x 0x%x 0x%x",
+						*(int *)(p->value + k * 8),
+						*(int *)(p->value + k * 8 + 4),
+						*(int *)(p->value + k * 8 + 8));
+				else
+					PR_INFO("%s    0x%x 0x%x 0x%x", indent,
+						*(int *)(p->value + k * 8),
+						*(int *)(p->value + k * 8 + 4),
+						*(int *)(p->value + k * 8 + 8));
+				if (k != (times - 1))
+					PR_INFO("\n");
+			}
+		}
+		PR_INFO(">\n");
+	} else if (type == PROP_HANDLE) {
+		struct device_node *tmp = of_find_node_by_phandle(
+			be32_to_cpup((u32 *)p->value));
+		int offset = 0;
+
+		if (tmp) {
+			PR_INFO("%s  %s=<&%s", indent, p->name, tmp->name);
+			offset = 1;
+		} else {
+			PR_INFO("%s  %s=<", indent, p->name);
+		}
+		if (p->length >= 4) {
+			int times = p->length / 4;
+
+			if (times) {
+				for (k = offset; k < times; k++)
+					PR_INFO("%d ",
+						*(int *)(p->value + k * 4));
+			}
+		}
+		PR_INFO(">\n");
+	} else {
+		PR_INFO("%s  %s length=%d\n", indent, p->name, p->length);
+		if (p->length) {
+			char *s = (unsigned char *)p->value;
+
+			PR_INFO("%s   ", indent);
+			for (i = 0; i < p->length; i++)
+				PR_INFO("0x%02x ", s[i]);
+			PR_INFO("\n");
+		}
+	}
+}
+
+void print_device_tree_node(struct device_node *node, int depth)
+{
+	int i = 0, len;
+	struct device_node *child;
+	struct property *p;
+	char *indent;
+
+	if (!node)
+		return;
+	len = (depth + 1) * 3;
+	indent = kmalloc(len, GFP_KERNEL);
+	if (!indent)
+		return;
+	for (i = 0; i < depth * 3; i++)
+		indent[i] = ' ';
+	indent[i] = '\0';
+	++depth;
+	PR_INFO("%s{%s(%s)_cell addr/size=%d/%d\n",
+		indent, node->name, node->full_name,
+		of_n_addr_cells(node), of_n_size_cells(node));
+	for_each_property_of_node(node, p)
+		print_property(node, p, indent);
+	for_each_child_of_node(node, child)
+		print_device_tree_node(child, depth);
+
+	PR_INFO("%s}\n", indent);
+	kfree(indent);
+}
+
+static u8 ipv4_plain_udp[] = {
+	0x00, 0x01, 0x01, 0x01, 0x01, 0x01, /*mac */
+	0x00, 0x10, 0x94, 0x00, 0x00, 0x02,
+	0x08, 0x00,	/*type */
+	0x45, 0x00, 0x00, 0x3E, 0x00, 0x00, 0x00, 0x00, 0xFF, 0x11, /*ip hdr*/
+	0x3A, 0x56, 0xC0, 0x55, 0x01, 0x02, 0xC0, 0x00, 0x00, 0x01,
+	0x04, 0x00, 0x00, 0x00, 0x00, 0x2A, 0x7A, 0x41, 0x00, 0x00, /*udp hdr*/
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00
+};
+
+static u8 ipv4_plain_tcp[1514] = {
+	0x00, 0x01, 0x01, 0x01, 0x01, 0x01, /*mac */
+	0x00, 0x10, 0x94, 0x00, 0x00, 0x02,
+	0x08, 0x00,	/*type */
+	0x45, 0x00, 0x00, 0x3E, 0x00, 0x00, 0x00, 0x00, 0xFF, 0x06, /*ip hdr*/
+	0x3A, 0x61, 0xC0, 0x55, 0x01, 0x02, 0xC0, 0x00, 0x00, 0x01,
+	0x04, 0x00, 0x04, 0x00, 0x00, 0x01, 0xE2, 0x40, 0x00, 0x03, /*tcp hdr*/
+	0x94, 0x47, 0x50, 0x10, 0x10, 0x00, 0x9F, 0xD9, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, /*data */
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00
+};
+
+/*flag: for dp_xmit
+ *pool: 1-CPU
+ *discard: discard bit in DMA descriptor, CQM HW will drop the packet
+ *priority: for skb->priority
+ */
+static int dp_send_packet(u8 *pdata, int len, char *devname, u32 flag,
+			  u32 pool, u32 discard, int priority, int color)
+{
+	struct sk_buff *skb;
+	dp_subif_t subif = { 0 };
+	#define PAD  32
+
+	if (pool == 1) {
+		len += PAD;
+		if (len < ETH_ZLEN)
+			len = ETH_ZLEN;
+		skb = alloc_skb(len, GFP_ATOMIC);
+		if (skb)
+			skb_reserve(skb, PAD);
+		PR_INFO_ONCE("Allocate CPU buffer\n");
+	} else {
+		skb = cbm_alloc_skb(len + 8, GFP_ATOMIC);
+		PR_INFO_ONCE("Allocate bm buffer\n");
+	}
+
+	if (unlikely(!skb)) {
+		PR_ERR("allocate cbm buffer fail\n");
+		return -1;
+	}
+
+	skb->DW0 = 0;
+	skb->DW1 = 0;
+	skb->DW2 = 0;
+	skb->DW3 = 0;
+	if (discard) {
+		((struct dma_tx_desc_3 *)&skb->DW3)->field.dic = 1;
+		PR_INFO_ONCE("discard bit set in DW3\n");
+	}
+	((struct dma_tx_desc_1 *)&skb->DW1)->field.color = color;
+	memcpy(skb->data, pdata, len);
+	skb->data[5] = (char)priority;
+	skb->len = 0;
+	skb_put(skb, len);
+	skb->dev = dev_get_by_name(&init_net, devname);
+	skb->priority = priority;
+
+	if (dp_get_netif_subifid(skb->dev, skb, NULL, skb->data, &subif, 0)) {
+		PR_ERR("dp_get_netif_subifid failed for %s\n",
+		       skb->dev->name);
+		dev_kfree_skb_any(skb);
+		return -1;
+	}
+
+	((struct dma_tx_desc_1 *)&skb->DW1)->field.ep = subif.port_id;
+	((struct dma_tx_desc_0 *)&skb->DW0)->field.dest_sub_if_id =
+		subif.subif;
+
+	if (dp_xmit(skb->dev, &subif, skb, skb->len, flag))
+		return -1;
+
+	return 0;
+}
+
+#define OPT_TX_DEV "[-i <dev_name>]"
+#define OPT_TX_NUM "[-n <pkt_num>]"
+#define OPT_TX_PROT "[-t <types: udp/tcp/raw>]"
+#define OPT_TX_POOL "[-p <pool:cpu/bm>]"
+#define OPT_TX_F_DISC "[-d]"
+#define OPT_TX_PRIO "[-c <class/prio range>]"
+#define OPT_TX_COLOR "[-o <color>]"
+#define OPT_TX1 (OPT_TX_DEV OPT_TX_NUM OPT_TX_PROT)
+#define OPT_TX2 (OPT_TX_POOL OPT_TX_F_DISC OPT_TX_PRIO OPT_TX_COLOR)
+#define OPT_TX (OPT_TX1 OPT_TX2)
+char l2_hdr[] = {0xff, 0xff, 0xff, 0xff, 0xff, 0xff,
+		 0x01, 0x01, 0x01, 0x01, 0x01, 0x11,
+		 0x12, 0x34};
+ssize_t proc_tx_pkt(struct file *file, const char *buf,
+		    size_t count, loff_t *ppos)
+{
+	int len = 0, c;
+	char data[100];
+	char *param_list[10];
+	int num, pkt_num = 1, times, pool = 0, disc = 0;
+	char *p = ipv4_plain_udp;
+	int size = sizeof(ipv4_plain_udp);
+	short prio_range = 1, color = 0;
+	int opt_offset;
+	char *optstring = "i:n:t:p:dc:o:";
+	char *optarg = 0;
+	char *dev_name = "eth1";
+
+	len = (count >= sizeof(data)) ? (sizeof(data) - 1) : count;
+	DP_DEBUG(DP_DBG_FLAG_DBG, "len=%d\n", len);
+
+	if (len <= 0) {
+		PR_ERR("Wrong len value (%d)\n", len);
+		return count;
+	}
+	if (copy_from_user(data, buf, len)) {
+		PR_ERR("copy_from_user fail");
+		return count;
+	}
+	data[len - 1] = 0; /* Make string */
+	num = dp_split_buffer(data, param_list, ARRAY_SIZE(param_list));
+	if (num <= 1)
+		goto help;
+	opt_offset = 0;
+	while ((c = dp_getopt(param_list, num,
+			      &opt_offset, &optarg, optstring)) > 0) {
+		if (optstring)
+			DP_DEBUG(DP_DBG_FLAG_DBG, "opt_offset=%d optarg=%s.\n",
+				 opt_offset, optarg);
+		switch (c) {
+		case 'i':
+			dev_name = optarg;
+			DP_DEBUG(DP_DBG_FLAG_DBG, "dev_name=%s\n", dev_name);
+			break;
+		case 'n':
+			pkt_num = dp_atoi(optarg);
+			PR_INFO("pkt_num=%d\n", pkt_num);
+			break;
+		case 't':
+			if (dp_strncmpi(optarg, "tcp", strlen("tcp")) == 0) {
+				p = ipv4_plain_tcp;
+				size = sizeof(ipv4_plain_tcp);
+			} else if (dp_strncmpi(optarg, "udp", strlen("udp")) == 0) {
+				p = ipv4_plain_udp;
+				size = sizeof(ipv4_plain_udp);
+			} else  {
+				PR_INFO("Wrong procol selected\n");
+				return count;
+			}
+			break;
+		case 'p':
+			if (dp_strncmpi(optarg, "cpu", strlen("cpu")) == 0) {
+				pool = 1;
+			} else if (dp_strncmpi(optarg, "bm", strlen("bm")) == 0) {
+				pool = 0;
+			} else  {
+				PR_INFO("Wrong procol selected\n");
+				return count;
+			}
+			break;
+		case 'd':
+			disc = 1;
+			break;
+		case 'c':
+			prio_range = dp_atoi(optarg);
+			if (prio_range <= 0)
+				prio_range = 1;
+			PR_INFO("prio_range=%d\n", prio_range);
+			break;
+		case 'o':
+			color = dp_atoi(optarg);
+			if (color < 0)
+				color = 0;
+			else if (color > 3)
+				color = 3;
+			PR_INFO("color=%d\n", color);
+			break;
+		default:
+			PR_INFO("Wrong command\n");
+			goto help;
+		}
+	}
+	if (c < 0) { /*c == 0 means reach end of list */
+		PR_INFO("Wrong command\n");
+		goto help;
+	}
+
+	if (!dev_name) { /*c == 0 means reach end of list */
+		PR_INFO("Must provide dev_name\n");
+		return count;
+	}
+
+	times = 0;
+	while (pkt_num--) {
+		if (dp_send_packet(p, size, dev_name, 0, pool, disc,
+				   times % prio_range, color))
+			break;
+		times++;
+	}
+	PR_INFO("sent %d packet already\n", times);
+	return count;
+help:   /*                        [0]         [1]         [2]     [3] [4]*/
+	PR_INFO("tx packet: echo %s\n", OPT_TX1);
+	PR_INFO("                %s\n", OPT_TX2);
+	return count;
+}
+
+static struct dp_proc_entry dp_proc_entries[] = {
+	/*name single_callback_t multi_callback_t/_start write_callback_t */
+#if defined(CONFIG_LTQ_DATAPATH_DBG) && CONFIG_LTQ_DATAPATH_DBG
+	{PROC_DBG, proc_dbg_read, NULL, NULL, proc_dbg_write},
+#endif
+	{PROC_PORT, NULL, proc_port_dump, proc_port_init, proc_port_write},
+	{PROC_MEM, NULL, NULL, NULL, proc_write_mem},
+	{PROC_DT, NULL, NULL, NULL, proc_dt_write},
+	{PROC_LOGICAL_DEV, NULL, NULL, NULL, proc_logical_dev_write},
+	{PROC_INST_DEV, NULL, proc_inst_dev_dump, proc_inst_dev_start, NULL},
+	{PROC_INST_MOD, NULL, proc_inst_mod_dump, proc_inst_mod_start, NULL},
+	{PROC_INST_HAL, NULL, proc_inst_hal_dump, NULL, NULL},
+	{PROC_INST, NULL, proc_inst_dump, NULL, NULL},
+	{PROC_TX_PKT, NULL, NULL, NULL, proc_tx_pkt},
+	{PROC_QOS, NULL, qos_dump, qos_dump_start, proc_qos_write},
+	{PROC_ASYM_VLAN, NULL, NULL, NULL, proc_asym_vlan},
+
+	/*the last place holder */
+	{NULL, NULL, NULL, NULL, NULL}
+};
+
+struct dentry *dp_proc_node;
+EXPORT_SYMBOL(dp_proc_node);
+
+struct dentry *dp_proc_install(void)
+{
+	dp_proc_node = debugfs_create_dir(DP_PROC_PARENT DP_PROC_NAME, NULL);
+
+	if (dp_proc_node) {
+		int i;
+
+		for (i = 0; i < ARRAY_SIZE(dp_proc_entries); i++)
+			dp_proc_entry_create(dp_proc_node,
+					     &dp_proc_entries[i]);
+	} else {
+		PR_ERR("datapath cannot create proc entry");
+		return NULL;
+	}
+
+	return dp_proc_node;
+}
+
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_proc_api.c b/drivers/net/ethernet/lantiq/datapath/datapath_proc_api.c
new file mode 100644
index 000000000000..9358835e13da
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/datapath_proc_api.c
@@ -0,0 +1,317 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include <linux/fs.h>
+#include<linux/slab.h>
+#include <linux/kernel.h>
+#include <net/datapath_proc_api.h>
+#include <net/datapath_api.h>
+#include "datapath.h"
+
+#undef local_dbg
+#define local_dbg(fmt, arg...) pr_debug(fmt, ##arg)
+int dp_strncmpi(const char *s1, const char *s2, size_t n)
+{
+	if (!s1 || !s2)
+		return 1;
+	return strncasecmp(s1, s2, n);
+}
+EXPORT_SYMBOL(dp_strncmpi);
+
+void dp_replace_ch(char *p, int len, char orig_ch, char new_ch)
+{
+	int i;
+
+	if (p)
+		for (i = 0; i < len; i++) {
+			if (p[i] == orig_ch)
+				p[i] = new_ch;
+		}
+}
+EXPORT_SYMBOL(dp_replace_ch);
+
+int dp_atoi(unsigned char *str)
+{
+	long long v = 0;
+	char *p = NULL;
+	int res;
+
+	if (!str)
+		return v;
+	dp_replace_ch(str, strlen(str), '.', 0);
+	dp_replace_ch(str, strlen(str), ' ', 0);
+	dp_replace_ch(str, strlen(str), '\r', 0);
+	dp_replace_ch(str, strlen(str), '\n', 0);
+	if (str[0] == 0)
+		return v;
+	if (str[0] == 'b' || str[0] == 'B') {
+		p = str + 1;
+		res = kstrtoll(p, 2, &v); /* binary */
+	} else if ((str[0] == '0') && ((str[1] == 'x') || (str[1] == 'X'))) {
+		p = str + 2;
+		res = kstrtoll(p, 16, &v); /* hex */
+	} else {
+		p = str;
+		res = kstrtoll(p, 10, &v); /* dec */
+	}
+	if (res)
+		v = 0;
+	return v;
+}
+EXPORT_SYMBOL(dp_atoi);
+
+/*Split buffer to multiple segment with seperater space.
+ *And put pointer to array[].
+ *By the way, original buffer will be overwritten with '\0' at some place.
+ */
+int dp_split_buffer(char *buffer, char *array[], int max_param_num)
+{
+	int i = 0;
+
+	if (!array)
+		return 0;
+	memset(array, 0, sizeof(array[0]) * max_param_num);
+	if (!buffer)
+		return 0;
+	while ((array[i] = strsep(&buffer, " ")) != NULL) {
+		size_t len = strlen(array[i]);
+		dp_replace_ch(array[i], len, ' ', 0);
+		dp_replace_ch(array[i], len, '\r', 0);
+		dp_replace_ch(array[i], len, '\n', 0);
+		len = strlen(array[i]);
+		if (!len)
+			continue;
+		i++;
+		if (i == max_param_num)
+			break;
+	}
+
+	return i;
+}
+EXPORT_SYMBOL(dp_split_buffer);
+
+void set_start_end_id(unsigned int new_start, unsigned int new_end,
+		      unsigned int max_start, unsigned int max_end,
+		      unsigned int default_start, unsigned int default_end,
+		      unsigned int *start, unsigned int *end)
+{
+	if (!start || !end)
+		return;
+
+	if (new_start > new_end) {
+		*start = default_start;
+		*end = default_end;
+	} else {
+		*start = new_start;
+		*end = new_end;
+	}
+
+	if (*start > max_start)
+		*start = default_start;
+
+	if (*end > max_end)
+		*end = default_end;
+}
+EXPORT_SYMBOL(set_start_end_id);
+
+/* cmd: command line, its format should be like
+ * cmd_len: the length of command line
+ * optcurser:
+ */
+int dp_getopt(char *cmd[], int cmd_size, int *cmd_offset,
+	      char **optarg, const char *optstring)
+{
+	char *p;
+	int offset;
+	int i;
+
+	if (!cmd || !cmd_offset || !optstring || !optarg)
+		return -1;
+	if (*cmd_offset >= cmd_size)
+		return 0;
+	offset = *cmd_offset;
+	while (1) {
+		p = cmd[offset];
+		if (p[0] != '-') {
+			offset++;
+			return -1; /*wrong format*/
+		}
+		for (i = 0; i < strlen(optstring); i++) {
+			if (optstring[i] != p[1])
+				continue;
+			/* match */
+			if (optstring[i + 1] == ':') { /*opt + value */
+				if (offset + 1 > cmd_size)
+					return -1;
+				*optarg = cmd[offset + 1];
+				offset += 2;
+			} else { /*no value */
+				*optarg = NULL;
+				offset += 1;
+			}
+			*cmd_offset = offset;
+			return (int)optstring[i];
+		}
+		return -1;
+	}
+	return -1;
+}
+EXPORT_SYMBOL(dp_getopt);
+
+static void *dp_seq_start(struct seq_file *s, loff_t *pos)
+{
+	struct dp_proc_file_entry *p = s->private;
+
+	if (p->pos < 0)
+		return NULL;
+
+	return p;
+}
+
+static void *dp_seq_next(struct seq_file *s, void *v, loff_t *pos)
+{
+	struct dp_proc_file_entry *p = s->private;
+
+	*pos = p->pos;
+
+	if (p->pos >= 0)
+		return p;
+	else
+		return NULL;
+}
+
+static void dp_seq_stop(struct seq_file *s, void *v)
+{
+}
+
+static int dp_seq_show(struct seq_file *s, void *v)
+{
+	struct dp_proc_file_entry *p = s->private;
+
+	if (p->pos >= 0) {
+		if (p->multi_callback) {
+			local_dbg("multiple call");
+			p->pos = p->multi_callback(s, p->pos);
+		} else if (p->single_callback) {
+			local_dbg("single call: %p", p->single_callback);
+			p->single_callback(s);
+			p->pos = -1;
+		}
+	}
+	return 0;
+}
+
+static const struct seq_operations dp_seq_ops = {
+	.start = dp_seq_start,
+	.next = dp_seq_next,
+	.stop = dp_seq_stop,
+	.show = dp_seq_show
+};
+
+void dummy_single_show(struct seq_file *s)
+{
+	seq_puts(s, "Cat Not implemented yet !\n");
+}
+
+static int dp_proc_open(struct inode *inode, struct file *file)
+{
+	struct seq_file *s;
+	struct dp_proc_file_entry *p;
+	struct dp_proc_entry *entry;
+	int ret;
+
+	ret = seq_open(file, &dp_seq_ops);
+	if (ret)
+		return ret;
+
+	s = file->private_data;
+	p = kmalloc(sizeof(*p), GFP_KERNEL);
+
+	if (!p) {
+		(void)seq_release(inode, file);
+		return -ENOMEM;
+	}
+	memset(p, 0, sizeof(*p));
+
+	entry = inode->i_private;
+
+	if (entry->multi_callback)
+		p->multi_callback = entry->multi_callback;
+	if (entry->single_callback)
+		p->single_callback = entry->single_callback;
+	else
+		p->single_callback = dummy_single_show;
+
+	if (entry->init_callback)
+		p->pos = entry->init_callback();
+	else
+		p->pos = 0;
+
+	s->private = p;
+
+	return 0;
+}
+
+static int dp_proc_release(struct inode *inode, struct file *file)
+{
+	struct seq_file *s;
+
+	s = file->private_data;
+	kfree(s->private);
+
+	return seq_release(inode, file);
+}
+
+static int dp_seq_single_show(struct seq_file *s, void *v)
+{
+	struct dp_proc_entry *p = s->private;
+
+	p->single_callback(s);
+	return 0;
+}
+
+static int dp_proc_single_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, dp_seq_single_show, inode->i_private);
+}
+
+#define MEM_ZERO(x, y) memset(&(x), 0, y)
+void dp_proc_entry_create(struct dentry *parent_node,
+			  struct dp_proc_entry *proc_entry)
+{
+	struct dentry *file;
+
+	if (!proc_entry || !proc_entry->name)
+		return;
+	MEM_ZERO(proc_entry->ops, sizeof(proc_entry->ops));
+	proc_entry->ops.owner = THIS_MODULE;
+
+	if (proc_entry->single_callback) {
+		proc_entry->ops.open = dp_proc_single_open;
+		proc_entry->ops.release = single_release;
+	} else if (proc_entry->multi_callback) {
+		proc_entry->ops.open = dp_proc_open;
+		proc_entry->ops.release = dp_proc_release;
+	} else { /*regard as single call with dummy show*/
+		proc_entry->ops.open = dp_proc_single_open;
+		proc_entry->ops.release = single_release;
+		proc_entry->single_callback = dummy_single_show;
+	}
+
+	proc_entry->ops.read = seq_read;
+	proc_entry->ops.llseek = seq_lseek;
+	proc_entry->ops.write = proc_entry->write_callback;
+	/*Don't know why checkpatch propose to use 0444
+	 *Instead of S_IRUGO ???
+	 */
+	file = debugfs_create_file(proc_entry->name, 0644/*(S_IFREG | 0444)*/,
+				   parent_node,
+				   proc_entry, &proc_entry->ops);
+}
+EXPORT_SYMBOL(dp_proc_entry_create);
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_proc_qos.c b/drivers/net/ethernet/lantiq/datapath/datapath_proc_qos.c
new file mode 100644
index 000000000000..1166a29ca0ea
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/datapath_proc_qos.c
@@ -0,0 +1,1464 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include <net/datapath_api.h>
+#include <net/datapath_proc_api.h>
+#include "datapath.h"
+
+#define PROC_MAX_BOX_LVL (DP_MAX_SCH_LVL + 1) /* Sched/Port both map to a box */
+/* max direct child per scheduler/port */
+#define PROC_DP_MAX_CHILD_PER_SCH_PORT DP_MAX_CHILD_PER_NODE
+#define PROC_MAX_Q_PER_PORT 16 /* max queues per port */
+#define PROC_DP_MAX_SCH_PER_PORT 4 /* max schedulers per port */
+#define PROC_DP_MAX_LEAF 8 /* max leaf per scheduler */
+
+struct location {
+	int x1, y1; /* start axis */
+	int x2, y2; /* end axis */
+};
+
+struct box_info;
+
+struct child_node {
+	int filled;
+	enum dp_node_type type;
+	union {
+		int qid;
+		struct box_info *box;
+		} box_qid;
+	struct location l;
+};
+
+struct box_info {
+	int filled;
+	int id;  /* physical id if available, otherwise -1 */
+	int node; /* node id if available, otherwise -1 */
+	int pir, cir, pbs, cbs;
+	int prn_leaf;  /* since PPV4 not support leaf,
+			* here generate leaf for layout printing
+			*/
+
+	int box_x, box_y, box_height; /* axis (x,y) for box */
+	int size;
+	int n_q, n_sch; /* direct child queue/scheduler */
+	struct box_info *p_box; /* parent box */
+	struct child_node child[PROC_DP_MAX_CHILD_PER_SCH_PORT];
+	struct location l; /* location */
+};
+
+struct q_print_info {
+	int q_num;
+	int q_id[PROC_MAX_Q_PER_PORT]; /* physical queue id if available,
+					*   otherwise -1
+					*/
+	int q_node_id[PROC_MAX_Q_PER_PORT]; /* queue node id if available,
+					     * otherwise -1
+					     */
+	int q_prn_leaf[PROC_MAX_Q_PER_PORT];  /* since PPV4 not support leaf,
+					       * here generate leaf for layout
+					       * printing
+					       */
+	int sch_lvl[PROC_MAX_Q_PER_PORT];
+
+	/* point to one of box entry */
+	struct box_info *sch_box[PROC_MAX_Q_PER_PORT][PROC_MAX_BOX_LVL];
+	struct box_info port_box;
+	int box_num;
+	struct box_info box[PROC_DP_MAX_SCH_PER_PORT]; /* need kmalloc/kfree */
+};
+
+static char *get_node_stat(int node_id, int type);
+char *get_node_pri(int node_id, int type);
+char *dp_port_flag_str(int cqm_deq_port, int flag);
+char *dp_port_dma_tx_str(int cqm_deq_port, int flag);
+static void conv_limit_to_str(int shaper_limit, char *buf, int size);
+
+struct box_info *find_box_via_nodeid(struct box_info *box,
+				     int box_num, int sch_node_id)
+{
+	int i;
+
+	for (i = 0; i < box_num; i++) {
+		if (!box[i].filled)
+			continue;
+		if (box[i].node != sch_node_id)
+			continue;
+		return &box[i];
+	}
+	return NULL;
+}
+
+struct box_info *find_child_box(
+	struct child_node child_list[PROC_DP_MAX_CHILD_PER_SCH_PORT],
+	struct box_info *curr_box)
+{
+	int i = 0;
+
+	while (child_list[i].filled) {
+		if (child_list[i].type != DP_NODE_SCH) {
+			i++;
+			continue;
+		}
+		if (child_list[i].box_qid.box == curr_box)
+			return curr_box;
+		i++;
+	}
+	return NULL;
+}
+
+void set_child_per_box(struct q_print_info *q_info)
+{
+	int i, j, idx;
+	struct box_info *p_box, *c_box;
+
+	/* Get child number and info */
+	for (i = 0; i < q_info->q_num; i++) { /* queue */
+		if (!q_info->sch_box[i][0]) {/* queue to port case */
+			p_box = &q_info->port_box;
+			idx = p_box->n_q + p_box->n_sch;
+
+			if (idx >= PROC_DP_MAX_CHILD_PER_SCH_PORT) {
+				PR_ERR("too many child: should <%d\n",
+				       PROC_DP_MAX_CHILD_PER_SCH_PORT);
+				return;
+			}
+			p_box->child[idx].filled = 1;
+			p_box->child[idx].box_qid.qid = q_info->q_id[i];
+			p_box->child[idx].type = DP_NODE_QUEUE;
+			q_info->q_prn_leaf[i] = idx;
+			p_box->n_q++;
+			continue;
+		}
+		/* queue to 1st scheduler */
+		p_box = q_info->sch_box[i][0];
+		idx = p_box->n_q + p_box->n_sch;
+
+		if (idx >= PROC_DP_MAX_CHILD_PER_SCH_PORT) {
+			PR_ERR("too many child: should <%d\n",
+			       PROC_DP_MAX_CHILD_PER_SCH_PORT);
+			return;
+		}
+		if (idx < 0) {
+			PR_ERR("wrong: idx(%d) should >= 0\n", idx);
+			return;
+		}
+		p_box->child[idx].filled = 1;
+		p_box->child[idx].box_qid.qid = q_info->q_id[i];
+		p_box->child[idx].type = DP_NODE_QUEUE;
+		q_info->q_prn_leaf[i] = idx;
+		p_box->n_q++;
+
+		/* scheduler to schduer/port */
+		for (j = 0; j < q_info->sch_lvl[i]; j++) {
+			if (j < (q_info->sch_lvl[i] - 1))
+				p_box = q_info->sch_box[i][j + 1];
+			else
+				p_box = &q_info->port_box;
+			c_box = q_info->sch_box[i][j];
+			idx = p_box->n_q + p_box->n_sch;
+			if (idx < 0) {
+				PR_ERR("wrong: idx(%d) should >= 0\n", idx);
+				return;
+			}
+			c_box->p_box = p_box;
+			if (find_child_box(p_box->child, c_box))
+				continue;
+			if (idx >= PROC_DP_MAX_CHILD_PER_SCH_PORT - 1) {
+				PR_ERR("too many child: should <%d\n",
+				       PROC_DP_MAX_CHILD_PER_SCH_PORT);
+				return;
+			}
+			p_box->child[idx].filled = 1;
+			p_box->child[idx].box_qid.box = c_box;
+			p_box->child[idx].type = DP_NODE_SCH;
+			c_box->prn_leaf = idx;
+			p_box->n_sch++;
+		}
+	}
+}
+
+#define PREFIX_SIZE_PER_BOX 1 /* for opening ----- */
+#define INFO_SIZE_PER_BOX   2 /* for box info, like node id and others */
+#define SUFIX_SIZE_PER_BOX  1 /* for closing ---- */
+#define EXTRA_SIZE_PER_BOX (PREFIX_SIZE_PER_BOX + INFO_SIZE_PER_BOX + \
+				SUFIX_SIZE_PER_BOX)
+#define PADDING_BETWEEN_BOX_X 2 /* axis x */
+#define PADDING_BETWEEN_BOX_Y 1 /* axis y */
+#define SIZE_PER_QUEUE      3
+#define BOX_WIDTH           20
+#define Q_WIDTH             18
+#define PORT_OTHER_INFO     20
+#define PORT_BOX_SUFFIX     22
+
+int set_location_size(struct box_info *box, int y)
+{
+	int i, y2 = 0, size = 0;
+
+	box->l.x1 = Q_WIDTH + box->box_x * (BOX_WIDTH + PADDING_BETWEEN_BOX_X);
+	box->l.x2 = box->l.x1 + BOX_WIDTH;
+	box->l.y1 = y;
+	y2 = box->l.y1 + PREFIX_SIZE_PER_BOX + INFO_SIZE_PER_BOX;
+	for (i = 0;
+	     (box->child[i].filled && (i < (box->n_q + box->n_sch)));
+		 i++) {
+		if (box->child[i].type == DP_NODE_QUEUE) {
+			box->child[i].l.x2 = box->l.x1;
+			box->child[i].l.y2 = y2;
+			size += SIZE_PER_QUEUE;
+			y2 += SIZE_PER_QUEUE;
+		} else if (box->child[i].type == DP_NODE_SCH) {
+			set_location_size(box->child[i].box_qid.box, y2);
+			box->child[i].l.x2 = box->l.x1;
+			box->child[i].l.y2 = y2;
+			size += box->child[i].box_qid.box->size;
+			y2 += box->child[i].box_qid.box->size;
+		}
+	}
+	y2 += SUFIX_SIZE_PER_BOX;
+	size += EXTRA_SIZE_PER_BOX;
+	box->l.y2 = y2;
+	box->size = size;
+	return 0;
+}
+
+int check_location(struct q_print_info *q_info)
+{
+	int i;
+
+	for (i = 0; i < q_info->box_num; i++) {
+		if ((q_info->box[i].l.x2 - q_info->box[i].l.x1) != BOX_WIDTH) {
+			PR_ERR("sched[%d] x1/x2: %d - %d should equal%d\n",
+			       q_info->box[i].node,
+			       q_info->box[i].l.x2,
+			       q_info->box[i].l.x1,
+			       q_info->box[i].l.x2 - q_info->box[i].l.x1);
+			return -1;
+		}
+		if (!q_info->box[i].p_box)
+			continue;
+		if ((q_info->box[i].p_box->l.x1 - q_info->box[i].l.x2) !=
+			PADDING_BETWEEN_BOX_X) {
+			PR_ERR("sched[%d]<->sched[%d]: %d - %d %s%d\n",
+			       q_info->box[i].node,
+			       q_info->box[i].p_box->node,
+			       q_info->box[i].p_box->l.x2,
+			       q_info->box[i].l.x1,
+			       "should equal",
+			       q_info->box[i].p_box->l.x2 -
+			       q_info->box[i].l.x1);
+			return -1;
+		}
+	}
+	return 0;
+}
+
+void virtual_print_box(struct box_info *box,
+		       struct box_info *p_box,
+		       char *buf, int rows, int cols)
+{
+	char *p;
+	int i, len;
+	char *stat = NULL;
+	char *info = NULL, *p_flag = NULL, *p_dma_tx = NULL;
+	struct dp_shaper_conf shaper = {0};
+	char buf_cir[6] = {0};
+	char buf_pir[6] = {0};
+
+	/* The format like below
+	 *         -----------------------
+	 *        |sched[%03d]            |
+	 *        |                       |
+	 *        |leaf[%2d]:kbps         |
+	 *        | cir/pir:%5d/%5d       |
+	 *        | cbs/pbs:%5d/%5d       |
+	 *        | ....                  |
+	 *        |                       |
+	 *         -----------------------
+	 */
+	p = &buf[cols * box->l.y1];
+	for (i = box->l.x1; i < box->l.x2; i++)
+		p[i] = '-';
+	p = &buf[cols * (box->l.y2 - 1)];
+	for (i = box->l.x1; i < box->l.x2; i++)
+		p[i] = '-';
+
+	for (i = 0; i < INFO_SIZE_PER_BOX; i++) {
+		p = &buf[cols * (box->l.y1 + 1 + i)];
+		p += box->l.x1 + 1;
+		if (i == 0) { /* print 1st info of box */
+			if (!p_box) { /* port box */
+				len = snprintf(p, BOX_WIDTH - 3,
+					       "port[%d/%d]",
+					       box->id, box->node);
+				p[len] = ' ';
+				stat = get_node_stat(box->id, DP_NODE_PORT);
+				shaper.id.cqm_deq_port = box->id;
+				shaper.type = DP_NODE_PORT;
+				dp_shaper_conf_get(&shaper, 0);
+				if ((shaper.cir == DP_MAX_SHAPER_LIMIT) ||
+				    (shaper.cir == DP_NO_SHAPER_LIMIT)) {
+					conv_limit_to_str(shaper.cir, buf_cir,
+							  sizeof(buf_cir));
+					DP_DEBUG(DP_DBG_FLAG_QOS,
+						 "port[%d] shaper=%d(%s)\n",
+						 shaper.id.cqm_deq_port,
+						 shaper.cir,
+						 buf_cir);
+				} else {
+					DP_DEBUG(DP_DBG_FLAG_QOS,
+						 "port[%d] shaper=%d\n",
+						 shaper.id.cqm_deq_port,
+						 shaper.cir);
+				}
+				p_flag = dp_port_flag_str(box->id, 0);
+				p_dma_tx = dp_port_dma_tx_str(box->id, 0);
+				DP_DEBUG(DP_DBG_FLAG_QOS,
+					 "port[%d] p_flag=%s\n",
+					 box->id, p_flag);
+			} else { /* sched box */
+				len = snprintf(p, BOX_WIDTH - 3,
+					       "sched[/%d]",  box->node);
+				stat = get_node_stat(box->node, DP_NODE_SCH);
+				info = get_node_pri(box->node, DP_NODE_SCH);
+				shaper.id.sch_id = box->node;
+				shaper.type = DP_NODE_SCH;
+				dp_shaper_conf_get(&shaper, 0);
+				if ((shaper.cir == DP_MAX_SHAPER_LIMIT) ||
+				    (shaper.cir == DP_NO_SHAPER_LIMIT)) {
+					conv_limit_to_str(shaper.cir, buf_cir,
+							  sizeof(buf_cir));
+					DP_DEBUG(DP_DBG_FLAG_QOS,
+						 "sched[%d] shaper=%d(%s)\n",
+						 shaper.id.sch_id, shaper.cir,
+						 buf_cir);
+				} else {
+					DP_DEBUG(DP_DBG_FLAG_QOS,
+						 "sched[%d] shaper=%d\n",
+						 shaper.id.sch_id, shaper.cir);
+				}
+				p[len] = ' ';
+			}
+		} else if (i == 1) {
+			len = snprintf(p, BOX_WIDTH - 3, " stat:%s",
+				       stat ? stat : "NULL");
+			p[len] = ' ';
+		}
+	}
+
+	for (i = box->l.y1 + 1; i < box->l.y2 - 1; i++) {
+		p = &buf[cols * i];
+		p[box->l.x1] = '|';
+	}
+	for (i = box->l.y1 + 1; i < box->l.y2 - 1; i++) {
+		p = &buf[cols * i];
+		p[box->l.x2 - 1] = '|';
+	}
+	if (!p_box) { /* port information */
+		p = &buf[cols * ((box->l.y1 + box->l.y2) / 2 - 1)];
+		p += box->l.x2;
+		len = snprintf(p, cols - box->l.x2 - 1,
+			       "--%s", p_flag ? p_flag : "");
+		if (len >= 0)
+			p[len] = ' ';
+
+		p += cols;
+		len = snprintf(p, cols - box->l.x2 - 1,
+			       "  %s", p_dma_tx ? p_dma_tx : "");
+		if (len >= 0)
+			p[len] = ' ';
+
+		p += cols;
+		conv_limit_to_str(shaper.cir, buf_cir, sizeof(buf_cir));
+		conv_limit_to_str(shaper.pir, buf_pir, sizeof(buf_pir));
+		len = snprintf(p, cols - box->l.x2 - 1,
+			       "  C/P:%5s/%5s", buf_cir, buf_pir);
+		p[len] = ' '; /* remove \0' added by snprintf */
+		p += cols;
+		len = snprintf(p, cols - box->l.x2 - 1,
+			       "  c/p:%5d/%5d", shaper.cbs, shaper.pbs);
+		p[len] = ' ';
+		return;
+	}
+
+	/* print link to the parent box */
+	p = &buf[cols * ((box->l.y1 + box->l.y2) / 2)];
+	for (i = box->l.x2; i < p_box->l.x1; i++)
+		p[i] = '-';
+
+	/* print leaf info in the parent box:sched/port */
+	p += p_box->l.x1 + 1; /* skip '|' */
+	for (i = 0; i < SIZE_PER_QUEUE; i++) {
+		if (i == 0) {
+			len = snprintf(p, BOX_WIDTH - 3,/* skip: | & | & null */
+				       "child[%d] %s", box->prn_leaf,
+				       info ? info : "");
+			p[len] = ' ';
+		} else if (i == 1) {
+			conv_limit_to_str(shaper.cir, buf_cir, sizeof(buf_cir));
+			conv_limit_to_str(shaper.pir, buf_pir, sizeof(buf_pir));
+			len = snprintf(p, BOX_WIDTH - 3,
+				       " C/P:%5s/%5s", buf_cir, buf_pir);
+			p[len] = ' ';
+		} else if (i == 2) {
+			len = snprintf(p, BOX_WIDTH - 3,
+				       " c/p:%5d/%5d", shaper.cbs, shaper.pbs);
+			p[len] = ' ';
+		}
+		/* move to next row */
+		p += cols;
+	}
+}
+
+void virtual_print_queues(struct q_print_info *q_info,
+			  char *buf, int rows, int cols)
+{
+	int i, j;
+	struct box_info *box;
+	int len, idx;
+	char *p;
+	char *stat = NULL;
+	char *info = NULL;
+	struct dp_shaper_conf shaper = {0};
+	char buf_cir[6] = {0};
+	char buf_pir[6] = {0};
+
+	for (i = 0; i < q_info->q_num; i++) {
+		if (q_info->sch_box[i][0])
+			box = q_info->sch_box[i][0];
+		else
+			box = &q_info->port_box;
+		idx = q_info->q_prn_leaf[i];
+		DP_DEBUG(DP_DBG_FLAG_QOS, "get_node_stat:queue=%d\n",
+			 q_info->q_id[i]);
+		stat = get_node_stat(q_info->q_id[i], DP_NODE_QUEUE);
+		info = get_node_pri(q_info->q_id[i], DP_NODE_QUEUE);
+		shaper.id.q_id = q_info->q_id[i];
+		shaper.type = DP_NODE_QUEUE;
+		dp_shaper_conf_get(&shaper, 0);
+		if ((shaper.cir == DP_MAX_SHAPER_LIMIT) ||
+		    (shaper.cir == DP_NO_SHAPER_LIMIT)) {
+			conv_limit_to_str(shaper.cir, buf_cir, sizeof(buf_cir));
+			DP_DEBUG(DP_DBG_FLAG_QOS, "q[%d] shaper=%d(%s)\n",
+				 shaper.id.q_id, shaper.cir, buf_cir);
+		} else {
+			DP_DEBUG(DP_DBG_FLAG_QOS, "q[%d] shaper=%d\n",
+				 shaper.id.q_id, shaper.cir);
+		}
+		p = &buf[cols * box->child[idx].l.y2];
+		len = snprintf(p, Q_WIDTH - 1, "q[%4d/%4d]",
+			       q_info->q_id[i], q_info->q_node_id[i]);
+		for (j = len; j < box->l.x1; j++)
+			p[j] = '-';
+
+		p = &buf[cols * (box->child[idx].l.y2 + 1)];
+		len = snprintf(p, Q_WIDTH - 1, "  stat:%s",
+			       stat ? stat : "");
+		p[len] = ' ';
+
+		/* print leaf info in the parent box:sched/port */
+		p = &buf[cols * box->child[idx].l.y2];
+		p += box->l.x1 + 1; /* skip '|' */
+		for (j = 0; j < SIZE_PER_QUEUE; j++) {
+			if (j == 0) {
+				len = snprintf(p, BOX_WIDTH - 3, /* skip:| and |
+								  * and null
+								  */
+					       "child[%d]:%s",
+					       q_info->q_prn_leaf[i],
+					       info ? info : "");
+				p[len] = ' ';
+			} else if (j == 1) {
+				conv_limit_to_str(shaper.cir, buf_cir,
+						  sizeof(buf_cir));
+				conv_limit_to_str(shaper.pir, buf_pir,
+						  sizeof(buf_pir));
+				len = snprintf(p, BOX_WIDTH - 3,
+					       " C/P:%5s/%5s",
+					       buf_cir, buf_pir);
+				p[len] = ' ';
+			} else if (j == 2) {
+				len = snprintf(p, BOX_WIDTH - 3,
+					       " c/p:%5d/%5d",
+					       shaper.cbs, shaper.pbs);
+				p[len] = ' ';
+			}
+			/* move to next row */
+			p += cols;
+		}
+	}
+}
+
+#define PRINT_QOS_DETAIL  0
+void print_all(struct seq_file *s, struct q_print_info *q_info)
+{
+	int cols = q_info->port_box.l.x2 + PORT_BOX_SUFFIX;
+	int rows = q_info->port_box.l.y2 + 1;
+	int i;
+	char *buf;
+	char *p;
+
+	buf = kmalloc(cols * rows + 1, GFP_KERNEL);
+	if (!buf) {
+		PR_ERR("fail to alloc %d bytes\n", cols * rows + 1);
+		return;
+	}
+	memset(buf, ' ', cols * rows);
+	buf[cols * rows] = 0;
+#if PRINT_QOS_DETAIL
+	seq_printf(s, "allocate buffer: %d bytes(%d * %d)\n",
+		   cols * rows, cols, rows);
+#endif
+
+	p = buf;
+	for (i = 0; i < rows; i++) {
+		p += cols;
+		*(p - 1) = 0;
+	}
+
+	/* print port box */
+	virtual_print_box(&q_info->port_box, NULL, buf, rows, cols);
+	for (i = 0; i < q_info->box_num; i++)
+		virtual_print_box(&q_info->box[i], q_info->box[i].p_box, buf,
+				  rows, cols);
+	/* print queue */
+	virtual_print_queues(q_info, buf, rows, cols);
+	p = buf;
+	for (i = 0; i < rows; i++) {
+		seq_printf(s, "%s\n", p);
+		p += cols;
+	}
+	kfree(buf);
+}
+
+/* print_box_lvl must bigger 1 than sch_lvl */
+struct q_print_info *collect_info(struct seq_file *s,
+				  struct dp_dequeue_res *res,
+				  int print_box_lvl)
+{
+	int i, j, size, curr_box_y = 0, curr_box_x;
+	struct q_print_info *q_info = NULL;
+	struct box_info *box;
+	char f_new_box;
+
+	if (!res || !res->num_q || (res->num_deq_ports < 1))
+		goto ERR_EXIT;
+
+	if (res->num_q >= PROC_MAX_Q_PER_PORT) {
+		seq_printf(s, "too many queues in one port:%d should <%d\n",
+			   res->num_q, PROC_MAX_Q_PER_PORT);
+		return NULL;
+	}
+
+	size = sizeof(*q_info) + 1;
+	q_info = kmalloc(size, GFP_KERNEL);
+	if (!q_info) {
+		PR_ERR("fail to alloc %d bytes\n", size);
+		return NULL;
+	}
+	memset(q_info, 0, size);
+
+	q_info->port_box.filled = 1;
+	q_info->port_box.id = res->q_res[0].cqm_deq_port;
+	q_info->port_box.node = res->q_res[0].qos_deq_port;
+	q_info->port_box.box_x = print_box_lvl - 1;
+	q_info->port_box.box_y = 0;
+
+	for (i = 0; i < res->num_q; i++) { /* q loop */
+		q_info->q_id[i] = res->q_res[i].q_id;
+		q_info->q_node_id[i] = res->q_res[i].q_node;
+		if (res->q_res[i].sch_lvl <= 0)
+			continue;
+		if (res->q_res[i].sch_lvl > PROC_MAX_BOX_LVL - 1) {
+			PR_ERR("Too many sched lvl(%d): expect<=%d\n",
+			       res->q_res[i].sch_lvl,
+			       PROC_MAX_BOX_LVL - 1);
+				goto ERR_EXIT;
+		}
+		f_new_box = 0;
+		curr_box_x = print_box_lvl - res->q_res[i].sch_lvl - 1;
+		for (j = 0; j < res->q_res[i].sch_lvl; j++) { /* each sched */
+			box = find_box_via_nodeid(q_info->box,
+						  q_info->box_num,
+						  res->q_res[i].sch_id[j]);
+			if (box) {
+				q_info->sch_box[i][j] = box;
+				continue;
+			}
+			/* create a new box */
+			memset(&q_info->box[q_info->box_num], 0,
+			       sizeof(q_info->box[q_info->box_num]));
+			q_info->box[q_info->box_num].filled = 1;
+			q_info->box[q_info->box_num].node =
+							res->q_res[i].sch_id[j];
+			q_info->box[q_info->box_num].id = -1; /* not valid */
+			q_info->box[q_info->box_num].box_x = curr_box_x + j;
+			q_info->box[q_info->box_num].box_y = curr_box_y;
+			q_info->sch_box[i][j] = &q_info->box[q_info->box_num];
+			q_info->box_num++;
+			if (q_info->box_num == ARRAY_SIZE(q_info->box)) {
+				PR_ERR("sched+port (%d) in one node: %s<%d\n",
+				       q_info->box_num,
+				       "expect",
+				       ARRAY_SIZE(q_info->box));
+				goto ERR_EXIT;
+			}
+			f_new_box = 1;
+		}
+		q_info->sch_lvl[i] = res->q_res[i].sch_lvl;
+		if (f_new_box)
+			curr_box_y++;
+	}
+	q_info->q_num = res->num_q;
+
+	/* sanity check */
+	for (i = 0; i < res->num_q; i++) {
+		if (!q_info->sch_lvl[i])
+			continue;
+		for (j = 0; j < q_info->sch_lvl[i]; j++) {
+			if (!q_info->sch_box[i][j]->filled) {
+				PR_ERR("sch_box[%d][%d].fill should 1:%d\n",
+				       i, j, q_info->sch_box[i][j]->filled);
+				goto ERR_EXIT;
+			}
+			if (q_info->sch_box[i][j]->n_q < 0) {
+				PR_ERR("sch_box[%d][%d].n_q should >=0:%d\n",
+				       i, j, q_info->sch_box[i][j]->n_q);
+				goto ERR_EXIT;
+			}
+			if (q_info->sch_box[i][j]->n_sch < 0) {
+				PR_ERR("sch_box[%d][%d].n_sch should >=0:%d\n",
+				       i, j, q_info->sch_box[i][j]->n_sch);
+				goto ERR_EXIT;
+			}
+		}
+	}
+	return q_info;
+ERR_EXIT:
+	kfree(q_info);
+	return NULL;
+}
+
+//static struct dp_dequeue_res res;
+static struct dp_queue_res q_res[PROC_MAX_Q_PER_PORT * 4];
+static int qos_layout_inst;
+static int qos_layout_max_lvl = PROC_MAX_BOX_LVL; /* sched/port box */
+static int max_tconf_idx;
+static int curr_tconf_idx;
+struct dp_dequeue_res tmp_res;
+struct dp_queue_res tmp_q_res[PROC_MAX_Q_PER_PORT] = {0};
+#define DP_PROC_BUF_LEN 20
+static char dma_flag[DP_PROC_BUF_LEN];
+static char port_flag[DP_PROC_BUF_LEN];
+
+static char *port_flag_str(int inst, int dp_port)
+{
+	int i;
+	struct pmac_port_info *port_info;
+
+	port_info = get_port_info(inst, dp_port);
+	if (!port_info)
+		return "";
+	for (i = 0; i < get_dp_port_type_str_size(); i++) {
+		if (port_info->alloc_flags & dp_port_flag[i])
+			return dp_port_type_str[i];
+	}
+	return "";
+}
+
+char *dp_port_flag_str(int cqm_deq_port, int flag)
+{
+	int i;
+	int inst = qos_layout_inst;
+
+	port_flag[0] = 0;
+	for (i = 0; i < dp_port_prop[inst].info.cap.max_num_dp_ports; i++) {
+		if ((cqm_deq_port >= dp_port_info[inst][i].deq_port_base) &&
+		    (cqm_deq_port < (dp_port_info[inst][i].deq_port_base +
+		     dp_port_info[inst][i].deq_port_num)))  {
+			if (i == 0) {
+				snprintf(port_flag, sizeof(port_flag),
+					 "CPU:%d", i);
+				return port_flag;
+			}
+			if (!dp_port_info[inst][i].alloc_flags)
+				continue;
+
+			snprintf(port_flag, sizeof(port_flag),
+				 "%s:%d",
+				 port_flag_str(inst, i),
+				 i);
+			return port_flag;
+		}
+	}
+	snprintf(port_flag, sizeof(port_flag), "?:%d", cqm_deq_port);
+	return port_flag;
+}
+
+char *dp_port_dma_tx_str(int cqm_deq_port, int flag)
+{
+	int i;
+	int inst = qos_layout_inst;
+
+	dma_flag[0] = 0;
+	for (i = 0; i < dp_port_prop[inst].info.cap.max_num_dp_ports; i++) {
+		if ((cqm_deq_port >= dp_port_info[inst][i].deq_port_base) &&
+		    (cqm_deq_port < (dp_port_info[inst][i].deq_port_base +
+		     dp_port_info[inst][i].deq_port_num))) {
+			if (i == 0) {
+				snprintf(dma_flag, sizeof(dma_flag), "-");
+				return dma_flag;
+			}
+			if (!dp_port_info[inst][i].alloc_flags)
+				continue;
+			snprintf(dma_flag, sizeof(dma_flag), "CH%x",
+				 dp_port_info[inst][i].dma_chan);
+			return dma_flag;
+		}
+	}
+	snprintf(dma_flag, sizeof(dma_flag), "?:%d", cqm_deq_port);
+	return dma_flag;
+}
+
+char *get_node_stat(int node_id, int type)
+{
+	struct dp_node_link_enable node_en = {0};
+	static char stat[20];
+
+	node_en.inst = qos_layout_inst;
+	node_en.id.q_id = node_id;
+	node_en.type = (enum dp_node_type)type;
+	stat[0] = 0;
+	if (dp_node_link_en_get(&node_en, 0)) {
+		strcpy(stat, "?");
+	} else {
+		if (node_en.en & DP_NODE_DIS)
+			strcat(stat, "Blk ");
+		if (node_en.en & DP_NODE_SUSPEND)
+			strcat(stat, "Susp");
+		if (node_en.en & DP_NODE_EN)
+			strcat(stat, "Normal");
+		if (strlen(stat) == 0)
+			strcat(stat, "??");
+	}
+
+	return stat;
+}
+
+char *get_node_pri(int node_id, int type)
+{
+	struct dp_node_prio node_prio = {0};
+	static char stat[20];
+	int len;
+
+	stat[0] = 0;
+	node_prio.inst = qos_layout_inst;
+	node_prio.id.q_id = node_id;
+	node_prio.type = (enum dp_node_type)type;
+	if (dp_qos_link_prio_get(&node_prio, 0)) {
+		strcpy(stat, "?");
+	} else {
+		if (node_prio.arbi == ARBITRATION_WRR)
+			strcpy(stat, "WRR");
+		else if (node_prio.arbi == ARBITRATION_SP)
+			strcpy(stat, "SP");
+		else if (node_prio.arbi == ARBITRATION_WSP)
+			strcpy(stat, "WSP");
+		else if (node_prio.arbi == ARBITRATION_WFQ)
+			strcpy(stat, "WFQ");
+		else
+			snprintf(stat, sizeof(stat), "?%d:", node_prio.arbi);
+
+		len = strlen(stat);
+		snprintf(stat + len, sizeof(stat) - len - 1, ":%d",
+			 node_prio.prio_wfq);
+	}
+	if (node_prio.type == DP_NODE_QUEUE)
+		DP_DEBUG(DP_DBG_FLAG_QOS, "q[%d] arbi=%d prio=%d\n",
+			 node_prio.id.q_id, node_prio.arbi, node_prio.prio_wfq);
+	else if (node_prio.type == DP_NODE_SCH)
+		DP_DEBUG(DP_DBG_FLAG_QOS, "sch[%d] arbi=%d prio=%d\n",
+			 node_prio.id.sch_id, node_prio.arbi,
+			 node_prio.prio_wfq);
+	else if (node_prio.type == DP_NODE_PORT)
+		DP_DEBUG(DP_DBG_FLAG_QOS, "port[%d] arbi=%d prio=%d\n",
+			 node_prio.id.cqm_deq_port, node_prio.arbi,
+			 node_prio.prio_wfq);
+	else
+		DP_DEBUG(DP_DBG_FLAG_QOS, "unknown type\n");
+	return stat;
+}
+
+void conv_limit_to_str(int shaper_limit, char *buf, int size)
+{
+	if (shaper_limit == DP_NO_SHAPER_LIMIT)
+		snprintf(buf, size, "NoLim");
+	else if (shaper_limit == DP_MAX_SHAPER_LIMIT)
+		snprintf(buf, size, "MaxLm");
+	else
+		snprintf(buf, size, "%d", shaper_limit);
+}
+
+int get_num_deq_ports(int inst, int dp_port)
+{
+	memset(&tmp_res, 0, sizeof(tmp_res));
+	tmp_res.inst = inst;
+	tmp_res.dp_port = dp_port;
+	tmp_res.q_res = NULL;
+	tmp_res.cqm_deq_idx = DEQ_PORT_OFFSET_ALL;
+	if (dp_deq_port_res_get(&tmp_res, 0))
+		return -1;
+	return tmp_res.num_deq_ports;
+}
+
+int get_res(int inst, int dp_port, int tconf_idx)
+{
+	memset(&tmp_res, 0, sizeof(tmp_res));
+	tmp_res.inst = inst;
+	tmp_res.dp_port = dp_port;
+	tmp_res.cqm_deq_idx = tconf_idx;
+	tmp_res.q_res = q_res;
+	tmp_res.q_res_size = ARRAY_SIZE(q_res);
+	DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+		 "get_res: dp_port=%d tconf_idx=%d\n",
+		 tmp_res.dp_port, tmp_res.cqm_deq_idx);
+	if (dp_deq_port_res_get(&tmp_res, 0)) {
+		PR_ERR("dp_deq_port_res_get fail: inst=%d dp_port=%d, %s=%d\n",
+		       qos_layout_inst, tmp_res.dp_port,
+		       "tconf_idx", curr_tconf_idx);
+		return -1;
+	}
+	return 0;
+}
+
+int dump_q_info_dbg(struct seq_file *s, struct dp_dequeue_res *res)
+{
+#if PRINT_QOS_DETAIL
+	struct dp_queue_res *q_res = res->q_res;
+	int i, j;
+
+	for (i = 0; i < res->num_q; i++) {
+		seq_printf(s, "q[%d]-", q_res[i].q_id);
+		for (j = 0; j < q_res[i].sch_lvl; j++)
+			seq_printf(s, "sched[%d]-", q_res[i].sch_id[j]);
+		seq_printf(s, "p[%d/%d]\n", q_res[i].cqm_deq_port,
+			   q_res[i].qos_deq_port);
+	}
+#endif
+	return 0;
+}
+
+int qos_dump(struct seq_file *s, int pos)
+{
+	struct q_print_info *q_info;
+	int i;
+
+	if (pos == 0) {
+		seq_puts(s, "Note:\n");
+		seq_puts(s, "  x/y :physical node/logical node\n");
+		seq_puts(s, "      :cqm dequeue port/ppv4 logical node\n");
+		seq_puts(s, "  Blk :blocked, ie new incoming packet will be dropped\n");
+		seq_puts(s, "  Susp:suspended, ie, packet in queue will not be scheduled\n");
+		seq_puts(s, "  C   :CIR in kbps\n");
+		seq_puts(s, "  P   :PIR in kbps\n");
+		seq_puts(s, "  c   :cbs in bytes\n");
+		seq_puts(s, "  p   :pbs in bytes\n");
+		seq_puts(s, "\n");
+	}
+	max_tconf_idx = get_num_deq_ports(qos_layout_inst, pos);
+	DP_DEBUG(DP_DBG_FLAG_QOS, "dp_port=%d  max_tconf=%d\n",
+		 pos, max_tconf_idx);
+	if (max_tconf_idx <= 0)
+		goto NEXT_DP_PORT;
+	curr_tconf_idx = 0;
+	for (i = 0; i < max_tconf_idx; i++) {
+		/* get qos resource and store into tmp_res */
+		if (get_res(qos_layout_inst, pos, i))
+			continue;
+		dump_q_info_dbg(s, &tmp_res);
+		if (!tmp_res.num_q)
+			continue;
+		q_info = collect_info(s, &tmp_res, qos_layout_max_lvl);
+		if (!q_info)
+			goto NEXT_DP_PORT;
+		set_child_per_box(q_info);
+		set_location_size(&q_info->port_box, 0);
+		if (check_location(q_info)) {
+			kfree(q_info);
+			continue;
+		}
+		print_all(s, q_info);
+		kfree(q_info);
+	}
+NEXT_DP_PORT:
+	if (!seq_has_overflowed(s))
+		pos++;
+	if (pos >= 16)
+		pos = -1;
+
+	return pos;
+}
+
+int qos_dump_start(void)
+{
+	struct dp_qos_level dp = {0};
+
+	qos_layout_inst = 0;
+	/* current just use maximum one. Later should check real QOS
+	 * configuration and get its maxim hirachy layers
+	 * qos_layout_max_lvl = PROC_MAX_BOX_LVL;
+	 */
+	dp_qos_level_get(&dp, 0);
+	qos_layout_max_lvl = dp.max_sch_lvl + 1;
+	max_tconf_idx = 0;
+	return 0;
+}
+
+void qos_create_qos_help(void)
+{
+	PR_INFO("QOS Command Help:\n\n");
+	PR_INFO("     Add Queue: echo add_q <qid> <schid>:<leaf> %s%s\n\n",
+		"<schid>:<leaf> <schid>:<leaf> <schid>:<leaf> <portid> ",
+		"> /sys/kernel/debug/dp/qos");
+	PR_INFO("            id: qid/schid_node/cbm_port\n");
+	PR_INFO("  Delete Queue: echo del_q qid > /sys/kernel/debug/dp/qos\n");
+	PR_INFO("  Delete Sched: echo del_sch schid > %s\n",
+		"/sys/kernel/debug/dp/qos");
+	PR_INFO("  SET PRIO: echo prio <id> <type> <arbi> <prio_wfq> %s\n",
+		"> /sys/kernel/debug/dp/qos");
+	PR_INFO("            id: phy_queue/sched_node\n");
+	PR_INFO("          type: queue/sched\n");
+	PR_INFO("          arbi: null/sp/wsp/wrr/wrr_wsp/wfq\n");
+	PR_INFO("    CFG SHAPER: echo shaper <cmd> <id> <type> %s\n",
+		"<cir> <pir> <cbs> <pbs> > /sys/kernel/debug/dp/qos");
+	PR_INFO("           cmd: add/remove/disable\n");
+	PR_INFO("            id: qid/sched_node/cbm_port\n");
+	PR_INFO("          type: queue/sched/port\n");
+	PR_INFO("           cir: no_limit/max/value\n");
+	PR_INFO("      SET NODE: echo set_node <id> <type> <cmd>... %s\n",
+		"> /sys/kernel/debug/dp/qos");
+	PR_INFO("            id: phy_queue/sched_node/cbm_port\n");
+	PR_INFO("          type: queue/sched/port\n");
+	PR_INFO("           cmd: enable(unblock)/disable(block)/%s\n",
+		"resume/suspend");
+	PR_INFO("                enable/disable: only for queue/port\n");
+	PR_INFO("                resume/suspend: for all nodes\n");
+}
+
+ssize_t proc_qos_write(struct file *file, const char *buf, size_t count,
+		       loff_t *ppos)
+{
+	int len, res;
+	char str[100];
+	char *param_list[16] = { 0 };
+	char *temp_list[3] = { 0 };
+	int i, idx = 2; /* index for sched_list */
+	unsigned int level = 0, num = 0;
+
+	len = (sizeof(str) > count) ? count : sizeof(str) - 1;
+	len -= copy_from_user(str, buf, len);
+	str[len] = 0;
+
+	if (!len)
+		return count;
+
+	num = dp_split_buffer(str, param_list, ARRAY_SIZE(param_list));
+	level = num - 3;
+
+	if (num <= 1 || num > ARRAY_SIZE(param_list) ||
+	    (dp_strncmpi(param_list[0], "help", strlen("help")) == 0))
+		qos_create_qos_help();
+	else if (dp_strncmpi(param_list[0], "add_q", strlen("add_q")) == 0) {
+		struct dp_qos_link cfg = {0};
+
+		if (num < 3) {
+			PR_INFO("Wrong Parameter(try help):echo help > %s\n",
+				"/sys/kernel/debug/dp/qos");
+			return count;
+		}
+
+		cfg.q_id = dp_atoi(param_list[1]);
+
+		cfg.cqm_deq_port = dp_atoi(param_list[num - 1]);
+
+		cfg.q_leaf = 0;
+		cfg.q_arbi = 1;
+		cfg.q_prio_wfq = 0;
+		cfg.n_sch_lvl = level;
+
+		if (!level)
+			PR_INFO("QID %d->", cfg.q_id);
+
+		for (i = 0; (i < (num - 3) && i < DP_MAX_SCH_LVL); i++) {
+			dp_replace_ch(param_list[i + idx],
+				      strlen(param_list[i + idx]), ':', ' ');
+			dp_split_buffer(param_list[i + idx], temp_list,
+					ARRAY_SIZE(temp_list));
+			cfg.sch[i].id = dp_atoi(temp_list[0]);
+			cfg.sch[i].leaf = dp_atoi(temp_list[1]);
+			cfg.sch[i].arbi = 1;
+			cfg.sch[i].prio_wfq = 0;
+
+			PR_INFO("SCH %d:LEAF %d->", cfg.sch[i].id,
+				cfg.sch[i].leaf);
+		}
+
+		PR_INFO("PORT %d\n", cfg.cqm_deq_port);
+		if (dp_link_add(&cfg, 0)) {
+			PR_ERR("dp_link_add failed\n");
+			return count;
+		}
+	} else if (dp_strncmpi(param_list[0], "del_q", strlen("del_q")) == 0) {
+		struct dp_node_alloc node = {0};
+
+		node.id.q_id = dp_atoi(param_list[1]);
+		node.type = DP_NODE_QUEUE;
+
+		if (dp_node_free(&node, DP_NODE_SMART_FREE)) {
+			PR_ERR("dp_node_free failed\n");
+			return count;
+		}
+	} else if (dp_strncmpi(param_list[0], "del_p", strlen("del_p")) == 0) {
+		struct dp_node_alloc node = {0};
+
+		if (dp_strncmpi(param_list[2], "sched", strlen("sched")) == 0) {
+			node.type = DP_NODE_SCH;
+			node.id.sch_id = dp_atoi(param_list[1]);
+		} else if (dp_strncmpi(param_list[2], "port",
+				       strlen("port")) == 0) {
+			node.type = DP_NODE_PORT;
+			node.id.cqm_deq_port = dp_atoi(param_list[1]);
+		} else {
+			PR_ERR("unknown type %s\n", param_list[2]);
+		}
+	} else if (dp_strncmpi(param_list[0], "del_sch",
+			       strlen("del_sch")) == 0) {
+		struct dp_node_link node_link = {0};
+		struct dp_node_alloc node = {0};
+
+		node_link.node_id.sch_id = dp_atoi(param_list[1]);
+		node_link.node_type = DP_NODE_SCH;
+		node.id.q_id = dp_atoi(param_list[1]);
+		node.type = DP_NODE_SCH;
+
+		if (dp_node_link_get(&node_link, 0))
+			PR_ERR("dp_node_link_get failed\n");
+
+		if (dp_node_free(&node, 0)) {
+			PR_ERR("dp_node_free failed\n");
+			return count;
+		}
+
+		while (1) {
+			node_link.node_id = node_link.p_node_id;
+			node_link.node_type = node_link.p_node_type;
+			node.id = node_link.p_node_id;
+			node.type = node_link.p_node_type;
+
+			res = dp_node_link_get(&node_link, 0);
+
+			if (dp_node_free(&node, 0))
+				PR_ERR("dp_node_free failed\n");
+
+			if (res) {
+				PR_ERR("dp_node_link_get failed\n");
+				break;
+			}
+		}
+
+		PR_INFO("\nSched %d deleted\n\n", node_link.node_id.sch_id);
+	} else if (dp_strncmpi(param_list[0], "prio", strlen("prio")) == 0) {
+		struct dp_node_prio node_prio = {0};
+
+		if (num < 3) {
+			PR_ERR("id, type are required!%s%s\n",
+			       "\n(try help):echo help > ",
+			       "/sys/kernel/debug/dp/qos");
+			return count;
+		}
+
+		if (dp_strncmpi(param_list[2], "queue", strlen("queue")) == 0) {
+			node_prio.type = DP_NODE_QUEUE;
+			node_prio.id.q_id = dp_atoi(param_list[1]);
+		} else if (dp_strncmpi(param_list[2], "sched",
+				       strlen("sched")) == 0) {
+			node_prio.type = DP_NODE_SCH;
+			node_prio.id.sch_id = dp_atoi(param_list[1]);
+		} else if (dp_strncmpi(param_list[2], "port",
+				       strlen("port")) == 0) {
+			node_prio.type = DP_NODE_PORT;
+			node_prio.id.cqm_deq_port = dp_atoi(param_list[1]);
+		} else {
+			PR_ERR("unknown type %s\n", param_list[2]);
+		}
+
+		if (dp_qos_link_prio_get(&node_prio, 0))
+			PR_ERR("dp_qos_link_prio_get failed\n");
+
+		if (dp_strncmpi(param_list[3], "null", strlen("null")) == 0) {
+			node_prio.arbi = ARBITRATION_NULL;
+		} else if (dp_strncmpi(param_list[3], "sp",
+			   strlen("sp")) == 0) {
+			node_prio.arbi = ARBITRATION_SP;
+		} else if (dp_strncmpi(param_list[3], "wsp",
+			   strlen("wsp")) == 0) {
+			node_prio.arbi = ARBITRATION_WSP;
+		} else if (dp_strncmpi(param_list[3], "wrr",
+			   strlen("wrr")) == 0) {
+			node_prio.arbi = ARBITRATION_WRR;
+		} else if (dp_strncmpi(param_list[3], "wsp_wrr",
+			   strlen("wsp_wrr")) == 0) {
+			node_prio.arbi = ARBITRATION_WSP_WRR;
+		} else if (dp_strncmpi(param_list[3], "wfq",
+			   strlen("wfq")) == 0) {
+			node_prio.arbi = ARBITRATION_WFQ;
+		} else {
+			PR_ERR("unknown type %s\n", param_list[3]);
+			return count;
+		}
+
+		if (dp_atoi(param_list[4]))
+			node_prio.prio_wfq = dp_atoi(param_list[4]);
+
+		if (dp_qos_link_prio_set(&node_prio, 0)) {
+			PR_ERR("dp_qos_link_prio_set failed\n");
+			return count;
+		}
+	} else if (dp_strncmpi(param_list[0], "shaper",
+			       strlen("shaper")) == 0) {
+		struct dp_shaper_conf shaper_cfg = {0};
+
+		if (num < 4) {
+			PR_ERR("cmd, id, type are required!%s%s\n",
+			       "\n(try help):echo help > ",
+			       "/sys/kernel/debug/dp/qos");
+			return count;
+		}
+
+		if (dp_strncmpi(param_list[1], "add", strlen("add")) == 0)
+			shaper_cfg.cmd = DP_SHAPER_CMD_ADD;
+		else if (dp_strncmpi(param_list[1], "remove",
+				     strlen("remove")) == 0)
+			shaper_cfg.cmd = DP_SHAPER_CMD_REMOVE;
+		else if (dp_strncmpi(param_list[1], "disable",
+				     strlen("disable")) == 0)
+			shaper_cfg.cmd = DP_SHAPER_CMD_DISABLE;
+		else
+			PR_ERR("unknown cmd try: echo help %s",
+			       "> /sys/kernel/debug/dp/qos");
+
+		if (dp_strncmpi(param_list[3], "queue", strlen("queue")) == 0) {
+			shaper_cfg.type = DP_NODE_QUEUE;
+			shaper_cfg.id.sch_id = dp_atoi(param_list[2]);
+		} else if (dp_strncmpi(param_list[3], "sched",
+				       strlen("sched")) == 0) {
+			shaper_cfg.type = DP_NODE_SCH;
+			shaper_cfg.id.sch_id = dp_atoi(param_list[2]);
+		} else if (dp_strncmpi(param_list[3], "port",
+				       strlen("port")) == 0) {
+			shaper_cfg.type = DP_NODE_PORT;
+			shaper_cfg.id.sch_id = dp_atoi(param_list[2]);
+		} else {
+			PR_ERR("unknown type %s\n", param_list[3]);
+			return count;
+		}
+
+		if (dp_shaper_conf_get(&shaper_cfg, 0))
+			PR_ERR("dp_shaper_conf_get failed\n");
+
+		if (dp_strncmpi(param_list[4], "no_limit",
+				strlen("no_limit")) == 0)
+			shaper_cfg.cir = DP_NO_SHAPER_LIMIT;
+		else if (dp_strncmpi(param_list[4], "max", strlen("max")) == 0)
+			shaper_cfg.cir = DP_MAX_SHAPER_LIMIT;
+		else
+			shaper_cfg.cir = dp_atoi(param_list[4]);
+
+		if (dp_atoi(param_list[5]))
+			shaper_cfg.pir = dp_atoi(param_list[5]);
+		if (dp_atoi(param_list[6]))
+			shaper_cfg.cbs = dp_atoi(param_list[6]);
+		if (dp_atoi(param_list[7]))
+			shaper_cfg.pbs = dp_atoi(param_list[7]);
+
+		if (dp_shaper_conf_set(&shaper_cfg, 0)) {
+			PR_ERR("dp_shaper_conf_set failed\n");
+			return count;
+		}
+	} else if (dp_strncmpi(param_list[0], "set_node",
+			       strlen("set_node")) == 0) {
+		struct dp_node_link_enable en_node = {0};
+
+		if (num < 4 || num > 5) {
+			PR_ERR("id, type, cmd are required!%s%s\n",
+			       "\n(try help):echo help > ",
+			       "/sys/kernel/debug/dp/qos");
+			return count;
+		}
+
+		if (dp_strncmpi(param_list[2], "queue", strlen("queue")) == 0) {
+			en_node.type = DP_NODE_QUEUE;
+			en_node.id.q_id = dp_atoi(param_list[1]);
+		} else if (dp_strncmpi(param_list[2], "sched",
+				       strlen("sched")) == 0) {
+			en_node.type = DP_NODE_SCH;
+			en_node.id.sch_id = dp_atoi(param_list[1]);
+		} else if (dp_strncmpi(param_list[2], "port",
+				       strlen("port")) == 0) {
+			en_node.type = DP_NODE_PORT;
+			en_node.id.cqm_deq_port = dp_atoi(param_list[1]);
+		} else {
+			PR_ERR("Incorrect parameter!%s%s%s\n", param_list[2],
+			       "\n(try help):echo help > ",
+			       "/sys/kernel/debug/dp/qos");
+			return count;
+		}
+
+		if (dp_strncmpi(param_list[3], "enable",
+				strlen("enable")) == 0) {
+			en_node.en |= DP_NODE_EN;
+		} else if (dp_strncmpi(param_list[3], "disable",
+				       strlen("disable")) == 0) {
+			en_node.en |= DP_NODE_DIS;
+		} else if (dp_strncmpi(param_list[3], "suspend",
+				       strlen("suspend")) == 0) {
+			en_node.en |= DP_NODE_SUSPEND;
+		} else if (dp_strncmpi(param_list[3], "resume",
+				       strlen("resume")) == 0) {
+			en_node.en |= DP_NODE_RESUME;
+		} else {
+			PR_ERR("Incorrect parameter!%s%s%s\n", param_list[3],
+			       "\n(try help):echo help > ",
+			       "/sys/kernel/debug/dp/qos");
+			return count;
+		}
+
+		if (num == 5) {
+			if (dp_strncmpi(param_list[4], "enable",
+					strlen("enable")) == 0) {
+				en_node.en |= DP_NODE_EN;
+			} else if (dp_strncmpi(param_list[4], "disable",
+					       strlen("disable")) == 0) {
+				en_node.en |= DP_NODE_DIS;
+			} else if (dp_strncmpi(param_list[4], "suspend",
+					       strlen("suspend")) == 0) {
+				en_node.en |= DP_NODE_SUSPEND;
+			} else if (dp_strncmpi(param_list[4], "resume",
+					       strlen("resume")) == 0) {
+				en_node.en |= DP_NODE_RESUME;
+			} else {
+				PR_ERR("Incorrect parameter!%s%s%s\n",
+				       param_list[4],
+				       "\n(try help):echo help > ",
+				       "/sys/kernel/debug/dp/qos");
+				return count;
+			}
+		}
+
+		if (dp_node_link_en_set(&en_node, 0)) {
+			PR_ERR("dp_node_link_en_set failed\n");
+			return count;
+		}
+	} else if (dp_strncmpi(param_list[0], "qmap_set",
+			       strlen("qmap_set")) == 0) {
+		struct dp_queue_map_set qmap_set = {0};
+
+		qmap_set.q_id = dp_atoi(param_list[1]);
+
+		if (dp_strncmpi(param_list[2], "mode0", strlen("mode0")) == 0) {
+			if (num < 10) {
+				PR_INFO("Wrong Parameter(try help):%s\n",
+					"echo help > /sys/kernel/debug/dp/qos");
+				return count;
+			}
+			qmap_set.qm_mode = DP_Q_MAP_MODE0;
+			qmap_set.map.map0.mpe1 = dp_atoi(param_list[3]);
+			qmap_set.map.map0.mpe2 = dp_atoi(param_list[4]);
+			qmap_set.map.map0.dp_port = dp_atoi(param_list[5]);
+			qmap_set.map.map0.flowid = dp_atoi(param_list[6]);
+			qmap_set.map.map0.dec = dp_atoi(param_list[7]);
+			qmap_set.map.map0.enc = dp_atoi(param_list[8]);
+			qmap_set.map.map0.class = dp_atoi(param_list[9]);
+		} else if (dp_strncmpi(param_list[2], "mode1",
+				       strlen("mode1")) == 0) {
+			if (num < 7) {
+				PR_INFO("Wrong Parameter(try help):%s\n",
+					"echo help > /sys/kernel/debug/dp/qos");
+				return count;
+			}
+			qmap_set.qm_mode = DP_Q_MAP_MODE1;
+			qmap_set.map.map1.mpe1 = dp_atoi(param_list[3]);
+			qmap_set.map.map1.mpe2 = dp_atoi(param_list[4]);
+			qmap_set.map.map1.dp_port = dp_atoi(param_list[5]);
+			qmap_set.map.map1.subif = dp_atoi(param_list[6]);
+		} else if (dp_strncmpi(param_list[2], "mode2",
+				       strlen("mode2")) == 0) {
+			if (num < 7) {
+				PR_INFO("Wrong Parameter(try help):%s\n",
+					"echo help > /sys/kernel/debug/dp/qos");
+				return count;
+			}
+			qmap_set.qm_mode = DP_Q_MAP_MODE2;
+			qmap_set.map.map2.mpe1 = dp_atoi(param_list[3]);
+			qmap_set.map.map2.mpe2 = dp_atoi(param_list[4]);
+			qmap_set.map.map2.dp_port = dp_atoi(param_list[5]);
+			qmap_set.map.map2.subif = dp_atoi(param_list[6]);
+			qmap_set.map.map2.class = dp_atoi(param_list[7]);
+		} else if (dp_strncmpi(param_list[2], "mode3",
+				       strlen("mode3")) == 0) {
+			if (num < 7) {
+				PR_INFO("Wrong Parameter(try help):%s\n",
+					"echo help > /sys/kernel/debug/dp/qos");
+				return count;
+			}
+			qmap_set.qm_mode = DP_Q_MAP_MODE3;
+			qmap_set.map.map3.mpe1 = dp_atoi(param_list[3]);
+			qmap_set.map.map3.mpe2 = dp_atoi(param_list[4]);
+			qmap_set.map.map3.dp_port = dp_atoi(param_list[5]);
+			qmap_set.map.map3.subif = dp_atoi(param_list[6]);
+			qmap_set.map.map3.class = dp_atoi(param_list[7]);
+		} else {
+			PR_ERR("unknown mode provided!\n");
+		}
+
+		if (dp_queue_map_set(&qmap_set, 0)) {
+			PR_ERR("dp_queue_map_set failed\n");
+			return count;
+		}
+	} else if (dp_strncmpi(param_list[0], "get_child",
+			       strlen("get_child")) == 0) {
+		struct dp_node_child node = {0};
+		int idx = 0;
+
+		if (dp_strncmpi(param_list[2], "sched", strlen("sched")) == 0) {
+			node.type = DP_NODE_SCH;
+			node.id.sch_id = dp_atoi(param_list[1]);
+		} else if (dp_strncmpi(param_list[2], "port",
+				       strlen("port")) == 0) {
+			node.type = DP_NODE_PORT;
+			node.id.cqm_deq_port = dp_atoi(param_list[1]);
+		} else {
+			PR_ERR("unknown type %s\n", param_list[2]);
+		}
+
+		if (dp_children_get(&node, 0)) {
+			PR_ERR("dp_children_get failed\n");
+			return count;
+		}
+		if (node.num)
+			PR_INFO("Node[%d] has {%d} Children!!\n",
+				node.id.q_id, node.num);
+		for (idx = 0; idx < PROC_DP_MAX_LEAF; idx++) {
+			if (node.child[idx].id.q_id) {
+				if (node.child[idx].type == DP_NODE_SCH)
+					PR_INFO("Child:[%d] is Sched:[%d]\n",
+						idx, node.child[idx].id.q_id);
+				else if (node.child[idx].type == DP_NODE_QUEUE)
+					PR_INFO("Child:[%d] is Q:[%d]\n",
+						idx, node.child[idx].id.q_id);
+				else
+					PR_INFO("Child:[%d] is FREE\n", idx);
+			}
+		}
+	} else if (dp_strncmpi(param_list[0], "q_link",
+			       strlen("q_link")) == 0) {
+		struct dp_qos_link q_link = {0};
+		int i = 0;
+
+		q_link.q_id = dp_atoi(param_list[1]);
+
+		if (dp_link_get(&q_link, 0)) {
+			PR_ERR("dp_link_get failed\n");
+			return count;
+		}
+		if (!q_link.n_sch_lvl) {
+			PR_INFO("Q[%d](arbi:%d|prio:%d)->PORT[%d]\n",
+				q_link.q_id, q_link.q_arbi, q_link.q_prio_wfq,
+				q_link.cqm_deq_port);
+		} else {
+			PR_INFO("Q[%d](arbi:%d|prio:%d)\n",
+				q_link.q_id, q_link.q_arbi,
+				q_link.q_prio_wfq);
+			for (i = 0; i < q_link.n_sch_lvl; i++) {
+				PR_INFO("%s(%d):SCH[%d](arbi:%d|prio:%d)\n",
+					"Parent level", i, q_link.sch[i].id,
+					q_link.sch[i].arbi,
+					q_link.sch[i].prio_wfq);
+			}
+			PR_INFO("Parent level(%d):PORT[%d] <Final Parent>\n",
+				i, q_link.cqm_deq_port);
+		}
+	} else if (dp_strncmpi(param_list[0], "get_level",
+			       strlen("get_level")) == 0) {
+		struct dp_qos_level dp = {0};
+
+		dp.inst = 0;
+
+		if (dp_qos_level_get(&dp, 0)) {
+			PR_ERR("dp_qos_level_get failed\n");
+			return count;
+		}
+		if (dp.max_sch_lvl)
+			PR_INFO("Q->SCH(%d)->PORT, level:[%d]\n",
+				dp.max_sch_lvl, dp.max_sch_lvl);
+		else
+			PR_INFO("Q->PORT, level:[%d]\n", dp.max_sch_lvl);
+	} else if (dp_strncmpi(param_list[0], "q_conf",
+			       strlen("q_conf")) == 0) {
+		struct dp_queue_conf q_conf = {0};
+
+		q_conf.inst = 0;
+		q_conf.q_id = dp_atoi(param_list[1]);
+
+		if (dp_queue_conf_get(&q_conf, 0)) {
+			PR_ERR("dp_queue_conf_get failed\n");
+			return count;
+		}
+		if (q_conf.act & DP_NODE_EN)
+			PR_INFO("Q(%d) action is ENABLED:[%d]\n",
+				q_conf.q_id, q_conf.act);
+		else
+			PR_INFO("Q(%d) action is BLOCKED:[%d]\n",
+				q_conf.q_id, q_conf.act);
+		if (q_conf.drop == DP_QUEUE_DROP_WRED) {
+			PR_INFO("Q(%d) is in WRED MODE:[%d]\n",
+				q_conf.q_id, q_conf.drop);
+			PR_INFO("Q(%d) is min_size_1:[%d]\n",
+				q_conf.q_id, q_conf.min_size[0]);
+			PR_INFO("Q(%d) is max_size_1:[%d]\n",
+				q_conf.q_id, q_conf.max_size[0]);
+			PR_INFO("Q(%d) is min_size_2:[%d]\n",
+				q_conf.q_id, q_conf.min_size[1]);
+			PR_INFO("Q(%d) is max_size_2:[%d]\n",
+				q_conf.q_id, q_conf.max_size[1]);
+			PR_INFO("Q(%d) is max_allowed:[%d]\n",
+				q_conf.q_id, q_conf.wred_max_allowed);
+			PR_INFO("Q(%d) is min_gauranteed:[%d]\n",
+				q_conf.q_id, q_conf.wred_min_guaranteed);
+			PR_INFO("Q(%d) is wred_slope_1:[%d]\n",
+				q_conf.q_id, q_conf.wred_slope[0]);
+			PR_INFO("Q(%d) is wred_slope_2:[%d]\n",
+				q_conf.q_id, q_conf.wred_slope[1]);
+		} else {
+			PR_INFO("Q(%d) is in DROP TAIL MODE:[%d]\n",
+				q_conf.q_id, q_conf.drop);
+			PR_INFO("Q(%d) is min_size_1:[%d]\n",
+				q_conf.q_id, q_conf.min_size[0]);
+			PR_INFO("Q(%d) is max_size_1:[%d]\n",
+				q_conf.q_id, q_conf.max_size[0]);
+			PR_INFO("Q(%d) is min_size_2:[%d]\n",
+				q_conf.q_id, q_conf.min_size[1]);
+			PR_INFO("Q(%d) is max_size_2:[%d]\n",
+				q_conf.q_id, q_conf.max_size[1]);
+		}
+	} else {
+		PR_INFO("Wrong Parameter:\n");
+		qos_create_qos_help();
+	}
+	return count;
+}
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_qos.c b/drivers/net/ethernet/lantiq/datapath/datapath_qos.c
new file mode 100644
index 000000000000..f10a5e3040a2
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/datapath_qos.c
@@ -0,0 +1,229 @@
+#include <net/datapath_api.h>
+#include <net/datapath_api_qos.h>
+#include "datapath.h"
+
+int dp_node_link_add(struct dp_node_link *info, int flag)
+
+{
+	if (!dp_port_prop[info->inst].info.dp_qos_platform_set)
+		return DP_FAILURE;
+	return dp_port_prop[info->inst].info.
+		dp_qos_platform_set(NODE_LINK_ADD, info, flag);
+}
+EXPORT_SYMBOL(dp_node_link_add);
+
+int dp_node_unlink(struct dp_node_link *info, int flag)
+{
+	if (!dp_port_prop[info->inst].info.dp_qos_platform_set)
+		return DP_FAILURE;
+	return dp_port_prop[info->inst].info.
+		dp_qos_platform_set(NODE_UNLINK, info, flag);
+}
+EXPORT_SYMBOL(dp_node_unlink);
+
+int dp_node_link_get(struct dp_node_link *info, int flag)
+{
+	if (!dp_port_prop[info->inst].info.dp_qos_platform_set)
+		return DP_FAILURE;
+	return dp_port_prop[info->inst].info.
+		dp_qos_platform_set(NODE_LINK_GET, info, flag);
+}
+EXPORT_SYMBOL(dp_node_link_get);
+
+int dp_node_link_en_set(struct dp_node_link_enable *en, int flag)
+{
+	if (!dp_port_prop[en->inst].info.dp_qos_platform_set)
+		return DP_FAILURE;
+	return dp_port_prop[en->inst].info.
+		dp_qos_platform_set(NODE_LINK_EN_SET, en, flag);
+}
+EXPORT_SYMBOL(dp_node_link_en_set);
+
+int dp_node_link_en_get(struct dp_node_link_enable *en, int flag)
+{
+	if (!dp_port_prop[en->inst].info.dp_qos_platform_set)
+		return DP_FAILURE;
+	return dp_port_prop[en->inst].info.
+		dp_qos_platform_set(NODE_LINK_EN_GET, en, flag);
+}
+EXPORT_SYMBOL(dp_node_link_en_get);
+
+int dp_link_add(struct dp_qos_link *cfg, int flag)
+{
+	if (!dp_port_prop[cfg->inst].info.dp_qos_platform_set)
+		return DP_FAILURE;
+	return dp_port_prop[cfg->inst].info.
+		dp_qos_platform_set(LINK_ADD, cfg, flag);
+}
+EXPORT_SYMBOL(dp_link_add);
+
+int dp_link_get(struct dp_qos_link *cfg, int flag)
+{
+	if (!dp_port_prop[cfg->inst].info.dp_qos_platform_set)
+		return DP_FAILURE;
+	return dp_port_prop[cfg->inst].info.
+			dp_qos_platform_set(LINK_GET, cfg, flag);
+}
+EXPORT_SYMBOL(dp_link_get);
+
+int dp_qos_link_prio_set(struct dp_node_prio *info, int flag)
+{
+	if (!dp_port_prop[info->inst].info.dp_qos_platform_set)
+		return DP_FAILURE;
+	return dp_port_prop[info->inst].info.
+			dp_qos_platform_set(LINK_PRIO_SET, info, flag);
+}
+EXPORT_SYMBOL(dp_qos_link_prio_set);
+
+int dp_qos_link_prio_get(struct dp_node_prio *info, int flag)
+{
+	if (!dp_port_prop[info->inst].info.dp_qos_platform_set)
+		return DP_FAILURE;
+	return dp_port_prop[info->inst].info.
+			dp_qos_platform_set(LINK_PRIO_GET, info, flag);
+}
+EXPORT_SYMBOL(dp_qos_link_prio_get);
+
+int dp_queue_conf_set(struct dp_queue_conf *cfg, int flag)
+{
+	if (!dp_port_prop[cfg->inst].info.dp_qos_platform_set)
+		return DP_FAILURE;
+	return dp_port_prop[cfg->inst].info.
+		dp_qos_platform_set(QUEUE_CFG_SET, cfg, flag);
+}
+EXPORT_SYMBOL(dp_queue_conf_set);
+
+int dp_queue_conf_get(struct dp_queue_conf *cfg, int flag)
+{
+	if (!dp_port_prop[cfg->inst].info.dp_qos_platform_set)
+		return DP_FAILURE;
+	return dp_port_prop[cfg->inst].info.
+		dp_qos_platform_set(QUEUE_CFG_GET, cfg, flag);
+}
+EXPORT_SYMBOL(dp_queue_conf_get);
+
+int dp_shaper_conf_set(struct dp_shaper_conf *cfg, int flag)
+{
+	if (!dp_port_prop[cfg->inst].info.dp_qos_platform_set)
+		return DP_FAILURE;
+	return dp_port_prop[cfg->inst].info.
+		dp_qos_platform_set(SHAPER_SET, cfg, flag);
+}
+EXPORT_SYMBOL(dp_shaper_conf_set);
+
+int dp_shaper_conf_get(struct dp_shaper_conf *cfg, int flag)
+{
+	return dp_port_prop[cfg->inst].info.
+		dp_qos_platform_set(SHAPER_GET, cfg, flag);
+}
+EXPORT_SYMBOL(dp_shaper_conf_get);
+
+int dp_node_alloc(struct dp_node_alloc *node, int flag)
+{
+	if (!dp_port_prop[node->inst].info.dp_qos_platform_set)
+		return DP_FAILURE;
+	return dp_port_prop[node->inst].info.
+		dp_qos_platform_set(NODE_ALLOC, node, flag);
+}
+EXPORT_SYMBOL(dp_node_alloc);
+
+int dp_node_free(struct dp_node_alloc *node, int flag)
+{
+	if (!dp_port_prop[node->inst].info.dp_qos_platform_set)
+		return DP_FAILURE;
+	return dp_port_prop[node->inst].info.
+		dp_qos_platform_set(NODE_FREE, node, flag);
+}
+EXPORT_SYMBOL(dp_node_free);
+
+int dp_node_children_free(struct dp_node_alloc *node, int flag)
+{
+	if (!dp_port_prop[node->inst].info.dp_qos_platform_set)
+		return DP_FAILURE;
+	return dp_port_prop[node->inst].info.
+		dp_qos_platform_set(NODE_CHILDREN_FREE, node, flag);
+}
+EXPORT_SYMBOL(dp_node_children_free);
+
+int dp_deq_port_res_get(struct dp_dequeue_res *res, int flag)
+{
+	dp_subif_t *subif;
+	struct pmac_port_info *port_info;
+
+	if (!res || !dp_port_prop[res->inst].info.dp_qos_platform_set)
+		return DP_FAILURE;
+	if (res->dev) { /*fill dp_port if dev is provided */
+		subif = kzalloc(sizeof(*subif), GFP_KERNEL);
+		if (!subif)
+			return DP_FAILURE;
+		dp_get_netif_subifid(res->dev, NULL, NULL, NULL, subif, 0);
+		if (!subif->subif_num) {
+			PR_ERR("Not found dev %s\n", res->dev->name);
+			return DP_FAILURE;
+		}
+		res->dp_port = subif->port_id;
+		kfree(subif);
+		subif = NULL;
+	}
+	port_info = &dp_port_info[res->inst][res->dp_port];
+	DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+		 "dp_deq_port_res_get: dp_port=%d tconf_idx=%d\n",
+		 res->dp_port, res->cqm_deq_idx);
+	return dp_port_prop[res->inst].info.
+		dp_qos_platform_set(DEQ_PORT_RES_GET, res, flag);
+}
+EXPORT_SYMBOL(dp_deq_port_res_get);
+
+int dp_counter_mode_set(struct dp_counter_conf *cfg, int flag)
+{
+	if (!dp_port_prop[cfg->inst].info.dp_qos_platform_set)
+		return DP_FAILURE;
+	return dp_port_prop[cfg->inst].info.
+		dp_qos_platform_set(COUNTER_MODE_SET, cfg, flag);
+}
+EXPORT_SYMBOL(dp_counter_mode_set);
+
+int dp_counter_mode_get(struct dp_counter_conf *cfg, int flag)
+{
+	if (!dp_port_prop[cfg->inst].info.dp_qos_platform_set)
+		return DP_FAILURE;
+	return dp_port_prop[cfg->inst].info.
+		dp_qos_platform_set(COUNTER_MODE_GET, cfg, flag);
+}
+EXPORT_SYMBOL(dp_counter_mode_get);
+
+int dp_queue_map_set(struct dp_queue_map_set *cfg, int flag)
+{
+	if (!dp_port_prop[cfg->inst].info.dp_qos_platform_set)
+		return DP_FAILURE;
+	return dp_port_prop[cfg->inst].info.
+		dp_qos_platform_set(QUEUE_MAP_SET, cfg, flag);
+}
+EXPORT_SYMBOL(dp_queue_map_set);
+
+int dp_queue_map_get(struct dp_queue_map_get *cfg, int flag)
+{
+	if (!dp_port_prop[cfg->inst].info.dp_qos_platform_set)
+		return DP_FAILURE;
+	return dp_port_prop[cfg->inst].info.
+		dp_qos_platform_set(QUEUE_MAP_GET, cfg, flag);
+}
+EXPORT_SYMBOL(dp_queue_map_get);
+
+int dp_children_get(struct dp_node_child *cfg, int flag)
+{
+	if (!dp_port_prop[cfg->inst].info.dp_qos_platform_set)
+		return DP_FAILURE;
+	return dp_port_prop[cfg->inst].info.
+		dp_qos_platform_set(NODE_CHILDREN_GET, cfg, flag);
+}
+EXPORT_SYMBOL(dp_children_get);
+
+int dp_qos_level_get(struct dp_qos_level *cfg, int flag)
+{
+	if (!dp_port_prop[cfg->inst].info.dp_qos_platform_set)
+		return DP_FAILURE;
+	return dp_port_prop[cfg->inst].info.
+		dp_qos_platform_set(QOS_LEVEL_GET, cfg, flag);
+}
+EXPORT_SYMBOL(dp_qos_level_get);
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_soc.c b/drivers/net/ethernet/lantiq/datapath/datapath_soc.c
new file mode 100644
index 000000000000..407558d7d2dd
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/datapath_soc.c
@@ -0,0 +1,60 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+#include<linux/init.h>
+#include<linux/module.h>
+#include <linux/kernel.h>
+#include <linux/version.h>
+#include <linux/if_ether.h>
+#include <linux/ethtool.h>
+#include <lantiq_soc.h>
+#include <net/datapath_api.h>
+#include "datapath.h"
+#include "datapath_instance.h"
+#include "datapath_swdev_api.h"
+
+#ifdef CONFIG_FALCONMX_CQM
+#define LTQ_DATAPATH_SOC_FALCON_MX
+#endif
+
+int request_dp(u32 flag)
+{
+	struct dp_inst_info info;
+
+#if IS_ENABLED(CONFIG_FALCONMX_CQM) || \
+	IS_ENABLED(CONFIG_LTQ_DATAPATH_DDR_SIMULATE_GSWIP31) /*testing only */
+	info.type = GSWIP31_TYPE;
+	info.ver = GSWIP31_VER;
+	info.ops[0] = gsw_get_swcore_ops(0);
+	info.ops[1] = gsw_get_swcore_ops(0);
+#else
+	info.type = GSWIP30_TYPE;
+	info.ver = GSWIP30_VER;
+	info.ops[0] = gsw_get_swcore_ops(0);
+	info.ops[1] = gsw_get_swcore_ops(1);
+#endif
+	info.cbm_inst = 0;
+	info.qos_inst = 0;
+	if (dp_request_inst(&info, flag)) {
+		PR_ERR("dp_request_inst failed\n");
+		return -1;
+	}
+	return 0;
+}
+
+int register_dp_cap(u32 flag)
+{
+#ifdef CONFIG_LTQ_DATAPATH_HAL_GSWIP31
+	register_dp_cap_gswip31(0);
+#endif
+
+#ifdef CONFIG_LTQ_DATAPATH_HAL_GSWIP30
+	register_dp_cap_gswip30(0);
+#endif
+	return 0;
+}
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_swdev.c b/drivers/net/ethernet/lantiq/datapath/datapath_swdev.c
new file mode 100644
index 000000000000..33bb7d81a400
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/datapath_swdev.c
@@ -0,0 +1,1134 @@
+/*
+ * Copyright (C) Intel Corporation
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include<linux/init.h>
+#include<linux/module.h>
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/version.h>
+#include <linux/if_ether.h>
+#include <linux/ethtool.h>
+#include <linux/proc_fs.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/clk.h>
+#include <linux/if_ether.h>
+#include <linux/if_bridge.h>
+#include <linux/clk.h>
+#include <linux/ip.h>
+#include <linux/if_vlan.h>
+#include <net/ip.h>
+#include <linux/ethtool.h>
+#include <net/rtnetlink.h>
+#include <generated/utsrelease.h>
+#include <net/datapath_api.h>
+#include "datapath_swdev.h"
+#include "datapath.h"
+#include "datapath_instance.h"
+
+static void dp_swdev_insert_bridge_id_entry(struct br_info *);
+static void dp_swdev_remove_bridge_id_entry(struct br_info *);
+static int dp_swdev_add_bport_to_list(struct br_info *br_item,
+				      int bport);
+static int dp_swdev_del_bport_from_list(struct br_info *br_item,
+					int bport);
+
+struct hlist_head
+	g_bridge_id_entry_hash_table[DP_MAX_INST][BR_ID_ENTRY_HASH_TABLE_SIZE];
+static spinlock_t dp_swdev_lock;
+
+static inline void swdev_lock(void)
+{
+	spin_lock_bh(&dp_swdev_lock);
+}
+
+static inline void swdev_unlock(void)
+{
+	spin_unlock_bh(&dp_swdev_lock);
+}
+
+u16 dp_swdev_cal_hash(unsigned char *name)
+{
+	size_t hash = 5381;
+
+	while (*name)
+		hash = 33 * hash ^ (unsigned char)*name++;
+	return (u16)(hash & 0x3F);
+}
+
+struct br_info *dp_swdev_bridge_entry_lookup(char *br_name,
+					     int inst)
+{
+	u16 idx;
+	struct br_info *br_item = NULL;
+	struct hlist_head *tmp;
+
+	idx = dp_swdev_cal_hash(br_name);
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "hash index:%d\n", idx);
+	tmp = (&g_bridge_id_entry_hash_table[inst][idx]);
+	hlist_for_each_entry(br_item, tmp, br_hlist) {
+		if (br_item) {
+			if (strcmp(br_name, br_item->br_device_name) == 0) {
+				DP_DEBUG(DP_DBG_FLAG_SWDEV,
+					 "hash entry found(%s)\n",
+					 br_name);
+				return br_item;
+			}
+		}
+	}
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "No hash entry found(%s)\n",
+		 br_name);
+	return NULL;
+}
+
+static void dp_swdev_remove_bridge_id_entry(struct br_info *br_item)
+{
+	/*TODO reset switch bridge configurations*/
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "hash del\n");
+	hlist_del(&br_item->br_hlist);
+	kfree(br_item);
+}
+
+static void dp_swdev_insert_bridge_id_entry(struct br_info
+					    *br_item)
+{
+	u16 idx;
+	struct hlist_head *tmp;
+
+	idx = dp_swdev_cal_hash(br_item->br_device_name);
+	tmp = (&g_bridge_id_entry_hash_table[br_item->inst][idx]);
+	hlist_add_head(&br_item->br_hlist, tmp);
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "hash added idx:%d bridge(%s)\n",
+		 idx, br_item->br_device_name);
+}
+
+int dp_swdev_bridge_id_entry_free(int instance)
+{
+	u32 index;
+	struct hlist_node *tmp;
+	struct br_info *br_item;
+
+	for (index = 0; index < BR_ID_ENTRY_HASH_TABLE_SIZE;
+		index++) {
+		hlist_for_each_entry_safe
+			(br_item, tmp,
+			 &g_bridge_id_entry_hash_table[instance][index],
+			 br_hlist) {
+			dp_swdev_remove_bridge_id_entry(br_item);
+		}
+	}
+	return 0;
+}
+
+int dp_swdev_bridge_id_entry_init(void)
+{
+	int i, j;
+
+	spin_lock_init(&dp_swdev_lock);
+	for (i = 0; i < DP_MAX_INST; i++)
+		for (j = 0; j < BR_ID_ENTRY_HASH_TABLE_SIZE; j++)
+			INIT_HLIST_HEAD(&g_bridge_id_entry_hash_table[i][j]);
+
+	return 0;/*<success>*/
+}
+
+static int dp_swdev_del_bport_from_list(struct br_info *br_item,
+					int bport)
+{
+	int found = 0;
+	struct bridge_member_port *temp_list = NULL;
+
+	list_for_each_entry(temp_list, &br_item->bp_list, list) {
+		if (temp_list->portid == bport) {
+			found = 1;
+			break;
+		}
+	}
+	if (found) {
+		list_del(&temp_list->list);
+		kfree(temp_list);
+		return 1;
+	}
+	return 0;
+}
+
+static int dp_swdev_add_bport_to_list(struct br_info *br_item,
+				      int bport)
+{
+	int found = 0;
+	struct bridge_member_port *bport_list = NULL;
+	struct bridge_member_port *temp_list = NULL;
+
+	list_for_each_entry(temp_list, &br_item->bp_list, list) {
+		if (temp_list->portid == bport) {
+			found = 1;
+			break;
+		}
+	}
+	if (found == 0) {
+		bport_list = (struct bridge_member_port *)
+			kmalloc(sizeof(struct bridge_member_port),
+				GFP_KERNEL);
+		if (!bport_list) {
+			PR_ERR("\n Node creation failed\n");
+			return -1;
+		}
+		bport_list->dev_reg_flag = br_item->flag;
+		bport_list->portid = bport;
+		DP_DEBUG(DP_DBG_FLAG_SWDEV, "bport:%d reg_flag:%d\n",
+			 bport_list->portid, bport_list->dev_reg_flag);
+		list_add(&bport_list->list, &br_item->bp_list);
+	}
+	return 0;
+}
+
+static int dp_swdev_clr_gswip_cfg(struct bridge_id_entry_item *br_item,
+				  u8 *addr)
+{
+	struct br_info *br_info;
+	int ret;
+
+	if (br_item->flags == BRIDGE_NO_ACTION) {
+		DP_DEBUG(DP_DBG_FLAG_SWDEV,
+			 "bport not added so no action required\n");
+		return 0;
+	}
+	br_info = dp_swdev_bridge_entry_lookup(br_item->br_device_name,
+					       br_item->inst);
+	if (!br_info)
+		return 0;
+	if (dp_swdev_del_bport_from_list(br_info, br_item->portid)) {
+		ret = dp_port_prop[br_item->inst].info.
+			swdev_bridge_port_cfg_reset(br_info,
+						    br_item->inst,
+						    br_item->portid);
+		if (ret == DEL_BRENTRY) {
+			dp_port_prop[br_item->inst].info.
+				swdev_free_brcfg(br_item->inst, br_item->fid);
+			dp_swdev_remove_bridge_id_entry(br_info);
+			DP_DEBUG(DP_DBG_FLAG_SWDEV,
+				 "rem bport(%d),bridge(%s)\n",
+				 br_item->portid,
+				 br_item->br_device_name);
+		}
+		DP_DEBUG(DP_DBG_FLAG_SWDEV, "rem bport(%d)\n",
+			 br_item->portid);
+		return 0;
+	}
+	DP_DEBUG(DP_DBG_FLAG_SWDEV,
+		 "No configuration,Pls check!!\n");
+	return 0;
+}
+
+static int dp_swdev_cfg_vlan(struct bridge_id_entry_item *br_item,
+			     struct net_device *dev)
+{
+	struct dp_dev *dp_dev;
+	u32 idx, inst;
+	int vap;
+
+	/*br_info = dp_swdev_bridge_entry_lookup(br_item->br_device_name,
+	 *br_item->inst);
+	 *if (!br_info)
+	 *	return 0;
+	 */
+	/*if (br_info->flag & LOGIC_DEV_REGISTER) {*/
+	if (br_item->flags & LOGIC_DEV_REGISTER) {
+		/*get_vlan_via_dev(dev, &vlan_prop);*/
+		idx = dp_dev_hash(dev, NULL);
+		dp_dev = dp_dev_lookup(&dp_dev_list[idx], dev, NULL, 0);
+		if (!dp_dev) {
+			PR_ERR("\n dp_dev NULL\n");
+			/* Cannot return -1 from here as this fn is
+			 * called by swdev commit phase
+			 */
+			return 0;
+		}
+		inst = br_item->inst;
+		vap = GET_VAP(dp_dev->ctp,
+			      dp_port_info[inst][dp_dev->ep].vap_offset,
+			      dp_port_info[inst][dp_dev->ep].vap_mask);
+		DP_DEBUG(DP_DBG_FLAG_SWDEV, "vap=%d ep=%d bp=%d\n",
+			 vap, dp_dev->ep, dp_dev->bp);
+		dp_port_prop[br_item->inst].info.dp_cfg_vlan(br_item->inst,
+							     vap, dp_dev->ep);
+	}
+	return 0;
+}
+
+static int dp_swdev_filter_vlan(struct net_device *dev,
+				const struct switchdev_obj *obj,
+				struct switchdev_trans *trans,
+				struct net_device *br_dev)
+{
+	struct br_info *br_info;
+	struct bridge_id_entry_item *br_item;
+	dp_subif_t subif = {0};
+
+	if (switchdev_trans_ph_prepare(trans)) {
+		/*Get current BPORT ID,instance from DP*/
+		if (dp_get_netif_subifid(dev, NULL, NULL, NULL, &subif, 0)) {
+			PR_ERR("%s dp_get_netif_subifid failed for %s\n",
+			       __func__, dev->name);
+			return -EOPNOTSUPP;
+		}
+		br_item = kmalloc(sizeof(*br_item), GFP_KERNEL);
+		if (!br_item)
+			//TODO need to check dequeue if no memory
+			return -ENOMEM;
+		br_item->inst = subif.inst;
+		/* current bridge member port*/
+		br_item->portid = subif.bport;
+		swdev_lock();
+		br_info = dp_swdev_bridge_entry_lookup(br_dev->name,
+						       subif.inst);
+		if (br_info) {
+			strcpy(br_item->br_device_name,
+			       br_info->br_device_name);
+			br_item->fid = br_info->fid;
+		}
+		switchdev_trans_item_enqueue(trans, br_item,
+					     kfree, &br_item->tritem);
+		swdev_unlock();
+		return 0;
+	}
+	br_item = switchdev_trans_item_dequeue(trans);
+	if (br_item)
+		dp_swdev_cfg_vlan(br_item, dev);
+	return 0;
+}
+
+static int dp_swdev_cfg_gswip(struct bridge_id_entry_item *br_item, u8 *addr)
+{
+	struct br_info *br_info;
+
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "britem flags:%x\n", br_item->flags);
+	if (br_item->flags & ADD_BRENTRY) {
+		DP_DEBUG(DP_DBG_FLAG_SWDEV, "Add br entry %s\n",
+			 br_item->br_device_name);
+		if ((dp_port_prop[br_item->inst].info.
+			swdev_bridge_cfg_set(br_item->inst,
+					     br_item->fid) == 0)) {
+			br_info = kmalloc(sizeof(*br_info), GFP_KERNEL);
+			if (!br_info) {
+				PR_ERR
+				("Switch cfg Failed as kmalloc %d bytes fail\n",
+				 sizeof(*br_info));
+				/*TODO need to check return value
+				 *for switchdev commit
+				 */
+				return 0;
+			}
+			br_info->fid = br_item->fid;
+			br_info->inst = br_item->inst;
+			br_info->cpu_port = ENABLE;
+			/* Logic dev flag added to verify if SWDEV registered
+			 * the logical i.e. VLAN device.Helpful during
+			 * br/bport delete
+			 */
+			if (br_item->flags & LOGIC_DEV_REGISTER) {
+				//br_item->flags &= ~LOGIC_DEV_REGISTER;
+				br_info->flag = LOGIC_DEV_REGISTER;
+			}
+			strcpy(br_info->br_device_name,
+			       br_item->br_device_name);
+			INIT_LIST_HEAD(&br_info->bp_list);
+			dp_swdev_insert_bridge_id_entry(br_info);
+			dp_swdev_add_bport_to_list(br_info,
+						   br_item->portid);
+			dp_port_prop[br_item->inst].info.
+				swdev_bridge_port_cfg_set(br_info,
+							  br_item->inst,
+							  br_item->portid);
+			br_item->flags &= ~ADD_BRENTRY;
+			DP_DEBUG(DP_DBG_FLAG_SWDEV,
+				 "added bport(%d),bridge(%s)\n",
+				 br_item->portid,
+				 br_info->br_device_name);
+			return 0;
+		}
+	} else {
+		br_info = dp_swdev_bridge_entry_lookup(br_item->br_device_name,
+						       br_item->inst);
+		if (!br_info)
+			return 0;
+		br_info->flag = 0;
+		if (br_item->flags & LOGIC_DEV_REGISTER)
+			br_info->flag = LOGIC_DEV_REGISTER;
+		dp_swdev_add_bport_to_list(br_info, br_item->portid);
+		dp_port_prop[br_item->inst].info.
+			swdev_bridge_port_cfg_set(br_info,
+						  br_item->inst,
+						  br_item->portid);
+		DP_DEBUG(DP_DBG_FLAG_SWDEV, "added bport(%d)\n",
+			 br_item->portid);
+		return 0;
+	}
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "No configuration,Pls check!!\n");
+	return 0;
+}
+
+static int dp_swdev_add_if(struct net_device *dev,
+			   const struct switchdev_attr *attr,
+			   struct switchdev_trans *trans,
+			   struct net_device *br_dev)
+{
+	struct br_info *br_info;
+	struct bridge_id_entry_item *br_item;
+	int br_id = 0;
+	struct net_device *base;
+	dp_subif_t subif = {0};
+	u32 flag = 0;
+	int port, inst;
+	u8 *addr = (u8 *)dev->dev_addr;
+
+	/* SWITCHDEV_TRANS_PREPARE phase */
+	if (switchdev_trans_ph_prepare(trans)) {
+		/*Get current BPORT ID,instance from DP*/
+		if (dp_get_netif_subifid(dev, NULL, NULL, NULL, &subif, 0)) {
+			DP_DEBUG(DP_DBG_FLAG_SWDEV,
+				 "get subifid failed for %s, chk if vlan dev\n",
+				 dev->name);
+			/*Check bridge port exists otherwise register
+			 *device with datapath  i.e. only in case of new
+			 *VLAN interface
+			 */
+			/*Check if dev is a VLAN device */
+			if (is_vlan_dev(dev)) {
+				base = get_base_dev(dev, -1);
+				DP_DEBUG(DP_DBG_FLAG_SWDEV,
+					 "base dev name:%s\n",
+					 base ? base->name : "NULL");
+				if (!base)
+					base = dev;
+				if (dp_get_netif_subifid(base, NULL, NULL,
+							 NULL, &subif, 0)) {
+					PR_ERR("dp_get_netif_subifid fail:%s\n",
+					       base->name);
+					return -EOPNOTSUPP;
+				}
+				port = subif.port_id;
+				inst = subif.inst;
+				subif.subif = -1;
+				if (dp_register_subif(dp_port_info[inst][port]
+						      .owner,
+						      dev, dev->name, &subif,
+						      DP_F_SUBIF_LOGICAL)) {
+					PR_ERR("dp_register_subif fail: %s\n",
+					       dev->name);
+					return -EOPNOTSUPP;
+				}
+				flag = LOGIC_DEV_REGISTER;
+				DP_DEBUG(DP_DBG_FLAG_SWDEV,
+					 "registered subif,bp=%d port=%d\n",
+					 subif.bport, subif.port_id);
+			} else {
+				return -EOPNOTSUPP;
+			}
+		}
+		br_item = kmalloc(sizeof(*br_item), GFP_KERNEL);
+		if (!br_item)
+			/*TODO need to check dequeue if no memory*/
+			return -ENOMEM;
+		br_item->inst = subif.inst;
+		/* current bridge member port*/
+		br_item->portid = subif.bport;
+		swdev_lock();
+		br_info = dp_swdev_bridge_entry_lookup(br_dev->name,
+						       subif.inst);
+		if (br_info) {
+			strcpy(br_item->br_device_name,
+			       br_info->br_device_name);
+			br_item->fid = br_info->fid;
+			br_item->flags = flag;
+		} else {
+			br_item->flags = ADD_BRENTRY | flag;
+			br_id = dp_port_prop[br_item->inst].info.
+				swdev_alloc_bridge_id(br_item->inst);
+			if (br_id) {
+				/* Store bridge information to add in the table.
+				 * This
+				 * info is used during switchdev commit phase
+				 */
+				strcpy(br_item->br_device_name, br_dev->name);
+				br_item->fid = br_id;
+			} else {
+				PR_ERR("Switch configuration failed\r\n");
+				kfree(br_item);
+				swdev_unlock();
+				return -EOPNOTSUPP;
+			}
+		}
+		switchdev_trans_item_enqueue(trans, br_item,
+					     kfree, &br_item->tritem);
+		swdev_unlock();
+		return 0;
+	}
+	/*configure switch in commit phase and it cannot return failure*/
+	swdev_lock();
+	br_item = switchdev_trans_item_dequeue(trans);
+	if (br_item) {
+		dp_swdev_cfg_gswip(br_item, addr);
+		if (br_item->flags & LOGIC_DEV_REGISTER)
+			/*do only for vlan flag*/
+			dp_swdev_cfg_vlan(br_item, dev);
+	}
+	swdev_unlock();
+	return 0;
+}
+
+static int dp_swdev_del_if(struct net_device *dev,
+			   const struct switchdev_attr *attr,
+			   struct switchdev_trans *trans,
+			   struct net_device *br_dev)
+{
+	struct br_info *br_info;
+	struct bridge_id_entry_item *br_item;
+	struct net_device *base, *master_dev;
+	struct bridge_member_port *temp_list = NULL;
+	dp_subif_t subif = {0};
+	int port, inst;
+	bool f_unlock = false;
+	u8 *addr = (u8 *)dev->dev_addr;
+
+	if (!rtnl_is_locked()) {
+		rtnl_lock();
+		f_unlock = true;
+	}
+	master_dev = netdev_master_upper_dev_get(attr->orig_dev);
+	if (f_unlock)
+		rtnl_unlock();
+
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "%s MASTER DEV %s\n", __func__,
+		 master_dev ? master_dev->name : "NULL");
+	/* SWITCHDEV_TRANS_PREPARE phase */
+	if (switchdev_trans_ph_prepare(trans)) {
+		/*Get current BR_PORT ID from DP*/
+		if (dp_get_netif_subifid(dev, NULL, NULL, NULL, &subif, 0)) {
+			DP_DEBUG(DP_DBG_FLAG_SWDEV,
+				 "dp_get_netif_subifid failed for %s\n",
+				 dev->name);
+		/*	if (!is_vlan_dev(dev))*/
+				return -EINVAL;
+		}
+		br_item = kmalloc(sizeof(*br_item), GFP_KERNEL);
+		if (!br_item)
+			/*TODO need to check dequeue if no memory*/
+			return -ENOMEM;
+		swdev_lock();
+		br_info = dp_swdev_bridge_entry_lookup(br_dev->name,
+						       subif.inst);
+		if (br_info) {
+			br_item->fid = br_info->fid;
+			br_item->inst = subif.inst;
+			/* current bridge member port*/
+			br_item->portid = subif.bport;
+			br_item->flags = BRIDGE_NO_ACTION;
+			strcpy(br_item->br_device_name,
+			       br_info->br_device_name);
+
+			list_for_each_entry(temp_list, &br_info->bp_list,
+					    list) {
+				if (temp_list->portid == br_item->portid) {
+					br_item->flags =
+						temp_list->dev_reg_flag;
+					break;
+				}
+			}
+		} else {
+			DP_DEBUG(DP_DBG_FLAG_SWDEV,
+				 "No bridge entry or bport to delete\r\n");
+			br_item->flags = BRIDGE_NO_ACTION;
+		}
+		/*TODO check return value & enqueue*/
+		switchdev_trans_item_enqueue(trans, br_item, kfree,
+					     &br_item->tritem);
+		swdev_unlock();
+		return 0;
+	}
+	/*TODO Check bridge port exists otherwise register device with datapath
+	 *i.e. only in case of new VLAN interface
+	 */
+	/*configure switch in commit phase and it cannot return failure*/
+	swdev_lock();
+	br_item = switchdev_trans_item_dequeue(trans);
+	if (br_item) {
+		dp_swdev_clr_gswip_cfg(br_item, addr);
+		/* De-Register Logical Dev i.e. VLAN DEV
+		 * if it is registered
+		 */
+		if (br_item->flags & LOGIC_DEV_REGISTER) {
+			base = get_base_dev(dev, -1);
+			DP_DEBUG(DP_DBG_FLAG_SWDEV,
+				 "base dev name:%s\n",
+				 base ? base->name : "NULL");
+			if (!base)
+				base = dev;
+			/*the previous sequence was running into a deadlock in
+			 * taking the swdev_lock
+			 */
+			dp_swdev_cfg_vlan(br_item, dev);
+			swdev_unlock();
+			if (dp_get_netif_subifid(base, NULL, NULL,
+						 NULL, &subif, 0)) {
+				PR_ERR("dp_get_netif_subifid fail:%s\n",
+				       base->name);
+				/*Cannot Return -EOPNOTSUPP
+				 * in swdev commit stage
+				 */
+				return 0;
+			}
+			port = subif.port_id;
+			inst = subif.inst;
+			if (dp_register_subif(dp_port_info[inst][port]
+						.owner,
+						dev, dev->name, &subif,
+						DP_F_DEREGISTER)) {
+				PR_ERR("dp_register_subif fail: %s\n",
+				       dev->name);
+				/*Cannot Return -EOPNOTSUPP
+				 * in swdev commit stage
+				 */
+				return 0;
+			}
+			swdev_lock();
+		}
+	}
+	swdev_unlock();
+	return 0;
+}
+
+int dp_del_br_if(struct net_device *dev, struct net_device *br_dev,
+		 int inst, int bport)
+{
+	struct br_info *br_info;
+	struct bridge_id_entry_item *br_item;
+	struct bridge_member_port *temp_list = NULL;
+	dp_subif_t subif = {0};
+	u8 *addr = (u8 *)dev->dev_addr;
+
+	br_item = kmalloc(sizeof(*br_item), GFP_KERNEL);
+	if (!br_item)
+		return -1;
+	swdev_lock();
+	br_info = dp_swdev_bridge_entry_lookup(br_dev->name, subif.inst);
+	if (br_info) {
+		br_item->fid = br_info->fid;
+		br_item->inst = inst;
+		br_item->portid = bport;
+		strcpy(br_item->br_device_name, br_info->br_device_name);
+
+		list_for_each_entry(temp_list, &br_info->bp_list, list) {
+			if (temp_list->portid == br_item->portid) {
+				br_item->flags =
+					temp_list->dev_reg_flag;
+				break;
+			}
+		}
+	} else {
+		DP_DEBUG(DP_DBG_FLAG_SWDEV,
+			 "No bridge entry or bport to delete\r\n");
+		br_item->flags = BRIDGE_NO_ACTION;
+	}
+
+	if (br_item)
+		dp_swdev_clr_gswip_cfg(br_item, addr);
+	swdev_unlock();
+	kfree(br_item);
+	return 0;
+}
+
+//#define CONFIG_LTQ_DATAPATH_SWDEV_TEST
+
+static int dp_swdev_port_attr_set(struct net_device *dev,
+				  const struct switchdev_attr *attr,
+				  struct switchdev_trans *trans)
+{
+	int err = -EOPNOTSUPP;
+	struct net_device *br_dev;
+	bool f_unlock = false;
+#ifdef CONFIG_LTQ_DATAPATH_SWDEV_TEST
+	{
+		struct net_device *br_dev =
+			netdev_master_upper_dev_get(attr->orig_dev);
+		char buf[100];
+		struct net_device *lower_dev;
+		struct list_head *iter;
+
+		buf[0] = 0;
+		if (br_dev) {
+			netdev_for_each_lower_dev(br_dev, lower_dev, iter) {
+				sprintf(buf + strlen(buf), "%s ",
+					lower_dev->name);
+			}
+		}
+		PR_INFO("flag=%d attr=%d stat=%d dev=%s orig/up_dev=%s/%s:%s\n",
+			attr->flags, attr->id, attr->u.stp_state,
+			dev->name,
+			attr->orig_dev ? attr->orig_dev->name : "NULL"
+			br_dev ? br_dev->name : "Null", buf);
+		return 0;
+	}
+#endif
+	/* switchdev attr orig dev -> bridge port dev pointer
+	 *then get the bridge dev from switchdev attr's orig dev
+	 */
+	if (!rtnl_is_locked()) {
+		rtnl_lock();
+		f_unlock = true;
+	}
+	br_dev = netdev_master_upper_dev_get(attr->orig_dev);
+	if (f_unlock)
+		rtnl_unlock();
+	if (!br_dev)
+		return -EOPNOTSUPP;
+#if 1
+	switch (attr->id) {
+	case SWITCHDEV_ATTR_ID_PORT_STP_STATE:
+		/*STP STATE forwading or ifconfig UP - add bridge*/
+		if (attr->u.stp_state == BR_STATE_FORWARDING) {
+			err = dp_swdev_add_if(attr->orig_dev,
+					      attr, trans, br_dev);
+		}
+		/*STP STATE disabled or ifconfig DOWN - del bridge*/
+		else if (attr->u.stp_state == BR_STATE_DISABLED) {
+			err = dp_swdev_del_if(attr->orig_dev,
+					      attr, trans, br_dev);
+			if (err != 0)
+				err = -EOPNOTSUPP;
+		} else {
+			return -EOPNOTSUPP;
+		}
+		break;
+	case SWITCHDEV_ATTR_ID_PORT_BRIDGE_FLAGS:
+		/* err = dp_swdev_port_attr_bridge_flags_set(dp_swdev_port,
+		 *attr->u.brport_flags,
+		 *trans);
+		 */
+		break;
+	case SWITCHDEV_ATTR_ID_BRIDGE_AGEING_TIME:
+		/*err = dp_swdev_port_attr_bridge_ageing_time_set(dp_swdev_port,
+		 *attr->u.ageing_time,
+		 *trans);
+		 */
+		break;
+	case SWITCHDEV_ATTR_ID_BRIDGE_VLAN_FILTERING:
+		/*err = dp_swdev_port_attr_bridge_br_vlan_set(dev,
+		 *attr->orig_dev,trans);
+		 */
+	default:
+		err = -EOPNOTSUPP;
+		break;
+	}
+	return err;
+#endif
+}
+
+static int dp_swdev_port_attr_get(struct net_device *dev,
+				  struct switchdev_attr *attr)
+{
+	struct net_device *br_dev;
+	struct br_info *br_info;
+	dp_subif_t subif = {0};
+	bool f_unlock = false;
+	/*For this api default err return value "-EOPNOTSUPP"
+	 * cannot be set as this blocks bridgeport offload_fwd_mark
+	 * setting at linux bridge level("nbp_switchdev_mark_set")
+	 */
+	if (!rtnl_is_locked()) {
+		rtnl_lock();
+		f_unlock = true;
+	}
+	br_dev = netdev_master_upper_dev_get(attr->orig_dev);
+	if (f_unlock)
+		rtnl_unlock();
+	if (!br_dev)
+		return 0;
+
+	if (dp_get_netif_subifid(dev, NULL, NULL, NULL, &subif, 0)) {
+		DP_DEBUG(DP_DBG_FLAG_SWDEV,
+			 "%s dp_get_netif_subifid failed for %s\n",
+			 __func__, dev->name);
+		return 0;
+	}
+
+	switch (attr->id) {
+	case SWITCHDEV_ATTR_ID_PORT_PARENT_ID:
+		br_info = dp_swdev_bridge_entry_lookup(br_dev->name,
+						       subif.inst);
+		if (!br_info)
+			return 0;
+		if (br_info->fid < 0)
+			return -EOPNOTSUPP;
+		attr->u.ppid.id_len = sizeof(br_info->fid);
+		memcpy(&attr->u.ppid.id, &br_info->fid, attr->u.ppid.id_len);
+		DP_DEBUG(DP_DBG_FLAG_SWDEV,
+			 "SWITCHDEV_ATTR_ID_PORT_PARENT_ID:%s fid=%d\n",
+			 attr->orig_dev ? attr->orig_dev->name : "NULL",
+			 br_info->fid);
+		//err = 0;
+		break;
+	case SWITCHDEV_ATTR_ID_PORT_BRIDGE_FLAGS:
+		break;
+	default:
+		return 0;
+	}
+	return 0;
+}
+
+static int dp_swdev_port_obj_add(struct net_device *dev,
+				 const struct switchdev_obj *obj,
+				 struct switchdev_trans *trans)
+{
+	int err = -EOPNOTSUPP;
+	struct net_device *br_dev;
+	bool f_unlock = false;
+#ifdef CONFIG_LTQ_DATAPATH_SWDEV_TEST
+	{
+		struct net_device *br_dev = netdev_master_upper_dev_get(dev);
+
+		PR_INFO
+		("obj_add: obj-id=%d flag=%d dev=%s orig_dev=%s up-dev=%s\n",
+		 obj->id, obj->flags,
+		 dev->name,
+		 obj->orig_dev ? obj->orig_dev->name : "NULL",
+		 br_dev ? br_dev->name : "Null");
+		return 0;
+	}
+	return err; //TODO
+
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "%s id:%d flags:%d dev name:%s\r\n",
+		 __func__, obj->id,
+		obj->flags, dev->name);
+	if (netif_is_bridge_port(dev))
+		DP_DEBUG(DP_DBG_FLAG_SWDEV,
+			 "dp_swdev_port_obj_add attr bridge port\r\n");
+	if (trans->ph_prepare == 1)
+		DP_DEBUG(DP_DBG_FLAG_SWDEV, "%s ph->prepare:%d\r\n",
+			 __func__, trans->ph_prepare);
+#endif
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "%s id:%d flags:%d dev name:%s\r\n",
+		 __func__, obj->id,
+		 obj->flags, dev->name);
+	if (!rtnl_is_locked()) {
+		rtnl_lock();
+		f_unlock = true;
+	}
+	br_dev = netdev_master_upper_dev_get(obj->orig_dev);
+	if (f_unlock)
+		rtnl_unlock();
+	if (!br_dev)
+		return err;
+	switch (obj->id) {
+	case SWITCHDEV_OBJ_ID_PORT_VLAN:
+		dp_swdev_filter_vlan(obj->orig_dev, obj, trans, br_dev);
+		break;
+	case SWITCHDEV_OBJ_ID_PORT_FDB:
+		break;
+	default:
+		err = -EOPNOTSUPP;
+		break;
+	}
+	return err;
+}
+
+static int dp_swdev_port_obj_del(struct net_device *dev,
+				 const struct switchdev_obj *obj)
+{
+	int err = -EOPNOTSUPP;
+	return err;//TODO
+#ifdef CONFIG_LTQ_DATAPATH_SWDEV_TEST
+	{
+		struct net_device *br_dev = netdev_master_upper_dev_get(dev);
+
+		PR_INFO
+		("obj_del: obj-id=%d flag=%d dev=%s orig_dev=%s up-dev=%s\n",
+		 obj->id, obj->flags,
+		 dev->name,
+		 obj->orig_dev ? obj->orig_dev->name : "NULL",
+		 br_dev ? br_dev->name : "Null");
+		return 0;
+	}
+#endif
+
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "dp_swdev_port_obj_del\r\n");
+	switch (obj->id) {
+	case SWITCHDEV_OBJ_ID_PORT_VLAN:
+		break;
+	default:
+		err = -EOPNOTSUPP;
+		break;
+	}
+	return err;
+}
+
+static int dp_swdev_port_fdb_dump(struct net_device *dev,
+				  struct switchdev_obj_port_fdb *fdb_obj,
+				  switchdev_obj_dump_cb_t *cb)
+{
+	int err = 0;
+	struct fdb_tbl *fdb_d = NULL;
+
+	list_for_each_entry(fdb_d, &fdb_tbl_list, fdb_list) {
+		if (fdb_d) {
+			if (fdb_d->port_dev != dev) {
+				continue;
+			} else {
+				ether_addr_copy(fdb_obj->addr, fdb_d->addr);
+				fdb_obj->ndm_state = NUD_PERMANENT;
+				//fdb->vid = 10;
+				err = cb(&fdb_obj->obj);
+				if (err)
+					return 0;
+			}
+		} else {
+			break;
+		}
+	}
+	return 0;
+}
+
+static int dp_swdev_port_obj_dump(struct net_device *dev,
+				  struct switchdev_obj *obj,
+				  switchdev_obj_dump_cb_t *cb)
+{
+	int err = -EOPNOTSUPP;
+
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "dp_swdev_port_obj_dump\r\n");
+	switch (obj->id) {
+#ifdef CONFIG_LTQ_DATAPATH_SWDEV_TEST
+	case SWITCHDEV_OBJ_ID_PORT_VLAN:
+		err = dp_swdev_port_vlan_dump(mlxsw_sp_port,
+					      SWITCHDEV_OBJ_PORT_VLAN(obj),
+					      cb);
+		break;
+#endif
+	case SWITCHDEV_OBJ_ID_PORT_FDB:
+		err = dp_swdev_port_fdb_dump(dev,
+					     SWITCHDEV_OBJ_PORT_FDB(obj),
+					     cb);
+		break;
+	default:
+		err = -EOPNOTSUPP;
+		break;
+	}
+	return err;
+}
+
+static int dp_ndo_bridge_setlink(struct net_device *dev,
+				 struct nlmsghdr *nlh,
+				 u16 flags)
+{
+	int status = 0;
+
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "ndo_bridge_setlink: dev=%s\n",
+		 dev ? dev->name : "NULL");
+#ifdef CONFIG_LTQ_DATAPATH_SWDEV_TEST
+	struct nlattr *attr, *br_spec;
+	int rem;
+	u16 mode = 0;
+
+	br_spec = nlmsg_find_attr(nlh, sizeof(struct ifinfomsg), IFLA_AF_SPEC);
+	if (!br_spec)
+		return -EINVAL;
+	nla_for_each_nested(attr, br_spec, rem) {
+		PR_INFO("nla_type(attr)=%d\n", nla_type(attr));
+		if (nla_type(attr) != IFLA_BRIDGE_MODE)
+			continue;
+		if (nla_len(attr) < sizeof(mode))
+			return -EINVAL;
+		mode = nla_get_u16(attr); /*like BRIDGE_MODE_VEPA */
+	}
+#endif
+	//return switchdev_port_bridge_setlink(dev, nlh, flags);
+	return status;
+}
+
+static inline int dp_ndo_bridge_getlink(struct sk_buff *skb, u32 pid,
+					u32 seq, struct net_device *dev,
+					u32 filter_mask, int nlflags)
+{
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "port_bridge_getlink: dev=%s\n",
+		 dev ? dev->name : "NULL");
+	if (!dev)
+		return -EINVAL;
+
+	return switchdev_port_bridge_getlink(skb, pid, seq, dev,
+					    filter_mask, nlflags);
+}
+
+int dp_ndo_bridge_dellink(struct net_device *dev, struct nlmsghdr *nlh,
+			  u16 flags)
+{
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "port_bridge_dellink: dev=%s\n",
+		 dev ? dev->name : "NULL");
+	if (!dev)
+		return -EINVAL;
+	return switchdev_port_bridge_dellink(dev, nlh, flags);
+}
+
+int dp_notif_br_alloc(struct net_device *br_dev)
+{
+	int br_id = 0;
+	struct br_info *br_info;
+
+	br_id = dp_port_prop[0].info.swdev_alloc_bridge_id(0);
+	if (br_id) {
+		if ((dp_port_prop[0].info.swdev_bridge_cfg_set(0, br_id)
+			== 0)) {
+			br_info = kmalloc(sizeof(*br_info), GFP_KERNEL);
+			if (!br_info) {
+				PR_ERR
+				("Switch cfg Failed as kmalloc %d bytes fail\n",
+				 sizeof(*br_info));
+				return -1;
+			}
+			br_info->fid = br_id;
+			br_info->inst = 0;
+			br_info->cpu_port = ENABLE;
+			br_info->flag = 0;
+			strcpy(br_info->br_device_name, br_dev->name);
+			INIT_LIST_HEAD(&br_info->bp_list);
+			dp_swdev_insert_bridge_id_entry(br_info);
+		} else {
+			PR_ERR("Switch configuration failed\r\n");
+			return -1;
+		}
+	} else {
+		PR_ERR("Switch bridge alloc failed\r\n");
+		return -1;
+	}
+	return br_id;
+}
+/*Register netdev_ops for switchdev*/
+static int dp_set_netdev_ops(struct dp_dev *dp_dev)
+{
+	if (!dp_dev)
+		return -1;
+	dp_dev->new_dev_ops.ndo_bridge_setlink = dp_ndo_bridge_setlink;
+	dp_dev->new_dev_ops.ndo_bridge_getlink = dp_ndo_bridge_getlink;
+	dp_dev->new_dev_ops.ndo_bridge_dellink = dp_ndo_bridge_dellink;
+	dp_dev->new_dev_ops.ndo_fdb_add = switchdev_port_fdb_add;
+	dp_dev->new_dev_ops.ndo_fdb_del = switchdev_port_fdb_del;
+	dp_dev->new_dev_ops.ndo_fdb_dump = switchdev_port_fdb_dump;
+	return 0;
+}
+/* This function registers the created port in datapath to switchdev */
+int dp_port_register_switchdev(struct dp_dev  *dp_dev,
+			       struct net_device *dp_port)
+{
+	if (!dp_port) {
+		PR_ERR("cannot support switchdev if dev is NULL\n");
+		return -1;
+	}
+	if (dp_port_prop[dp_dev->inst].info.swdev_flag == 1) {
+		if (!dp_port->netdev_ops) {
+			PR_ERR("netdev_ops not defined\n");
+			return -1;
+		}
+		/*backup ops*/
+		if (!dp_dev->old_dev_ops) {
+			dp_dev->old_dev_ops = NULL;
+			dp_dev->old_dev_ops = dp_port->netdev_ops;
+			dp_dev->new_dev_ops = *dp_port->netdev_ops;
+			dp_set_netdev_ops(dp_dev);
+			dp_port->netdev_ops = (const struct net_device_ops *)
+						&dp_dev->new_dev_ops;
+		} else if (dp_port->netdev_ops ==
+				(const struct net_device_ops *)
+				&dp_dev->new_dev_ops) {
+			dp_set_netdev_ops(dp_dev);
+		} else {
+			PR_ERR("error in old dev ops assignment\n");
+		}
+		dp_dev->old_swdev_ops = NULL;
+		if (dp_dev->old_swdev_ops)
+			dp_dev->old_swdev_ops = (struct switchdev_ops *)
+							dp_port->switchdev_ops;
+		if (dp_port->switchdev_ops)
+			dp_dev->new_swdev_ops = *dp_port->switchdev_ops;
+		/*tune new ops */
+		dp_dev->new_swdev_ops.switchdev_port_attr_get =
+							dp_swdev_port_attr_get;
+		dp_dev->new_swdev_ops.switchdev_port_attr_set =
+							dp_swdev_port_attr_set;
+		dp_dev->new_swdev_ops.switchdev_port_obj_add =
+							dp_swdev_port_obj_add;
+		dp_dev->new_swdev_ops.switchdev_port_obj_del =
+							dp_swdev_port_obj_del;
+		dp_dev->new_swdev_ops.switchdev_port_obj_dump =
+							dp_swdev_port_obj_dump;
+#ifdef CONFIG_LTQ_DATAPATH_SWDEV_TEST
+		dp_dev->new_dev_ops.ndo_vlan_rx_add_vid =
+				dp_swdev_vlan_rx_add,
+		dp_dev->new_dev_ops.ndo_vlan_rx_kill_vid =
+				dp_swdev_vlan_rx_kill,
+		dp_dev->new_dev_ops.ndo_change_proto_down =
+				dp_swdev_port_change_proto_down,
+		dp_dev->new_dev_ops.ndo_neigh_destroy =
+				dp_swdev_port_neigh_destroy,
+#endif
+		/*change to new ops */
+		dp_port->switchdev_ops =
+			(const struct switchdev_ops *)&dp_dev->new_swdev_ops;
+		DP_DEBUG(DP_DBG_FLAG_SWDEV,
+			 "dp_port_register_switchdev done:%s\n",
+			 dp_port->name);
+	}
+	return 0;
+}
+
+void dp_port_deregister_switchdev(struct dp_dev *dp_dev,
+				  struct net_device *dev)
+{
+	struct net_device *br_dev;
+	bool f_unlock = false;
+
+	/* Workaround for ethernet ifconfig down case
+	 * to remove port from switchdev as dev is de-registered
+	 * from DP lib
+	 */
+	if (netif_is_bridge_port(dev)) {
+		if (!rtnl_is_locked()) {
+			rtnl_lock();
+			f_unlock = true;
+		}
+		br_dev = netdev_master_upper_dev_get(dev);
+		if (f_unlock)
+			rtnl_unlock();
+		DP_DEBUG(DP_DBG_FLAG_SWDEV, "Upper br.device name:%s\n",
+			 br_dev->name);
+		if (dp_del_br_if(dev, br_dev, dp_dev->inst, dp_dev->bp)) {
+			DP_DEBUG(DP_DBG_FLAG_SWDEV,
+				 "del br intf port in DP fail:%s\n",
+				 dev->name);
+		}
+	}
+	if (dp_dev->old_swdev_ops)
+		dev->switchdev_ops = dp_dev->old_swdev_ops;
+	if (dp_dev->old_dev_ops) {
+		if (dev->netdev_ops != dp_dev->old_dev_ops) {
+			dev->netdev_ops = dp_dev->old_dev_ops;
+			dp_dev->old_dev_ops = NULL;
+		}
+	}
+}
+
+void dp_switchdev_exit(void)
+{
+	int i;
+
+	for (i = 0; i < DP_MAX_INST; i++)
+		dp_swdev_bridge_id_entry_free(i);
+}
+
+int dp_switchdev_init(void)
+{
+	dp_swdev_bridge_id_entry_init();
+	return 0;
+}
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_swdev.h b/drivers/net/ethernet/lantiq/datapath/datapath_swdev.h
new file mode 100644
index 000000000000..167cc9843d6b
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/datapath_swdev.h
@@ -0,0 +1,69 @@
+#ifndef DATAPATH_SWDEV_H
+#define DATAPATH_SWDEV_H
+
+#include <linux/if.h>
+//TODO added below header for local compilation
+#include <linux/etherdevice.h>
+#include <net/switchdev.h>
+#include <net/switch_api/lantiq_gsw_api.h> /*Switch related structures */
+#include <net/switch_api/lantiq_gsw.h>
+
+#define ENABLE 1
+#define DISABLE 0
+
+#define BRIDGE_ID_ENTRY_HASH_SHIFT			0
+#define BRIDGE_ID_ENTRY_HASH_LENGTH			6
+#define BRIDGE_ID_ENTRY_HASH_MASK ((BIT(BRIDGE_ID_ENTRY_HASH_LENGTH)) - 1)
+#define BR_ID_ENTRY_HASH_TABLE_SIZE BIT(BRIDGE_ID_ENTRY_HASH_LENGTH)
+#define BRIDGE_ID_ENTRY_HASH_TABLE_VALUE(x) \
+	(((u32)(x) >> BRIDGE_ID_ENTRY_HASH_SHIFT) & BRIDGE_ID_ENTRY_HASH_MASK)
+
+#define ADD_BRENTRY	0x00000001
+#define DEL_BRENTRY	0x00000002
+#define BRIDGE_NO_ACTION 0x00000004
+#define LOGIC_DEV_REGISTER 0x00000008
+
+struct bridge_id_entry_item {
+	u8 br_device_name[IFNAMSIZ];
+	u16 fid;
+	u32 flags;
+	u32 portid; /*Bridge port*/
+	u32 inst;
+	struct switchdev_trans_item tritem;
+};
+
+struct br_info {
+	struct hlist_node br_hlist;
+	u8 br_device_name[IFNAMSIZ];
+	u8  cpu_port; /*To set CPU port as member or not*/
+	u16 fid;
+	u32 flag;
+	u32 inst;
+	struct list_head bp_list;
+};
+
+struct bridge_member_port {
+	struct list_head list;
+	u32 portid;
+	u32 dev_reg_flag;
+	u16 bport[8];
+};
+
+struct fdb_tbl {
+	struct list_head fdb_list;
+	struct net_device *port_dev;
+	u8 addr[ETH_ALEN];
+	__be16 vid;
+};
+
+extern struct list_head fdb_tbl_list;
+extern struct hlist_head
+	g_bridge_id_entry_hash_table[DP_MAX_INST][BR_ID_ENTRY_HASH_TABLE_SIZE];
+int dp_swdev_bridge_id_entry_free(int instance);
+int dp_swdev_bridge_id_entry_init(void);
+struct br_info *dp_swdev_bridge_entry_lookup(char *br_name, int inst);
+u16 crc_cal(const u8 *data, u16 len);
+u16 dp_swdev_cal_hash(u8 *dev_name);
+void dp_switchdev_exit(void);
+
+#endif /*DATAPATH_SWDEV_H*/
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_swdev_api.h b/drivers/net/ethernet/lantiq/datapath/datapath_swdev_api.h
new file mode 100644
index 000000000000..6fbcc0ceb3d6
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/datapath_swdev_api.h
@@ -0,0 +1,22 @@
+/*
+ * Copyright (C) Intel Corporation
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#ifndef DATAPATH_SWIHDEV_API_H
+#define DATAPATH_SWIHDEV_API_H
+
+int dp_switchdev_init(void);
+int dp_port_register_switchdev(struct dp_dev *dp_dev,
+			       struct net_device *dp_port);
+void dp_port_deregister_switchdev(struct dp_dev *dp_dev,
+				  struct net_device *dp_port);
+int dp_del_br_if(struct net_device *dev, struct net_device *br_dev,
+		 int inst, int bport);
+int dp_notif_br_alloc(struct net_device *br_dev);
+
+#endif
+/*DATAPATH_SWIHDEV_API_H*/
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_tc_asym_vlan.c b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_tc_asym_vlan.c
new file mode 100644
index 000000000000..d2025137bb69
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_tc_asym_vlan.c
@@ -0,0 +1,804 @@
+/*
+ * Copyright (C) Intel Corporation
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include <net/switch_api/gsw_flow_ops.h>
+#include <net/datapath_api.h>
+#include <net/datapath_api_vlan.h>
+#include "../datapath.h"
+/***************************
+ *	Code for TC VLAN
+ ***************************/
+
+static int update_bp(struct core_ops *ops,
+		     u32 bpid,
+		     int ingress,
+		     GSW_VLANFILTER_alloc_t *pfilter,
+		     GSW_EXTENDEDVLAN_alloc_t *pextvlan)
+{
+	int ret;
+	GSW_BRIDGE_portConfig_t bpcfg1, bpcfg2;
+
+	memset(&bpcfg1, 0, sizeof(bpcfg1));
+	memset(&bpcfg2, 0, sizeof(bpcfg2));
+	bpcfg1.nBridgePortId = bpid;
+	bpcfg2.nBridgePortId = bpid;
+	if (ingress) {
+		bpcfg1.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_INGRESS_VLAN_FILTER
+				| GSW_BRIDGE_PORT_CONFIG_MASK_INGRESS_VLAN;
+		if (!pfilter) {
+			bpcfg2.bIngressVlanFilterEnable = LTQ_FALSE;
+		} else {
+			bpcfg2.bIngressVlanFilterEnable = LTQ_TRUE;
+			bpcfg2.nIngressVlanFilterBlockId =
+				pfilter->nVlanFilterBlockId;
+			bpcfg2.nIngressVlanFilterBlockSize = 0;
+		}
+		if (!pextvlan) {
+			bpcfg2.bIngressExtendedVlanEnable = LTQ_FALSE;
+		} else {
+			bpcfg2.bIngressExtendedVlanEnable = LTQ_TRUE;
+			bpcfg2.nIngressExtendedVlanBlockId =
+				pextvlan->nExtendedVlanBlockId;
+			bpcfg2.nIngressExtendedVlanBlockSize = 0;
+		}
+	} else {
+		bpcfg1.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_VLAN_FILTER1
+				| GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_VLAN;
+		if (!pfilter) {
+			bpcfg2.bEgressVlanFilter1Enable = LTQ_FALSE;
+		} else {
+			bpcfg2.bEgressVlanFilter1Enable = LTQ_TRUE;
+			bpcfg2.nEgressVlanFilter1BlockId =
+				pfilter->nVlanFilterBlockId;
+			bpcfg2.nEgressVlanFilter1BlockSize = 0;
+		}
+		if (!pextvlan) {
+			bpcfg2.bEgressExtendedVlanEnable = LTQ_FALSE;
+		} else {
+			bpcfg2.bEgressExtendedVlanEnable = LTQ_TRUE;
+			bpcfg2.nEgressExtendedVlanBlockId =
+				pextvlan->nExtendedVlanBlockId;
+			bpcfg2.nEgressExtendedVlanBlockSize = 0;
+		}
+	}
+	bpcfg2.eMask = bpcfg1.eMask;
+
+	ret = ops->gsw_brdgport_ops.BridgePort_ConfigGet(ops, &bpcfg1);
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	ret = ops->gsw_brdgport_ops.BridgePort_ConfigSet(ops, &bpcfg2);
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	if (ingress) {
+		if (bpcfg1.bIngressVlanFilterEnable != LTQ_FALSE) {
+			GSW_VLANFILTER_alloc_t alloc = {0};
+
+			alloc.nVlanFilterBlockId =
+				bpcfg1.nIngressVlanFilterBlockId;
+			ops->gsw_vlanfilter_ops.VlanFilter_Free(ops, &alloc);
+		}
+		if (bpcfg1.bIngressExtendedVlanEnable != LTQ_FALSE) {
+			GSW_EXTENDEDVLAN_alloc_t alloc = {0};
+
+			alloc.nExtendedVlanBlockId =
+				bpcfg1.nIngressExtendedVlanBlockId;
+			ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+		}
+	} else {
+		if (bpcfg1.bEgressVlanFilter1Enable != LTQ_FALSE) {
+			GSW_VLANFILTER_alloc_t alloc = {0};
+
+			alloc.nVlanFilterBlockId =
+				bpcfg1.nEgressVlanFilter1BlockId;
+			ops->gsw_vlanfilter_ops.VlanFilter_Free(ops, &alloc);
+		}
+		if (bpcfg1.bEgressExtendedVlanEnable != LTQ_FALSE) {
+			GSW_EXTENDEDVLAN_alloc_t alloc = {0};
+
+			alloc.nExtendedVlanBlockId =
+				bpcfg1.nEgressExtendedVlanBlockId;
+			ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+		}
+	}
+
+	return 0;
+}
+
+static int update_ctp(struct core_ops *ops,
+		      u32 lpid,
+		      u32 subifidg,
+		      int ingress,
+		      GSW_EXTENDEDVLAN_alloc_t *pextvlan)
+{
+	int ret;
+	GSW_CTP_portConfig_t ctpcfg1 = {0}, ctpcfg2 = {0};
+
+	ctpcfg1.nLogicalPortId = lpid;
+	ctpcfg2.nLogicalPortId = lpid;
+	ctpcfg1.nSubIfIdGroup = subifidg;
+	ctpcfg2.nSubIfIdGroup = subifidg;
+	if (ingress) {
+		ctpcfg1.eMask = GSW_CTP_PORT_CONFIG_MASK_INGRESS_VLAN;
+		if (!pextvlan) {
+			ctpcfg2.bIngressExtendedVlanEnable = LTQ_FALSE;
+		} else {
+			ctpcfg2.bIngressExtendedVlanEnable = LTQ_TRUE;
+			ctpcfg2.nIngressExtendedVlanBlockId =
+				pextvlan->nExtendedVlanBlockId;
+			ctpcfg2.nIngressExtendedVlanBlockSize = 0;
+		}
+	} else {
+		ctpcfg1.eMask = GSW_CTP_PORT_CONFIG_MASK_EGRESS_VLAN;
+		if (!pextvlan) {
+			ctpcfg2.bEgressExtendedVlanEnable = LTQ_FALSE;
+		} else {
+			ctpcfg2.bEgressExtendedVlanEnable = LTQ_TRUE;
+			ctpcfg2.nEgressExtendedVlanBlockId =
+				pextvlan->nExtendedVlanBlockId;
+			ctpcfg2.nEgressExtendedVlanBlockSize = 0;
+		}
+	}
+	ctpcfg2.eMask = ctpcfg1.eMask;
+
+	ret = ops->gsw_ctp_ops.CTP_PortConfigGet(ops, &ctpcfg1);
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	ret = ops->gsw_ctp_ops.CTP_PortConfigSet(ops, &ctpcfg2);
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	if (ingress) {
+		if (ctpcfg1.bIngressExtendedVlanEnable != LTQ_FALSE) {
+			GSW_EXTENDEDVLAN_alloc_t alloc = {0};
+
+			alloc.nExtendedVlanBlockId =
+				ctpcfg1.nIngressExtendedVlanBlockId;
+			ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+		}
+	} else {
+		if (ctpcfg1.bEgressExtendedVlanEnable != LTQ_FALSE) {
+			GSW_EXTENDEDVLAN_alloc_t alloc = {0};
+
+			alloc.nExtendedVlanBlockId =
+				ctpcfg1.nEgressExtendedVlanBlockId;
+			ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+		}
+	}
+
+	return 0;
+}
+
+static int vlan_filter_mode(struct dp_pattern_vlan *pout)
+{
+	int k;
+
+	k = 0;
+	if (pout->tpid != DP_VLAN_PATTERN_NOT_CARE)
+		k |= 1 << 4;
+	if (pout->proto != DP_VLAN_PATTERN_NOT_CARE)
+		k |= 1 << 3;
+	if (pout->dei != DP_VLAN_PATTERN_NOT_CARE)
+		k |= 1 << 2;
+	if (pout->prio != DP_VLAN_PATTERN_NOT_CARE)
+		k |= 1 << 1;
+	if (pout->vid != DP_VLAN_PATTERN_NOT_CARE)
+		k |= 1;
+
+	return k;
+}
+
+static int tc_vlan_filter(struct core_ops *ops,
+			  struct dp_tc_vlan *vlan,
+			  struct dp_tc_vlan_info *info)
+{
+	/* default return 1 to indicate Extended VLAN is required */
+	int ret = 1;
+	int total = vlan->n_vlan0 + vlan->n_vlan1 + vlan->n_vlan2;
+	int untagged = -1;
+	int tagged = -1;
+	int mode = -1;
+	GSW_VLANFILTER_alloc_t alloc;
+	GSW_VLANFILTER_config_t *pcfg;
+	int i, j, k;
+
+	/* bridge port */
+	pcfg = kmalloc_array(total, sizeof(*pcfg), GFP_KERNEL);
+	if (!pcfg)
+		return -ENOMEM;
+	memset(pcfg, 0, sizeof(*pcfg) * total);
+
+	j = 0;
+
+	/* untagged rule */
+	for (i = 0; i < vlan->n_vlan0; i++) {
+		if (!vlan->vlan0_list[i].def)
+	/* VLAN filter for untagged packet have default rule only */
+			goto EXIT;
+		if ((vlan->vlan0_list[i].act.act & DP_VLAN_ACT_FWD)) {
+		/* default value was set to drop */
+			if (untagged > 0)
+				goto EXIT;
+		/* default value is forward */
+			untagged = 0;
+		} else if ((vlan->vlan0_list[i].act.act & DP_VLAN_ACT_DROP)) {
+		/* default value was set to forward */
+			if (untagged == 0)
+				goto EXIT;
+		/* default value is drop */
+			untagged = 1;
+		} else
+		/* packet editing required */
+			goto EXIT;
+		continue;
+	}
+
+	/* 1-tag rule */
+	for (i = 0; i < vlan->n_vlan1; i++) {
+		if (vlan->vlan1_list[i].def) {
+			if ((vlan->vlan1_list[i].act.act & DP_VLAN_ACT_FWD)) {
+			/* default value was set to drop */
+				if (tagged > 0)
+					goto EXIT;
+				tagged = 0;
+			} else if ((vlan->vlan1_list[i].act.act &
+				    DP_VLAN_ACT_DROP)) {
+			/* default value was set to forward */
+				if (tagged == 0)
+					goto EXIT;
+				tagged = 1;
+			} else
+			/* packet editing required */
+				goto EXIT;
+			continue;
+		}
+		/* Action other than FWD/DROP is not accepted */
+		if ((vlan->vlan1_list[i].act.act & DP_VLAN_ACT_FWD))
+			pcfg[j].bDiscardMatched = LTQ_FALSE;
+		else if ((vlan->vlan1_list[i].act.act & DP_VLAN_ACT_DROP))
+			pcfg[j].bDiscardMatched = LTQ_TRUE;
+		else
+			goto EXIT;
+		/* VLAN filter only support VID, PCP, or TCI only */
+		k = vlan_filter_mode(&vlan->vlan1_list[i].outer);
+		switch (k) {
+		case 1:
+			pcfg[j].eVlanFilterMask = GSW_VLAN_FILTER_TCI_MASK_VID;
+			pcfg[j].nVal = (u32)vlan->vlan1_list[i].outer.vid;
+			break;
+		case 2:
+			pcfg[j].eVlanFilterMask = GSW_VLAN_FILTER_TCI_MASK_PCP;
+			pcfg[j].nVal = (u32)vlan->vlan1_list[i].outer.prio;
+			break;
+		case 7:
+			pcfg[j].eVlanFilterMask = GSW_VLAN_FILTER_TCI_MASK_TCI;
+			pcfg[j].nVal =
+				(u32)vlan->vlan1_list[i].outer.vid & 0xFFF;
+			pcfg[j].nVal |=
+				((u32)vlan->vlan1_list[i].outer.prio & 7) << 13;
+			pcfg[j].nVal |=
+				((u32)vlan->vlan1_list[i].outer.dei & 1) << 12;
+			break;
+		default:
+			goto EXIT;
+		}
+		if (mode < 0)
+			mode = k;
+		else if (mode != k)
+			goto EXIT;
+		j++;
+	}
+
+	/* 2-tag rule */
+	for (i = 0; i < vlan->n_vlan2; i++) {
+		if (vlan->vlan2_list[i].def) {
+			if ((vlan->vlan2_list[i].act.act & DP_VLAN_ACT_FWD)) {
+				if (tagged > 0)
+					goto EXIT;
+				tagged = 0;
+			} else if ((vlan->vlan2_list[i].act.act &
+				    DP_VLAN_ACT_DROP)) {
+				if (tagged == 0)
+					goto EXIT;
+				tagged = 1;
+			} else {
+				goto EXIT;
+			}
+			continue;
+		}
+		/* Action other than FWD/DROP is not accepted */
+		if ((vlan->vlan2_list[i].act.act & DP_VLAN_ACT_FWD))
+			pcfg[j].bDiscardMatched = LTQ_FALSE;
+		else if ((vlan->vlan2_list[i].act.act & DP_VLAN_ACT_DROP))
+			pcfg[j].bDiscardMatched = LTQ_TRUE;
+		else
+			goto EXIT;
+		/* VLAN filter only support VID, PCP, or TCI only */
+		k = vlan_filter_mode(&vlan->vlan2_list[i].outer);
+		switch (k) {
+		case 1:
+			pcfg[j].eVlanFilterMask = GSW_VLAN_FILTER_TCI_MASK_VID;
+			pcfg[j].nVal = (u32)vlan->vlan2_list[i].outer.vid;
+			break;
+		case 2:
+			pcfg[j].eVlanFilterMask = GSW_VLAN_FILTER_TCI_MASK_PCP;
+			pcfg[j].nVal = (u32)vlan->vlan2_list[i].outer.prio;
+			break;
+		case 7:
+			pcfg[j].eVlanFilterMask = GSW_VLAN_FILTER_TCI_MASK_TCI;
+			pcfg[j].nVal = (u32)vlan->vlan2_list[i].outer.vid &
+				       0xFFF;
+			pcfg[j].nVal |= ((u32)
+					 vlan->vlan2_list[i].outer.prio & 7)
+					 << 13;
+			pcfg[j].nVal |= ((u32)
+					 vlan->vlan2_list[i].outer.dei & 1)
+					 << 12;
+			break;
+		default:
+			goto EXIT;
+		}
+		if (mode < 0)
+			mode = k;
+		else if (mode != k)
+			goto EXIT;
+		/* Inner VLAN should be "don't care" for all fields */
+		k = vlan_filter_mode(&vlan->vlan2_list[i].inner);
+		if (k != 0)
+			goto EXIT;
+		j++;
+	}
+
+	if (untagged < 0)
+		untagged = 0;
+	if (tagged < 0)
+		tagged = 0;
+
+	/* Allocate VLAN filter */
+	memset(&alloc, 0, sizeof(alloc));
+	alloc.nNumberOfEntries = (u32)j;
+	alloc.bDiscardUntagged = untagged ? LTQ_TRUE : LTQ_FALSE;
+	alloc.bDiscardUnmatchedTagged = tagged ? LTQ_TRUE : LTQ_FALSE;
+	ret = ops->gsw_vlanfilter_ops.VlanFilter_Alloc(ops, &alloc);
+	if (ret != GSW_statusOk) {
+		ret = -EIO;
+		goto EXIT;
+	}
+	DP_DEBUG(DP_DBG_FLAG_PAE, "VlanFilter_Alloc - %u[%u]\n",
+		 alloc.nVlanFilterBlockId, alloc.nNumberOfEntries);
+
+	/* Configure each VLAN filter */
+	for (i = 0; i < j; i++) {
+		pcfg[i].nVlanFilterBlockId = alloc.nVlanFilterBlockId;
+		pcfg[i].nEntryIndex = (u32)i;
+		ret = ops->gsw_vlanfilter_ops.VlanFilter_Set(ops, &pcfg[i]);
+		if (ret != GSW_statusOk) {
+			ops->gsw_vlanfilter_ops.VlanFilter_Free(ops, &alloc);
+			ret = -EIO;
+			goto EXIT;
+		}
+	}
+
+	/* Update bridge port */
+	ret = update_bp(ops,
+			(u32)info->bp,
+			vlan->dir == DP_DIR_INGRESS,
+			&alloc,
+			NULL);
+	if (ret != 0)
+		ops->gsw_vlanfilter_ops.VlanFilter_Free(ops, &alloc);
+
+EXIT:
+	kfree(pcfg);
+	return ret;
+}
+
+static int write_vtetype(struct core_ops *ops, u16 val)
+{
+	int ret;
+	GSW_register_t reg = {0};
+
+	reg.nRegAddr = 0xA42;
+	reg.nData = val;
+	ret = ops->gsw_common_ops.RegisterSet(ops, &reg);
+	if (ret == GSW_statusOk)
+		return 0;
+	else
+		return -EIO;
+}
+
+static int ext_vlan_filter_cfg(struct core_ops *ops,
+			       GSW_EXTENDEDVLAN_filterVLAN_t *pcfg,
+			       struct dp_pattern_vlan *pattern)
+{
+	int ret = 0;
+
+	if (pattern->prio == DP_VLAN_PATTERN_NOT_CARE) {
+		pcfg->bPriorityEnable = LTQ_FALSE;
+	} else {
+		pcfg->bPriorityEnable = LTQ_TRUE;
+		pcfg->nPriorityVal = pattern->prio;
+	}
+	if (pattern->vid == DP_VLAN_PATTERN_NOT_CARE) {
+		pcfg->bVidEnable = LTQ_FALSE;
+	} else {
+		pcfg->bVidEnable = LTQ_TRUE;
+		pcfg->nVidVal = pattern->vid;
+	}
+	if (pattern->tpid == DP_VLAN_PATTERN_NOT_CARE) {
+		pcfg->eTpid = GSW_EXTENDEDVLAN_FILTER_TPID_NO_FILTER;
+	} else if (pattern->tpid == ETH_P_8021Q) {
+		pcfg->eTpid = GSW_EXTENDEDVLAN_FILTER_TPID_8021Q;
+	} else {
+		pcfg->eTpid = GSW_EXTENDEDVLAN_FILTER_TPID_VTETYPE;
+		ret = write_vtetype(ops, (u16)pattern->tpid);
+	}
+	if (pattern->dei == DP_VLAN_PATTERN_NOT_CARE)
+		pcfg->eDei = GSW_EXTENDEDVLAN_FILTER_DEI_NO_FILTER;
+	else if (pattern->dei == 0)
+		pcfg->eDei = GSW_EXTENDEDVLAN_FILTER_DEI_0;
+	else
+		pcfg->eDei = GSW_EXTENDEDVLAN_FILTER_DEI_1;
+
+	return ret;
+}
+
+static int ext_vlan_insert(struct core_ops *ops,
+			   GSW_EXTENDEDVLAN_treatmentVlan_t *pcfg,
+			   struct dp_act_vlan *act,
+			   unsigned int idx)
+{
+	int ret;
+
+	DP_DEBUG(DP_DBG_FLAG_PAE, "act->tpid[%u]: 0x%04x\n",
+		 idx, act->tpid[idx]);
+	switch (act->tpid[idx]) {
+	case CP_FROM_INNER:
+		pcfg->eTpid = GSW_EXTENDEDVLAN_TREATMENT_INNER_TPID;
+		break;
+	case CP_FROM_OUTER:
+		pcfg->eTpid = GSW_EXTENDEDVLAN_TREATMENT_OUTER_TPID;
+		break;
+	case ETH_P_8021Q:
+		pcfg->eTpid = GSW_EXTENDEDVLAN_TREATMENT_8021Q;
+		break;
+	default:
+		pcfg->eTpid = GSW_EXTENDEDVLAN_TREATMENT_VTETYPE;
+		ret = write_vtetype(ops, (u16)act->tpid[idx]);
+		if (ret)
+			return ret;
+	}
+
+	switch (act->vid[idx]) {
+	case CP_FROM_INNER:
+		pcfg->eVidMode = GSW_EXTENDEDVLAN_TREATMENT_INNER_VID;
+		break;
+	case CP_FROM_OUTER:
+		pcfg->eVidMode = GSW_EXTENDEDVLAN_TREATMENT_OUTER_VID;
+		break;
+	default:
+		pcfg->eVidMode = GSW_EXTENDEDVLAN_TREATMENT_VID_VAL;
+		pcfg->eVidVal = (u32)act->vid[idx];
+	}
+
+	switch (act->dei[idx]) {
+	case CP_FROM_INNER:
+		pcfg->eDei = GSW_EXTENDEDVLAN_TREATMENT_INNER_DEI;
+		break;
+	case CP_FROM_OUTER:
+		pcfg->eDei = GSW_EXTENDEDVLAN_TREATMENT_OUTER_DEI;
+		break;
+	case 0:
+		pcfg->eDei = GSW_EXTENDEDVLAN_TREATMENT_DEI_0;
+		break;
+	case 1:
+		pcfg->eDei = GSW_EXTENDEDVLAN_TREATMENT_DEI_1;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	switch (act->prio[idx]) {
+	case CP_FROM_INNER:
+		pcfg->ePriorityMode = GSW_EXTENDEDVLAN_TREATMENT_INNER_PRORITY;
+		break;
+	case CP_FROM_OUTER:
+		pcfg->ePriorityMode = GSW_EXTENDEDVLAN_TREATMENT_OUTER_PRORITY;
+		break;
+	case DERIVE_FROM_DSCP:
+		pcfg->ePriorityMode = GSW_EXTENDEDVLAN_TREATMENT_DSCP;
+		break;
+	default:
+		pcfg->ePriorityMode = GSW_EXTENDEDVLAN_TREATMENT_PRIORITY_VAL;
+		pcfg->ePriorityVal = (u32)act->prio[idx];
+		break;
+	}
+	return 0;
+}
+
+static int ext_vlan_action_cfg(struct core_ops *ops,
+			       GSW_EXTENDEDVLAN_treatment_t *pcfg,
+			       struct dp_act_vlan *act)
+{
+	int ret;
+
+	DP_DEBUG(DP_DBG_FLAG_PAE, "act->act: 0x%02x\n", (unsigned int)act->act);
+
+	/* forward without modification */
+	if ((act->act & DP_VLAN_ACT_FWD))
+		return 0;
+
+	/* drop the packet */
+	if ((act->act & DP_VLAN_ACT_DROP)) {
+		pcfg->eRemoveTag = GSW_EXTENDEDVLAN_TREATMENT_DISCARD_UPSTREAM;
+		return 0;
+	}
+
+	/* remove tag */
+	if ((act->act & DP_VLAN_ACT_POP)) {
+		DP_DEBUG(DP_DBG_FLAG_PAE, "act->pop_n: %d\n", act->pop_n);
+		switch (act->pop_n) {
+		case 1:
+			pcfg->eRemoveTag =
+				GSW_EXTENDEDVLAN_TREATMENT_REMOVE_1_TAG;
+			break;
+		case 2:
+			pcfg->eRemoveTag =
+				GSW_EXTENDEDVLAN_TREATMENT_REMOVE_2_TAG;
+			break;
+		default:
+			return -EINVAL;
+		}
+	}
+
+	if (!(act->act & DP_VLAN_ACT_PUSH))
+		return 0;
+
+	switch (act->push_n) {
+	case 2:
+		pcfg->bAddInnerVlan = LTQ_TRUE;
+		ret = ext_vlan_insert(ops, &pcfg->sInnerVlan, act, 1);
+		if (ret)
+			return ret;
+	case 1:
+		pcfg->bAddOuterVlan = LTQ_TRUE;
+		return ext_vlan_insert(ops, &pcfg->sOuterVlan, act, 0);
+	default:
+		return -EINVAL;
+	}
+}
+
+static int ext_vlan_cfg(struct core_ops *ops,
+			GSW_EXTENDEDVLAN_config_t *pcfg,
+			int def,
+			int ethertype,
+			struct dp_pattern_vlan *outer,
+			struct dp_pattern_vlan *inner,
+			struct dp_act_vlan *act)
+{
+	const int ethertype_map[] = {
+		DP_VLAN_PATTERN_NOT_CARE,
+		GSW_EXTENDEDVLAN_FILTER_ETHERTYPE_NO_FILTER,
+		DP_PROTO_IP4,
+		GSW_EXTENDEDVLAN_FILTER_ETHERTYPE_IPOE,
+		DP_PROTO_PPPOE,
+		GSW_EXTENDEDVLAN_FILTER_ETHERTYPE_PPPOE,
+		DP_PROTO_ARP,
+		GSW_EXTENDEDVLAN_FILTER_ETHERTYPE_ARP,
+		DP_PROTO_IP6,
+		GSW_EXTENDEDVLAN_FILTER_ETHERTYPE_IPV6IPOE,
+		DP_PROTO_EAPOL,
+		GSW_EXTENDEDVLAN_FILTER_ETHERTYPE_EAPOL,
+	};
+	int ret;
+	unsigned int i;
+
+	for (i = 0; i < ARRAY_SIZE(ethertype_map); i += 2) {
+		if (ethertype == ethertype_map[i])
+			break;
+	}
+	if (i >= ARRAY_SIZE(ethertype_map))
+		return -EINVAL;
+	pcfg->sFilter.eEtherType = (u32)ethertype_map[i + 1];
+
+	switch ((inner ? 2 : 0) | (outer ? 1 : 0)) {
+	case 0:
+		DP_DEBUG(DP_DBG_FLAG_PAE, "Untagged Rule\n");
+		/* untagged rule */
+		if (def)	/* default untagged rule is not supported */
+			return -EINVAL;
+		pcfg->sFilter.sInnerVlan.eType =
+			GSW_EXTENDEDVLAN_FILTER_TYPE_NO_TAG;
+		pcfg->sFilter.sOuterVlan.eType =
+			GSW_EXTENDEDVLAN_FILTER_TYPE_NO_TAG;
+		break;
+	case 1:
+		DP_DEBUG(DP_DBG_FLAG_PAE, "1-tag Rule\n");
+		/* 1-tag rule */
+		pcfg->sFilter.sInnerVlan.eType =
+			GSW_EXTENDEDVLAN_FILTER_TYPE_NO_TAG;
+		if (def) {
+			/* default 1-tag rule */
+			pcfg->sFilter.sOuterVlan.eType =
+				GSW_EXTENDEDVLAN_FILTER_TYPE_DEFAULT;
+			break;
+		}
+		pcfg->sFilter.sOuterVlan.eType =
+			GSW_EXTENDEDVLAN_FILTER_TYPE_NORMAL;
+		ret = ext_vlan_filter_cfg(ops,
+					  &pcfg->sFilter.sOuterVlan,
+					  outer);
+		if (ret)
+			return ret;
+		break;
+	case 3:
+		DP_DEBUG(DP_DBG_FLAG_PAE, "2-tag Rule\n");
+		/* 2-tag rule */
+		if (def) {
+			/* default 2-tag rule */
+			pcfg->sFilter.sOuterVlan.eType =
+				GSW_EXTENDEDVLAN_FILTER_TYPE_NO_FILTER;
+			pcfg->sFilter.sInnerVlan.eType =
+				GSW_EXTENDEDVLAN_FILTER_TYPE_DEFAULT;
+			break;
+		}
+		pcfg->sFilter.sOuterVlan.eType =
+			GSW_EXTENDEDVLAN_FILTER_TYPE_NORMAL;
+		pcfg->sFilter.sInnerVlan.eType =
+			GSW_EXTENDEDVLAN_FILTER_TYPE_NORMAL;
+		ret = ext_vlan_filter_cfg(ops,
+					  &pcfg->sFilter.sOuterVlan,
+					  outer);
+		if (ret)
+			return ret;
+		ret = ext_vlan_filter_cfg(ops,
+					  &pcfg->sFilter.sInnerVlan,
+					  inner);
+		if (ret)
+			return ret;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	ret = ext_vlan_action_cfg(ops, &pcfg->sTreatment, act);
+	if (ret)
+		return ret;
+
+	return ops->gsw_extvlan_ops.ExtendedVlan_Set(ops, pcfg);
+}
+
+static int tc_ext_vlan(struct core_ops *ops,
+		       struct dp_tc_vlan *vlan,
+		       struct dp_tc_vlan_info *info)
+{
+	int ret;
+	int total = vlan->n_vlan0 + vlan->n_vlan1 + vlan->n_vlan2;
+	GSW_EXTENDEDVLAN_alloc_t alloc = {0};
+	GSW_EXTENDEDVLAN_config_t cfg;
+	int i, j;
+
+	alloc.nNumberOfEntries = (u32)total;
+	ret = ops->gsw_extvlan_ops.ExtendedVlan_Alloc(ops, &alloc);
+	if (ret != GSW_statusOk)
+		return -EIO;
+	DP_DEBUG(DP_DBG_FLAG_PAE, "ExtendedVlan_Alloc - %u[%u]\n",
+		 alloc.nExtendedVlanBlockId,
+		 alloc.nNumberOfEntries);
+
+	j = 0;
+
+	/* untagged rule */
+	for (i = 0; i < vlan->n_vlan0; i++) {
+		memset(&cfg, 0, sizeof(cfg));
+		cfg.nExtendedVlanBlockId = alloc.nExtendedVlanBlockId;
+		cfg.nEntryIndex = j;
+		j++;
+		ret = ext_vlan_cfg(ops,
+				   &cfg,
+				   vlan->vlan0_list[i].def,
+				   vlan->vlan0_list[i].outer.proto,
+				   NULL,
+				   NULL,
+				   &vlan->vlan0_list[i].act);
+		if (ret != 0)
+			goto ERROR;
+	}
+
+	/* 1-tag rule */
+	for (i = 0; i < vlan->n_vlan1; i++) {
+		memset(&cfg, 0, sizeof(cfg));
+		cfg.nExtendedVlanBlockId = alloc.nExtendedVlanBlockId;
+		cfg.nEntryIndex = j;
+		j++;
+		ret = ext_vlan_cfg(ops,
+				   &cfg,
+				   vlan->vlan1_list[i].def,
+				   vlan->vlan1_list[i].outer.proto,
+				   &vlan->vlan1_list[i].outer,
+				   NULL,
+				   &vlan->vlan1_list[i].act);
+		if (ret != 0)
+			goto ERROR;
+	}
+
+	/* 2-tag rule */
+	for (i = 0; i < vlan->n_vlan2; i++) {
+		memset(&cfg, 0, sizeof(cfg));
+		cfg.nExtendedVlanBlockId = alloc.nExtendedVlanBlockId;
+		cfg.nEntryIndex = j;
+		j++;
+		ret = ext_vlan_cfg(ops,
+				   &cfg,
+				   vlan->vlan2_list[i].def,
+				   vlan->vlan2_list[i].outer.proto,
+				   &vlan->vlan2_list[i].outer,
+				   &vlan->vlan2_list[i].inner,
+				   &vlan->vlan2_list[i].act);
+		if (ret != 0)
+			goto ERROR;
+	}
+
+	if (info->dev_type == 0) {
+		/* Configure CTP */
+		ret = update_ctp(ops,
+				 (u32)info->dp_port,
+				 (u32)info->subix,
+				 vlan->dir == DP_DIR_INGRESS,
+				 &alloc);
+	} else {
+		/* Configure bridge port */
+		ret = update_bp(ops,
+				(u32)info->bp,
+				vlan->dir == DP_DIR_INGRESS,
+				NULL,
+				&alloc);
+	}
+	if (ret == 0)
+		return 0;
+
+ERROR:
+	ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+	return ret;
+}
+
+/*tc_vlan_set_31:
+ *param[in] vlan: directly from DP core API's parameter
+ *param[in] info: traslated via vlan->dev.
+ *       if it is BP/pmapper dev, info->dev_type=DP_DEV_TYPE_BP_PMAPPER {
+ *                                info->bp is set
+ *            if vlan->def_apply == DP_VLAN_APPLY_CTP,
+ *                                info->subix is set also
+ *       } elif it is CTP/GEM dev, info->dev_type=DP_DEV_TYPE_CTP_GEM {
+ *                                info->subix is set
+ *       }
+ *flag: reserved for future
+ */
+int tc_vlan_set_31(struct core_ops *ops,
+		   struct dp_tc_vlan *vlan,
+		   struct dp_tc_vlan_info *info,
+		   int flag)
+{
+	/* If it's bridge port, try to configure VLAN filter. */
+	if (info->dev_type != 0) {
+		int ret;
+
+		ret = tc_vlan_filter(ops, vlan, info);
+		/* Either managed to configure VLAN filter
+		 * or error happens in GSW API
+		 */
+		if (ret <= 0)
+			return ret;
+		/* VLAN filter can't cover the case and need extended VLAN */
+	}
+
+	/* Configure extended VLAN */
+	return tc_ext_vlan(ops, vlan, info);
+}
diff --git a/include/net/datapath_api.h b/include/net/datapath_api.h
new file mode 100644
index 000000000000..a2f629fc416a
--- /dev/null
+++ b/include/net/datapath_api.h
@@ -0,0 +1,1040 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#ifndef DATAPATH_API_H
+#define DATAPATH_API_H
+
+#include <linux/skbuff.h>
+#include <linux/etherdevice.h>
+#include <linux/atmdev.h>
+
+#ifndef DATAPATH_HAL_LAYER
+#if IS_ENABLED(CONFIG_FALCONMX_CQM) || \
+	IS_ENABLED(CONFIG_LTQ_DATAPATH_DDR_SIMULATE_GSWIP31) /*testing only */
+#include <net/datapath_api_gswip31.h>
+#else /*GRX500 GSWIP30*/
+#include <net/datapath_api_gswip30.h>
+#endif
+#endif /*DATAPATH_HAL_LAYER */
+#include <net/datapath_api_vlan.h>
+#include <net/switch_api/lantiq_gsw_api.h>
+#include <net/switch_api/lantiq_gsw_flow.h>
+#include <net/switch_api/lantiq_gsw.h>
+#include <net/switch_api/gsw_dev.h>
+#include <net/switch_api/gsw_flow_ops.h>
+#ifdef CONFIG_LTQ_DATAPATH_CPUFREQ
+#include <linux/cpufreq.h>
+#include <cpufreq/ltq_cpufreq.h>
+
+#endif /*CONFIG_LTQ_DATAPATH_CPUFREQ*/
+
+/*! @mainpage Datapath Manager API
+ * @section Basic Datapath Registration API
+ * @section Datapath QOS HAL
+ *
+ * @file datapath_api.h
+ * @brief This file contains all the API for datapath manager on the GRX500 and
+ *future system. This will actually be split into different header files,
+ *but collected together for understanding here.
+ */
+/*! @defgroup Datapath_MGR Datapath Manager Basic API
+ *@brief All API and defines exported by Datapath Manager
+ */
+/*! @{ */
+/*! @defgroup Datapath_Driver_Defines Datapath Driver Defines
+ *@brief Defines used in the Datapath Driver
+ */
+/*! @defgroup Datapath_Driver_Structures Datapath Driver Structures
+ *@brief Datapath Configuration Structures
+ */
+/*! @defgroup Datapath_Driver_API Datapath Driver Manager API
+ *@brief  Datapath Driver Manager API
+ *@brief  Datapath Driver Manager API
+ */
+/*! @defgroup PPA_Accel_API PPA Acceleration Driver API
+ *@brief PPA Acceleration Driver API used for learning and getting
+ *the information necessary to accelerate a flow
+ */
+/*! @defgroup Datapath_API_QOS Datapath QOS Manager API
+ *@brief  Datapath QOS Manager API
+ */
+/*! @} */
+#define DP_MAX_INST  1  /*!< @brief maximum DP instance to support. It can be
+			 *  change as as needed
+			 */
+#define DP_TX_CAL_CHKSUM 1 /*!< @brief Need calculate PMAC. \n
+			    *  Note, make sure pmac place holder already have\n
+			    *   or set flag DP_TX_INSERT_PMAC to insert it
+			    */
+#define DP_TX_DSL_FCS        2	/*!< @brief For DSL FCS Checksum calculation */
+#define DP_TX_INSERT_PMAC    4	/*!< @brief Oly for special test purpose */
+#define DP_TX_OAM            8	/*!< @brief OAM packet */
+#define DP_TX_TO_DL_MPEFW    0x10/*!< @brief Send Pkt directly to DL FW */
+#define DP_MAX_ETH_ALEN 6  /*!< @brief MAC Header Size */
+#define DP_MAX_PMAC_LEN     8  /*!< @brief Maximum PMAC Header Size */
+
+/*! @addtogroup Datapath_Driver_Structures */
+/*! @brief  PPA Sub-interface Data structure
+ *@param port_id  Datapath Port Id corresponds to PMAC Port Id
+ *@param subif    Sub-interface Id info. In GRX500, this 15 bits,
+ *		only 13 bits in PAE are handled [14, 11:0]
+ *\note
+ */
+enum DP_API_STATUS {
+	DP_FAILURE = -1,  /*!< failure */
+	DP_SUCCESS = 0, /*!< succeed */
+};
+
+#define DP_F_ENUM_OR_STRING(name, value, short_name) {name = value} /*!< @brief
+								     *  macro
+								     *  for
+								     *  enum
+								     */
+
+/*! @brief Enumerator DP_F_FLAG */
+enum DP_F_FLAG {
+	DP_F_DEREGISTER   = BIT(0), /*!< For de-allocate port only */
+	DP_F_FAST_ETH_LAN = BIT(1), /*!< For Ethernet LAN device */
+	DP_F_FAST_ETH_WAN = BIT(2), /*!< For Ethernet WAN device */
+	DP_F_FAST_WLAN =    BIT(3), /*!< For DirectConnect WLAN device*/
+	DP_F_FAST_DSL =     BIT(4), /*!< For DSL device */
+	DP_F_DIRECT =       BIT(5), /*!< For PPA Directpath/LitePath*/
+	DP_F_LOOPBACK =     BIT(6), /*!< For packet redirect back to GSWIP
+				     *! For bridging/routing acceleration after
+				     *de-tunnel or others, like ipsec case
+				     */
+	DP_F_DIRECTLINK =  BIT(7), /*!< For DirectLink/QCA device */
+	DP_F_SUBIF_LOGICAL = BIT(8), /*!< For logical device, like VLAN device
+				      *It is used by dp_register_subif
+				      */
+	DP_F_ACA =          BIT(9), /*!< For peripheral device with ACA*/
+	DP_F_ALLOC_EXPLICIT_SUBIFID = BIT(10), /*!< For logical device which
+						* need explicit subif/VAP
+						* request.
+						*! For GRX350, seems need this
+						*flag becase of VLAN talbe
+						*handling inside PPA.
+						*! For Falcon-MX, normally no
+						*need this flag.
+						*Used by dp_register_subif
+						*/
+	DP_F_FAST_WLAN_EXT = BIT(11), /*!< 9-bit WLAN stations's device */
+	DP_F_GPON     = BIT(12), /*!< For GPON device */
+	DP_F_EPON     = BIT(13), /*!< For EPON device */
+	DP_F_GINT     = BIT(14), /*!< For GINT device */
+	DP_F_NO_SWDEV = BIT(15), /*!< For those device which don't want
+				  *auto switchdev framework support,
+				  *like no need for auto-bridging via ip/brctl
+				  */
+	DP_F_SHARE_RES = BIT(16), /*!< Wave600 multiple radio share same ACA */
+
+	/*Note Below Flags are ued by CBM/CQE driver only */
+	DP_F_MPE_ACCEL =   BIT(25), /*!< For MPE path config, used by CBM only*/
+	DP_F_CHECKSUM =    BIT(26), /*!< For HW chksum offload path config.
+				     *Used by CBM only
+				     */
+	DP_F_DIRECTPATH_RX = BIT(27), /*!< For PPA Directpath/LitePath's RX PATH
+				       *Used by CBM only
+				       */
+	DP_F_DONTCARE =    BIT(28), /*!< ??? Used by CBM only */
+	DP_F_LRO =         BIT(29), /*!< For LRO path config. used by CBM only*/
+	DP_F_FAST_DSL_DOWNSTREAM = BIT(30), /*!< For VRX318/518 downstream path.
+					     *used by CBM only
+					     */
+	DP_F_DSL_BONDING =         BIT(31), /*!< For DSL BONDING path config
+					     *to configu two UMT
+					     *used by CBM only
+					     */
+	/*Note, once add a new entry here int the enum,
+	 *need to add new item in below macro DP_F_FLAG_LIST
+	 */
+};
+
+/*! @brief DP_F_FLAG_LIST Note:per bit one variable */
+#define DP_F_FLAG_LIST  { \
+	DP_F_ENUM_OR_STRING(DP_F_DEREGISTER,   "De-Register"), \
+	DP_F_ENUM_OR_STRING(DP_F_FAST_ETH_LAN, "ETH_LAN"), \
+	DP_F_ENUM_OR_STRING(DP_F_FAST_ETH_WAN, "ETH_WAN"),\
+	DP_F_ENUM_OR_STRING(DP_F_FAST_WLAN,    "FAST_WLAN"),\
+	DP_F_ENUM_OR_STRING(DP_F_FAST_DSL,     "DSL"),\
+	DP_F_ENUM_OR_STRING(DP_F_DIRECT,        "DirectPath"), \
+	DP_F_ENUM_OR_STRING(DP_F_LOOPBACK,      "Tunne_loop"),\
+	DP_F_ENUM_OR_STRING(DP_F_DIRECTLINK,    "DirectLink"),\
+	DP_F_ENUM_OR_STRING(DP_F_SUBIF_LOGICAL, "LogicalDev"), \
+	DP_F_ENUM_OR_STRING(DP_F_ACA,                 "ACA"),\
+	DP_F_ENUM_OR_STRING(DP_F_ALLOC_EXPLICIT_SUBIFID, "Explicit_subif"), \
+	DP_F_ENUM_OR_STRING(DP_F_FAST_WLAN_EXT,       "EXT_WLAN"),\
+	DP_F_ENUM_OR_STRING(DP_F_GPON,                "GPON"),\
+	DP_F_ENUM_OR_STRING(DP_F_EPON,                "EPON"),\
+	DP_F_ENUM_OR_STRING(DP_F_GINT,                "GINT"),\
+	DP_F_ENUM_OR_STRING(DP_F_NO_SWDEV,            "NO_SWITCHDEV"),\
+	DP_F_ENUM_OR_STRING(DP_F_SHARE_RES,            "SHARE_ACA"),\
+	DP_F_ENUM_OR_STRING(DP_F_MPE_ACCEL,     "MPE_FW"), \
+	DP_F_ENUM_OR_STRING(DP_F_CHECKSUM,      "HW Chksum"),\
+	DP_F_ENUM_OR_STRING(DP_F_DIRECTPATH_RX, "Directpath_RX"),\
+	DP_F_ENUM_OR_STRING(DP_F_DONTCARE,      "DontCare"),\
+	DP_F_ENUM_OR_STRING(DP_F_LRO,           "LRO"), \
+	DP_F_ENUM_OR_STRING(DP_F_FAST_DSL_DOWNSTREAM, "DSL_Down"),\
+	DP_F_ENUM_OR_STRING(DP_F_DSL_BONDING,         "DSL_Bonding") \
+}
+
+#define DP_F_PORT_TUNNEL_DECAP  DP_F_LOOPBACK /*!< @brief Just for
+					       *  back-compatible since
+					       *  CBM is using old macro
+					       *  DP_F_PORT_TUNNEL_DECAP
+					       */
+#define DP_COC_REQ_DP	1 /*!< @brief COC request from Datapath itself */
+#define DP_COC_REQ_ETHERNET	2 /*!< @brief COC request from ethernet */
+#define DP_COC_REQ_VRX318	4 /*!< @brief COC request from vrx318 */
+
+/*! @brief pmapper mode */
+enum DP_PMAP_MODE {
+	DP_PMAP_PCP = 1,  /*!< PCP Mapper:with omci unmark frame option 1
+			    *    ie, derive pcp fields from default
+			    */
+	DP_PMAP_DSCP,     /*!< PCP Mapper with omci unmark frame option 0,
+			   *    ie, derive pcp fields from dscp bits
+			   */
+	DP_PMAP_DSCP_ONLY, /*!< DSCP mapper only: PON not using it */
+	DP_PMAP_MAX       /*!< Not valid */
+};
+
+#define DP_PMAP_PCP_NUM 8  /*!< @brief  Max pcp entries supported per pmapper*/
+#define DP_PMAP_DSCP_NUM 64 /*!<@brief  Max dscp entries supported per pmapper*/
+#define DP_MAX_CTP_PER_DEV  64  /*!< @brief  max CTP per dev:
+				 *  Note: its value should be like
+				 *     max(DP_PMAP_DSCP_NUM, DP_PMAP_PCP_NUM)
+				 */
+#define DP_PMAPPER_DISCARD_CTP 0xFFFF  /*!<@brief Discard ctp flag for pmapper*/
+/*! @brief structure for pmapper */
+struct dp_pmapper {
+	u32 pmapper_id;  /*!<pmapper_id : pmapper id*/
+	enum DP_PMAP_MODE mode;  /*!< mode: pcp or dscp mapper*/
+	u16 def_ctp;  /*!< default map: used for below cases:
+		       *  pcp mode: for non-vlan packet case
+		       *  dscp mode: for non-ip packet case
+		       *  but according to PON OMCI requirement, in fact, it
+		       *  should drop.
+		       */
+	u16 pcp_map[DP_PMAP_PCP_NUM];  /*!< For pcp mapper.  Should be
+					*non-zero since CTP 0 reserved
+					*/
+	u16 dscp_map[DP_PMAP_DSCP_NUM]; /*!< For dscp mapper*/
+};
+
+/*! @brief structure for dp_subif */
+typedef struct dp_subif {
+	s32 port_id; /*!< Datapath Port Id corresponds to PMAC Port Id */
+	int inst;  /*!< dp instance id */
+	int bport;  /*!< output: valid only for API dp_get_netif_subifid in the
+		     *  GSWIP 3.1 or above
+		     *  bridge port ID
+		     */
+	int subif_num; /*!< valid subif/ctp num.
+			*   output for dp_get_netif_subifid,
+			*   no use for dp_register_subif_ext
+			*/
+	union {
+		s32 subif; /*!< Sub-interface Id as HW defined
+			    * in full length
+			    * In GRX500/Falcon-MX, it is 15 bits
+			    */
+		s32 subif_list[DP_MAX_CTP_PER_DEV]; /*!< subif list */
+	};
+
+	int lookup_mode; /*!< CQM lookup mode for this device (dp_port based)*/
+	int alloc_flag; /*!< the flag is used during dp_alloc_port
+			 *   output for dp_get_netif_subifid
+			 *   no use for dp_register_subif_ext
+			 *   This is requested by PPA/DCDP to get original flag
+			 *   the caller provided to DP during
+			 *   dp_alloc_port
+			 */
+	int subif_flag[DP_MAX_CTP_PER_DEV]; /*!< the flag is used during dp_register_subif_ext
+					     *   output for dp_get_netif_subifid
+					     *   no use for dp_register_subif_ext
+					     *   This is requested by PPA/DCDP to get original flag
+					     *   the caller provided to DP during
+					     *   dp_register_subif_ext
+					     */
+	u32 flag_bp : 1; /*!< output: flag to indicate whether this device is
+			  *   bridge port device or GEM device
+			  *   if it is bridge port device, it will be set to 1
+			  *   otherwise, it is 0
+			  *   Valid only for API output of dp_get_netif_subifid
+			  *   It will be used for asymmetric VLAN case
+			  *   in case need call API dp_vlan_set to apply VLAN
+			  *   rule to CTP or bridge port
+			  */
+} dp_subif_t;
+
+typedef dp_subif_t PPA_SUBIF; /*!< @brief structure type dp_subif PPA_SUBIF*/
+
+struct vlan_prop {
+	u8 num;
+	u16 out_proto, out_vid;
+	u16 in_proto, in_vid;
+	struct net_device *base;
+};
+
+/*! @brief struct for dp_drv_mib */
+typedef struct dp_drv_mib {
+	u64 rx_drop_pkts;  /*!< rx drop pkts */
+	u64 rx_error_pkts; /*!< rx error pkts */
+	u64 tx_drop_pkts; /*!< tx drop pkts */
+	u64 tx_error_pkts; /*!< tx error  pkts */
+	u64 tx_pkts; /*!< tx pkts */
+	u64 tx_bytes; /*!< tx bytes */
+} dp_drv_mib_t;
+
+typedef int32_t(*dp_rx_fn_t)(struct net_device *rxif, struct net_device *txif,
+	struct sk_buff *skb, int32_t len);/*!< @brief   Device Receive
+					   *   Function callback for packets
+					   */
+typedef int32_t(*dp_stop_tx_fn_t)(struct net_device *dev);/*!< @brief   The
+							   * Driver Stop
+							   *Tx function
+							   *callback
+							   */
+typedef int32_t(*dp_restart_tx_fn_t)(struct net_device *dev); /*!< @brief Driver
+							       * Restart Tx
+							       * function
+							       * callback
+							       */
+typedef int32_t(*dp_reset_mib_fn_t)(dp_subif_t *subif, int32_t flag);/*!< @brief
+								      *Driver
+								      *reset
+								      *its mib
+								      *counter
+								      *callback
+								      **/
+typedef int32_t(*dp_get_mib_fn_t)(dp_subif_t *subif, dp_drv_mib_t *,
+	int32_t flag); /*!< @brief   Driver get mib counter of the
+			*specified subif interface.
+			*/
+typedef int32_t(*dp_get_netif_subifid_fn_t)(struct net_device *netif,
+	struct sk_buff *skb, void *subif_data, uint8_t dst_mac[DP_MAX_ETH_ALEN],
+	dp_subif_t *subif, uint32_t flags);	/*!< @brief   get subifid */
+#if defined(CONFIG_LTQ_DATAPATH_CPUFREQ) && defined(CONFIG_LTQ_CPUFREQ)
+typedef int32_t(*dp_coc_confirm_stat)(enum ltq_cpufreq_state new_state,
+	enum ltq_cpufreq_state old_st, uint32_t f); /*!< @brief Confirm state
+						     *   by COC
+						     */
+#endif
+/*!
+ *@brief Datapath Manager Registration Callback
+ *@param rx_fn  Rx function callback
+ *@param stop_fn    Stop Tx function callback for flow control
+ * *@param restart_fn    Start Tx function callback for flow control
+ *@param get_subifid_fn    Get Sub Interface Id of netif
+ *@note
+ */
+typedef struct dp_cb {
+	dp_rx_fn_t rx_fn;	/*!< Rx function callback */
+	dp_stop_tx_fn_t stop_fn;/*!< Stop Tx function callback for
+				 *flow control
+				 */
+	dp_restart_tx_fn_t restart_fn;	/*!< Start Tx function callback
+					 *! For flow control
+					 */
+	dp_get_netif_subifid_fn_t get_subifid_fn; /*!< Get Sub Interface Id
+						   *of netif/netdevice
+						   */
+	dp_reset_mib_fn_t reset_mib_fn;  /*!< reset registered device's network
+					  *mib counters
+					  */
+	dp_get_mib_fn_t get_mib_fn; /*!< reset registered device's
+				     *network mib counters
+				     */
+#ifdef CONFIG_LTQ_DATAPATH_CPUFREQ
+	dp_coc_confirm_stat dp_coc_confirm_stat_fn; /*!< once COC confirm the
+						     *state changed, Datatpath
+						     *will notify Ethernet/
+						     *VRX318 driver and
+						     *Ethernet/VRX318 driver
+						     *need to enable/disable
+						     *interrupt or change
+						     *threshold accordingly
+						     */
+#endif
+} dp_cb_t;
+
+/*!
+ *@brief Ingress PMAC port configuration from Datapath Manager
+ *@param tx_dma_chan  Tx DMA channel Number for which PMAC
+ *		configuration is to be done
+ *@param err_disc     Is Discard Error Enable
+ *@param pmac    Is Ingress PMAC Hdr Present
+ *@param def_pmac   Is Default PMAC Header configured for Tx DMA Channel
+ *@param def_pmac_pmap Is PortMap to be used from Default PMAC hdr (else use
+ *		Ingress PMAC hdr)
+ *@param def_pmac_en_pmap Is PortMap Enable to be used from Default PMAC hrd
+ *		(else use Ingress PMAC hdr)
+ *@param def_pmac_tc  Is TC (class) to be used from Default PMAC hdr
+ *		(else use Ingress PMAC hdr)
+ *@param def_pmac_en_tc  Are TC bits to be used for TC from Default PMAC hdr
+ *		(else use Ingress PMAC hdr)
+ *		Alternately, EN/DE/MPE1/MPE2 bits can be used for TC
+ *@param def_pmac_subifid  Is Sub-interfaceId to be taken from Default PMAC hdr
+ *		(else use Ingress PMAC hdr)
+ *@param def_pmac_src_port Packet Source Port determined from Default PMAC hdr
+ *		(else use Ingress PMAC hdr)
+ *@param res_ing	Reserved bits
+ *@param def_pmac_hdr Default PMAC header configuration for the Tx DMA channel
+ *		Useful if Src Port does not send PMAC header with packet
+ *@note
+ */
+typedef struct ingress_pmac {
+	uint32_t tx_dma_chan:8;	/*!< Tx DMA channel Number for which PMAC
+				 * configuration is to be done
+				 */
+	uint32_t err_disc:1;	/*!< Is Discard Error Enable */
+	uint32_t pmac:1;	/*!< Is Ingress PMAC Hdr Present */
+	uint32_t def_pmac:1;	/*!< Is Ingress PMAC Hdr Present */
+	uint32_t def_pmac_pmap:1;	/*!< Is Default PMAC Header configured
+					 * for Tx DMA Channel
+					 */
+	uint32_t def_pmac_en_pmap:1;	/*!< Is PortMap Enable to be used from
+					 *  Default PMAC hrd (else use Ingress
+					 * PMAC hdr)
+					 */
+	uint32_t def_pmac_tc:1;	/*!< Is TC (class) to be used from Default PMAC
+				 *  hdr (else use Ingress PMAC hdr)
+				 */
+	uint32_t def_pmac_en_tc:1;	/*!< Are TC bits to be used for TC from
+					 * Default PMAC hdr (else use Ingress
+					 * PMAC hdr)
+					 * Alternately, EN/DE/MPE1/MPE2 bits
+					 * can be used for TC
+					 */
+	uint32_t def_pmac_subifid:2; /*!< Is Sub-interfaceId to be taken from
+				      *Default PMAC hdr (else use Ingress PMAC
+				      *hdr)
+				      */
+	uint32_t def_pmac_src_port:1; /*!< Packet Source Port determined from
+				       *Default PMAC hdr (else use Ingress
+				       *PMAC hdr)
+				       */
+	uint32_t res_ing:15;	/*!< Reserved bits */
+	u8 def_pmac_hdr[DP_MAX_PMAC_LEN]; /*!< Default PMAC header config
+					   *   For the Tx DMA channel.
+					   *   Useful if Src Port does not
+					   *   send PMAC header with
+					   *   packet
+					   */
+} ingress_pmac_t;
+
+/*!
+ *@brief Egress PMAC port configuration from Datapath Manager
+ *@param rx_dma_chan  Rx DMA channel Number for which PMAC configuration
+ *		is to be done
+ *@param rm_l2hdr	If Layer 2 Header is to be removed before Egress
+ *		(for eg. for IP interfaces like LTE)
+ *@param num_l2hdr_bytes_rm If rm_l2hdr=1,then number of L2 hdr bytes to be
+ *		removed
+ *@param fcs	If FCS is enabled on the port
+ *@param pmac If PMAC header is enabled on the port
+ *@param dst_port  Destination Port Identifier
+ *@param res_eg  Reserved bits
+ *@note
+ */
+typedef struct egress_pmac {
+	uint32_t rx_dma_chan:8;	/*!< Rx DMA channel Number for which PMAC
+				 *  configuration is to be done
+				 */
+	uint32_t rm_l2hdr:1;   /*!< If Layer 2 Header is to be removed
+				*before Egress (for eg. for IP interfaces like
+				*LTE)
+				*/
+
+	uint32_t num_l2hdr_bytes_rm:8;/*!< If rm_l2hdr=1,
+				       *then number of L2 hdr bytes to be
+				       *removed
+				       */
+	uint32_t fcs:1;		/*!< If FCS is enabled on the port */
+	uint32_t pmac:1;	/*!< If PMAC header is enabled on the port */
+	uint32_t redir:1;	/*!< Enable redirection flag. GSWIP-3.1 only.
+				 *Overwritten by bRes1DW0Enable and nRes1DW0.
+				 */
+	uint32_t bsl_seg:1;	/*!< Allow (False) or not allow (True)
+				 * segmentation during buffer selection.
+				 * GSWIP-3.1 only. Overwritten by
+				 * bResDW1Enable and nResDW1.
+				 */
+	uint32_t dst_port:8;	/*!< Destination Port Identifier */
+	uint32_t res_endw1:1;	/*!< If false, nResDW1 is ignored*/
+	uint32_t res_dw1:4;	/*!< reserved field in DMA descriptor - DW1*/
+	uint32_t res1_endw0:1;	/*!< If false, nRes1DW0 is ignored.*/
+	uint32_t res1_dw0:3;	/*!< reserved field in DMA descriptor - DW0*/
+	uint32_t res2_endw0:1;	/*!< If false, nRes2DW0 is ignored.*/
+	uint32_t res2_dw0:2;	/*!< reserved field in DMA descriptor - DW0*/
+	uint32_t tc_enable:1;	/*!< Selector for traffic class bits */
+	uint32_t traffic_class:8;/*!< If tc_enable=true,sets egress queue
+				  *	traffic class.
+				  */
+	uint32_t flow_id:8; /*!< flow id msb*/
+	uint32_t dec_flag:1; /*!< If tc_enable=false,sets decryption flag*/
+	uint32_t enc_flag:1; /*!< If tc_enable=false,sets encryption flag*/
+	uint32_t mpe1_flag:1; /*!< If tc_enable=false,mpe1 marked flag valid*/
+	uint32_t mpe2_flag:1; /*!< If tc_enable=false,mpe1 marked flag valid*/
+	uint32_t res_eg:5; /*!< Reserved bits */
+} egress_pmac_t;
+
+/*!
+ *@brief struct dp_subif_stats_t
+ */
+typedef struct dp_subif_stats_t {
+	u64 rx_bytes; /*!< received bytes*/
+	u64 rx_pkts; /*!< received packets*/
+	u64 rx_disc_pkts; /*!< received discarded packets*/
+	u64 rx_err_pkts; /*!< received errored packets*/
+	u64 tx_bytes; /*!< transmitted bytes*/
+	u64 tx_pkts; /*!< transmitted packets*/
+	u64 tx_disc_pkts; /*!< transmitted discarded packets*/
+	u64 tx_err_pkts; /*!< transmitted errored packets*/
+} dp_subif_stats_t;
+
+/*!
+ *@brief enum EG_PMAC_F
+ */
+enum EG_PMAC_F {
+	/*1 bit one flag */
+	EG_PMAC_F_L2HDR_RM = 0x1, /*!< eg_pmac.numBytesRem/bRemL2Hdr valid*/
+	EG_PMAC_F_FCS = 0x2, /*!< mean eg_pmac.bFcsEna valid*/
+	EG_PMAC_F_PMAC = 0x4, /*!< mean eg_pmac.bPmacEna valid */
+	EG_PMAC_F_RXID = 0x8, /*!< mean eg_pmac.nRxDmaChanId valid */
+	EG_PMAC_F_RESDW1 = 0x10, /*!< mean eg_pmac.nResDW1 valid */
+	EG_PMAC_F_RES1DW0 = 0x20, /*!< mean eg_pmac.nRes1DW0 valid */
+	EG_PMAC_F_RES2DW0 = 0x40, /*!< mean eg_pmac.nRes2DW0 valid */
+	EG_PMAC_F_TCENA = 0x80, /*!< mean eg_pmac.bTCEnable valid */
+	EG_PMAC_F_DECFLG = 0x100, /*!< mean eg_pmac.bDecFlag valid */
+	EG_PMAC_F_ENCFLG = 0x200, /*!< mean eg_pmac.bEncFlag valid */
+	EG_PMAC_F_MPE1FLG = 0x400, /*!< mean eg_pmac.bMpe1Flag valid */
+	EG_PMAC_F_MPE2FLG = 0x800, /*!< mean eg_pmac.bMpe2Flag valid */
+	EG_PMAC_F_RESDW1EN = 0x1000, /*!< mean eg_pmac.res_endw1 valid */
+	EG_PMAC_F_RES1DW0EN = 0x2000, /*!< mean eg_pmac.res1_endw0 valid */
+	EG_PMAC_F_RES2DW0EN = 0x4000, /*!< mean eg_pmac.res2_endw0 valid */
+	EG_PMAC_F_REDIREN = 0x8000, /*!< mean eg_pmac.redir valid */
+	EG_PMAC_F_BSLSEG = 0x10000, /*!< mean eg_pmac.bsl_seg valid */
+};
+
+/*! @brief EG_PMAC Flags */
+enum IG_PMAC_F {
+	/*1 bit one flag */
+	IG_PMAC_F_ERR_DISC = BIT(0), /*!< mean ig_pmac.bErrPktsDisc valid */
+	IG_PMAC_F_PRESENT = BIT(1),  /*!< mean ig_pmac.bPmacPresent valid */
+	IG_PMAC_F_SUBIF = BIT(2),  /*!< mean ig_pmac.bSubIdDefault valid */
+	IG_PMAC_F_SPID = BIT(3),  /*!< mean ig_pmac.bSpIdDefault valid */
+	IG_PMAC_F_CLASSENA = BIT(4),  /*!< mean ig_pmac.bClassEna valid */
+	IG_PMAC_F_CLASS = BIT(5),  /*!< mean ig_pmac.bClassDefault valid */
+	IG_PMAC_F_PMAPENA = BIT(6),  /*!< mean ig_pmac.bPmapEna valid */
+	IG_PMAC_F_PMAP = BIT(7),  /*!< mean ig_pmac.bPmapDefault valid */
+	IG_PMAC_F_PMACHDR1 = BIT(8),  /*!< mean ig_pmac.defPmacHdr[1] valid */
+	IG_PMAC_F_PMACHDR2 = BIT(9),  /*!< mean ig_pmac.defPmacHdr[2] valid */
+	IG_PMAC_F_PMACHDR3 = BIT(10),  /*!< mean ig_pmac.defPmacHdr[3] valid */
+	IG_PMAC_F_PMACHDR4 = BIT(11),  /*!< mean ig_pmac.defPmacHdr[4] valid */
+	IG_PMAC_F_PMACHDR5 = BIT(12),  /*!< mean ig_pmac.defPmacHdr[5] valid */
+	IG_PMAC_F_PMACHDR6 = BIT(13),  /*!< mean ig_pmac.defPmacHdr[6] valid */
+	IG_PMAC_F_PMACHDR7 = BIT(14),  /*!< mean ig_pmac.defPmacHdr[7] valid */
+	IG_PMAC_F_PMACHDR8 = BIT(15),  /*!< mean ig_pmac.defPmacHdr[8] valid */
+};
+
+/*! @brief Paser Flags */
+enum PASER_FLAG {
+	F_MPE_NONE = 0x1, /*!< Need set MPE1=0 and MPE2=0*/
+	F_MPE1_ONLY = 0x2, /*!< Need set MPE1=1 and MPE2=0 */
+	F_MPE2_ONLY = 0x4, /*!< Need set MPE1=0 and MPE2=1 */
+	F_MPE1_MPE2 = 0x8, /*!< Need set MPE1=1 and MPE2=1 */
+};
+
+/*! @brief PASER_VALUE Flags */
+enum PASER_VALUE {
+	DP_PARSER_F_DISABLE = 0,  /*!< Without Paser Header and Offset */
+	DP_PARSER_F_HDR_ENABLE = 1,/*!< With Paser Header, but without Offset */
+	DP_PARSER_F_HDR_OFFSETS_ENABLE = 2,  /*!< with Paser Header and Offset*/
+};
+
+/*!
+ *@brief Datapath Manager Port PMAC configuration structure
+ *@param ig_pmac  Ingress PMAC configuration
+ *@param eg_pmac  Egress PMAC configuration
+ *@note GSW_PMAC_Ig_Cfg_t/GSW_PMAC_Eg_Cfg_t defined in GSWIP driver:
+ *	<xway/switch-api/lantiq_gsw_api.h>
+ */
+typedef struct dp_pmac_cfg {
+	u32 ig_pmac_flags;	/*!< one bit for one ingress_pmac_t fields */
+	u32 eg_pmac_flags;	/*!< one bit for one egress_pmac_t fields */
+	ingress_pmac_t ig_pmac;	/*!< Ingress PMAC configuration */
+	egress_pmac_t eg_pmac;	/*!< Egress PMAC configuration */
+} dp_pmac_cfg_t;
+
+/*! @brief struct pon_subif_d */
+struct pon_subif_d {
+	s32 tcont_idx; /*!< relative tconf_idx map to CQE PON dequeuer port */
+	s8 pcp;/*!< 0~7:valid pcp
+		*   -1: non valid pcp value
+		*/
+};
+
+/*! @brief enum DP_SUBIF_DATA_FLAG */
+enum DP_SUBIF_DATA_FLAG {
+	DP_SUBIF_AUTO_NEW_Q = BIT(0), /*!< create new queue for this subif */
+	DP_SUBIF_SPECIFIC_Q = BIT(1), /*!< use the already configured queue as
+				       *  specified by q_id in
+				       *  \struct dp_subif_data. This queue can
+				       *  be created by caller itself, or
+				       *  by last call of dp_register_subif_ext
+				       */
+};
+
+/*! @brief struct dp_subif_data */
+struct dp_subif_data {
+	s8 deq_port_idx;  /*!< [in] range: 0 ~ its max deq_port_num - 1
+			   *  For PON, it is tcont_idx,
+			   *  For other device, normally its value is zero
+			   */
+	enum DP_SUBIF_DATA_FLAG flag_ops; /*!< flags */
+	int q_id; /*!< [in,out]:
+		   * [in]: valid only if DP_SUBIF_SPECIFIC_Q set in
+		   *       \ref flag_ops
+		   * [out]: queue alloted or reused for this subif
+		   * Note: this queue can be created by caller,
+		   *         or by dp_register_subif_ext itself in Pmapper case
+		   */
+	struct net_device *ctp_dev; /*Optional CTP device if there is one bridge
+				     *port device
+				     */
+};
+
+/*! @brief enum DP_F_DATA_RESV_CQM_PORT */
+enum dp_port_data_flag {
+	DP_F_DATA_RESV_CQM_PORT = BIT(0), /*!< need reserve cqm multiple ports*/
+	DP_F_DATA_ALLOC = BIT(1),
+	DP_F_DATA_EVEN_FIRST = BIT(2), /*!< reserve dp_port in even number*/
+	DP_F_DATA_RESV_Q = BIT(3), /*!< reserve QOS queue */
+	DP_F_DATA_RESV_SCH = BIT(4), /*!< reserve QOS scheduler */
+};
+
+/*! @brief typedef struct dp_port_data */
+struct dp_port_data {
+	int flag_ops; /*!< flag operation, refer to enum dp_port_data_flag */
+	u32 resv_num_port; /*!< valid only if DP_F_DATA_RESV_CQM_PORT is set.
+			    * the number of cqm dequeue port to reserve.
+			    * Currently mainly for Wave600 multiple radio but
+			    * sharing same cqm dequeue port
+			    */
+	u32 start_port_no; /*!< valid only if DP_F_DATA_RESV_CQM_PORT is set */
+
+	int num_resv_q; /*!< input:reserve the required number of queues. Valid
+			 *   only if DP_F_DATA_RESV_Q bit valid in \ref flag_ops
+			 */
+	int num_resv_sched; /*!< input:reserve required number of schedulers.
+			     *   Valid only if DP_F_DATA_RESV_SCH bit valid in
+			     *   \ref flag_ops
+			     */
+	int deq_port_base; /*!< output: the CQM dequeue port base. Mainly for
+			    *          PON
+			    */
+};
+
+/*! @brief typedef struct dp_dev_data */
+struct dp_dev_data {
+	int resv; /*!< just for reserve*/
+};
+
+/*! @addtogroup Datapath_Driver_API */
+/*! @brief  Datapath Allocate Datapath Port aka PMAC port
+ *	port may map to an exclusive netdevice like in the case of
+ *	ethernet LAN ports. In other cases like WLAN, the physical port is a
+ *	Radio port, while netdevices are Virtual Access Points (VAPs)
+ *	In this case, the  AP netdevice can be passed
+ *Alternately, driver_port & driver_id will be used to identify this port
+ *@param[in] owner  Kernel module pointer which owns the port
+ *@param[in] dev pointer to Linux netdevice structure (optional), can be NULL
+ *@param[in] dev_port Physical Port Number of this device managed by the driver
+ *@param[in] port_id Optional port_id number requested. Usually, 0 and
+ *	allocated by driver
+ *@param[in] pmac_cfg PMAC related configuration parameters
+ *@param[in] flags :Various special Port flags like WAVE500, VRX318 etc ...
+ *	-  DP_F_DEALLOC_PORT :Deallocate the already allocated port
+ *@return  Returns PMAC Port number, -1 on ERROR
+ */
+int32_t dp_alloc_port(struct module *owner, struct net_device *dev,
+		      u32 dev_port, int32_t port_id,
+		      dp_pmac_cfg_t *pmac_cfg, uint32_t flags);
+
+/*! @brief  Datapath Allocate Datapath Port aka PMAC port
+ *	port may map to an exclusive netdevice like in the case of
+ *	ethernet LAN ports. In other cases like WLAN, the physical port is a
+ *	Radio port, while netdevices are Virtual Access Points (VAPs)
+ *	In this case, the  AP netdevice can be passed
+ *Alternately, driver_port & driver_id will be used to identify this port
+ *@param[in] inst the DP instance, start from 0. At SOC side, it is always 0.
+ *           For pherpheral device, normally it is non-zero.
+ *@param[in] owner  Kernel module pointer which owns the port
+ *@param[in] dev pointer to Linux netdevice structure (optional), can be NULL
+ *@param[in] dev_port Physical Port Number of this device managed by the driver
+ *@param[in] port_id Optional port_id number requested. Usually, 0 and
+ *	allocated by driver
+ *@param[in] pmac_cfg PMAC related configuration parameters
+ *@param[in,out] data to pass the peripheral information
+ *@param[in] flags :Various special Port flags like WAVE500, VRX318 etc ...
+ *	-  DP_F_DEALLOC_PORT :Deallocate the already allocated port
+ *@return  Returns PMAC Port number, -1 on ERROR
+ */
+int32_t dp_alloc_port_ext(int inst, struct module *owner,
+			  struct net_device *dev,
+			  u32 dev_port, int32_t port_id,
+			  dp_pmac_cfg_t *pmac_cfg,
+			  struct dp_port_data *data,
+			  uint32_t flags);
+
+/*! @brief  Higher layer Driver Datapath registration API
+ *@param[in] owner  Kernel module pointer which owns the port
+ *@param[in] port_id Port Id returned by alloc() function
+ *@param[in] dp_cb  Datapath driver callback structure
+ *@param[in] flags :Special input flags to alloc routine
+ *		- F_DEREGISTER :Deregister the device
+ *@return 0 - OK / -1 - Correct Return Value
+ *@note
+ */
+int32_t dp_register_dev(struct module *owner, uint32_t port_id,
+			dp_cb_t *dp_cb, uint32_t flags);
+
+/*! @brief  Higher layer Driver Datapath registration API
+ *@param[in] inst the DP instance, start from 0. At SOC side, it is always 0.
+ *           For pherpheral device, normally it is non-zero.
+ *@param[in] owner  Kernel module pointer which owns the port
+ *@param[in] port_id Port Id returned by alloc() function
+ *@param[in] dp_cb  Datapath driver callback structure
+ *@param[in,out] data to pass the peripheral information
+ *@param[in] flags :Special input flags to alloc routine
+ *		- F_DEREGISTER :Deregister the device
+ *@return 0 - OK / -1 - Correct Return Value
+ *@note
+ */
+int32_t dp_register_dev_ext(int inst, struct module *owner,
+			    u32 port_id,
+			    dp_cb_t *dp_cb,
+			    struct dp_dev_data *data,
+			    uint32_t flags);
+
+/*! @brief  Allocates datapath subif number to a sub-interface netdevice
+ *Sub-interface value must be passed to the driver
+ *port may map to an exclusive netdevice like in the case of ethernet LAN ports
+ *@param[in] owner  Kernel module pointer which owns the port
+ *@param[in] dev pointer to Linux netdevice structure, only for VRX318 driver,
+ *it can be NULL. All other driver's, must provide valid dev pointer.
+ *@param[in] subif_name pointer
+ *@param[in,out] subif_id pointer to subif_id structure including port_id
+ *@param[in] flags :
+ *	DP_F_DEREGISTER - De-register already registered subif/vap
+ *@return Port Id  / IFX_FAILURE
+ *@note
+ */
+int32_t dp_register_subif(struct module *owner, struct net_device *dev,
+			  char *subif_name, dp_subif_t *subif_id,
+			  uint32_t flags);
+
+/*! @brief  Allocates datapath subif number to a sub-interface netdevice
+ *Sub-interface value must be passed to the driver
+ *port may map to an exclusive netdevice like in the case of ethernet LAN ports
+ *@param[in] inst the DP instance, start from 0. At SOC side, it is always 0.
+ *           For pherpheral device, normally it is non-zero.
+ *@param[in] owner  Kernel module pointer which owns the port
+ *@param[in] dev pointer to Linux netdevice structure, only for VRX318 driver,
+ *it can be NULL. All other driver's, must provide valid dev pointer.
+ *@param[in] subif_name pointer
+ *@param[in,out] subif_id pointer to subif_id structure including port_id
+ *@param[in,out] data to pass the peripheral information
+ *@param[in] flags :
+ *	DP_F_DEREGISTER - De-register already registered subif/vap
+ *@return Port Id  / IFX_FAILURE
+ *@note
+ */
+int32_t dp_register_subif_ext(
+	int inst,
+	struct module *owner,
+	struct net_device *dev,
+	char *subif_name,
+	dp_subif_t *subif_id,
+	struct dp_subif_data *data,
+	uint32_t flags);
+
+/*! @brief  Transmit packet to low-level Datapath driver
+ *@param[in] rx_if  Rx If netdevice pointer - optional
+ *@param[in] rx_subif  Rx rx_subif netdevice pointer
+ *@param[in] skb  pointer to packet buffer like sk_buff
+ *@param[in] len    Length of packet to transmit
+ *@param[in] flags :Reserved
+ *@return 0 if OK  / -1 if error
+ *@note
+ */
+int32_t dp_xmit(struct net_device *rx_if, dp_subif_t *rx_subif,
+		struct sk_buff *skb, int32_t len, uint32_t flags);
+
+/*! @brief  Check if network interface like WLAN is a fastpath interface
+ *Sub-interface value must be passed to the driver
+ *	port may map to an exclusive netdevice like in the case of
+ *	ethernet LAN ports.
+ *@param[in] netif  pointer to stack network interface structure
+ *@param[in,out] subif pointer to subif_id structure including port_id
+ *@param[in] ifname  Interface Name
+ *@param[in] flags :Reserved
+ *@return 1 if WLAN fastpath interface like WAVE500 / 0 otherwise
+ *@note  Prototype of PPA_DP_CHECK_IF_NETIF_FASTPATH part of the callback
+ *		structure. Such a function needs to be defined by client driver
+ *		like the WAVE500 WLAN driver. This API is used by the PPA Stack
+ *		AL to check during acceleration learning and configuration
+ */
+
+int32_t dp_check_if_netif_fastpath_fn(struct net_device *netif,
+				      dp_subif_t *subif, char *ifname,
+				      uint32_t flags);
+
+/*! @brief  Get Pkt dst-if Sub-if value
+ *	Sub-interface value must be passed to the driver
+ *	port may map to an exclusive netdevice like in the case of ethernet
+ *	LAN ports.
+ *@param[in] netif  pointer to stack network interface structure through
+ *		which packet to be Tx
+ *@param[in] skb pointer to sk_buff structure that carries packet destination
+ *		info
+ *@param[in,out] subif_data pointer to subif_data structure including port_id
+ *@param[in] dst_mac  Destiantion MAC address to which packet is addressed
+ *@param[in,out] subif pointer to subif_id structure including port_id
+ *@param[in] flags :Reserved
+ *@return 0 if subifid found; -1 otherwise
+ *@note  Prototype of PPA_DP_GET_NETIF_SUBIF function. Not implemented in PPA
+ *	Datapath, but in client driver like WAVE500 WLAN driver
+ *@note  Either skbuff parameters to be used  or dst_mac to determine subifid
+ *	For WAVE500 driver, this will be the StationId + VAP on the basis of
+ *	the dst mac. This function is only to be used by the PPA to program
+ *	acceleration entries. The client driver is still expected to fill
+ *	in Sub-interface id when transmitting to the underlying datapath driver
+ */
+int32_t dp_get_netif_subifid(struct net_device *netif, struct sk_buff *skb,
+			     void *subif_data, uint8_t dst_mac[DP_MAX_ETH_ALEN],
+			     dp_subif_t *subif, uint32_t flags);
+
+/*! @brief  The API is for CBM to send received packets(skb) to dp lib. Datapath
+ *	lib will do basic packet parsing and forwards it to related drivers,\n
+ *	like ethernet driver, wifi and lte and so on. Noted.
+ *	It is a chained skb and dp lib will split it before send it to
+ *	related drivers
+ *@param[in] skb  pointer to packet buffer like sk_buffer
+ *@param[in] flags  reserved for futures
+ *@return 0 if OK / -1 if error
+ */
+int32_t dp_rx(struct sk_buff *skb, uint32_t flags);
+/*!
+ *@brief  The API is for configuing PMAC based on deque port
+ *@param[in] port  Egress Port
+ *@param[in] pmac_cfg Structure of ingress/egress parameters for setting PMAC
+ *	   configuration
+ *@return 0 if OK / -1 if error
+ */
+
+enum DP_F_STATS_ENUM {
+	DP_F_STATS_SUBIF = 1 << 0, /*!< Flag to get network device subif
+				    *   mib counter
+				    */
+	DP_F_STATS_PAE_CPU = 1 << 1 /*!< Flag to get CPU network mib counter*/
+};
+
+/*!
+ *@brief  The API is for dp_get_netif_stats
+ *@param[in] dev pointer to Linux netdevice structure, only for VRX318 driver,
+ *@param[in,out] subif_id pointer to subif_id structure including port_id
+ *@param[in] path_stats stats for path
+ *@param[in] flags  reserved for futures
+ *@return 0 if OK / -1 if error
+ */
+int dp_get_netif_stats(struct net_device *dev, dp_subif_t *subif_id,
+		       struct rtnl_link_stats64 *path_stats, uint32_t flags);
+
+/*!
+ *@brief  The API is for dp_clear_netif_stats
+ *@param[in] dev pointer to Linux netdevice structure, only for VRX318 driver,
+ *@param[in,out] subif_id pointer to subif_id structure including port_id
+ *@param[in] flag  reserved for futures
+ *@return 0 if OK / -1 if error
+ */
+int dp_clear_netif_stats(struct net_device *dev, dp_subif_t *subif_id,
+			 uint32_t flag);
+
+/*!
+ *@brief  The API is for dp_get_port_subitf_via_ifname
+ *@param[in] ifname  Interface Name
+ *@param[in,out] subif pointer to subif_id structure including port_id
+ *@return 0 if OK / -1 if error
+ */
+int dp_get_port_subitf_via_ifname(char *ifname, dp_subif_t *subif);
+
+/*!
+ *@brief  The API is for dp_get_port_subitf_via_dev
+ *@param[in] dev pointer to Linux netdevice structure, only for VRX318 driver,
+ *@param[in,out] subif pointer to subif_id structure including port_id
+ *@return 0 if OK / -1 if error
+ */
+int dp_get_port_subitf_via_dev(struct net_device *dev,
+			       dp_subif_t *subif);
+#ifdef CONFIG_LTQ_DATAPATH_CPUFREQ
+
+/*!
+ *@brief  The API is for dp_get_port_subitf_via_dev
+ *@param[in,out] new_state pointer to structure ltq_cpufreq_threshold,
+ *@param[in] flag flag
+ *@return 0 if OK / -1 if error
+ */
+int dp_coc_new_stat_req(enum ltq_cpufreq_state new_state, uint32_t flag);
+
+/*!
+ *@brief  The API is for dp_get_port_subitf_via_dev
+ *@param[in,out] threshold pointer to structure ltq_cpufreq_threshold,
+ *@param[in] flags flags
+ *@return 0 if OK / -1 if error
+ */
+/*! DP's submodule to call it */
+int dp_set_rmon_threshold(struct ltq_cpufreq_threshold *threshold,
+			  uint32_t flags);
+#endif /*! CONFIG_LTQ_DATAPATH_CPUFREQ*/
+
+/*! @brief enum ltq_cpufreq_state */
+enum ltq_cpufreq_state;
+/*! get port flag. for TMU proc file cat /proc/tmu/queue1 and /proc/tmu/eqt */
+u32 get_dp_port_flag(int k);
+
+/*! @brief set parser: to enable/disable pmac header
+ *@param[in] flag flag
+ *@param[in] cpu cpu
+ *@param[in] mpe1 mpe1
+ *@param[in] mpe2 mpe2
+ *@param[in] mpe3 mpe3
+ *@return 0 if OK / -1 if error
+ */
+int dp_set_gsw_parser(u8 flag, u8 cpu, u8 mpe1, u8 mpe2, u8 mpe3);
+
+/*! @brief get parser configuration
+ *@param[in] cpu cpu
+ *@param[in] mpe1 mpe1
+ *@param[in] mpe2 mpe2
+ *@param[in] mpe3 mpe3
+ *@return 0 if OK / -1 if error
+ */
+int dp_get_gsw_parser(u8 *cpu, u8 *mpe1, u8 *mpe2, u8 *mpe3);
+
+/*! @brief set pmac configuration
+ *@param[in] inst DP instance ID
+ *@param[in] port DP port ID
+ *@param[in] pmac_cfg point of pmac configuration need to set
+ *@return 0 if OK / -1 if error
+ */
+int dp_pmac_set(int inst, u32 port, dp_pmac_cfg_t *pmac_cfg);
+
+#define DP_MAX_NAME  20 /*!< max name length in character */
+/*! struct dp_cap: dp capability per instance */
+struct dp_cap {
+	int inst; /*!< Datapath instance id */
+
+	u32 tx_hw_chksum:1;  /*!< output: HW checksum offloading support flag
+			      *   for tx path
+			      *   0 - not support
+			      *   1: support
+			      */
+	u32 rx_hw_chksum:1;  /*!< output: HW checksum verification support flag
+			      *   for rx path
+			      *   0 - not support
+			      *   1: support
+			      */
+	u32 hw_tso: 1; /*!< output: HW TSO offload support for TX path */
+	u32 hw_gso: 1; /*!< output: HW GSO offload support for TX path */
+
+	char qos_eng_name[DP_MAX_NAME]; /*!< QOS engine name in string */
+	char pkt_eng_name[DP_MAX_NAME]; /*!< Packet Engine Name String */
+	int max_num_queues; /*!< max number of QOS queue supported */
+	int max_num_scheds; /*!< max number of QOS scheduler supported */
+	int max_num_deq_ports; /*!< max number of CQM dequeue port */
+	int max_num_qos_ports; /*!< max number of QOS dequeue port */
+	int max_num_dp_ports; /*!< max number of dp port */
+	int max_num_subif_per_port; /*!< max number of subif per dp_port */
+	int max_num_subif; /*!< max number of subif supported. Maybe no meaning?
+			    */
+	int max_num_bridge_port; /*!< max number of bridge port */
+};
+
+/*!
+ *@brief  The API is for dp_get_cap
+ *@param[in,out] cap dp_cap pointer, caller must provide the buffer
+ *@param[in] flag for future
+ *@return 0 if OK / -1 if error
+ */
+int dp_get_cap(struct dp_cap *cap, int flag);
+
+/*!
+ *@brief  The API is for dp_get_module_owner
+ *@param[in] ep dp_port ID
+ *@return module owner pointer if success, otherwise NULL
+ */
+struct module *dp_get_module_owner(int ep);
+
+/*!
+ *@brief  Set the minimum frame length on the DP Port
+ *@param[in] dp_port dp_port ID
+ *@param[in] min_frame_len minimal frame size for this DP port ID
+ *@param[in] flag Reserved
+ *@return return 0 if OK / -1 if error
+ */
+
+int dp_set_min_frame_len(s32 dp_port,
+			 s32 min_frame_len,
+			 uint32_t flags);
+
+/*!
+ *@brief  Enable/Disable forwarding RX packet to specified netif or ifname
+ *@param[in] netif netowrk device pointer. if NULL, then check ifname
+ *@param[in] ifname if netif == NULL, then check ifname
+ *@param[in] rx_enable: 1 enable rx for this device, otherwise
+ *@param[in] flag:
+ *            DP_RX_ENABLE: enable rx, ie, allow forwarding rx pkt to this dev
+ *            DP_RX_ENABLE: stop rx, ie, DP should drop rx pkt for this dev
+ *@return return 0 if OK / -1 if error
+ */
+#define DP_RX_ENABLE  1
+#define DP_RX_DISABLE 0
+int dp_rx_enable(struct net_device *netif, char *ifname, uint32_t flags);
+
+/*!
+ *@brief Datapath Manager Pmapper Configuration Set
+ *@param[in] dev: network device point to set pmapper
+ *@param[in] mapper: buffer to get pmapper configuration
+ *@param[in] flag: reserve for future
+ *@return Returns 0 on succeed and -1 on failure
+ *@note  for pcp mapper case, all 8 mapping must be configured properly
+ *       for dscp mapper case, all 64 mapping must be configured properly
+ *       def ctp will match non-vlan and non-ip case
+ *	For drop case, assign CTP value == DP_PMAPPER_DISCARD_CTP
+ */
+int dp_set_pmapper(struct net_device *dev, struct dp_pmapper *mapper, u32 flag);
+
+/*!
+ *@brief Datapath Manager Pmapper Configuration Get
+ *@param[in] dev: network device point to set pmapper
+ *@param[out] mapper: buffer to get pmapper configuration
+ *@param[in] flag: reserve for future
+ *@return Returns 0 on succeed and -1 on failure
+ *@note  for pcp mapper case, all 8 mapping must be configured properly
+ *       for dscp mapper case, all 64 mapping must be configured properly
+ *       def ctp will match non-vlan and non-ip case
+ *	 For drop case, assign CTP value == DP_PMAPPER_DISCARD_CTP
+ */
+int dp_get_pmapper(struct net_device *dev, struct dp_pmapper *mapper, u32 flag);
+
+#endif /*DATAPATH_API_H */
+
diff --git a/include/net/datapath_api_qos.h b/include/net/datapath_api_qos.h
new file mode 100644
index 000000000000..2b57a3ae7c15
--- /dev/null
+++ b/include/net/datapath_api_qos.h
@@ -0,0 +1,1503 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+/*!
+ * @file Datapath_api_qos.h
+ *
+ *  @brief &nbsp; Datapath QOS HAL API
+ *<PRE>
+ *  Purpose: try to provide common QOS HAL API and hide HW difference inside
+ *           Datapath QOS HAL
+ *  Note of QOS related new supports in datapath basic APIs:
+ *    1) dp_alloc_port_ext:
+ *         input: to reserve the required number of queues/schedulers
+ *         output: cqm dequeue port base for this module
+ *    2) dp_register_subif:
+ *         output: cqm_deq_port/qos_deq_port/bridge port;
+ *    3) dp_register_subif_ex:
+ *         input: caller can specify the queue if needed. But make sure this
+ *                queue is already configured properly before calling this API
+ *         output: cqm_deq_port/qos_deq_port/bridge port;
+ * <PRE>
+ * Example
+ * Queue       Sched ID: leaf/weight        Sched ID: leaf/weight    Egress Port
+ *             |-----------------|    |----------------------|
+ * Queue 17----|1  :0/0          |    |2 :0/0                |  Shaper CIR=100
+ *             |                 |    |                      |     |
+ *             |                 |    |                      |     |
+ *             |                 |----|                      |-------- 7
+ *             |                 |    |                      |
+ *             |                 |    |                      |
+ * Queue 18----|---:1/0          |    |                      |
+ *             |-----------------|    |----------------------|
+
+ * </PRE>
+ * Method 1 with basic link node API dp_node_link_add from scratch (exampe1)
+ *          struct dp_node_link info;
+ *          int flag = 0;
+ *          int dp_inst = 0;
+ *          struct dp_shaper_conf shaper;
+ *          struct dp_node_link_enable enable;
+ *          memset(&info, 0, sizeof(info));
+ *
+ *          connect queue 17 to first scheduler 1
+ *          info.inst = dp_inst;
+ *          info.arbi = ARBITRATION_WSP_WRR;
+ *          info.prio_wfq = 0;
+ *          info.node_type = DP_NODE_QUEUE;
+ *          info.node_id.q_id = 17;
+ *          info.p_node_type = DP_NODE_SCH;
+ *          info.p_node_id.sch_id = 1;
+ *          if (dp_node_link_add(&info, 0) == DP_FAILURE) {
+ *             return -1;
+ *          }
+ *
+ *          connect scheduler 1 to first scheduler 2
+ *          info.arbi = ARBITRATION_WSP_WRR;
+ *          info.prio_wfq = 0;
+ *          info.node_type = DP_NODE_SCH;
+ *          info.node_id.sch_id = 1;
+ *          info.p_node_type = DP_NODE_SCH;
+ *          info.p_node_id.sch_id = 2;
+ *          if (dp_node_link_add(&info, 0) == DP_FAILURE) {
+ *             return -1;
+ *          }
+ *          int flag = 0;
+ *          connect scheduler 2 to egress port 7
+ *          info.arbi = ARBITRATION_WSP_WRR;
+ *          info.prio_wfq = 0;
+ *          info.node_type = DP_NODE_SCH;
+ *          info.node_id.sch_id = 2;
+ *          info.p_node_type = DP_NODE_PORT;
+ *          info.p_node_id.cqm_deq_port = 7;
+ *          if (dp_node_link_add(&info, 0) == DP_FAILURE) {
+ *              return -1;
+ *          }
+ *
+ *          connect queue 18 to first scheduler 1
+ *          info.inst = dp_inst;
+ *          info.arbi = ARBITRATION_WSP_WRR;
+ *          info.prio_wfq = 0;
+ *          info.node_type = DP_NODE_QUEUE;
+ *          info.node_id.q_id = 18;
+ *          info.p_node_type = DP_NODE_SCH;
+ *          info.p_node_id.sch_id = 1;
+ *          if (dp_node_link_add(&info, 0) == DP_FAILURE) {
+ *             return -1;
+ *          }
+ *
+ *          set port shaper
+ *          struct dp_shaper_conf shaper;
+ *          memset(&shaper, 0, sizeof(shaper));
+ *          shaper.type = DP_NODE_SCH;
+ *          shaper.id.sch_id = 2;
+ *          shaper.cir = 100;
+ *          shaper.pir = 100;
+ *          dp_shaper_conf_set(&shaper, flag);
+ *
+ *          enable queue/scheduler
+ *          enable.type = DP_NODE_QUEUE;
+ *          enable.id.q_id = 17;
+ *          enable.en = DP_NODE_EN;
+ *          dp_node_link_en_set(&enable, DP_NODE_AUTO_SMART_ENABLE);
+ *          enable.id.q_id = 18;
+ *          dp_node_link_en_set(&enable, DP_NODE_AUTO_SMART_ENABLE);
+ *
+ *
+ * Method 2 with basic link node API dp_link_add from scratch (example2)
+ *          int flag = DP_NODE_AUTO_SMART_ENABLE;
+ *          int dp_inst = 0;
+ *          struct dp_shaper_conf shaper;
+ *          struct dp_qos_link full_link;
+ *          int dp_port = 3;
+ *
+ *          memset (&full_link, 0, sizeof(full_link));
+ *          full_link.inst = dp_inst;
+ *          full_link.cqm_deq_port = 7;
+ *          full_link.q_id = 1;
+ *          full_link.q_arbi = ARBITRATION_WSP_WRR;
+ *          full_link.q_prio_wfq = 0;
+ *          full_link.n_sch_lvl = 2;
+ *          full_link.sch[0].id = 1;
+ *          full_link.sch[0].arbi = ARBITRATION_WSP_WRR;
+ *          full_link.sch[0].prio_wfq = 0;
+ *          full_link.sch[1].id = 2;
+ *          full_link.sch[1].arbi = ARBITRATION_WSP_WRR;
+ *          full_link.sch[1].prio_wfq = 0;
+ *          full_link.dp_port = dp_port;
+ *          if (dp_link_add(&full_link,flag) == DP_FAILURE) {
+ *              return -1;
+ *          }
+ *          full_link.q_id = 2;
+ *          if (dp_link_add(&full_link,flag) == DP_FAILURE) {
+ *              return -1;
+ *          }
+ *
+ *          set port shaper
+ *          struct dp_shaper_conf shaper;
+ *          memset(&shaper, 0, sizeof(shaper));
+ *          shaper.type = DP_NODE_SCH;
+ *          shaper.id.sch_id = 2;
+ *          shaper.cir = 100;
+ *          shaper.pir = 100;
+ *          dp_shaper_conf_set(&shaper, flag);
+ *
+ * Example 3 with PON OMCI and one pmapper case
+ *
+ *          int inst = 0;
+ *          struct module *owner = NULL;
+ *          u32 dev_port = 0;
+ *          int32_t dp_port = 0;
+ *          struct net_device *omci_dev = NULL;
+ *          struct net_device *pmapper_dev = NULL;
+ *          struct dp_port_data port_data = {0};
+ *          uint32_t flags = 0;
+ *          dp_cb_t cb;
+ *          dp_subif_t subif_id;
+ *          struct dp_subif_data subif_data;
+ *          int32_t res;
+ *
+ *          flags = DP_F_GPON;
+ *          port_data.flag_ops = DP_F_DATA_RESV_Q | DP_F_DATA_RESV_SCH;
+ *          port_data.num_resv_q = 64;
+ *          port_data.num_resv_sched = 64;
+ *          dp_port = dp_alloc_port_ext(inst, owner, omci_dev, dev_port,
+ *                        dp_port, NULL, &port_data, flags);
+ *          if (dp_port == DP_FAILURE) {
+ *              pr_err("Fail to allocate port\n");
+ *              return -1;
+ *          }
+ *
+ *          it needs to register device, ie, cb properly.
+ *          At least, it needs set below callbacks:
+ *          1) rx_fn
+ *          2) get_subifid_fn: mainly for cpu tx path to fill in proper CTP, esp
+ *             for pmapper related cases
+ *          flags = 0;
+ *          res = dp_register_dev_ext(inst, owner, dp_port,
+ *                   &cb, NULL, flags);
+ *          if (res == DP_FAILURE) {
+ *               pr_err("Fail to register dev\n");
+ *              dp_alloc_port_ext(inst, owner, omci_dev, dev_port,
+ *              dp_port, NULL, &port_data, DP_F_DEREGISTER);
+ *              return -1;
+ *          }
+ *
+ *          Register OMCI Dev
+ *          First it needs to create omci dev and store in omci_dev
+ *          Once it is ready, call below dp_register_subif_ext API.
+ *          After dp_register_subif_ext, below QOS structure will be created
+ *          Queue       Sched ID: leaf/weight         Egress Port
+ *                      |----------------------|
+ *          Queue xx----|1  :0/0               |
+ *                      |                      |
+ *                      |                      |
+ *                      |                      |---- ----- 26
+ *                      |                      |
+ *                      |                      |
+ *                      |                      |
+ *                      |----------------------|
+ *          flags = 0;
+ *          subif_id.inst = inst;
+ *          subif_id.port_id = dp_port;
+ *          subif_id.subif_num = 1;
+ *          subif_id.subif = 0; OCMI CTP
+ *          subif_data.deq_port_idx = 0;
+ *          res = dp_register_subif_ext(inst, owner, omci_dev, omci_dev->name,
+ *                           &subif_id, &subif_data, flags);
+ *          if (res == DP_FAILURE) {
+ *              pr_err("Fail to regisster subif port\n");
+ *              dp_register_dev_ext(inst, owner, dp_port,
+ *                            NULL, NULL, DP_F_DEREGISTER);
+ *              dp_alloc_port_ext(inst, owner, omci_dev, dev_port,
+ *                     dp_port, NULL,
+ *                     &port_data, DP_F_DEREGISTER);
+ *          return -1;
+ *          }
+ *
+ *          Register pmapper device
+ *          First it needs to create new Dev and store in pmapper_dev
+ *          Once it is ready, call below dp_register_subif_ext API.
+ *          After dp_register_subif_ext, below QOS structure will be created:
+ *          Queue       Sched ID: leaf/weight         Egress Port
+ *                      |----------------------|
+ *          Queue xx----|x  :0/0               |
+ *                      |                      |
+ *                      |                      |
+ *                      |                      |---- ----- 26
+ *                      |                      |
+ *                      |                      |
+ *                      |----------------------|
+ *
+ *          Queue yy----|y  :0/0               |
+ *                      |                      |
+ *                      |                      |
+ *                      |                      |---- ----- 27
+ *                      |                      |
+ *                      |                      |
+ *                      |----------------------|
+ *
+ *          subif_id.subif = 1; 1st CTP for one pmapper
+ *          subif_data.flag_ops = DP_SUBIF_Q_PER_CTP | DP_SUBIF_PCP;
+ *          subif_data.deq_port_idx = 1;
+ *          subif_data.pcp = 0;
+ *          res = dp_register_subif_ext(inst, owner, pmapper_dev,
+ *                     pmapper_dev->name,
+ *                     &subif_id, &subif_data, flags);
+ *          if (res == DP_FAILURE) {
+ *               pr_err("Fail to regisster subif port\n");
+ *               return -1;
+ *          }
+
+ *          Register 2nd CTP based on same Dev and same pmapper
+ *          After dp_register_subif_ext,below QOS structure will be created:
+ *          Queue       Sched ID: leaf/weight         Egress Port
+ *                      |----------------------|
+ *          Queue xx----|x  :0/0               |
+ *                      |                      |
+ *                      |                      |
+ *                      |                      |---- ----- 26
+ *                      |                      |
+ *                      |                      |
+ *                      |----------------------|
+ *
+ *          Queue yy----|y  :0/0               |
+ *                      |                      |
+ *                      |                      |
+ *                      |                      |---- ----- 27
+ *          Queue zz----|    1/0               |
+ *                      |                      |
+ *                      |----------------------|
+ *
+ *          subif_id.subif = 2; 2nd Data CTP
+ *          DP_SUBIF_Q_PER_CTP bit used to create a new queue.
+ *          Otherwise, it will share same queue yy
+ *
+ *          subif_data.flag_ops = DP_SUBIF_Q_PER_CTP | DP_SUBIF_PCP;
+ *          subif_data.deq_port_idx = 1;
+ *          subif_data.pcp = 1;
+ *          res = dp_register_subif_ext(inst, owner, omci_dev, omci_dev->name,
+ *                    &subif_id, &subif_data, flags);
+ *          if (res == DP_FAILURE) {
+ *               pr_err("Fail to regisster subif port\n");
+ *              return -1;
+ *          }
+ *
+ *
+ * Note:
+ *  1) DP QOS HAL API needs maintain below mapping:
+ *     a) Physical queue id (user)  <---> Queue Node ID (ppv4)
+ *     b) Physical CQM dequeue port <--> QOS dequeue port Node ID (ppv4)
+ * </PRE>
+ */
+/*! @defgroup Datapath_QOS Datapath QOS HAL
+ *@brief All API and defines exported by Datapath QOS HAL
+ */
+/*! @{ */
+/*!
+ * @defgroup APIs_link_related QOS link subgroup
+ * @brief HAL API for link related operation:
+ *        Single link node operation: dp_node_link_add
+ *                                    dp_node_link_get
+ *                                    dp_node_unlink
+ *        Smart full link path operation: dp_link_add
+ * @defgroup APIs_link_node_en QOS node enable subgroup
+ * @brief HAL API for link node enable:dp_node_link_en_set/get
+ * @defgroup APIs_link_node_prio QOS node priority subgroup
+ * @brief HAL API for link node priority setting: dp_qos_link_prio_set/get
+ * @defgroup APIs_dp_queue_conf QOS queue configuration subgroup
+ * @brief HAL API for link queue conf: dp_queue_conf_set/get
+ * @defgroup APIs_dp_shaper_conf_set QOS shaper subgroup
+ * @brief HAL API for shaper/bandwidth setting: dp_shaper_conf_set/get
+ * @defgroup APIs_dp_node_alloc QOS node allocation subgroup
+ * @brief HAL API for link node (queue/scheduler) allocation: dp_node_alloc/free
+ * @defgroup APIs_dp_queue_map_set QOS queue mapping subgroup
+ * @brief HAL API for queue mapping: dp_queue_map_set/get
+ * @defgroup APIs_dp_qos_example PON registration and QOS example
+ * @brief HAL API example1/example2/gpon_example3
+ */
+ /*! @} */
+#ifndef DP_QOS_API_H
+#define DP_QOS_API_H
+#define DP_NODE_SMART_FREE 1 /*< @brief flag to free node
+			      *  and its parent if no child
+			      */
+
+#define DP_MAX_SCH_LVL  3 /*!< @brief max number of hierarchy QOS layers
+			   *   supported.
+			   *  can change MACRO for more layers
+			   */
+#define DP_NODE_AUTO_ID -1  /*!< @brief auto allocate a node for queue and
+			     *   scheduler
+			     */
+#define DP_AUTO_LEAF -1 /*!< @brief auto generate leaf in the scheduler.
+			 *  For TMU QOS's scheduler, leaf need to set
+			 */
+#define DP_MAX_COLORS  3  /*!< @brief support max number of color */
+
+#define DP_NO_SHAPER_LIMIT 0xFFFFFFFE /*!< @brief no limit of shaper*/
+#define DP_MAX_SHAPER_LIMIT 0xFFFFFFFF /*!< @brief max limit of shaper*/
+
+/*! @brief QOS Link Node Type: Queue, scheduler and dequeue port*/
+enum dp_node_type {
+	DP_NODE_UNKNOWN = 0,  /*!< Unallocated node*/
+	DP_NODE_QUEUE,  /*!< queue node*/
+	DP_NODE_SCH,   /*!< scheduler node*/
+	DP_NODE_PORT   /*!< port node*/
+};
+
+/*! @brief QOS Link Node ID:
+ *  @note for queue id or scheduler id, for add API, if its value equals to
+ *        DP_NODE_AUTO_ID, add API will auto allocate a
+ *        queue or scheduler for it
+ */
+union dp_node_id {
+	int q_id;  /*!< queue physical id */
+	int sch_id; /*!< scheduler id:
+		     *    for pp, it is logical
+		     *    for TMU, it is physical
+		     */
+	int cqm_deq_port; /*!< cbm/cqem dequeue port */
+};
+
+/*! @brief node flag to enable/disable/keep current setting */
+enum dp_node_en {
+	DP_NODE_DIS = BIT(0),     /*!< disable node:drop new incoming pkt
+				   *   for ppv4, valid queue/port only
+				   */
+	DP_NODE_EN = BIT(1),      /*!< enable node:allow to enqueue
+				   *   for ppv4, valid queue/port only
+				   */
+	DP_NODE_SUSPEND = BIT(2), /*!< Suspend this node no scheduling:
+				   *   Not for TMU
+				   *   for ppv4, valid queue/sched/port
+				   */
+	DP_NODE_RESUME = BIT(3)   /*!< Resume scheduling for this node:
+				   *  Not for TMU
+				   *  for ppv4,valid queue/sched/port
+				   */
+};
+
+/*! @brief arbitration method used for the node in its parents scheduler/ports*/
+enum dp_arbitate {
+	ARBITRATION_NULL = 0, /*!< No arbitrate */
+	ARBITRATION_WRR,  /*!< round robin: for ppv4 */
+	ARBITRATION_SP,  /*!< strict priority: for TMU */
+	ARBITRATION_WSP,  /*!< strict priority: for ppv4 */
+	ARBITRATION_WSP_WRR, /*!< combination of strict priority + round robin:
+			      *   ppv4 also not support at present
+			      */
+	ARBITRATION_WFQ,     /*!< fwq: for TMU/ppv4*/
+};
+
+/*! @brief dp_node_flag */
+enum dp_node_flag {
+	DP_NODE_SMART_UNLINK = BIT(0),/*!< Unlink this specified node together
+				       *   with all child
+				       *  @note it is for unlink API only
+				       */
+	DP_NODE_AUTO_FREE_RES = BIT(1),/*!< auto free QOS resource after unlink.
+					*  @note it is for unlink API only
+					*/
+	DP_NODE_AUTO_ENABLE = BIT(2),/*!< DP will auto enable this node if it is
+				      *  set.
+				      *  @note it is for link_add related API
+				      */
+	DP_NODE_AUTO_SMART_ENABLE = BIT(3),/*!< DP will auto enable all nodes
+					    *   in the node's full path:
+					    *  @note it is for link_add related
+					    *   API only
+					    */
+
+	DP_ALLOC_RESV_ONLY = BIT(4),/*!< Only allocate node from this device's
+				     *  resverved resource.
+				     *  Reservation can be done via
+				     *  dp_alloc_port_ext
+				     *  @note it is for link_alloc/link_add
+				     *  related API only
+				     */
+	DP_ALLOC_GLOBAL_ONLY = BIT(5),/*!< Only allocate node from the system
+				       * global free resource.
+				       *  @note it is for link_alloc/link_add
+				       *  related API only
+				       */
+	DP_ALLOC_GLOBAL_FIRST  = BIT(6),/*!< allocate node follow the sequence:
+					 * 1) system global free resource
+					 * 2) device reserved resource
+					 * @note it is for link_alloc/link_add
+					 *  related API only
+					 */
+};
+
+/*! @brief QOS link node atribute to setup/remove/get a link note */
+struct dp_node_link {
+	int inst; /*!< input: dp instance. For SOC side, it is always zero.
+		   *         Anyway just call dp_get_netif_subifid to get it
+		   */
+	int dp_port;   /*!< input[optional]: based on it to
+			*    get the free node if there is reservation
+			*    during dp_alloc_port_ext
+			*/
+	enum dp_node_type node_type; /*!< input node type:can be queue/scheduler
+				      *  /port
+				      */
+	union dp_node_id node_id; /*!< input node id.
+				   *  if id == DP_NODE_AUTO_ID, DP will
+				   *  allocate a free node from the reserved
+				   *  pool or global pool and set node id
+				   */
+
+	enum dp_arbitate arbi;  /*!< <PRE>arbitration method used in its parents
+				 *  for this node
+				 * for dp_node_link_add: it is in  input
+				 * for dp_node_link_get: it is in  output
+				 * for dp_node_unlink: no use </PRE>
+				 */
+	int prio_wfq; /*!< <PRE>node priority
+		       * for dp_node_link_add: it is in  input
+		       * for dp_node_link_get: it is in  output
+		       * for dp_node_unlink: no use </PRE>
+		       */
+
+	int leaf; /*!< <PRE>The leaf in the parent of this node
+		   *  Only valid for TMU,valid value: 0 - 7
+		   * for dp_node_link_add: it is in  input
+		   * for dp_node_link_get: it is in  output
+		   * for dp_node_unlink: no use</PRE>
+		   */
+	enum dp_node_type p_node_type;  /*!< <PRE>parent node type: scheduler/
+					 *  dequeue port
+					 *  for dp_node_link_add: it is input
+					 *  for dp_node_link_get: it is output
+					 *  for dp_node_unlink: no use</PRE>
+					 */
+	union dp_node_id p_node_id; /*!< <PRE>parent id
+				     * for dp_node_link_add: it is in input
+				     *   if type is scheduler
+				     *     if id == DP_NODE_AUTO_ID
+				     *        DP will allocate a free node
+				     *        from the reserved or global pool
+				     *        and set p_node_id
+				     *     else use user specified parent id
+				     *   else type is port
+				     *        User must provide valid
+				     *        cqm_deq_port value
+				     * for dp_node_link_get: it is in output
+				     * for dp_node_unlink: no use
+				     * </PRE>
+				     */
+	union dp_node_id cqm_deq_port; /*!< <PRE>input/optional:
+					*  for TMU queue link setup, it is
+					*  required to specify dequeue port
+					*  for dp_node_link_add: it is in input
+					*  for dp_node_link_get: it is in output
+					*  for dp_node_unlink: no use</PRE>
+					*/
+};
+
+/*! \ingroup APIs_link_related
+ * @brief Add a Link node for queue and scheduler.
+ * @param [in,out] info struct dp_node_link *info
+ * @param [in] flag int flag, refer to enum \ref dp_node_flag
+ * @return [out] integer value: return DP_SUCCESS if succeed,
+ *                              otherwise, return DP_FAILURE
+ * @note
+ *  For queue: it is used to attach a queue to its parent, ie, scheduler <br />
+ *  For scheduler: it is used to attach a scheduler to its parent:
+ *      another scheduler or egress port <br />
+ *  If it is used to move an existing scheduler to a new scheduler but still
+ *     with original dequeue port, DP need to do necessary protection
+ *     as dp_node_unlink API <br />
+ *  If caller want to directly move an existing scheduler to a different cqm
+ *     dequeue port, it is not allowed, esp for TMU case in order to make
+ *     the QOS HAL API simple. <br />
+ *  If caller really want to move the node to another dequeue port, it needs to
+ *     unlink the node and re-link to a new dequeue port<br />
+ *  If flag DP_NODE_AUTO_ENABLE/DP_NODE_AUTO_SMART_ENABLE is set, it will auto
+ *     enable related node or nodes as specified, including its indirect and
+ *     direct parents.<br />
+ *  If flag DP_ALLOC_XXX related is set, it will try to allocate a new node
+ *     as specified priority from reserved pool or global pool<br />
+ */
+int dp_node_link_add(struct dp_node_link *info, int flag);
+
+/*! \ingroup APIs_link_related
+ * @brief unlink a QOS node for its QOS configuration: queue or scheduler
+ * @param [in,out] info struct dp_node_link *info
+ * @param [in] flag int flag, refer to enum \ref dp_node_flag
+ * @return [out] integer value: return DP_SUCCESS if succeed
+ *                              otherwise, return DP_FAILURE
+ *  @note It is used to unlink a specified node or including its full path
+ *        depends on the flag set
+ * <PRE>
+ *  1) if it is to unlink a queue node, DP will do necessary work as below:
+ *        re-map the lookup entry to a drop queue
+ *        flushing the queue: not including those already dequeue by CQM
+ *        disable the queue
+ *        unlink the queue node as specified from its parent== Not needed?
+ *  2) if it is to unlink a scheduler node, DP will do necessary work as below:
+ *       a) if there is still child linked to this scheduler yet
+ *             i) without flag DP_NODE_SMART_UNLINK set:
+ *                   return DP_FAILURE
+ *             ii) with flag DP_NODE_SMART_UNLINK set:
+ *                   unlink its all child first,== Not needed?
+ *                   then unlink this node as specified.== Not needed?
+ *  Note:
+       a) This API only unlik the node from the QOS setup by default, but the
+ *        node itself is not freed.
+ *     b) If the caller realy want to free the unlinked node automatically,
+ *        it needs to specify flag with DP_NODE_AUTO_FREE_RES
+ *     c) If DP_NODE_SMART_UNLINK set: DP will unlink all its child note also
+ *     d) Normally top layer caller should not delete all nodes under one
+ *        specific CQM dequeue port. But if really need to do, just set proper
+ *        node_type and and node_id. -- Need take care
+ * </PRE>
+ */
+int dp_node_unlink(struct dp_node_link *info, int flag);
+
+/*! \ingroup APIs_link_related
+ * @brief get a node's link configration
+ * @param [in,out] info struct dp_node_link *info
+ * @param [in] flag int flag
+ * @return [out] integer value: return DP_SUCCESS if succeed,
+ *                              otherwise, return DP_FAILURE
+ */
+int dp_node_link_get(struct dp_node_link *info, int flag);
+
+/*! @brief struct for enable/disabling a link node */
+struct dp_node_link_enable {
+	int inst; /*!< input: dp instance. For SOC side, it is always zero */
+	enum dp_node_type type; /*!< link node type */
+	union dp_node_id id; /*!< link node id */
+	enum dp_node_en en;  /*!< action flag: enable/disable */
+};
+
+/*! \ingroup APIs_link_node_en
+ * @brief enable/disable a link node
+ * @param [in,out] en pointer to struct dp_node_link_enable *en
+ * @param [in] flag int flag
+ * @return [out] integer value: return DP_SUCCESS if succeed,
+ *                              otherwise, return DP_FAILURE
+ * @note if flag DP_NODE_AUTO_SMART_ENABLE is set, it will auto enable all
+ * related indrect and dirct parents
+ */
+int dp_node_link_en_set(struct dp_node_link_enable *en, int flag);
+
+/*! \ingroup APIs_link_node_en
+ * @brief get enable/disable status for a link node
+ * @param [in,out] en struct dp_node_link_enable *en
+ * @param [in] flag int flag
+ * @return [out] integer value: return DP_SUCCESS if succeed,
+ *                              otherwise, return DP_FAILURE
+ */
+int dp_node_link_en_get(struct dp_node_link_enable *en, int flag);
+
+/*! @brief structure for multiple scheduler attribute */
+struct dp_sch_lvl {
+	int id;     /*!< scheduler id:
+		     *    for ppv4, it is scheduler node id.
+		     *    for TMU, it is physical id
+		     * sch_id >=0: set up the link for this queue
+		     * sch_id==DP_NODE_AUTO_ID: alloc a free scheduler and
+		     *                          set up the link
+		     */
+	int prio_wfq; /*!< scheduler output priority to its next scheduler
+		       *  or egress port
+		       */
+	int leaf;     /*!< scheduler output leaf to the next scheduler ???
+		       *  Since only valid for TMU case, shall is auto shift the
+		       *  the queue according to the prio_wfq to get the leaf ??
+		       */
+	enum dp_arbitate arbi; /*!< arbitration method used for this link node
+				* in its parents
+				*/
+};
+
+/*!
+ *  @brief QOS full link note's attribute in order to quickly
+ *   set up a full QoS full from like below:
+ *   queue -> scheduler -> .... -> dequeue port
+ */
+struct dp_qos_link {
+	int inst; /*!< input: dp instance. For SOC side, it is always zero */
+	int dp_port;   /*!< input[optional]: based on it to
+			*    get the free node if there is reservation
+			*    during dp_alloc_port_ext and flag
+			*    DP_NODE_AUTO_ID is set
+			*/
+	int cqm_deq_port; /*!< cbm dequeue port */
+	int q_id;  /*!< physical queue id
+		    *    id >=0: queue id specified by caller itself
+		    *    id==DP_NODE_AUTO_ID: alloc a free queue and set up
+		    */
+	enum dp_arbitate q_arbi;/*!< arbitration method used in its parents for
+				 *  for this queue
+				 */
+	int q_prio_wfq; /*!<  queue priority
+			 *    input for dp_qos_link_add API
+			 *    output for dp_qos_link_get API
+			 */
+	int q_leaf;   /*!<  The leaf to the scheduler input ???
+		       *   not valid for ppv4
+		       */
+	int n_sch_lvl; /*!<  The number of Scheduler provided in the
+			*   variable sch[DP_MAX_SCH_LVL].
+			*   it should be less than DP_MAX_SCH_LVL
+			*    input for dp_qos_link_add API
+			*    output for dp_qos_link_get API
+			*/
+	struct dp_sch_lvl sch[DP_MAX_SCH_LVL]; /*!<scheduler info*/
+};
+
+/*! \ingroup APIs_link_related
+ * @brief A quick way to add a full queue path with a single API call.
+ * @param [in,out] cfg struct dp_qos_link *cfg
+ * @param [in] flag int flag
+ * @return [out] integer value: return DP_SUCCESS if succeed
+ *                              otherwise, return DP_FAILURE
+ * @note <PRE> It supports below example with a single API call:
+ *    queue -> egress port, or
+ *    queue -> scheduler -> egress port  or
+ *    queue -> scheduler -> ... -> scheduler -> egress port
+ *  if flag DP_NODE_AUTO_SMART_ENABLE is set, it will auto enable related nodes
+ *    dp_link_add in this full path
+ * </PRE>
+ */
+int dp_link_add(struct dp_qos_link *cfg, int flag);
+
+/*! \ingroup APIs_link_related
+ * @brief A quick way to get a full queue path configure instead of multipel
+ *  calling of dp_node_link_get
+ *  just based on input parameter inst, deq_port and q_id;
+ * @param [in,out] cfg struct dp_qos_link *cfg
+ * @param [in] flag  int flag
+ * @return [out] integer value: return DP_SUCCESS if succeed,
+ *                              otherwise, return DP_FAILURE
+ */
+int dp_link_get(struct dp_qos_link *cfg, int flag);
+
+/*! @brief dp_node_prio*/
+struct dp_node_prio {
+	int inst; /*!< input: dp instance. For SOC side, it is always zero */
+	enum dp_node_type type; /*!< Link Node */
+	union dp_node_id id; /*!< input node id */
+	enum dp_arbitate arbi; /*!< arbitration method used for this link node
+				* in its parents(scheduler/egress port)
+				*/
+	int prio_wfq; /*!< priority for WSP:
+		       *    For TMU strict priority: 0-(highest) 1023-(lowest)
+		       *    But don't know PPv4 behavior yet ????
+		       *  priority for WFQ mode, it is WFQ weight.
+		       */
+};
+
+/*! \addtogroup APIs_link_node_prio
+ * @brief APIs_link_node_prio group
+ * @param [in,out] info struct dp_node_prio *info
+ * @param [in] flag int flag
+ * @return [out] integer value: return DP_SUCCESS if succeed,
+ *                              otherwise, return DP_FAILURE
+ * @note ????? need to change leaf for TMU case ????.
+ * Waiting Purnendu to confirm.
+ *  @{
+ */
+/*!
+ *  @brief function dp_qos_link_prio_set
+ */
+int dp_qos_link_prio_set(struct dp_node_prio *info, int flag);
+/*!
+ *  @brief function dp_qos_link_prio_get
+ */
+int dp_qos_link_prio_get(struct dp_node_prio *info, int flag);
+/*! @} */
+
+/*! @brief dp_q_drop_mode*/
+enum dp_q_drop_mode {
+	DP_QUEUE_DROP_TAIL, /*!< tail drop mode. */
+	DP_QUEUE_DROP_WRED,  /*!< wred mode */
+};
+
+/*! @brief dp_color*/
+enum dp_color {
+	DP_COLOR_GREEN = 0, /*!< green color */
+	DP_COLOR_YELLOW,   /*!< yellow color */
+	DP_COLOR_RED,  /*!< red color */
+};
+
+/*! @brief dp_q_size_unit*/
+enum dp_q_size_unit {
+	DP_COLOR_PKT = 0, /*!< in packet*/
+	DP_COLOR_BYTE,   /*!< in bytes*/
+};
+
+/*! @brief dp_queue_conf*/
+struct dp_queue_conf {
+	int inst; /*!< input: dp instance. For SOC side, it is always zero */
+	int q_id; /*!< input: q_id */
+	enum dp_node_en act;  /*!< enable/disable/suspend/resume queue */
+	enum dp_q_drop_mode drop; /*!< TMU: wred/tail drop, how about PP?? */
+	enum dp_q_size_unit unit; /*!< queue size unit:packet/bytes */
+	u32 min_size[DP_MAX_COLORS]; /*!< queue minimal size, If QOCC less than
+				      * this setting, should be no drop
+				      */
+	u32 max_size[DP_MAX_COLORS]; /*!< queue maximum size, If QOCC more than
+				      * this setting, should be dropped.
+				      * For tail drop mode, it is not valid
+				      */
+	u32 wred_slope[DP_MAX_COLORS];/*!< in percent, for example, 1 means 1%*/
+	u32 wred_min_guaranteed; /*!< ??? from ppv4 */
+	u32 wred_max_allowed; /*!< ??? from ppv4 */
+};
+
+/*! \addtogroup APIs_dp_queue_conf
+ * @brief Set(add/remove/disable) shaper/bandwidth based on its node
+ * @param [in,out] cfg struct dp_shaper_conf *cfg
+ * @param [in] flag int flag
+ * @return [out] integer value: return DP_SUCCESS if succeed,
+ *                              otherwise, return DP_FAILURE
+ *  @{
+ */
+/*!
+ *  @brief function to set queue configuration
+ */
+int dp_queue_conf_set(struct dp_queue_conf *cfg, int flag);
+/*!
+ *  @brief function to get queue configuration
+ */
+int dp_queue_conf_get(struct dp_queue_conf *cfg, int flag);
+/*! @} */
+
+/*! @brief enum dp_shaper_cmd */
+enum dp_shaper_cmd {
+	DP_SHAPER_CMD_ADD = 0, /*!< add shaper */
+	DP_SHAPER_CMD_REMOVE,  /*!< remove shaper */
+	DP_SHAPER_CMD_ENABLE,  /*!< ppv4 does not support enable
+				* just use ADD shaper
+				*/
+	DP_SHAPER_CMD_DISABLE, /*!< disable the shaper: no limit
+				* for ppv4 does not support disable
+				* instead just remove shaper
+				*/
+};
+
+/*! @brief dp_shaper_conf */
+struct dp_shaper_conf {
+	int inst; /*!< input: dp instance. For SOC side, it is always zero */
+
+	enum dp_shaper_cmd cmd; /*!< command */
+	enum dp_node_type type; /*!< Shaper type */
+	union dp_node_id id;  /*!< node id for queue/scheduler/port*/
+	u32 cir;  /*!< bandwidth in kbps */
+	u32 pir; /*!< bandwidth in kbps. PPV4 support ?? */
+	u32 cbs;  /*!< cbs. PPV4 support ?? */
+	u32 pbs;  /*!< pbs. PPV4 support ?? */
+};
+
+/*! \addtogroup APIs_dp_shaper_conf_set
+ * @brief Set(add/remove/disable) shaper/bandwidth based on its node
+ * @param [in,out] cfg struct dp_shaper_conf *cfg
+ * @param [in] flag int flag
+ * @return [out] integer value: return DP_SUCCESS if succeed,
+ *                              otherwise, return DP_FAILURE
+ *  @{
+ */
+/*!
+ *  @brief function to set shaper configuration based on its node
+ */
+int dp_shaper_conf_set(struct dp_shaper_conf *cfg, int flag);
+/*!
+ *  @brief function to get shaper configuration based on its node
+ */
+int dp_shaper_conf_get(struct dp_shaper_conf *cfg, int flag);
+/*! @} */
+
+/*! @brief dp_node_alloc */
+struct dp_node_alloc {
+	int inst; /*!< input: dp instance. For SOC side, it is always zero */
+	int dp_port;   /*!< input[optional]: based on it to
+			*    get the free node if there is reservation
+			*    during dp_alloc_port_ext
+			*/
+
+	enum dp_node_type type; /*!< node type: queue or scheduler */
+	union dp_node_id id;  /*!< <PRE>node id: if id == DP_NODE_AUTO_ID,
+			       *  allocate a free node from global pool or from
+			       *  its reserved pool
+			       *  otherwise provided by the caller itself.</PRE>
+			       */
+	//int cqm_dq_port; /*Added for qos slim driver only*/
+};
+
+/*! \ingroup APIs_dp_node_alloc
+ * @brief allocate a node(queue or scheduler)
+ * @param [in,out] node struct dp_node_info *node
+ * @param [in] flag int, refer to enum \ref dp_node_flag
+ * @return [out] integer value: return DP_SUCCESS if succeed,
+ *                              otherwise, return DP_FAILURE
+ */
+int dp_node_alloc(struct dp_node_alloc *node, int flag);
+
+/*! \ingroup APIs_dp_node_alloc
+ * @brief free a node: queue or scheduler
+ * @param [in,out] node struct dp_node_alloc *node
+ * @param [in] flag int flag
+ *             if flag DP_NODE_SMART_FREE set try to free parent if no child
+ * @return [out] integer value: return DP_SUCCESS if succeed,
+ *                              otherwise, return DP_FAILURE
+ */
+int dp_node_free(struct dp_node_alloc *node, int flag);
+
+/*! \ingroup dp_node_children_free
+ * @brief free all children based on specified parent node
+ * @param [in,out] node struct dp_node_alloc *node
+ * @param [in] flag int flag
+ * @return [out] integer value: return DP_SUCCESS if succeed,
+ *				otherwise, return DP_FAILURE
+ */
+int dp_node_children_free(struct dp_node_alloc *node, int flag);
+
+/*! @brief dp_queue_res */
+struct dp_queue_res {
+	int q_id; /*!< queue id */
+	int q_node; /*!< queue logica node id. For debugging only */
+	int sch_lvl; /*!< number of scheduler layers configured for this queue*/
+	int sch_id[DP_MAX_SCH_LVL]; /*!< Scheduler information.
+				     *  @note the scheduler of sch_id[0] is the
+				     *   one which the queue attached if there
+				     *   scheduler used for this queue
+				     */
+	int leaf[DP_MAX_SCH_LVL]; /*!< leaf information. Valid only for TMU HW
+				   *
+				   */
+	int cqm_deq_port; /*!< cqm dequeue port: absolute port id */
+	int qos_deq_port; /*!< qos dequeue port: Normally user no need to know*/
+};
+
+#define DEQ_PORT_OFFSET_ALL -1 /*!< @brief Port offset all */
+
+/*! @brief dp_dequeue_res */
+struct dp_dequeue_res {
+	int inst; /*!< input: dp instance. For SOC side, it is always zero */
+	struct net_device *dev; /*!< <PRE>input: get resource for this dev
+				 *         NULL Allowed, for example  ATM/CPU
+				 *         If valid, then dp_port no use</PRE>
+				 */
+	int dp_port;  /*!< input: if DEV NULL, dp_port must be valid,
+		       *  otherwise no use
+		       */
+	int cqm_deq_idx; /*!< <PRE>get resource as specified dequeue port
+			  *  offset (relative)
+			  *  If it is DEQ_PORT_OFFSET_ALL, it means
+			  *  get all resource under that dev/dep_port
+			  *  related device.
+			  *  DEQ_PORT_OFFSET_ALL is mainly for PON/CPU case,
+			  *  since multiple dequeue port applied
+			  *  For pon case, cqm_deq_port is like tcont idx
+			  * </PRE>
+			  */
+	int cqm_deq_port; /*!< <PRE>get resource as specified dequeue port
+			   *  absolution dequeue port
+			   *  output only:
+			   *    for cqm_deq_idx, cqm_deq_port is
+			   *           matched absolute cqm dequeue port
+			   *     if cqm_deq_idx == DEQ_PORT_OFFSET_ALL,
+			   *            it is the base of cqm dequeue port
+			   * </PRE>
+			   */
+
+	int num_deq_ports; /*!< <PRE>output: The number of dequeue port this
+			    *    dev have.
+			    *    Normally this value should be 1.
+			    *    For GPON case, if cqm_deq_port ==
+			    *        DEQ_PORT_OFFSET_ALL, then it will be the
+			    *    max number of dequeue port. In falcon-mx, it is
+			    *    64.</PRE>
+			    */
+	int num_q; /*!< output: the number of queues*/
+	int q_res_size; /*!< input: to indicate q num can be stored in q_res*/
+	struct dp_queue_res *q_res;/*!< output: resource output.
+				    *  @note caller should allocate the memory.
+				    *  <PRE>Procedure:
+				    *  1st call with res NULL to get the num_q;
+				    *  then allocate memory: sizeof(*res)*num_q
+				    *  2nd call with valid res pointer to get
+				    *  real queue information</PRE>
+				    */
+};
+
+/*! @brief Function deque dp_deq_port_res_get
+ * @param [in,out] res struct dp_dequeue_res *res
+ * @param [in] flag int flag
+ * @return [out] integer value: return DP_SUCCESS if succeed,
+ *                              otherwise, return DP_FAILURE
+ */
+int dp_deq_port_res_get(struct dp_dequeue_res *res, int flag);
+
+/*! @brief dp_counter_type */
+enum dp_counter_type {
+	DP_ENQUEUE_MIB_MODE = 0 /*!< For TMU Enqueue mib mode setting:pkt/bytes
+				 *  Not sure about PPv4 yet
+				 */
+};
+
+/*! @brief dp_counter_conf */
+struct dp_counter_conf {
+	int inst; /*!< input: dp instance. For SOC side, it is always zero */
+	enum dp_counter_type counter_type; /*!< counter_type to get/set */
+	int id;  /*!< depends on mode, it can be queue id/scheduler id/.... */
+	int mode; /*!< value to set */
+};
+
+/*! @brief Function dp_counter_mode_set
+ * @param [in,out] cfg struct dp_counter_conf *cfg
+ * @param [in] flag int flag
+ * @return [out] integer value: return DP_SUCCESS if succeed,
+ *                              otherwise, return DP_FAILURE
+ */
+int dp_counter_mode_set(struct dp_counter_conf *cfg, int flag);
+
+/*! @brief Function dp_counter_mode_get
+ * @param [in,out] cfg struct dp_counter_conf *cfg
+ * @param [in] flag int flag
+ * @return [out] integer value: return DP_SUCCESS if succeed
+ *                              otherwise, return DP_FAILURE
+ */
+int dp_counter_mode_get(struct dp_counter_conf *cfg, int flag);
+
+/*! @brief dp_q_map_mode */
+enum dp_q_map_mode {
+	DP_Q_MAP_MODE0 = 0, /*!< flowid[7-6] dec/enc mpe2/1 dp_port class[3-0]*/
+	DP_Q_MAP_MODE1, /*!< subif[7-4] mpe2/1 dp_port subif[3-0] */
+	DP_Q_MAP_MODE2, /*!< subif_hi[11-8] mpe2/1 dp_port class[3-0] */
+	DP_Q_MAP_MODE3 /*!< subif_hi[4-0] mpe2/1 dp_port class[2-0] */
+};
+
+/*! @brief dp_q_map_mode0 */
+struct dp_q_map_mode0 {
+	u32	mpe1; /*!< MPE1 Flag: 1 bit*/
+	u32	mpe2; /*!< MPE2 Flag:1 bit */
+	u32	dp_port; /*!< logical port id: 4 bits*/
+	u32	flowid; /*!< FlowId (Bits 7:6): 2 bit */
+	u32	dec; /*!< VPN Decrypt flag: 1 bit*/
+	u32	enc; /*!< VPN Encrypt flag: 1 bit*/
+	u32	class; /*!< Traffic Class: 4 bits*/
+};
+
+/*! @brief dp_q_map_mode0_mask dont' care bit per field */
+struct dp_q_map_mode0_mask {
+	u32	flowid:1; /*!< FlowId don't care */
+	u32	dec:1; /*!< DEC Decrypt flag don't care */
+	u32	enc:1; /*!< ENC Encrypt flag don't care */
+	u32	mpe1:1; /*!< MPE1 Flag don't care */
+	u32	mpe2:1; /*!< MPE2 Flag don't care */
+	u32	dp_port:4; /*!< logical port don't care */
+	u32	class:4; /*!< Traffic Class don't care */
+};
+
+/*! @brief dp_q_map_mode1*/
+struct dp_q_map_mode1 {
+	u32	mpe1; /*!< MPE1 Flag */
+	u32	mpe2; /*!< MPE2 Flag */
+	u32	dp_port; /*!< logical port */
+	u32	subif; /*!< subif_hi[7-0]*/
+};
+
+/*! @brief dp_q_map_mode1_mask dont' care bit per field */
+struct dp_q_map_mode1_mask {
+	u32	subif:1; /*!< subif_hi[7-4] don't care */
+	u32	mpe1:1; /*!< MPE1 Flag don't care */
+	u32	mpe2:1; /*!< MPE2 Flag don't care */
+	u32	dp_port:4; /*!< logical port don't care */
+};
+
+/*! @brief dp_q_map_mode2*/
+struct dp_q_map_mode2 {
+	u32	mpe1; /*!< MPE1 Flag: 1 bit */
+	u32	mpe2; /*!< MPE2 Flag: 1 bit */
+	u32	dp_port; /*!< logical port: 4 bits*/
+	u32	subif; /*!< subif_hi[7-4]*/
+	u32	class; /*!< class[3-0]*/
+};
+
+/*! @brief dp_q_map_mode2_mask dont' care bit per field */
+struct dp_q_map_mode2_mask {
+	u32	subif:1; /*!< subif_hi[7-4] don't care */
+	u32	mpe1:1; /*!< MPE1 Flag don't care */
+	u32	mpe2:1; /*!< MPE2 Flag don't care */
+	u32	dp_port:4; /*!< logical port don't care */
+	u32	class:1; /*!< subif_hi[3-0] don't care */
+};
+
+/*! @brief dp_q_map_mode3*/
+struct dp_q_map_mode3 {
+	u32	mpe1; /*!< MPE1 Flag: 1 bits */
+	u32	mpe2; /*!< MPE2 Flag: 1 bits */
+	u32	dp_port; /*!< logical port: 4 bits */
+	u32	subif; /*!< subif_hi[4-0]: 5 bits*/
+	u32	class; /*!< class[2-0]: 3 bits */
+};
+
+/*! @brief dp_q_map_mode3_mask dont' care bit per field */
+struct dp_q_map_mode3_mask {
+	u32	subif:1; /*!< subif_hi[7-4] don't care */
+	u32	mpe1:1; /*!< MPE1 Flag don't care */
+	u32	mpe2:1; /*!< MPE2 Flag don't care */
+	u32	dp_port:4; /*!< logical port don't care */
+	u32	class:1; /*!< subif_hi[3-0] don't care */
+};
+
+/*! @brief dp_q_map*/
+union dp_q_map {
+	struct dp_q_map_mode0 map0; /*!< dp_q_map_f_mode0 setting */
+	struct dp_q_map_mode1 map1; /*!< dp_q_map_f_mode1 setting */
+	struct dp_q_map_mode2 map2; /*!< dp_q_map_f_mode2 setting */
+	struct dp_q_map_mode3 map3; /*!< dp_q_map_f_mode3 setting */
+};
+
+/*! @brief dp_q_map_mask*/
+union dp_q_map_mask {
+	struct dp_q_map_mode0_mask mask0; /*!< don't care mask for this mode*/
+	struct dp_q_map_mode1_mask mask1; /*!< don't care mask for this mode*/
+	struct dp_q_map_mode2_mask mask2; /*!< don't care mask for this mode*/
+	struct dp_q_map_mode3_mask mask3; /*!< don't care mask for this mode*/
+};
+
+/*! @brief queue_map_set*/
+struct dp_queue_map_set {
+	int inst; /*!< input: dp instance. For SOC side, it is always zero */
+	int q_id; /*!< queue id */
+	enum dp_q_map_mode qm_mode; /*!< map mode */
+	union dp_q_map map;  /*!< lookup map value */
+	union dp_q_map_mask mask; /*!< lookup map don't care flag setting:
+				   *  1 - means don't care this bit setting
+				   */
+};
+
+/*! @brief queue_map_entry*/
+struct dp_queue_map_entry {
+	enum dp_q_map_mode qm_mode; /*!< map mode */
+	union dp_q_map qmap;    /*!< map setting */
+};
+
+/*! @brief queue_map_get*/
+struct dp_queue_map_get {
+	int inst; /*!< input: dp instance. For SOC side, it is always zero */
+	int q_id; /*!<  queue id */
+	int num_entry; /*!< output: the number of entries mapped to specified
+			*           qid
+			*/
+	int qmap_size; /*!< number of qmap_entry buffer*/
+	struct dp_queue_map_entry *qmap_entry; /*!< caller need to provide
+						*   buffer.otherwise, only
+						*   return num_entries.
+						*/
+};
+
+/*! \ingroup APIs_dp_queue_map_set
+ * @brief function dp_queue_map_set
+ * @param [in,out] cfg struct dp_queue_map_set *cfg
+ * @param [in] flag unsigned int flag
+ * @return [out] integer value: return DP_SUCCESS if succeed,
+ *                              otherwise, return DP_FAILURE
+ */
+int dp_queue_map_set(struct dp_queue_map_set *cfg, int flag);
+
+/*! \ingroup APIs_dp_queue_map_set
+ * @brief function dp_queue_map_get
+ * @param [in,out] cfg struct dp_queue_map_get *cfg
+ * @param [in] flag unsigned int flag
+ * @return [out] integer value: return DP_SUCCESS if succeed,
+ *                              otherwise, return DP_FAILURE
+ */
+int dp_queue_map_get(struct dp_queue_map_get *cfg, int flag);
+
+#define DP_MAX_CHILD_PER_NODE 8 /*!< Maximum child number per node */
+
+/*! @brief dp_node*/
+struct dp_node {
+	union dp_node_id id; /*!< node ID */
+	enum dp_node_type type; /*!< node type */
+};
+
+/*! @brief queue_map_get*/
+struct dp_node_child {
+	int inst; /*!< input: dp instance. For SOC side, it is always zero */
+	union dp_node_id id; /*!< input: node ID */
+	enum dp_node_type type; /*!< input: node type: only for sched/port */
+	int num; /*!< output: number of child provided in child[] array */
+	struct dp_node child[DP_MAX_CHILD_PER_NODE]; /*!< output: child arary*/
+};
+
+/*! \ingroup dp_children_get
+ * @brief function dp_children_get
+ * @param [in,out] cfg struct dp_node_child *cfg
+ * @param [in] flag int flag
+ * @return [out] integer value: return DP_SUCCESS if succeed,
+ *                              otherwise, return DP_FAILURE
+ */
+int dp_children_get(struct dp_node_child *cfg, int flag);
+
+/*! @brief dp_qos_level */
+struct dp_qos_level {
+	int inst; /* input: dp instance. For SOC side, it is always zero */
+	int max_sch_lvl; /* output: max scheduler level */
+};
+
+/*! \ingroup dp_qos_level_get
+ * @brief function dp_qos_level_get
+ * @param [in,out] struct dp_qos_level *dp
+ * @param [in] flag int flag
+ * @return [out] integer value: return DP_SUCCESS if succeed,
+ *                              otherwise, return DP_FAILURE
+ */
+int dp_qos_level_get(struct dp_qos_level *dp, int flag);
+
+#ifdef ENABLE_QOS_EXAMPLE
+/*! \ingroup APIs_dp_qos_example
+ * @brief example1: use basic node_link API to create complex QOS structure
+ * @note
+ * <PRE> <PRE>
+ * Queue       Sched ID: leaf/weight        Sched ID: leaf/weight    Egress Port
+ *             |-----------------|    |----------------------|
+ * Queue 17----|1  :0/0          |    |2 :0/0                |  Shaper CIR=100
+ *             |                 |    |                      |     |
+ *             |                 |    |                      |     |
+ *             |                 |----|                      |-------- 7
+ *             |                 |    |                      |
+ *             |                 |    |                      |
+ * Queue 18----|---:1/0          |    |                      |
+ *             |-----------------|    |----------------------|
+ *</PRE></PRE>
+ */
+static inline int example1(void)
+{
+	struct dp_node_link info;
+	int flag = 0;
+	int dp_inst = 0;
+	struct dp_shaper_conf shaper;
+	struct dp_node_link_enable enable;
+
+	memset(&info, 0, sizeof(info));
+
+	/*connect queue 17 to first scheduler 1*/
+	info.inst = dp_inst;
+	info.arbi = ARBITRATION_WSP_WRR;
+	info.prio_wfq = 0;
+	info.node_type = DP_NODE_QUEUE;
+	info.node_id.q_id = 17;
+	info.p_node_type = DP_NODE_SCH;
+	info.p_node_id.sch_id = 1;
+	if (dp_node_link_add(&info, 0) == DP_FAILURE)
+		return -1;
+
+	/*connect scheduler 1 to first scheduler 2*/
+	info.arbi = ARBITRATION_WSP_WRR;
+	info.prio_wfq = 0;
+	info.node_type = DP_NODE_SCH;
+	info.node_id.sch_id = 1;
+	info.p_node_type = DP_NODE_SCH;
+	info.p_node_id.sch_id = 2;
+	if (dp_node_link_add(&info, 0) == DP_FAILURE) {
+		/* Note: need to unlink some already added node.
+		 * For demo purpose, here skip it
+		 */
+		return -1;
+	}
+
+	/*connect scheduler 2 to egress port 7*/
+	info.arbi = ARBITRATION_WSP_WRR;
+	info.prio_wfq = 0;
+	info.node_type = DP_NODE_SCH;
+	info.node_id.sch_id = 2;
+	info.p_node_type = DP_NODE_PORT;
+	info.p_node_id.cqm_deq_port = 7;
+	if (dp_node_link_add(&info, 0) == DP_FAILURE) {
+		/* Note: need to unlink some already added node.
+		 * For demo purpose, here skip it
+		 */
+		return -1;
+	}
+
+	/*connect queue 18 to first scheduler 1*/
+	info.inst = dp_inst;
+	info.arbi = ARBITRATION_WSP_WRR;
+	info.prio_wfq = 0;
+	info.node_type = DP_NODE_QUEUE;
+	info.node_id.q_id = 18;
+	info.p_node_type = DP_NODE_SCH;
+	info.p_node_id.sch_id = 1;
+	if (dp_node_link_add(&info, 0) == DP_FAILURE) {
+		/* Note: need to unlink some already added node.
+		 * For demo purpose, here skip it
+		 */
+		return -1;
+	}
+
+	/* set port shaper*/
+	memset(&shaper, 0, sizeof(shaper));
+	shaper.type = DP_NODE_SCH;
+	shaper.id.sch_id = 2;
+	shaper.cir = 100;
+	shaper.pir = 100;
+	dp_shaper_conf_set(&shaper, flag);
+
+	/*Enable queue/scheduler */
+	enable.type = DP_NODE_QUEUE;
+	enable.id.q_id = 17;
+	enable.en = DP_NODE_EN;
+	dp_node_link_en_set(&enable, DP_NODE_AUTO_SMART_ENABLE);
+	enable.id.q_id = 18;
+	enable.en = DP_NODE_EN;
+	dp_node_link_en_set(&enable, DP_NODE_AUTO_SMART_ENABLE);
+
+	return 0;
+}
+
+/*! \ingroup APIs_dp_qos_example
+ * @brief example2: use smart link API to create complex QOS structure
+ * @note
+ * <PRE> <PRE>
+ * Queue       Sched ID: leaf/weight        Sched ID: leaf/weight    Egress Port
+ *             |-----------------|    |----------------------|
+ * Queue 17----|1  :0/0          |    |2 :0/0                |  Shaper CIR=100
+ *             |                 |    |                      |     |
+ *             |                 |    |                      |     |
+ *             |                 |----|                      |-------- 7
+ *             |                 |    |                      |
+ *             |                 |    |                      |
+ * Queue 18----|---:1/0          |    |                      |
+ *             |-----------------|    |----------------------|
+ *</PRE></PRE>
+ */
+static inline int example2(void)
+{
+	/* Method 2 with basic link node API dp_link_add from scratch*/
+	int flag = DP_NODE_AUTO_SMART_ENABLE;
+	int dp_inst = 0;
+	struct dp_shaper_conf shaper;
+	struct dp_qos_link full_link;
+	int dp_port = 3;
+
+	memset(&full_link, 0, sizeof(full_link));
+	full_link.inst = dp_inst;
+	full_link.cqm_deq_port = 7;
+	full_link.q_id = 1;
+	full_link.q_arbi = ARBITRATION_WSP_WRR;
+	full_link.q_prio_wfq = 0;
+	full_link.n_sch_lvl = 2;
+	full_link.sch[0].id = 1;
+	full_link.sch[0].arbi = ARBITRATION_WSP_WRR;
+	full_link.sch[0].prio_wfq = 0;
+	full_link.sch[1].id = 2;
+	full_link.sch[1].arbi = ARBITRATION_WSP_WRR;
+	full_link.sch[1].prio_wfq = 0;
+	full_link.dp_port = dp_port;
+	if (dp_link_add(&full_link, flag) == DP_FAILURE) {
+		pr_err("dp_link_add fail\n");
+		return -1;
+	}
+	full_link.q_id = 2;
+	if (dp_link_add(&full_link, flag) == DP_FAILURE) {
+		pr_err("dp_link_add fail\n");
+		return -1;
+	}
+
+	/*set port shaper*/
+	memset(&shaper, 0, sizeof(shaper));
+	shaper.type = DP_NODE_SCH;
+	shaper.id.sch_id = 2;
+	shaper.cir = 100;
+	shaper.pir = 100;
+	dp_shaper_conf_set(&shaper, flag);
+
+	return 0;
+}
+
+/*! \ingroup APIs_dp_qos_example
+ * @brief gpon_example3:OMCI GEM port and one PMAPPER with basic QOS
+ * <PRE><PRE>
+ * Queue       Sched ID: leaf/weight         Egress Port
+ *             |----------------------|
+ * Queue xx----|x  :0/0               |
+ *             |                      |
+ *             |                      |
+ *             |                      |---- ----- 26
+ *             |                      |
+ *             |                      |
+ *             |----------------------|
+ *
+ * Queue yy----|y  :0/0               |
+ *             |                      |
+ *             |                      |
+ *             |                      |---- ----- 27
+ *             |                      |
+ *             |                      |
+ *             |----------------------|
+ * </PRE></PRE>
+ */
+static inline int gpon_example3(void)
+{
+	int inst = 0;
+	struct module *owner = NULL;
+	u32 dev_port = 0;
+	s32 dp_port = 0;
+	struct net_device *omci_dev = NULL;
+	struct net_device *pmapper_dev = NULL;
+	struct dp_port_data port_data = {0};
+	u32 flags = 0;
+	dp_cb_t cb;
+	dp_subif_t subif_id;
+	struct dp_subif_data subif_data;
+	s32 res;
+
+	/*Allocate dp_port and reserve QOS resource if needed.
+	 *Note: must set moudle owner properly.
+	 */
+	flags = DP_F_GPON;
+	port_data.flag_ops = DP_F_DATA_RESV_Q | DP_F_DATA_RESV_SCH;
+	port_data.num_resv_q = 64;
+	port_data.num_resv_sched = 64;
+	dp_port = dp_alloc_port_ext(inst, owner, omci_dev, dev_port,
+				    dp_port, NULL, &port_data, flags);
+	if (dp_port == DP_FAILURE) {
+		pr_err("Fail to allocate port\n");
+		return -1;
+	}
+
+	/*it needs to register device, ie, cb properly.
+	 * At least, it needs set below callbacks:
+	 * 1) rx_fn
+	 * 2) get_subifid_fn: mainly for cpu tx path to fill in proper CTP, esp
+	 *    for pmapper related cases
+	 */
+	flags = 0;
+	res = dp_register_dev_ext(inst, owner, dp_port,
+				  &cb, NULL, flags);
+	if (res == DP_FAILURE) {
+		pr_err("Fail to register dev\n");
+		dp_alloc_port_ext(inst, owner, omci_dev, dev_port,
+				  dp_port, NULL, &port_data, DP_F_DEREGISTER);
+		return -1;
+	}
+
+	/* Register OMCI Dev
+	 * First it needs to create omci dev and store in omci_dev
+	 * Once it is ready, call below dp_register_subif_ext API.
+	 * After dp_register_subif_ext, below QOS structure will be created
+	 * Queue       Sched ID: leaf/weight         Egress Port
+	 *             |----------------------|
+	 * Queue xx----|1  :0/0               |
+	 *             |                      |
+	 *             |                      |
+	 *             |                      |---- ----- 26
+	 *             |                      |
+	 *             |                      |
+	 *             |                      |
+	 *             |----------------------|
+	 */
+	flags = 0;
+	subif_id.inst = inst;
+	subif_id.port_id = dp_port;
+	subif_id.subif_num = 1;
+	subif_id.subif = 0; /*OCMI CTP */
+	subif_data.deq_port_idx = 0;
+	res = dp_register_subif_ext(inst, owner, omci_dev, omci_dev->name,
+				    &subif_id, &subif_data, flags);
+	if (res == DP_FAILURE) {
+		pr_err("Fail to regisster subif port\n");
+		dp_register_dev_ext(inst, owner, dp_port,
+				    NULL, NULL, DP_F_DEREGISTER);
+		dp_alloc_port_ext(inst, owner, omci_dev, dev_port,
+				  dp_port, NULL,
+				  &port_data, DP_F_DEREGISTER);
+		return -1;
+	}
+
+	/* Register pmapper device
+	 * First it needs to create new Dev and store in pmapper_dev
+	 * Once it is ready, call below dp_register_subif_ext API.
+	 * After dp_register_subif_ext, below QOS structure will be created:
+	 * Queue       Sched ID: leaf/weight         Egress Port
+	 *             |----------------------|
+	 * Queue xx----|x  :0/0               |
+	 *             |                      |
+	 *             |                      |
+	 *             |                      |---- ----- 26
+	 *             |                      |
+	 *             |                      |
+	 *             |----------------------|
+
+	 * Queue yy----|y  :0/0               |
+	 *             |                      |
+	 *             |                      |
+	 *             |                      |---- ----- 27
+	 *             |                      |
+	 *             |                      |
+	 *             |----------------------|
+	 *
+	 */
+	subif_id.subif = 1; /*2nd CTP for one pmapper*/
+	subif_data.flag_ops = DP_SUBIF_Q_PER_CTP | DP_SUBIF_PCP;
+	subif_data.deq_port_idx = 1;
+	subif_data.pcp = 0;
+	res = dp_register_subif_ext(inst, owner, pmapper_dev, pmapper_dev->name,
+				    &subif_id, &subif_data, flags);
+	if (res == DP_FAILURE) {
+		pr_err("Fail to regisster subif port\n");
+		return -1;
+	}
+
+	/* Register 2nd CTP based on same Dev and same pmapper
+	 * After dp_register_subif_ext, below QOS structure will be created:
+	 * Queue       Sched ID: leaf/weight         Egress Port
+	 *             |----------------------|
+	 * Queue xx----|x  :0/0               |
+	 *             |                      |
+	 *             |                      |
+	 *             |                      |---- ----- 26
+	 *             |                      |
+	 *             |                      |
+	 *             |----------------------|
+
+	 * Queue yy----|y  :0/0               |
+	 *             |                      |
+	 *             |                      |
+	 *             |                      |---- ----- 27
+	 * Queue zz----|    1/0               |
+	 *             |                      |
+	 *             |----------------------|
+	 */
+
+	subif_id.subif = 2; /*1st Data CTP */
+	/* DP_SUBIF_Q_PER_CTP bit used to create a new queue.
+	 * Otherwise, it will share same queue yy
+	 */
+	subif_data.flag_ops = DP_SUBIF_Q_PER_CTP | DP_SUBIF_PCP;
+	subif_data.deq_port_idx = 1;
+	subif_data.pcp = 1;
+	res = dp_register_subif_ext(inst, owner, omci_dev, omci_dev->name,
+				    &subif_id, &subif_data, flags);
+	if (res == DP_FAILURE) {
+		pr_err("Fail to regisster subif port\n");
+		return -1;
+	}
+
+	return 0;
+}
+
+/*! @note below API not need to provide:
+ * 1) dp_get_dc_umt_pid:only valid for GRX500, will be merged into existing API
+ *    dp_get_netif_subifid
+ * 2) dp_queue_enable(flush): no need to export since will be integraded inside
+ *    DP API, like dp_node_del
+ * 3) dp_reserved_resource_get: no hardcoded reservation will be take. No need
+ */
+#endif
+#endif
+
diff --git a/include/net/datapath_api_skb.h b/include/net/datapath_api_skb.h
new file mode 100644
index 000000000000..9b49c8b8a53e
--- /dev/null
+++ b/include/net/datapath_api_skb.h
@@ -0,0 +1,179 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#ifndef DATAPATH_API_SKB_H
+#define DATAPATH_API_SKB_H
+
+#include <linux/types.h>
+
+#ifdef CONFIG_LTQ_DATAPATH_MPE_FASTHOOK_TEST
+
+#define MAX_FASTHOOK_PHYDEV 10
+enum LTQ_MPE_FASTHOOK_SKB_FLAG {
+	ACC_HOOK_RX,/*hook point: device rx hook */
+	ACC_HOOK_NETFILTER,/*hook point: netfilter hook */
+	ACC_HOOK_TX	/*hook point: device tx hook */
+};
+
+enum LTQ_MPE_FASTHOOK_SKB_ACT {
+	ACC_HOOK_CONTINUE,/*continue next hook */
+	ACC_HOOK_DONE /* current packet is soft ware accelerated alreadys */
+};
+
+enum LTQ_MPE_FASTHOOK_RETURN_VALUE {
+	PKT_CONTINUE,/*continue next hook */
+	PKT_CONSUMED,
+	PKT_DONE/* current packet is soft ware accelerated alreadys */
+};
+
+/* Note,since there is multiple accelertion engineer with different capability,
+ * suggest to use MACRO in the hook for different learning,
+ * otherwise, it needs to check MPE HAL layer and affects performace
+ */
+enum LTQ_MPE_FASTHOOK_SKB_ACC_RES_FLAG {
+	ACC_UNKNOWN = 0,/* Don't know whether can be accelerated or not */
+	ACC_CANNOT = 1,/* Cannot be acclerated by any accelerator engineer */
+	ACC_DONOT_ = 2,/* Don't do acceleration even it can be accelerated*/
+	ACC_SOFTWARE_OK = 4,/* Can be accelerated by software acceleration */
+	ACC_SWITCH_OK = 8,/* Can be accelerated by HW Accelerator */
+	ACC_MPE_OK = 16,/* Can be accelerated by MPE accelerator*/
+	ACC_PAE_OK = 32,/* Can be accelerated by PPE FW, */
+};
+
+typedef union {
+	u32 ip; /*!< the storage buffer for ipv4 */
+#ifdef CONFIG_IPV6
+	u32 ip6[4];/*!< the storage buffer for ipv6 */
+#endif
+} LTQ_MPE_FASTHOOK_IPADDR;
+
+struct ltq_mpe_fasthook_session_info {
+	/* buffer for parsing skb during rx_hook and tx_hook */
+	u32 vlan_num: 2; /* for vlan header number: for both rx/tx_hook */
+	u32 ppp_flag: 1; /* pppoe header: for both rx/tx_hook */
+
+	u32 dslite_flag: 1; /*dslite header flag: for both rx/tx_hook */
+	u32 rd_flag: 1;
+	u32 gre_flag: 1;
+	u32 route_flag: 1; /* 1 means it is routing packet */
+	u32 tcp_flag: 1; /* 1 means tcp packet */
+	u32 tcp_establish_flag: 1; /* 1 means tcp fully estabilished */
+	u32 multicast_flag: 1;
+	u32 l2tp_flag: 1;
+	u32 l2tp_controlmsg_flag: 1;
+	u8 ip_proto: 8;
+
+	LTQ_MPE_FASTHOOK_IPADDR src_ip, dst_ip;
+	u16 src_port, dst_port;
+	u16 tos;
+
+	/* buffer for parsing skb during tx_hook */
+	u32 inner_vid;/* inner_vid: for tx_hook only */
+	u32 outer_vid;/* outer_vid: for tx_hook only */
+	u32 pppheader_offset;
+	u32 dsliteheader_offset;
+	u32 rdheader_offset;
+	u32 inneripheader_offset;
+	u32 greheader_offset;
+	u32 grekey_en;
+	u32 previous_ipheadertype; /*Incase of gre and l2tp*/
+	u32 eogre_inner_macheader_offset;/*Incase of gre*/
+	u32 l2tpheader_offset; /*Incase of l2tp*/
+	u32 l2tpheader_udp_offset; /*Incase of l2tp*/
+	u32 l2tpversion; /*Incase of l2tp*/
+	u16 pppoe_session_id;/* pppoe_session_id */
+	u32 ttl;
+	u8 src_mac[6], dst_mac[6];
+	u8 ip_version;
+	struct net_device *phydev[MAX_FASTHOOK_PHYDEV];
+
+	/* Mainly for multicast or bridging learning */
+	atomic_t tx_count;/* the times tx hook triggered based on same skb */
+	atomic_t referece_count;/*do learn-decsion and free memory if zero */
+};
+
+#define FASTHOOK_KEY_SIZE  64
+struct dp_mpe_fasthook_info {
+	struct ltq_mpe_fasthook_session_info *rx_info;
+	struct ltq_mpe_fasthook_session_info *tx_info;
+	struct ltq_mpe_fasthook_session_info *tmp_tx_info;
+	char key[FASTHOOK_KEY_SIZE];
+
+	/*learning decision */
+	u32 acc_hook_point: 2; /* Refer to LTQ_MPE_FASTHOOK_SKB_HOOK_FLAG*/
+	u32 acc_hook_action: 2; /* Need skip next hooks or not */
+	u8 acc_learn_result;/* Multiple bit may be set  */
+};
+#endif
+
+#ifdef CONFIG_LTQ_DATAPATH_ETH_OAM
+enum DP_OAM_FLAG {
+	DP_OAM_OAM = 1 << 0,
+	DP_OAM_INS = 1 << 1,
+	DP_OAM_EXT = 1 << 2,
+	DP_OAM_ONE_STEP = 1 << 3,
+	DP_OAM_PTP = 1 << 4,
+	DP_OAM_RECORDID = 1 << 5
+};
+
+struct dp_oam {
+	u32 flag; /*flat to indicate which field is valid or not*/
+
+	u32 oam:1; /*from/to pmac*/
+	u32 ins:1; /*from/to pmac*/
+	u32 ext:1; /*from/to pmac*/
+	u32 one_step:1; /*from/to pmac*/
+	u32 ptp:1; /*from/to pmac*/
+	u16 record_id:12; /*from/to pmac*/
+};
+#endif
+
+struct ltq_dp_skb {
+#ifdef CONFIG_LTQ_DATAPATH_MPE_FASTHOOK_TEST
+	struct dp_mpe_fasthook_info *mpe;
+#endif
+#ifdef CONFIG_LTQ_DATAPATH_ETH_OAM
+	struct dp_oam oam_rx;
+	struct dp_oam oam_tx;
+#endif
+};
+
+#ifdef CONFIG_LTQ_DATAPATH_MPE_FASTHOOK_TEST
+extern int (*ltq_mpe_fasthook_free_fn)(struct sk_buff *);
+extern int (*ltq_mpe_fasthook_tx_fn)(struct sk_buff *, u32, void *);
+extern int (*ltq_mpe_fasthook_rx_fn)(struct sk_buff *, u32, void *);
+#endif
+
+static inline void dp_skb_cp(const struct ltq_dp_skb *old,
+			     struct ltq_dp_skb *new)
+{
+#ifdef CONFIG_LTQ_DATAPATH_ETH_OAM
+	/*Need further check whether need copy oam info */
+#endif
+
+#ifdef CONFIG_LTQ_DATAPATH_MPE_FASTHOOK_TEST
+	if (!old->mpe)
+		return;
+	new->mpe = old->mpe;
+	if (old->mpe->rx_info)
+		atomic_add(1,
+			   &old->mpe->rx_info->referece_count);
+#endif
+}
+
+static inline void dp_skb_free(struct sk_buff *skb)
+{
+#ifdef CONFIG_LTQ_DATAPATH_MPE_FASTHOOK_TEST
+	if (ltq_mpe_fasthook_free_fn)
+		ltq_mpe_fasthook_free_fn(skb);
+#endif
+}
+
+#endif	/*DATAPATH_API_SKB_H */
+
diff --git a/include/net/datapath_api_vlan.h b/include/net/datapath_api_vlan.h
new file mode 100644
index 000000000000..4783d5681620
--- /dev/null
+++ b/include/net/datapath_api_vlan.h
@@ -0,0 +1,172 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+#ifndef DATAPATH_VLAN_H
+#define DATAPATH_VLAN_H
+#include <linux/if_ether.h>
+
+#define DP_VLAN_PATTERN_NOT_CARE -1  /*Don't care the specified pattern field,
+				      *ie, match whatever value supported
+				      */
+#define DP_VLAN_NUM 2  /*Maximum number of vlan tag supported */
+#define DP_MAX_EXT_VLAN_RULES 64 /* Maximum number of VLAN rules
+				  * per dev per direction.
+				  * It is mainly for performance purpose
+				  */
+struct dp_pattern_vlan {
+	int prio; /* match exact VLAN tag priority: 0-7
+		   * DP_VLAN_PATTERN_PRIO_NOT_CARE: don't care
+		   */
+	int vid; /* match exact VID: 0 - 4095
+		  * DP_VLAN_PATTERN_NOT_CARE: don't care
+		  */
+	int tpid; /* match exact TPID: 0x8100 or another configured TPID
+		   * via FDMA_VTETYPE
+		   * Can only match two TPID
+		   * DP_VLAN_PATTERN_NOT_CARE: don't care
+		   */
+	int dei; /* match exact DEI: 0 or 1
+		  * DP_VLAN_PATTERN_NOT_CARE: don't care
+		  */
+#define DP_PROTO_IP4    ETH_P_IP /*IP packet 0x0800*/
+#define DP_PROTO_PPPOE  ETH_P_PPP_DISC /*PPPoE packet: 0x8863 & 0x8864) */
+#define DP_PROTO_ARP    ETH_P_ARP /*ARP 0x0806*/
+#define DP_PROTO_IP6    ETH_P_IPV6 /*IPv6 packet 0x86DD*/
+#define DP_PROTO_EAPOL  ETH_P_PAE /*EAPOL packet 0x888E */
+
+	int proto;/* match exact ether type (protocol) as defined
+		   * or
+		   * DP_VLAN_PATTERN_NOT_CARE: match all proto/ether type
+		   */
+};
+
+struct dp_act_vlan {
+#define DP_VLAN_ACT_FWD    BIT(0)  /*forward packet without editing */
+#define DP_VLAN_ACT_DROP   BIT(1)  /*drop packet */
+#define DP_VLAN_ACT_POP    BIT(2)  /*pop/remove VLAN */
+#define DP_VLAN_ACT_PUSH   BIT(3)  /*push/insert VLAN */
+	int act;  /* if act == DP_VLAN_ACT_FWD, forward packet without editing
+		   * if act == DP_VLAN_ACT_DROP, drop the packet
+		   * if act == DP_VLAN_ACT_POP, remove VLAN
+		   * if act == DP_VLAN_ACT_PUSH, insert VLAN
+		   */
+	int pop_n;  /*the number of VLAN tag to pop
+		     *the valid number: 1 or 2
+		     */
+	int push_n;  /*the number of VLAN tag to push
+		      *the valid number: 1 or 2
+		      */
+#define CP_FROM_INNER		-1 /*copy from inner VLAN header*/
+#define CP_FROM_OUTER		-2 /*copy from inner VLAN header*/
+#define DERIVE_FROM_DSCP	-3 /* prio is derived from DSCP */
+	int tpid[DP_VLAN_NUM]; /* the tpid of VLAN to push:
+				*  support two TPID 0x8100 and
+					  another programmable TPID
+				* or
+				*  copy from recv pkt's inner tag(CP_FROM_INNER)
+				*  copy from recv pkt's outer tag(CP_FROM_OUTER)
+				*/
+	int vid[DP_VLAN_NUM];  /* the VID of VLAN to push:
+				*  support range: 0 - 4095
+				* or
+				*  copy from recv pkt's inner tag(CP_FROM_INNER)
+				*  copy from recv pkt's outer tag(CP_FROM_OUTER)
+				*/
+	int dei[DP_VLAN_NUM];  /* the DEI of VLAN to push:
+				*  support range: 0 - 1
+				* or
+				*  copy from recv pkt's inner tag(CP_FROM_INNER)
+				*  copy from recv pkt's outer tag(CP_FROM_OUTER)
+				*  keep existing value
+				*/
+	int prio[DP_VLAN_NUM]; /* the prority of VLAN to push:
+				*  support range: 0 - 7
+				* or
+				*  copy from recv pkt's inner tag(CP_FROM_INNER)
+				*  copy from recv pkt's outer tag(CP_FROM_OUTER)
+				*/
+};
+
+struct dp_vlan0 {
+#define DP_VLAN_DEF_DROP 1
+#define DP_VLAN_DEF_ACCEPT 0
+	int def;		/* default rule for untagged packet */
+	struct dp_pattern_vlan outer;	/* match pattern.
+					 * only proto is valid for this case
+					 */
+	struct dp_act_vlan act; /*action once matched */
+};
+
+struct dp_vlan1 {
+	int def;		/* default rule for 1-tag packet */
+	struct dp_pattern_vlan outer;	/*outer VLAN match pattern */
+	struct dp_act_vlan act; /*action once matched */
+};
+
+struct dp_vlan2 {
+	int def;		/* default rule for 2-tag packet */
+	struct dp_pattern_vlan outer;	/*outer VLAN match pattern */
+	struct dp_pattern_vlan inner;	/*inner VLAN match pattern */
+	struct dp_act_vlan act; /*action once matched */
+};
+
+struct dp_tc_vlan {
+	struct net_device *dev;  /*bridge port device or its CTP device */
+
+#define DP_VLAN_APPLY_CTP 1  /*apply to BP for CTP device */
+	int def_apply; /* By default, ie, def_apply == 0,
+			*  Apply rule to bridge port if it is a bridge port dev
+			*  Apply rule to ctp if it is CTP device
+			* But for UNI dev, normally only bridge port dev and not
+			* its CTP dev, in this case, caller need to specify flag
+			* DP_VLAN_APPLY_CTP to apply VLAN to its CTP port.
+			*/
+
+#define DP_DIR_INGRESS 0
+#define DP_DIR_EGRESS  1
+	int dir; /* DP_DIR_INGRESS(0) and DP_DIR_EGRESS(1) */
+
+	int n_vlan0, n_vlan1, n_vlan2; /*size of vlan0/vlan1/2_list*/
+	struct dp_vlan0 *vlan0_list; /* non-vlan matching rules,
+				      * ie, ether type matching only
+				      */
+	struct dp_vlan1 *vlan1_list; /* single vlan matching rules */
+	struct dp_vlan2 *vlan2_list; /* double vlan matching rules */
+
+};
+
+/* API dp_vlan_set: Asymmetric VLAN handling via TC command
+ * This API is used to set VLAN on CTP/Bridge port in specified direction:
+ * ingress or egress
+ * It will automatically apply VLAN on vlan->dev in the specified direction
+ * Note: every time to call this API, caller need to collect all VLAN
+ *       information for this CTP or brige port based on specified direction
+ * return:
+ *    DP_FAILURE: no resource or not find such dev
+ *    DP_SUCCESS: succeed
+ *
+ * Treatment outer TPID/DEI example: (3 bits)
+ * 000 Copy TPID (and DEI, if present) from the inner tag of the received frame.
+ *     tpid=CP_FROM_INNER   dei=CP_FROM_INNER
+ * 001 Copy TPID (and DEI, if present) from the outer tag of the received frame.
+ *     tpid= CP_FROM_OUTER	dei=CP_FROM_OUTER
+ * 010 Set TPID = output TPID attribute value, copy DEI bit from the inner tag
+ *     of the received frame
+ *      tpid=specified_tpid   dei=CP_FROM_INNER
+ * 011 Set TPID = output TPID, copy DEI from the outer tag of the received frame
+ *     tpid=specified_tpid   dei=CP_FROM_OUTER
+ * 100 Set TPID = 0x8100
+ *     tpid=0x8100  dei=0
+ * 101 Reserved
+ * 110 Set TPID = output TPID, DEI = 0
+ *     tpid=specified_tpid   dei=0
+ * 111 Set TPID = output TPID, DEI = 1
+ *     tpid=specified_tpid   dei=1
+ */
+int dp_vlan_set(struct dp_tc_vlan *vlan, int flag);
+#endif /*DATAPATH_VLAN_H*/
diff --git a/include/net/datapath_inst.h b/include/net/datapath_inst.h
new file mode 100644
index 000000000000..4f0082ab706b
--- /dev/null
+++ b/include/net/datapath_inst.h
@@ -0,0 +1,155 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#ifndef DATAPATH_INST_H
+#define DATAPATH_INST_H
+
+#include <net/switch_api/lantiq_gsw_api.h> /*Switch related structures */
+#include <net/switch_api/lantiq_gsw_flow.h>
+#include <net/switch_api/lantiq_gsw.h>
+#include <net/switch_api/gsw_dev.h>
+
+struct logic_dev;
+struct logic_dev;
+struct subif_platform_data;
+struct pmac_port_info;
+struct pmac_port_info2;
+struct gsw_itf;
+
+#define DP_MAX_GSW_HANDLE 2 /*! max GSW instance per SOC */
+
+/*! enum for DP HW capability type */
+enum DP_HW_CAP_TYPE {
+	GSWIP30_TYPE = 0,
+	GSWIP31_TYPE
+};
+
+/*! enum for DP HW version */
+enum DP_HW_CAP_VER {
+	GSWIP30_VER = 0,
+	GSWIP31_VER
+};
+
+struct dp_inst_info {
+	int inst; /*! for register, it will be filled by DP,
+		   *  for de-register, the caller fill the instance id
+		   */
+	enum DP_HW_CAP_TYPE type;  /*! HW type */
+	enum DP_HW_CAP_VER ver;  /*! HE version */
+	struct core_ops *ops[DP_MAX_GSW_HANDLE]; /*! GSWIP ops handler*/
+	int cbm_inst;  /*! CBM instance for this DP instance*/
+	int qos_inst; /*! QOS instance for this DP instance*/
+};
+
+struct dp_tc_vlan_info;
+struct inst_info {
+	enum DP_HW_CAP_TYPE type;
+	enum DP_HW_CAP_VER ver;
+	int max_ports;
+	int max_port_subifs;
+	struct dp_cap cap;
+	int (*dp_platform_set)(int inst, u32 flag);
+	int (*port_platform_set)(int inst, u8 ep, struct dp_port_data *data,
+				 uint32_t flags);
+	int (*subif_platform_set_unexplicit)(int inst, int port_id,
+					     struct logic_dev *dev,
+					     u32 flag);
+	int (*port_platform_set_unexplicit)(int inst, int port_id,
+					    struct logic_dev *dev,
+					    u32 flag);
+	int (*subif_platform_set)(int inst, int portid, int subif_ix,
+				  struct subif_platform_data *data,
+				  u32 flags);
+	int (*proc_print_ctp_bp_info)(struct seq_file *s, int inst,
+				      struct pmac_port_info *port,
+				      int subif_index, u32 flag);
+	void (*init_dma_pmac_template)(int portid, uint32_t flags);
+	int (*not_valid_rx_ep)(int ep);
+	void (*set_pmac_subif)(struct pmac_tx_hdr *pmac, int32_t subif);
+	void (*update_port_vap)(int inst, u32 *ep, int *vap,
+				struct sk_buff *skb,
+				struct pmac_rx_hdr *pmac, char *decryp);
+	int (*check_csum_cap)(void);
+	void (*get_dma_pmac_templ)(int index, struct pmac_tx_hdr *pmac,
+				   struct dma_tx_desc_0 *desc_0,
+				   struct dma_tx_desc_1 *desc_1,
+				   struct pmac_port_info2 *dp_info);
+	int (*get_itf_start_end)(struct gsw_itf *itf_info, u16 *start,
+				 u16 *end);
+	void (*dump_rx_dma_desc)(struct dma_rx_desc_0 *desc_0,
+				 struct dma_rx_desc_1 *desc_1,
+				 struct dma_rx_desc_2 *desc_2,
+				 struct dma_rx_desc_3 *desc_3);
+	void (*dump_tx_dma_desc)(struct dma_tx_desc_0 *desc_0,
+				 struct dma_tx_desc_1 *desc_1,
+				 struct dma_tx_desc_2 *desc_2,
+				 struct dma_tx_desc_3 *desc_3);
+	void (*dump_rx_pmac)(struct pmac_rx_hdr *pmac);
+	void (*dump_tx_pmac)(struct pmac_tx_hdr *pmac);
+	int (*supported_logic_dev)(int inst, struct net_device *dev,
+				   char *subif_name);
+	int (*dp_pmac_set)(int inst, u32 port, dp_pmac_cfg_t *pmac_cfg);
+	int (*dp_set_gsw_parser)(u8 flag, u8 cpu, u8 mpe1, u8 mpe2, u8 mpe3);
+	int (*dp_get_gsw_parser)(u8 *cpu, u8 *mpe1, u8 *mpe2, u8 *mpe3);
+	int (*dp_qos_platform_set)(int cmd_id, void *cfg, int flag);
+	int (*dp_get_port_vap_mib)(dp_subif_t *subif_id, void *priv,
+				   struct rtnl_link_stats64 *stats,
+				   u32 flags);
+	int (*dp_clear_netif_mib)(dp_subif_t *subif, void *priv, u32 flag);
+	int (*dp_set_gsw_pmapper)(int inst, int bport, int lport,
+				  struct dp_pmapper *mapper, u32 flag);
+	int (*dp_get_gsw_pmapper)(int inst, int bport, int lport,
+				  struct dp_pmapper *mapper, u32 flag);
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_SWITCHDEV)
+	int swdev_flag;
+	int (*swdev_alloc_bridge_id)(int inst);
+	int (*swdev_free_brcfg)(int inst, u16 fid);
+	int (*swdev_bridge_cfg_set)(int inst, u16 fid);
+	int (*swdev_bridge_port_cfg_set)(struct br_info *br_item, int inst,
+					 int port);
+	int (*swdev_bridge_port_cfg_reset)(struct br_info *br_item,
+					   int inst, int bport);
+	int (*dp_mac_set)(int bport, int fid, int inst, u8 *addr);
+	int (*dp_mac_reset)(int bport, int fid, int inst, u8 *addr);
+	int (*dp_cfg_vlan)(int inst, int vap, int ep);
+#endif
+	int (*dp_tc_vlan_set)(struct core_ops *ops, struct dp_tc_vlan *vlan,
+			      struct dp_tc_vlan_info *info,
+			      int flag);
+};
+
+struct dp_inst {
+	enum DP_HW_CAP_TYPE type;
+	enum DP_HW_CAP_VER ver;
+	struct inst_info info;
+};
+
+struct dp_hw_cap {
+	u8 valid;
+	struct inst_info info;
+};
+
+struct inst_property {
+	u8 valid;
+	struct inst_info info;
+	/*driver should know which HW to configure, esp for PCIe case */
+	struct core_ops *ops[DP_MAX_GSW_HANDLE];
+	int cbm_inst;
+	int qos_inst;
+	void *priv_hal; /*private data per HAL */
+};
+
+int register_dp_cap_gswip30(int flag);
+int register_dp_cap_gswip31(int flag);
+int register_dp_hw_cap(struct dp_hw_cap *info, u32 flag);
+
+/*! request a new DP instance based on its HW type/version */
+int dp_request_inst(struct dp_inst_info *info, u32 flag);
+
+#endif /* DATAPATH_INST_H */
diff --git a/include/net/datapath_proc_api.h b/include/net/datapath_proc_api.h
new file mode 100644
index 000000000000..ff59a7621530
--- /dev/null
+++ b/include/net/datapath_proc_api.h
@@ -0,0 +1,84 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#ifndef DATAPATH_PROC_H
+#define DATAPATH_PROC_H
+
+#include <linux/kernel.h>	/*kmalloc */
+#include <linux/ctype.h>
+#include <linux/debugfs.h>	/*file_operations */
+#include <linux/seq_file.h>	/*seq_file */
+#include <linux/uaccess.h>	/*copy_from_user */
+
+#define set_ltq_dbg_flag(v, e, f) do {;\
+	if ((e) > 0)\
+		(v) |= (uint32_t)(f);\
+	else\
+		(v) &= (uint32_t)(~f); } \
+	while (0)
+
+typedef void (*dp_proc_single_callback_t) (struct seq_file *);
+typedef int (*dp_proc_callback_t) (struct seq_file *, int);
+typedef int (*dp_proc_init_callback_t) (void);
+typedef ssize_t(*dp_proc_write_callback_t) (struct file *file,
+					     const char __user *input,
+					     size_t size, loff_t *loff);
+
+struct dp_proc_file_entry {
+	dp_proc_callback_t multi_callback;
+	dp_proc_single_callback_t single_callback;
+	int pos;
+	int single_call_only;
+};
+
+struct dp_proc_entry {
+	char *name;
+	dp_proc_single_callback_t single_callback;
+	dp_proc_callback_t multi_callback;
+	dp_proc_init_callback_t init_callback;
+	dp_proc_write_callback_t write_callback;
+	struct file_operations ops;
+};
+
+int dp_getopt(char *cmd[], int cmd_size, int *cmd_offset,
+	      char **optarg, const char *optstring);
+
+void dp_proc_entry_create(struct dentry *parent_node,
+			  struct dp_proc_entry *proc_entry);
+
+int dp_atoi(unsigned char *str);
+int dp_strncmpi(const char *s1, const char *s2, size_t n);
+void dp_replace_ch(char *p, int len, char orig_ch, char new_ch);
+
+/*Split buffer to multiple segment with separator space.
+ *And put pointer to array[].
+ *By the way, original buffer will be overwritten with '\0' at some place.
+ */
+int dp_split_buffer(char *buffer, char *array[], int max_param_num);
+
+#ifdef LTQ_DATAPATH_MPE_FASTHOOK_TEST
+/*add the macro in order to be back-compatible with old MPE FW HOOK */
+#define ltq_proc_entry dp_proc_entry
+#define ltq_proc_file_entry dp_proc_file_entry
+
+#define ltq_proc_entry_create dp_proc_entry_create
+
+#define ltq_atoi dp_atoi
+#define ltq_strncmpi dp_strncmpi
+#define ltq_replace_ch dp_replace_ch
+#define ltq_remove_leading_whitespace dp_remove_leading_whitespace
+#define ltq_split_buffer dp_split_buffer
+#endif /*LTQ_DATAPATH_MPE_FASTHOOK_TEST*/
+
+void set_start_end_id(unsigned int new_start, unsigned int new_end,
+		      unsigned int max_start, unsigned int max_end,
+		      unsigned int default_start, unsigned int default_end,
+		      unsigned int *start, unsigned int *end);
+
+#endif				/*DATAPATH_PROC_H */
