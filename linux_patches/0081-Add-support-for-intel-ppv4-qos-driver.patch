From 1d70ca7323a50255fe210eb844ca6678ce659c88 Mon Sep 17 00:00:00 2001
From: Hua Ma <hua.ma@linux.intel.com>
Date: Thu, 21 Jun 2018 17:38:26 +0800
Subject: [PATCH] Add support for intel ppv4 qos driver

---
 drivers/net/ethernet/lantiq/ppv4/bm/Makefile       |   10 +
 .../net/ethernet/lantiq/ppv4/bm/pp_bm_debugfs.c    |  431 +++
 drivers/net/ethernet/lantiq/ppv4/bm/pp_bm_drv.c    | 2361 ++++++++++++++++
 drivers/net/ethernet/lantiq/ppv4/bm/pp_bm_drv.h    |  241 ++
 .../ethernet/lantiq/ppv4/bm/pp_bm_drv_internal.h   |  220 ++
 drivers/net/ethernet/lantiq/ppv4/qos/Makefile      |    7 +
 .../net/ethernet/lantiq/ppv4/qos/pp_qos_common.h   |  182 ++
 .../net/ethernet/lantiq/ppv4/qos/pp_qos_debugfs.c  |  841 ++++++
 drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_fw.c   | 2813 ++++++++++++++++++++
 drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_fw.h   |  167 ++
 .../net/ethernet/lantiq/ppv4/qos/pp_qos_kernel.h   |   75 +
 .../net/ethernet/lantiq/ppv4/qos/pp_qos_linux.c    |  569 ++++
 drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_main.c | 2082 +++++++++++++++
 .../net/ethernet/lantiq/ppv4/qos/pp_qos_uc_defs.h  |  522 ++++
 .../ethernet/lantiq/ppv4/qos/pp_qos_uc_wrapper.h   |   34 +
 .../net/ethernet/lantiq/ppv4/qos/pp_qos_utils.c    | 2627 ++++++++++++++++++
 .../net/ethernet/lantiq/ppv4/qos/pp_qos_utils.h    |  684 +++++
 17 files changed, 13866 insertions(+)

diff --git a/drivers/net/ethernet/lantiq/ppv4/bm/Makefile b/drivers/net/ethernet/lantiq/ppv4/bm/Makefile
new file mode 100644
index 000000000000..87cd75f19981
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/ppv4/bm/Makefile
@@ -0,0 +1,10 @@
+#
+# Makefile for PPv4 BM driver.
+#
+
+obj-$(CONFIG_LTQ_PPV4) += ppv4_bm.o
+ppv4_bm-y 	:= pp_bm_drv.o pp_bm_debugfs.o
+#ccflags-y	+= -Iinclude/net -DNO_FW -DPP_QOS_DISABLE_CMDQ
+ccflags-y	+= -Iinclude/net -DQOS_CPU_UC_SAME_ENDIANESS
+
+#ccflags-$(CONFIG_LTQ_PPV4_QOS_TEST) += -DPP_QOS_TEST
diff --git a/drivers/net/ethernet/lantiq/ppv4/bm/pp_bm_debugfs.c b/drivers/net/ethernet/lantiq/ppv4/bm/pp_bm_debugfs.c
new file mode 100644
index 000000000000..fa290fc0b9df
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/ppv4/bm/pp_bm_debugfs.c
@@ -0,0 +1,431 @@
+/*
+ * GPL LICENSE SUMMARY
+ *
+ *  Copyright(c) 2017 Intel Corporation.
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of version 2 of the GNU General Public License as
+ *  published by the Free Software Foundation.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *  The full GNU General Public License is included in this distribution
+ *  in the file called LICENSE.GPL.
+ *
+ *  Contact Information:
+ *  Intel Corporation
+ *  2200 Mission College Blvd.
+ *  Santa Clara, CA  97052
+ */
+
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/platform_device.h>
+#include <linux/debugfs.h>
+#include "pp_bm_drv_internal.h"
+
+#define PP_BM_DEBUGFS_DIR "ppv4_bm"
+
+typedef int (*file_func)(struct seq_file *, void *);
+
+static int dbg_dump_hw_stats(struct seq_file *f, void *unused);
+static int dbg_dump_sw_stats(struct seq_file *f, void *unused);
+static int dbg_pop(struct seq_file *f, void *unused);
+static int dbg_push(struct seq_file *f, void *unused);
+static int dbg_test_dma(struct seq_file *f, void *unused);
+static int dbg_test_init(struct seq_file *f, void *unused);
+
+struct bm_debugfs_file_info {
+	const char	*name;
+	file_func	func;
+};
+
+static struct bm_debugfs_file_info bm_debugfs_files[] = {
+		{"hw_stats", dbg_dump_hw_stats},
+		{"sw_stats", dbg_dump_sw_stats},
+		{"pop", dbg_pop},
+		{"push", dbg_push},
+		{"test_dma", dbg_test_dma},
+		{"test_init", dbg_test_init},
+};
+
+//!< debugfs dir
+static struct dentry *bm_dir;
+static u16 policy_pop_test;
+static struct bmgr_buff_info buff_info;
+
+static void dump_pools(struct seq_file *f, struct bmgr_driver_private *pdata)
+{
+	u8	idx;
+	struct bmgr_pool_db_entry *pool;
+
+	// Dump all pools
+	seq_puts(f, "============= POOLS =============\n");
+	for (idx = 0; idx < pdata->driver_db.num_pools; idx++) {
+		pool = &pdata->driver_db.pools[idx];
+
+		if (!pool->is_busy)
+			continue;
+
+		seq_printf(f, "Active pool %d:\n", idx);
+		seq_printf(f, "\t\tNum allocated buffers: %d\n",
+			pool->num_allocated_buffers);
+		seq_printf(f, "\t\tNum deallocated buffers: %d\n",
+			pool->num_deallocated_buffers);
+		seq_printf(f, "\t\tInternal table address: %x\n",
+			(phys_addr_t)pool->internal_pointers_tables);
+		seq_printf(f, "\t\tNum buffers: %d\n",
+			pool->pool_params.num_buffers);
+		seq_printf(f, "\t\tSize of buffer: %d\n",
+			pool->pool_params.size_of_buffer);
+		seq_printf(f, "\t\tGroup id: %d\n",
+			pool->pool_params.group_id);
+		seq_printf(f, "\t\tBase address low: 0x%x\n",
+			pool->pool_params.base_addr_low);
+		seq_printf(f, "\t\tBase address high: 0x%x\n",
+			pool->pool_params.base_addr_high);
+		seq_printf(f, "\t\tFlags: %d\n",
+			pool->pool_params.flags);
+	}
+}
+
+static void dump_groups(struct seq_file *f, struct bmgr_driver_private *pdata)
+{
+	struct bmgr_group_db_entry	*group;
+	u8	idx;
+	u8	idx2;
+	u32	num = 0;
+
+	// Dump all groups
+	seq_puts(f, "============= GROUPS ============\n");
+	for (idx = 0; idx < pdata->driver_db.num_groups; idx++) {
+		group = &pdata->driver_db.groups[idx];
+
+		if (group->num_pools_in_group) {
+			seq_printf(f, "Active group %d:\n", idx);
+			seq_printf(f, "\t\tAvailable buffers: %d\n",
+				group->available_buffers);
+			seq_printf(f, "\t\tReserved buffers: %d\n",
+				group->reserved_buffers);
+			seq_puts(f, "\t\tPools in group:");
+			num = group->num_pools_in_group;
+			for (idx2 = 0; idx2 < num; idx2++) {
+				if (group->pools[idx2])
+					seq_printf(f, " %d", idx2);
+			}
+			seq_puts(f, "\n");
+		}
+	}
+}
+
+static void dump_policies(struct seq_file *f, struct bmgr_driver_private *pdata)
+{
+	struct bmgr_policy_db_entry *policy;
+	struct bmgr_pool_in_policy_info *pool_in_policy;
+	u16	idx;
+	u8	idx2;
+	u32	num = 0;
+	u32	pool_id = 0;
+	u32	max_allowed = 0;
+
+	// Dump all policies
+	seq_puts(f, "============= POLICIES ==========\n");
+	for (idx = 0; idx < pdata->driver_db.num_policies; idx++) {
+		policy = &pdata->driver_db.policies[idx];
+
+		if (!policy->is_busy)
+			continue;
+
+		seq_printf(f, "Active policy %d:\n", idx);
+		seq_printf(f, "\t\tNum allocated buffers: %d\n",
+			policy->num_allocated_buffers);
+		seq_printf(f, "\t\tNum deallocated buffers: %d\n",
+			policy->num_deallocated_buffers);
+		seq_printf(f, "\t\tMax allowed: %d\n",
+			policy->policy_params.max_allowed);
+		seq_printf(f, "\t\tMin guaranteed: %d\n",
+			policy->policy_params.min_guaranteed);
+		seq_printf(f, "\t\tGroup id: %d\n",
+			policy->policy_params.group_id);
+		seq_printf(f, "\t\tFlags: %d\n",
+			policy->policy_params.flags);
+		seq_puts(f, "\t\tPools in policy:\n");
+		num = policy->policy_params.num_pools_in_policy;
+		for (idx2 = 0; idx2 < num; idx2++) {
+			pool_in_policy =
+				&policy->policy_params.pools_in_policy[idx2];
+			pool_id = pool_in_policy->pool_id;
+			max_allowed = pool_in_policy->max_allowed;
+
+			seq_printf(f, "\t\t\t%d. id %d, max allowed %d\n",
+				idx2, pool_id, max_allowed);
+		}
+	}
+}
+
+static void dump_db(struct seq_file *f, struct bmgr_driver_private *pdata)
+{
+	seq_puts(f, "=================================\n");
+	seq_puts(f, "====== BUFFER MANAGER DUMP ======\n");
+	seq_puts(f, "=================================\n");
+
+	// Dump all DB general counters
+	seq_puts(f, "======= GENERAL COUNTERS ========\n");
+	seq_printf(f, "Number of active pools:    %d\n",
+		pdata->driver_db.num_pools);
+	seq_printf(f, "Number of active groups:   %d\n",
+		pdata->driver_db.num_groups);
+	seq_printf(f, "Number of active policies: %d\n",
+		pdata->driver_db.num_policies);
+
+	dump_pools(f, pdata);
+	dump_groups(f, pdata);
+	dump_policies(f, pdata);
+
+	seq_puts(f, "=================================\n");
+	seq_puts(f, "========== END OF DUMP ==========\n");
+	seq_puts(f, "=================================\n");
+}
+
+static int dbg_dump_hw_stats(struct seq_file *f, void *unused)
+{
+	struct bmgr_driver_private *pdata;
+
+	pdata = dev_get_drvdata(f->private);
+
+	print_hw_stats();
+
+	return 0;
+}
+
+static int dbg_dump_sw_stats(struct seq_file *f, void *unused)
+{
+	struct bmgr_driver_private *pdata;
+
+	pdata = dev_get_drvdata(f->private);
+
+	dump_db(f, pdata);
+
+	return 0;
+}
+
+static int dbg_test_dma(struct seq_file *f, void *unused)
+{
+	struct bmgr_driver_private *pdata;
+
+	pdata = dev_get_drvdata(f->private);
+
+	bmgr_test_dma(8);
+
+	return 0;
+}
+
+static int dbg_test_init(struct seq_file *f, void *unused)
+{
+	struct bmgr_driver_private	*pdata;
+	struct bmgr_pool_params		pool_params;
+	struct bmgr_policy_params	policy_params;
+	u8				pool_id;
+	u8				policy_id;
+	u8				num_buffers = 10;
+	u8				size_of_buffer = 64;
+	void				*ptr = NULL;
+
+	pdata = dev_get_drvdata(f->private);
+
+	if (bmgr_driver_init() != 0) {
+		seq_puts(f, "dbg_test_init(): bmgr_driver_init failed\n");
+		return 0;
+	}
+
+	ptr = devm_kzalloc(f->private,
+			   size_of_buffer * num_buffers,
+			   GFP_KERNEL);
+	if (!ptr)
+		return 0;
+
+	pool_params.base_addr_low = (u32)ptr;
+
+	pool_params.base_addr_high = 0;
+	pool_params.size_of_buffer = size_of_buffer;
+	pool_params.num_buffers = num_buffers;
+	pool_params.group_id = 0;
+	pool_params.flags = 0;
+	if (bmgr_pool_configure(&pool_params, &pool_id) != 0) {
+		seq_puts(f, "dbg_test_init(): bmgr_pool_configure failed\n");
+		return 0;
+	}
+
+	seq_printf(f, "dbg_test_init(): configured pool_id %d with %d buffers (of %d), group id 0, address 0x%x\n",
+		pool_id, num_buffers, size_of_buffer, (u32)ptr);
+
+	policy_params.flags = 0;
+	policy_params.group_id = 0;
+	policy_params.max_allowed = num_buffers / 2;
+	policy_params.min_guaranteed = num_buffers / 5;
+	policy_params.num_pools_in_policy = 1;
+	policy_params.pools_in_policy[0].max_allowed = num_buffers;
+	policy_params.pools_in_policy[0].pool_id = 0;
+	if (bmgr_policy_configure(&policy_params, &policy_id) != 0) {
+		seq_puts(f, "dbg_test_init(): bmgr_policy_configure failed\n");
+		return 0;
+	}
+
+	seq_printf(f, "dbg_test_init(): configured policy_id %d with max allowed %d, min guaranteed %d, pool 0 (max allowed %d) group 0\n",
+		policy_id, num_buffers / 2, num_buffers / 5, num_buffers);
+
+	return 0;
+}
+
+// Currently pop one buffer from policies 0-3 in increament order
+static int dbg_pop(struct seq_file *f, void *unused)
+{
+	struct bmgr_driver_private *pdata;
+
+	pdata = dev_get_drvdata(f->private);
+
+	memset(&buff_info, 0x0, sizeof(buff_info));
+
+	buff_info.num_allocs = 1;
+	buff_info.policy_id = policy_pop_test;
+
+	bmgr_pop_buffer(&buff_info);
+
+	policy_pop_test++;
+	if (policy_pop_test == 4)
+		policy_pop_test = 0;
+
+	seq_printf(f, "pop buffer address 0x%x (high 0x%x) from policy %d pool %d\n",
+		buff_info.addr_low[0], buff_info.addr_high[0],
+		buff_info.policy_id, buff_info.pool_id[0]);
+
+	return 0;
+}
+
+// Currently push only last pop-ed buffer
+static int dbg_push(struct seq_file *f, void *unused)
+{
+	struct bmgr_driver_private *pdata;
+
+	pdata = dev_get_drvdata(f->private);
+
+	bmgr_push_buffer(&buff_info);
+
+	seq_printf(f, "push buffer address 0x%x (high 0x%x) from policy %d pool %d\n",
+		buff_info.addr_low[0], buff_info.addr_high[0],
+		buff_info.policy_id, buff_info.pool_id[0]);
+
+	return 0;
+}
+
+int bm_dbg_dev_init(struct platform_device *pdev)
+{
+	struct bmgr_driver_private *pdata;
+	struct dentry *dent;
+	int err;
+	u32 num_files = sizeof(bm_debugfs_files)/sizeof(struct bm_debugfs_file_info);
+	u16 ind = 0;
+
+	if (!pdev) {
+		dev_err(&pdev->dev, "Invalid platform device\n");
+		return -ENODEV;
+	}
+
+	pdata = platform_get_drvdata(pdev);
+	if (!pdata) {
+		dev_err(&pdev->dev, "Invalid platform device data\n");
+		return -ENODEV;
+	}
+
+	pdata->debugfs_info.dir = bm_dir;
+
+	dent = debugfs_create_u32("reg_bar_offset", 0644,
+				  pdata->debugfs_info.dir,
+				  &pdata->debugfs_info.virt2phyoff);
+	if (IS_ERR_OR_NULL(dent)) {
+		err = (int)PTR_ERR(dent);
+		dev_err(&pdev->dev,
+			"debugfs_create_u32 failed creating reg_bar_offset with %d\n",
+			err);
+		return err;
+	}
+
+	dent = debugfs_create_u32("ddr_bar_offset", 0644,
+				  pdata->debugfs_info.dir,
+				  &pdata->debugfs_info.ddrvirt2phyoff);
+	if (IS_ERR_OR_NULL(dent)) {
+		err = (int)PTR_ERR(dent);
+		dev_err(&pdev->dev,
+			"debugfs_create_u32 failed creating ddr_bar_offset with %d\n",
+			err);
+		return err;
+	}
+
+	dent = debugfs_create_u32("fpga_bar_offset", 0644,
+				  pdata->debugfs_info.dir,
+				  &pdata->debugfs_info.fpga_offset);
+	if (IS_ERR_OR_NULL(dent)) {
+		err = (int)PTR_ERR(dent);
+		dev_err(&pdev->dev,
+			"debugfs_create_u32 failed creating fpga_offset with %d\n",
+			err);
+		return err;
+	}
+
+	for (ind = 0; ind < num_files; ind++) {
+		dent = debugfs_create_devm_seqfile(&pdev->dev,
+						   bm_debugfs_files[ind].name,
+						   pdata->debugfs_info.dir,
+						   bm_debugfs_files[ind].func);
+		if (IS_ERR_OR_NULL(dent)) {
+			err = (int)PTR_ERR(dent);
+			dev_err(&pdev->dev,
+				"debugfs_create_file failed creating %s with %d\n",
+				bm_debugfs_files[ind].name,
+				err);
+			return err;
+		}
+	}
+
+	return 0;
+}
+
+void bm_dbg_dev_clean(struct platform_device *pdev)
+{
+//	struct bmgr_driver_private *pdata;
+
+//	if (pdev) {
+//		pdata = platform_get_drvdata(pdev);
+//		if (pdata)
+//			debugfs_remove_recursive(pdata->dbg_info.dir);
+//	}
+}
+
+int bm_dbg_module_init(void)
+{
+	int rc;
+	struct dentry *dir;
+
+	dir = debugfs_create_dir(PP_BM_DEBUGFS_DIR, NULL);
+	if (IS_ERR_OR_NULL(dir)) {
+		rc = (int)PTR_ERR(dir);
+		pr_err("debugfs_create_dir failed with %d\n", rc);
+		return rc;
+	}
+
+	bm_dir = dir;
+
+	return 0;
+}
+
+void bm_dbg_module_clean(void)
+{
+	debugfs_remove_recursive(bm_dir); // NULL check occurs internally
+}
diff --git a/drivers/net/ethernet/lantiq/ppv4/bm/pp_bm_drv.c b/drivers/net/ethernet/lantiq/ppv4/bm/pp_bm_drv.c
new file mode 100644
index 000000000000..6dcfa2b6341c
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/ppv4/bm/pp_bm_drv.c
@@ -0,0 +1,2361 @@
+/*
+ * GPL LICENSE SUMMARY
+ *
+ *  Copyright(c) 2017 Intel Corporation.
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of version 2 of the GNU General Public License as
+ *  published by the Free Software Foundation.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *  The full GNU General Public License is included in this distribution
+ *  in the file called LICENSE.GPL.
+ *
+ *  Contact Information:
+ *  Intel Corporation
+ *  2200 Mission College Blvd.
+ *  Santa Clara, CA  97052
+ */
+
+#include <linux/kernel.h>
+#include <linux/platform_device.h>
+#include <linux/of_device.h>
+#include <linux/debugfs.h>
+#include <linux/moduleparam.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+#include <linux/spinlock.h>
+#include <linux/module.h>
+#include <linux/delay.h>
+#include <linux/time.h>
+#include <linux/dma-mapping.h>
+
+#include "pp_bm_drv.h"
+#include "pp_bm_drv_internal.h"
+#include "pp_bm_regs.h"
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Intel Corporation");
+MODULE_AUTHOR("obakshe");
+MODULE_DESCRIPTION("Intel(R) PPv4 buffer_manager Driver");
+MODULE_VERSION(BMGR_DRIVER_VERSION);
+
+////////////////////
+//// PROTOTYPES ////
+////////////////////
+
+// Static function only. External function are declared in the h file
+static int buffer_manager_probe(struct platform_device *pdev);
+static int buffer_manager_remove(struct platform_device *pdev);
+
+//////////////////////////
+//// Global variables ////
+//////////////////////////
+
+#define RC_SUCCESS	(0)
+
+static const struct of_device_id bm_match[] = {
+	{ .compatible = "intel,falconmx-bm" },
+	{},
+};
+MODULE_DEVICE_TABLE(of, bm_match);
+
+//!< Platform driver
+static struct platform_driver	g_bm_platform_driver = {
+	.probe = buffer_manager_probe,
+	.remove = buffer_manager_remove,
+	.driver = {
+			.owner = THIS_MODULE,
+			.name = "buffer_manager",
+#ifdef CONFIG_OF
+			.of_match_table = bm_match,
+#endif
+		  },
+};
+
+//!< global pointer for private database
+static struct bmgr_driver_private	*this;
+
+void __iomem	*bm_config_addr_base;
+void __iomem	*bm_policy_mngr_addr_base;
+void __iomem	*uc_mcdma0_config_addr_base;
+
+#define BM_BASE		(bm_config_addr_base)		// (0x18B00000)
+#define BM_RAM_BASE	(bm_policy_mngr_addr_base)	// (0x18B10000)
+
+#define IO_VIRT(phy_addr)	((phy_addr) + this->debugfs_info.virt2phyoff)
+#define DDR_VIRT(phy_addr)	((phy_addr) + this->debugfs_info.ddrvirt2phyoff)
+
+//#define RD_DDR32(addr)	(*(volatile u32 *)(DDR_VIRT(addr)))
+#define WR_DDR32(addr, var)	((*(volatile u32 *)(DDR_VIRT(addr))) = var)
+
+#define RD_REG_32(addr)	__raw_readl((volatile u32 *)(addr))
+#define WR_REG_32(addr, val)	__raw_writel(val, (volatile u32 *)(addr))
+//#define WR_REG_32(addr, var)	((*(volatile u32 *)(IO_VIRT(addr))) = var)
+//#define RD_REG_32(addr)	(*(volatile u32 *)(IO_VIRT(addr)))
+
+//////////////
+//// APIs ////
+//////////////
+
+/**************************************************************************
+ *! \fn	bmgr_db_lock
+ **************************************************************************
+ *
+ *  \brief	This function locks the DB in a recursion-save manner
+ *
+ *  \return	void
+ *
+ **************************************************************************/
+static inline void bmgr_db_lock(void)
+{
+	spin_lock_bh(&this->driver_db.db_lock);
+}
+
+/**************************************************************************
+ *! \fn	bmgr_db_unlock
+ **************************************************************************
+ *
+ *  \brief	This function unlocks the DB in a recursion-save manner
+ *
+ *  \return	void
+ *
+ **************************************************************************/
+static inline void bmgr_db_unlock(void)
+{
+	spin_unlock_bh(&this->driver_db.db_lock);
+}
+
+/**************************************************************************
+ *! \fn	buffer_manager_db_init
+ **************************************************************************
+ *
+ *  \brief	Initialize the DB
+ *
+ *  \param	priv:	buffer_manager private data pointer
+ *
+ *  \return	0 on success, other error code on failure
+ *
+ **************************************************************************/
+static s32 buffer_manager_db_init(struct bmgr_driver_private *priv)
+{
+	struct bmgr_driver_db	*db = &priv->driver_db;
+
+	if (!db) {
+		pr_err("buffer_manager_db_init(): db was not initialized successfuly!\n");
+		return -EINVAL;
+	}
+
+	memset(db, 0, sizeof(struct bmgr_driver_db));
+
+	spin_lock_init(&db->db_lock);
+
+	pr_info("buffer_manager_db_init(): db was initialized successfuly\n");
+
+	return 0;
+}
+
+static struct resource *get_resource(struct platform_device *pdev,
+				     const char *name,
+				     unsigned int type,
+				     size_t size)
+{
+	struct resource *r;
+	size_t sz;
+	struct device *dev;
+
+	dev = &pdev->dev;
+
+	r = platform_get_resource_byname(pdev, type, name);
+	if (!r) {
+		dev_err(dev, "Could not get %s resource\n", name);
+		return NULL;
+	}
+
+	sz = resource_size(r);
+	if (size && sz != size) {
+		dev_err(dev, "Illegal size %zu for %s resource expected %zu\n",
+			sz, name, size);
+		return NULL;
+	}
+
+	return r;
+}
+
+static void print_resource(struct device *dev,
+			   const char *name,
+			   struct resource *r)
+{
+
+	dev_info(dev, "%s memory resource: start(0x%08zX), size(%zu)\n",
+		 name,
+		 (size_t)(uintptr_t)r->start,
+		 (size_t)(uintptr_t)resource_size(r));
+}
+
+static void __iomem *get_resource_mapped_addr(struct platform_device *pdev,
+					      const char *name,
+					      unsigned int size)
+{
+	struct resource	*res;
+
+	res = get_resource(pdev, name, IORESOURCE_MEM, size);
+	if (!res)
+		return NULL;
+
+	print_resource(&pdev->dev, name, res);
+
+	return devm_ioremap_resource(&pdev->dev, res);
+}
+
+/**************************************************************************
+ *! \fn	buffer_manager_probe
+ **************************************************************************
+ *
+ *  \brief	probe platform device hook
+ *
+ *  \param	pdev:	platform device pointer
+ *
+ *  \return	0 on success, other error code on failure
+ *
+ **************************************************************************/
+static int buffer_manager_probe(struct platform_device *pdev)
+{
+	int				ret;
+	struct bmgr_driver_private	*priv;
+	u32				val;
+	struct device_node		*node;
+	int				err;
+
+	dev_info(&pdev->dev, "BM probe...\n");
+
+	node = pdev->dev.of_node;
+
+	priv = devm_kzalloc(&pdev->dev, sizeof(*priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	platform_set_drvdata(pdev, priv);
+	priv->pdev = pdev;
+	priv->enabled = true;
+	this = priv;
+
+	ret = buffer_manager_db_init(priv);
+	if (ret) {
+		kfree(priv);
+		return -ENOMEM;
+	}
+
+	bm_config_addr_base = get_resource_mapped_addr(pdev, "reg-config", 0x10000);
+	bm_policy_mngr_addr_base = get_resource_mapped_addr(pdev, "ram-config", 0x10000);
+	uc_mcdma0_config_addr_base = get_resource_mapped_addr(pdev, "uc-mcdma0", 0x200);
+
+	if (!bm_config_addr_base ||
+	    !bm_policy_mngr_addr_base ||
+	    !uc_mcdma0_config_addr_base) {
+		dev_err(&pdev->dev, "failed to request and remap io ranges\n");
+		return -ENOMEM;
+	}
+
+	err = of_property_read_u32(node, "intel,max-policies", &val);
+	if (err) {
+		dev_err(&pdev->dev,
+			"Could not get max policies from DT, error is %d\n",
+			err);
+		return -ENODEV;
+	}
+
+	dev_info(&pdev->dev, "max-policies = %d\n", val);
+	this->driver_db.max_policies = val;
+	if (this->driver_db.max_policies > PP_BMGR_MAX_POLICIES) {
+		dev_err(&pdev->dev, "max policies %d is %d\n",
+			this->driver_db.max_policies, PP_BMGR_MAX_POLICIES);
+		return -ENODEV;
+	}
+
+	err = of_property_read_u32(node, "intel,max-groups", &val);
+	if (err) {
+		dev_err(&pdev->dev,
+			"Could not get max groups from DT, error is %d\n",
+			err);
+		return -ENODEV;
+	}
+
+	dev_info(&pdev->dev, "max-groups = %d\n", val);
+	this->driver_db.max_groups = val;
+	if (this->driver_db.max_groups > PP_BMGR_MAX_GROUPS) {
+		dev_err(&pdev->dev, "max groups %d is %d\n",
+			this->driver_db.max_groups, PP_BMGR_MAX_GROUPS);
+		return -ENODEV;
+	}
+
+	err = of_property_read_u32(node, "intel,max-pools", &val);
+	if (err) {
+		dev_err(&pdev->dev,
+			"Could not get max pools from DT, error is %d\n",
+			err);
+		return -ENODEV;
+	}
+
+	dev_info(&pdev->dev, "max-pools = %d\n", val);
+	this->driver_db.max_pools = val;
+	if (this->driver_db.max_pools > PP_BMGR_MAX_POOLS) {
+		dev_err(&pdev->dev, "max pools %d is %d\n",
+			this->driver_db.max_pools, PP_BMGR_MAX_POOLS);
+		return -ENODEV;
+	}
+
+	bm_dbg_dev_init(pdev);
+
+	return 0;
+}
+
+/**************************************************************************
+ *! \fn	buffer_manager_remove
+ **************************************************************************
+ *
+ *  \brief	probe platform device hook
+ *
+ *  \param	pdev:	platform device pointer
+ *
+ *  \return	0 on success, other error code on failure
+ *
+ **************************************************************************/
+static int buffer_manager_remove(struct platform_device *pdev)
+{
+	struct bmgr_driver_private	*priv = platform_get_drvdata(pdev);
+
+	priv->enabled = 0;
+
+	kfree(priv);
+
+	platform_set_drvdata(pdev, NULL);
+	this = NULL;
+
+	bm_dbg_dev_clean(pdev);
+
+	dev_info(&pdev->dev, "buffer_manager_remove(): remove done\n");
+
+	return 0;
+}
+
+/**************************************************************************
+ *! \fn	buffer_manager_driver_init
+ **************************************************************************
+ *
+ *  \brief	Init platform device driver
+ *
+ *  \return	0 on success, other error code on failure
+ *
+ **************************************************************************/
+static int __init buffer_manager_driver_init(void)
+{
+	int ret;
+
+	bm_dbg_module_init();
+
+	ret = platform_driver_register(&g_bm_platform_driver);
+	if (ret < 0) {
+		pr_err("platform_driver_register failed (%d)\n", ret);
+		return ret;
+	}
+
+	pr_info("buffer manager driver init done\n");
+
+	return 0;
+}
+
+/**************************************************************************
+ *! \fn	buffer_manager_driver_exit
+ **************************************************************************
+ *
+ *  \brief	Exit platform device driver
+ *
+ *  \return	0 on success, other error code on failure
+ *
+ **************************************************************************/
+static void __exit buffer_manager_driver_exit(void)
+{
+	platform_driver_unregister(&g_bm_platform_driver);
+
+	bm_dbg_module_clean();
+
+	pr_info("buffer manager driver exit done\n");
+}
+
+/*************************************************/
+/**		Module Declarations		**/
+/*************************************************/
+//module_init(buffer_manager_driver_init);
+arch_initcall(buffer_manager_driver_init);
+module_exit(buffer_manager_driver_exit);
+
+void copy_dma(u32 src, u32 dst, u32 flags)
+{
+	// base-0x18850000
+	u32 MCDMA0_BASE_ADDR = (u32)uc_mcdma0_config_addr_base;
+	u32 MCDMA_SRC_OFFSET = 0;
+	u32 MCDMA_DST_OFFSET = 0x4;
+	u32 MCDMA_CONTROL_OFFSET = 0x8;
+	u32 mcdma_channel = 0;
+	u32 active_bit = (1 << 30);
+	u32 pxp_offset = 0;
+	struct timespec start_ts;
+	struct timespec end_ts;
+	u32 DMA0_SRC_ADDR = MCDMA0_BASE_ADDR +
+		MCDMA_SRC_OFFSET + 0x40 * mcdma_channel;
+	u32 DMA0_DST_ADDR = MCDMA0_BASE_ADDR +
+		MCDMA_DST_OFFSET + 0x40 * mcdma_channel;
+	u32 DMA0_CTRL_ADDR = MCDMA0_BASE_ADDR +
+		MCDMA_CONTROL_OFFSET + 0x40 * mcdma_channel;
+
+	WR_REG_32(DMA0_SRC_ADDR, src - pxp_offset); //source address
+	WR_REG_32(DMA0_DST_ADDR, dst - pxp_offset); //destination address
+	pr_info("Copying from ADDR 0x%x to ADDR 0x%x\n",
+		src - pxp_offset, dst - pxp_offset);
+	// 8 bytes burst, total size is 8 byte which is 64 bits
+	WR_REG_32(DMA0_CTRL_ADDR, flags);
+	pr_info("SLEEP\n");
+	msleep(100);
+
+	getnstimeofday(&start_ts);
+
+	// wait for Active bit 30 will be 0
+	while ((RD_REG_32(DMA0_CTRL_ADDR) & active_bit) != 0) {
+		getnstimeofday(&end_ts);
+		if ((timespec_sub(end_ts, start_ts).tv_sec) > 5) {
+			pr_err("!!!!!!! DMA TIMEOUT !!!!!!!\n");
+			return;
+		}
+	}
+
+	msleep(100);
+	pr_info("Copying DONE (control 0x%x)\n", RD_REG_32(DMA0_CTRL_ADDR));
+}
+
+/**************************************************************************
+ *! \fn	bmgr_wait_for_init_completion
+ **************************************************************************
+ *
+ *  \brief	This function reads the init bit and waits for completion
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+static s32 bmgr_wait_for_init_completion(void)
+{
+	u32	st;
+	struct timespec start_ts;
+	struct timespec end_ts;
+
+	pr_info("Waiting for operation complete....");
+
+	getnstimeofday(&start_ts);
+
+	do {
+		getnstimeofday(&end_ts);
+		if ((timespec_sub(end_ts, start_ts).tv_sec) > 5) {
+			pr_err("!!!!!!! WAIT COMPLETETION TIMEOUT !!!!!!!\n");
+			return -EINVAL;
+		}
+		st = (RD_REG_32(BMGR_STATUS_REG_ADDR(BM_BASE)) & (1));
+	} while (st);
+
+	pr_info("Done\n");
+
+	return RC_SUCCESS;
+}
+
+/**************************************************************************
+ *! \fn	bmgr_is_pool_params_valid
+ **************************************************************************
+ *
+ *  \brief	Check wheather pool parameters are valid
+ *
+ *  \param	pool_params:	Pool param from user
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+static s32 bmgr_is_pool_params_valid(
+		const struct bmgr_pool_params * const pool_params)
+{
+	// Validity check
+	if (!pool_params) {
+		pr_err("bmgr_is_pool_params_valid: pool_params is NULL\n");
+		return -EINVAL;
+	}
+
+	if (pool_params->size_of_buffer < BMGR_MIN_POOL_BUFFER_SIZE) {
+		pr_err("bmgr_is_pool_params_valid: size_of_buffer %d should be smaller than %d\n",
+		       pool_params->size_of_buffer,
+		       BMGR_MIN_POOL_BUFFER_SIZE);
+		return -EINVAL;
+	}
+
+	if (pool_params->size_of_buffer % BMGR_MIN_POOL_BUFFER_SIZE) {
+		pr_err("bmgr_is_pool_params_valid: size_of_buffer %d must be aligned to %d bytes\n",
+		       pool_params->size_of_buffer,
+		       BMGR_MIN_POOL_BUFFER_SIZE);
+		return -EINVAL;
+	}
+
+	if (pool_params->base_addr_low & 0x3F) {
+		pr_err("bmgr_is_pool_params_valid: base_addr_low %d must be aligned to %d bytes\n",
+		       pool_params->base_addr_low,
+		       BMGR_MIN_POOL_BUFFER_SIZE);
+		return -EINVAL;
+	}
+
+	// Num_buffers can be up to 2^24
+	if (pool_params->num_buffers >= 0x1000000) {
+		pr_err("Number of buffers can be up to 0x1000000\n");
+		return -EINVAL;
+	}
+
+	if (pool_params->group_id >= this->driver_db.max_groups) {
+		pr_err("bmgr_is_pool_params_valid: group_id %d must be smaller than %d\n",
+		       pool_params->group_id, this->driver_db.max_groups);
+		return -EINVAL;
+	}
+
+	return RC_SUCCESS;
+}
+
+/**************************************************************************
+ *! \fn	bmgr_is_policy_params_valid
+ **************************************************************************
+ *
+ *  \brief	Check wheather policy parameters are valid
+ *
+ *  \param	policy_params:	Policy param from user
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+static s32 bmgr_is_policy_params_valid(
+		const struct bmgr_policy_params * const policy_params)
+{
+	if (!policy_params) {
+		pr_err("bmgr_is_policy_params_valid: policy_params is NULL\n");
+		return -EINVAL;
+	}
+
+	if (policy_params->group_id >= this->driver_db.max_groups) {
+		pr_err("group_id %d >= %d\n",
+		       policy_params->group_id, this->driver_db.max_groups);
+		return -EINVAL;
+	}
+
+	if (policy_params->num_pools_in_policy > PP_BMGR_MAX_POOLS_IN_POLICY) {
+		pr_err("num_pools_in_policy %d should be up to %d\n",
+		       policy_params->num_pools_in_policy,
+		       PP_BMGR_MAX_POOLS_IN_POLICY);
+		return -EINVAL;
+	}
+
+	return RC_SUCCESS;
+}
+
+/**************************************************************************
+ *! \fn	bmgr_set_control
+ **************************************************************************
+ *
+ *  \brief	Sets the control register
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+static s32 bmgr_set_control(void)
+{
+	// Buffer manager client enable
+	WR_REG_32(BMGR_CTRL_REG_ADDR(BM_BASE), 0x1);
+
+	return RC_SUCCESS;
+}
+
+static u32 bmgr_get_control(void)
+{
+	return RD_REG_32(BMGR_CTRL_REG_ADDR(BM_BASE));
+}
+
+/**************************************************************************
+ *! \fn	bmgr_enable_min_guarantee_per_pool
+ **************************************************************************
+ *
+ *  \brief	Enables/Disables minimum guarantee per pool
+ *
+ *  \param	pool_id:	Pool ID
+ *  \param	enable:		True to enable, false to disable
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+static s32 bmgr_enable_min_guarantee_per_pool(u8 pool_id, u8 enable)
+{
+	u32	value;
+	u32	mask;
+
+	// Validity check
+	if (pool_id >= this->driver_db.max_pools) {
+		pr_err("pool_id %d out of range %d\n",
+		       pool_id, this->driver_db.max_pools);
+		return -EINVAL;
+	}
+
+	// Read value
+	value = RD_REG_32(BMGR_POOL_MIN_GRNT_MASK_REG_ADDR(BM_BASE));
+	mask = BIT(pool_id);
+
+	if (enable) {
+		value |= mask;
+	} else {
+		mask = ~mask;
+		value &= mask;
+	}
+
+	WR_REG_32(BMGR_POOL_MIN_GRNT_MASK_REG_ADDR(BM_BASE), value);
+
+	return RC_SUCCESS;
+}
+
+/**************************************************************************
+ *! \fn	bmgr_pool_enable
+ **************************************************************************
+ *
+ *  \brief	Enable pool
+ *
+ *  \param	pool_id:	Pool ID
+ *  \param	enable:		True to enable, false to disable
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+static s32 bmgr_pool_enable(u8 pool_id, bool enable)
+{
+	u32	value;
+	u32	mask;
+
+	// Validity check
+	if (pool_id >= this->driver_db.max_pools) {
+		pr_err("pool_id %d out of range %d\n",
+		       pool_id, this->driver_db.max_pools);
+		return -EINVAL;
+	}
+
+	// Read value
+	value = RD_REG_32(BMGR_POOL_ENABLE_REG_ADDR(BM_BASE));
+	mask = (BIT(pool_id) << 16) | BIT(pool_id);
+
+	if (enable) {
+		// Pool Enable
+		value |= mask;
+	} else {
+		// Pool disable
+		mask = ~mask;
+		value &= mask;
+	}
+
+	WR_REG_32(BMGR_POOL_ENABLE_REG_ADDR(BM_BASE), value);
+
+	return RC_SUCCESS;
+}
+
+/**************************************************************************
+ *! \fn	bmgr_pool_reset_fifo
+ **************************************************************************
+ *
+ *  \brief	Resets the pool fifo
+ *
+ *  \param	pool_id:	Pool ID
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+static s32 bmgr_pool_reset_fifo(u8 pool_id)
+{
+	u32	reset_reg;
+
+	// Validity check
+	if (pool_id >= this->driver_db.max_pools) {
+		pr_err("pool_id %d out of range %d\n",
+		       pool_id, this->driver_db.max_pools);
+		return -EINVAL;
+	}
+
+	reset_reg = RD_REG_32(BMGR_POOL_FIFO_RESET_REG_ADDR(BM_BASE));
+
+	// Set the reset bit to 0
+	reset_reg &= ~BIT(pool_id);
+
+	// Pool FIFO Reset
+	WR_REG_32(BMGR_POOL_FIFO_RESET_REG_ADDR(BM_BASE), reset_reg);
+
+	return RC_SUCCESS;
+}
+
+/**************************************************************************
+ *! \fn	bmgr_configure_ocp_master
+ **************************************************************************
+ *
+ *  \brief	Configures the burst size and number of bursts
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+static s32 bmgr_configure_ocp_master(void)
+{
+	// OCP Master burst size
+	// 64B burst for all pools
+	WR_REG_32(BMGR_OCPM_BURST_SIZE_REG_ADDR(BM_BASE), 0);
+
+	// OCP Master number of bursts
+	// 1 burst for all pools
+	WR_REG_32(BMGR_OCPM_NUM_OF_BURSTS_REG_ADDR(BM_BASE), 0);
+
+	return RC_SUCCESS;
+}
+
+/**************************************************************************
+ *! \fn	bmgr_get_ocp_burst_size
+ **************************************************************************
+ *
+ *  \brief	Gets burst size
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+static u32 bmgr_get_ocp_burst_size(void)
+{
+	return RD_REG_32(BMGR_OCPM_BURST_SIZE_REG_ADDR(BM_BASE));
+}
+
+/**************************************************************************
+ *! \fn	bmgr_set_pool_size
+ **************************************************************************
+ *
+ *  \brief	Sets pool size
+ *
+ *  \param	pool_id:	Pool ID
+ *  \param	num_buffers:	Number of buffers in pool
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+static s32 bmgr_set_pool_size(u8 pool_id, u32 num_buffers)
+{
+	u32	burst_size = 0;
+	u32	reg = 0;
+	u32	mask = 3 << (2 * pool_id);
+
+	if (num_buffers > 128) {
+		burst_size = 3; // 512B
+	} else if (num_buffers > 64) {
+		burst_size = 2; // 256B
+	} else if (num_buffers > 32) {
+		burst_size = 1; // 128B
+	} else if (num_buffers > 16)	{
+		burst_size = 0; // 64B
+	} else {
+		pr_err("bmgr_set_pool_size(): minimum valid num_buffers (%d) is 16\n",
+		       num_buffers);
+		return -EINVAL;
+	}
+
+	// num buffer X pointer size must be
+	// multiplier of the burst size in bytes
+	if ((num_buffers % (1 << (4 + burst_size))) != 0) {
+		pr_err("bmgr_set_pool_size(): num_buffers %d must be multiplier of %d\n",
+		       num_buffers, 1 << (4 + burst_size));
+		return -EINVAL;
+	}
+
+	reg = RD_REG_32(BMGR_OCPM_BURST_SIZE_REG_ADDR(BM_BASE));
+	burst_size <<= (2 * pool_id);
+
+	reg &= ~(mask);
+	reg |= burst_size;
+
+	// OCP Master burst size
+	WR_REG_32(BMGR_OCPM_BURST_SIZE_REG_ADDR(BM_BASE), reg);
+
+	// Sets number of buffers in pools
+	WR_REG_32(BMGR_POOL_SIZE_REG_ADDR(BM_BASE, pool_id), num_buffers);
+
+	return RC_SUCCESS;
+}
+
+/**************************************************************************
+ *! \fn	bmgr_get_pool_size
+ **************************************************************************
+ *
+ *  \brief	Returns the number of buffers in pool
+ *
+ *  \param	pool_id:	Pool ID
+ *
+ *  \return	Number of buffers in pool
+ *
+ **************************************************************************/
+static u32 bmgr_get_pool_size(u8 pool_id)
+{
+	return RD_REG_32(BMGR_POOL_SIZE_REG_ADDR(BM_BASE, pool_id));
+}
+
+/**************************************************************************
+ *! \fn	bmgr_set_group_available_buffers
+ **************************************************************************
+ *
+ *  \brief  Sets the available buffers in group.
+ *          This is used to for better HW performance
+ *
+ *  \param	group_id:		Group ID
+ *  \param	available_buffers:	Available buffres in group
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+static s32 bmgr_set_group_available_buffers(u8 group_id,
+					    u32 available_buffers)
+{
+	WR_REG_32(BMGR_GROUP_AVAILABLE_BUFF_REG_ADDR(BM_BASE, group_id),
+		  available_buffers);
+
+	return RC_SUCCESS;
+}
+
+/**************************************************************************
+ *! \fn	bmgr_get_group_available_buffers
+ **************************************************************************
+ *
+ *  \brief	Returns the available buffers in group
+ *
+ *  \param	group_id:	Group ID
+ *
+ *  \return	Available buffers in group
+ *
+ **************************************************************************/
+static u32 bmgr_get_group_available_buffers(u8 group_id)
+{
+	return RD_REG_32(BMGR_GROUP_AVAILABLE_BUFF_REG_ADDR(BM_BASE,
+			 group_id));
+}
+
+/**************************************************************************
+ *! \fn	bmgr_set_group_reserved_buffers
+ **************************************************************************
+ *
+ *  \brief	Sets the number of reserved buffers in group
+ *
+ *  \param	group_id:		Group ID
+ *  \param	reserved_buffers:	reserved buffers in group
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+static s32 bmgr_set_group_reserved_buffers(u8 group_id, u32 reserved_buffers)
+{
+	WR_REG_32(BMGR_GROUP_RESERVED_BUFF_REG_ADDR(BM_BASE, group_id),
+		  reserved_buffers);
+
+	return RC_SUCCESS;
+}
+
+/**************************************************************************
+ *! \fn	bmgr_get_group_reserved_buffers
+ **************************************************************************
+ *
+ *  \brief	Returns the number of reserved buffers in group
+ *
+ *  \param	group_id:	Group ID
+ *
+ *  \return	Number of reserved buffers in group
+ *
+ **************************************************************************/
+static u32 bmgr_get_group_reserved_buffers(u8 group_id)
+{
+	return RD_REG_32(BMGR_GROUP_RESERVED_BUFF_REG_ADDR(BM_BASE,
+			 group_id));
+}
+
+/**************************************************************************
+ *! \fn	bmgr_set_pcu_fifo_base_address
+ **************************************************************************
+ *
+ *  \brief	Sets the pcu fifo base address (In SRAM)
+ *
+ *  \param	pool_id:	Pool ID
+ *  \param	base_address:	PCU Fifo base address
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+static s32 bmgr_set_pcu_fifo_base_address(u8 pool_id, u32 base_address)
+{
+	WR_REG_32(BMGR_POOL_PCU_FIFO_BASE_ADDR_REG_ADDR(BM_BASE, pool_id),
+		  base_address);
+
+	return RC_SUCCESS;
+}
+
+/**************************************************************************
+ *! \fn	bmgr_get_pcu_fifo_base_address
+ **************************************************************************
+ *
+ *  \brief	Returns the pcu fifo base address (In SRAM)
+ *
+ *  \param	pool_id:	Pool ID
+ *
+ *  \return	PCU Fifo base address
+ *
+ **************************************************************************/
+static u32 bmgr_get_pcu_fifo_base_address(u8 pool_id)
+{
+	return RD_REG_32(BMGR_POOL_PCU_FIFO_BASE_ADDR_REG_ADDR(BM_BASE,
+			 pool_id));
+}
+
+/**************************************************************************
+ *! \fn	bmgr_set_pcu_fifo_size
+ **************************************************************************
+ *
+ *  \brief	Sets the PCU fifo size
+ *
+ *  \param	pool_id:	Pool ID
+ *  \param	fifo_size:	PCU Fifo size
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+static s32 bmgr_set_pcu_fifo_size(u8 pool_id, u32 fifo_size)
+{
+	WR_REG_32(BMGR_POOL_PCU_FIFO_SIZE_REG_ADDR(BM_BASE, pool_id),
+		  fifo_size);
+
+	return RC_SUCCESS;
+}
+
+/**************************************************************************
+ *! \fn	bmgr_get_pcu_fifo_size
+ **************************************************************************
+ *
+ *  \brief	Returns the PCU fifo size
+ *
+ *  \param	pool_id:	Pool ID
+ *
+ *  \return	PCU Fifo size
+ *
+ **************************************************************************/
+static u32 bmgr_get_pcu_fifo_size(u8 pool_id)
+{
+	return RD_REG_32(BMGR_POOL_PCU_FIFO_SIZE_REG_ADDR(BM_BASE, pool_id));
+}
+
+/**************************************************************************
+ *! \fn	bmgr_set_pcu_fifo_occupancy
+ **************************************************************************
+ *
+ *  \brief	Sets the PCU fifo occupancy
+ *
+ *  \param	pool_id:	Pool ID
+ *  \param	fifo_occupancy:	Occupancy of the pool
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+static s32 bmgr_set_pcu_fifo_occupancy(u8 pool_id, u32 fifo_occupancy)
+{
+	WR_REG_32(BMGR_POOL_PCU_FIFO_OCCUPANCY_REG_ADDR(BM_BASE, pool_id),
+		  fifo_occupancy);
+
+	return RC_SUCCESS;
+}
+
+/**************************************************************************
+ *! \fn	bmgr_get_pcu_fifo_occupancy
+ **************************************************************************
+ *
+ *  \brief	Returns the PCU fifo occupancy
+ *
+ *  \param	pool_id:	Pool ID
+ *
+ *  \return	Occupancy of the pool
+ *
+ **************************************************************************/
+static u32 bmgr_get_pcu_fifo_occupancy(u8 pool_id)
+{
+	return RD_REG_32(BMGR_POOL_PCU_FIFO_OCCUPANCY_REG_ADDR(BM_BASE,
+			 pool_id));
+}
+
+/**************************************************************************
+ *! \fn	bmgr_set_pcu_fifo_prog_empty
+ **************************************************************************
+ *
+ *  \brief	Sets the PCU fifo empty threshold
+ *
+ *  \param	pool_id:	Pool ID
+ *  \param	threshold:	PCU fifo empty threshold
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+static s32 bmgr_set_pcu_fifo_prog_empty(u8 pool_id, u32 threshold)
+{
+	WR_REG_32(BMGR_POOL_PCU_FIFO_PROG_EMPTY_REG_ADDR(BM_BASE, pool_id),
+		  threshold);
+
+	return RC_SUCCESS;
+}
+
+/**************************************************************************
+ *! \fn	bmgr_get_pcu_fifo_prog_empty
+ **************************************************************************
+ *
+ *  \brief	Returns the PCU fifo empty threshold
+ *
+ *  \param	pool_id:	Pool ID
+ *
+ *  \return	PCU fifo empty threshold
+ *
+ **************************************************************************/
+static u32 bmgr_get_pcu_fifo_prog_empty(u8 pool_id)
+{
+	return RD_REG_32(BMGR_POOL_PCU_FIFO_PROG_EMPTY_REG_ADDR(BM_BASE,
+			 pool_id));
+}
+
+/**************************************************************************
+ *! \fn	bmgr_set_pcu_fifo_prog_full
+ **************************************************************************
+ *
+ *  \brief	Sets the PCU fifo full threshold
+ *
+ *  \param	pool_id:	Pool ID
+ *  \param	threshold:	PCU fifo full threshold
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+static s32 bmgr_set_pcu_fifo_prog_full(u8 pool_id, u32 threshold)
+{
+	WR_REG_32(BMGR_POOL_PCU_FIFO_PROG_FULL_REG_ADDR(BM_BASE, pool_id),
+		  threshold);
+
+	return RC_SUCCESS;
+}
+
+/**************************************************************************
+ *! \fn	bmgr_get_pcu_fifo_prog_full
+ **************************************************************************
+ *
+ *  \brief	Returns the PCU fifo prog full threshold
+ *
+ *  \param	pool_id:	Pool ID
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+static u32 bmgr_get_pcu_fifo_prog_full(u8 pool_id)
+{
+	return RD_REG_32(BMGR_POOL_PCU_FIFO_PROG_FULL_REG_ADDR(BM_BASE,
+			 pool_id));
+}
+
+/**************************************************************************
+ *! \fn	bmgr_set_ext_fifo_base_addr_low
+ **************************************************************************
+ *
+ *  \brief	Sets the external fifo base low address
+ *
+ *  \param	pool_id:	Pool ID
+ *  \param	low_address:	External fifo base low address
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+static s32 bmgr_set_ext_fifo_base_addr_low(u8 pool_id, u32 low_address)
+{
+	WR_REG_32(BMGR_POOL_EXT_FIFO_BASE_ADDR_LOW_REG_ADDR(BM_BASE, pool_id),
+		  low_address);
+
+	return RC_SUCCESS;
+}
+
+/**************************************************************************
+ *! \fn	bmgr_get_ext_fifo_base_addr_low
+ **************************************************************************
+ *
+ *  \brief	Returns the external fifo base low address
+ *
+ *  \param	pool_id:	Pool ID
+ *
+ *  \return	External fifo base low address
+ *
+ **************************************************************************/
+static u32 bmgr_get_ext_fifo_base_addr_low(u8 pool_id)
+{
+	return RD_REG_32(BMGR_POOL_EXT_FIFO_BASE_ADDR_LOW_REG_ADDR(BM_BASE,
+			 pool_id));
+}
+
+/**************************************************************************
+ *! \fn	bmgr_set_ext_fifo_base_addr_high
+ **************************************************************************
+ *
+ *  \brief	Sets the external fifo base high address
+ *
+ *  \param	pool_id:	Pool ID
+ *  \param	high_address:	External fifo base high address
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+static s32 bmgr_set_ext_fifo_base_addr_high(u8 pool_id, u32 high_address)
+{
+	WR_REG_32(BMGR_POOL_EXT_FIFO_BASE_ADDR_HIGH_REG_ADDR(BM_BASE, pool_id),
+		  high_address);
+
+	return RC_SUCCESS;
+}
+
+/**************************************************************************
+ *! \fn	bmgr_get_ext_fifo_base_addr_high
+ **************************************************************************
+ *
+ *  \brief	Returns the external fifo base high address
+ *
+ *  \param	pool_id:	Pool ID
+ *
+ *  \return	External fifo base high address
+ *
+ **************************************************************************/
+static u32 bmgr_get_ext_fifo_base_addr_high(u8 pool_id)
+{
+	return RD_REG_32(BMGR_POOL_EXT_FIFO_BASE_ADDR_HIGH_REG_ADDR(BM_BASE,
+			 pool_id));
+}
+
+/**************************************************************************
+ *! \fn	bmgr_set_ext_fifo_occupancy
+ **************************************************************************
+ *
+ *  \brief	Sets external fifo occupancy
+ *
+ *  \param	pool_id:	Pool ID
+ *  \param	occupancy:	External fifo occupancy
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+static s32 bmgr_set_ext_fifo_occupancy(u8 pool_id, u32 occupancy)
+{
+	WR_REG_32(BMGR_POOL_EXT_FIFO_OCC_REG_ADDR(BM_BASE, pool_id),
+		  occupancy);
+
+	return RC_SUCCESS;
+}
+
+/**************************************************************************
+ *! \fn	bmgr_get_ext_fifo_occupancy
+ **************************************************************************
+ *
+ *  \brief	Returns the external fifo occupancy
+ *
+ *  \param	pool_id:	Pool ID
+ *
+ *  \return	External fifo occupancy
+ *
+ **************************************************************************/
+static u32 bmgr_get_ext_fifo_occupancy(u8 pool_id)
+{
+	return RD_REG_32(BMGR_POOL_EXT_FIFO_OCC_REG_ADDR(BM_BASE, pool_id));
+}
+
+/**************************************************************************
+ *! \fn	bmgr_get_pool_allocated_counter
+ **************************************************************************
+ *
+ *  \brief	Returns the number of allocated buffers in pool
+ *
+ *  \param	pool_id:	Pool ID
+ *
+ *  \return	Number of allocated buffers
+ *
+ **************************************************************************/
+static u32 bmgr_get_pool_allocated_counter(u8 pool_id)
+{
+	return RD_REG_32(BMGR_POOL_ALLOCATED_COUNTER_REG_ADDR(BM_BASE,
+			 pool_id));
+}
+
+/**************************************************************************
+ *! \fn	bmgr_get_pool_pop_counter
+ **************************************************************************
+ *
+ *  \brief	Returns the number of pop operations on pool
+ *
+ *  \param	pool_id:	Pool ID
+ *
+ *  \return	Number of pop operations
+ *
+ **************************************************************************/
+static u32 bmgr_get_pool_pop_counter(u8 pool_id)
+{
+	return RD_REG_32(BMGR_POOL_POP_COUNTER_REG_ADDR(BM_BASE,
+			 pool_id));
+}
+
+/**************************************************************************
+ *! \fn	bmgr_get_pool_push_counter
+ **************************************************************************
+ *
+ *  \brief	Returns the number of push operations on pool
+ *
+ *  \param	pool_id:	Pool ID
+ *
+ *  \return	Number of push operations
+ *
+ **************************************************************************/
+static u32 bmgr_get_pool_push_counter(u8 pool_id)
+{
+	return RD_REG_32(BMGR_POOL_PUSH_COUNTER_REG_ADDR(BM_BASE, pool_id));
+}
+
+/**************************************************************************
+ *! \fn	bmgr_get_pool_burst_write_counter
+ **************************************************************************
+ *
+ *  \brief	Returns the burst write counter
+ *
+ *  \param	pool_id:	Pool ID
+ *
+ *  \return	Burst write counter
+ *
+ **************************************************************************/
+static u32 bmgr_get_pool_burst_write_counter(u8 pool_id)
+{
+	return RD_REG_32(BMGR_DDR_BURST_WRITE_COUNTER_REG_ADDR(BM_BASE,
+			 pool_id));
+}
+
+/**************************************************************************
+ *! \fn	bmgr_get_pool_burst_read_counter
+ **************************************************************************
+ *
+ *  \brief	Returns the burst read counter
+ *
+ *  \param	pool_id:	Pool ID
+ *
+ *  \return	Burst read counter
+ *
+ **************************************************************************/
+static u32 bmgr_get_pool_burst_read_counter(u8 pool_id)
+{
+	return RD_REG_32(BMGR_DDR_BURST_READ_COUNTER_REG_ADDR(BM_BASE,
+			 pool_id));
+}
+
+/**************************************************************************
+ *! \fn	bmgr_set_pool_watermark_low_threshold
+ **************************************************************************
+ *
+ *  \brief	Sets the watermark low threshold
+ *
+ *  \param	pool_id:	Pool ID
+ *  \param	threshold:	low threshold
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+static s32 bmgr_set_pool_watermark_low_threshold(u8 pool_id, u32 threshold)
+{
+	WR_REG_32(BMGR_POOL_WATERMARK_LOW_THRESHOLD_REG_ADDR(BM_BASE, pool_id),
+		  threshold);
+
+	return RC_SUCCESS;
+}
+
+/**************************************************************************
+ *! \fn	bmgr_get_pool_watermark_low_threshold
+ **************************************************************************
+ *
+ *  \brief	Returns the watermark low threshold
+ *
+ *  \param	pool_id:	Pool ID
+ *
+ *  \return	Low threshold
+ *
+ **************************************************************************/
+static u32 bmgr_get_pool_watermark_low_threshold(u8 pool_id)
+{
+	return RD_REG_32(BMGR_POOL_WATERMARK_LOW_THRESHOLD_REG_ADDR(BM_BASE,
+			 pool_id));
+}
+
+/**************************************************************************
+ *! \fn	bmgr_get_policy_null_counter
+ **************************************************************************
+ *
+ *  \brief	Returns the policy null counter
+ *
+ *  \param	policy_id:	Policy ID
+ *
+ *  \return	Policy null counter
+ *
+ **************************************************************************/
+static u32 bmgr_get_policy_null_counter(u8 policy_id)
+{
+	return RD_REG_32(BMGR_POLICY_NULL_COUNTER_REG_ADDR(BM_BASE,
+			 policy_id));
+}
+
+/**************************************************************************
+ *! \fn	bmgr_set_policy_max_allowed_per_policy
+ **************************************************************************
+ *
+ *  \brief	Sets policy max allowed buffers per policy
+ *
+ *  \param	policy_id:	Policy ID
+ *  \param	max_allowed:	Max allowed per this policy
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+static s32 bmgr_set_policy_max_allowed_per_policy(u8 policy_id,
+						  u32 max_allowed)
+{
+	WR_REG_32(BMGR_POLICY_MAX_ALLOWED_ADDR(BM_RAM_BASE, policy_id),
+		  max_allowed);
+
+	return RC_SUCCESS;
+}
+
+/**************************************************************************
+ *! \fn	bmgr_get_policy_max_allowed_per_policy
+ **************************************************************************
+ *
+ *  \brief	Returns the policy max allowed buffers per policy
+ *
+ *  \param	policy_id:	Policy ID
+ *
+ *  \return	Max allowed per this policy
+ *
+ **************************************************************************/
+static u32 bmgr_get_policy_max_allowed_per_policy(u8 policy_id)
+{
+	return RD_REG_32(BMGR_POLICY_MAX_ALLOWED_ADDR(BM_RAM_BASE,
+			 policy_id));
+}
+
+/**************************************************************************
+ *! \fn	bmgr_set_policy_min_guaranteed_per_policy
+ **************************************************************************
+ *
+ *  \brief	Sets minimum guaranteed per policy
+ *
+ *  \param	min_guaranteed:	Minimum guaranteed per policy
+ *  \param	policy_id:	Policy ID
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+static s32 bmgr_set_policy_min_guaranteed_per_policy(u8 policy_id,
+						     u32 min_guaranteed)
+{
+	WR_REG_32(BMGR_POLICY_MIN_GUARANTEED_ADDR(BM_RAM_BASE, policy_id),
+		  min_guaranteed);
+
+	return RC_SUCCESS;
+}
+
+/**************************************************************************
+ *! \fn	bmgr_get_policy_min_guaranteed_per_policy
+ **************************************************************************
+ *
+ *  \brief	Returns minimum guaranteed per policy
+ *
+ *  \param	policy_id:	Policy ID
+ *
+ *  \return	Minimum guaranteed per policy
+ *
+ **************************************************************************/
+static u32 bmgr_get_policy_min_guaranteed_per_policy(u8 policy_id)
+{
+	return RD_REG_32(BMGR_POLICY_MIN_GUARANTEED_ADDR(BM_RAM_BASE,
+			 policy_id));
+}
+
+/**************************************************************************
+ *! \fn	bmgr_set_policy_group_association
+ **************************************************************************
+ *
+ *  \brief	Maps group to policy
+ *
+ *  \param	policy_id:	Policy ID
+ *  \param	group_id:	Group ID
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+static s32 bmgr_set_policy_group_association(u8 policy_id, u8 group_id)
+{
+	WR_REG_32(BMGR_POLICY_GROUP_ASSOCIATED_ADDR(BM_RAM_BASE, policy_id),
+		  (u32)group_id);
+
+	return RC_SUCCESS;
+}
+
+/**************************************************************************
+ *! \fn	bmgr_get_policy_group_association
+ **************************************************************************
+ *
+ *  \brief	Returns the mapped group to this policy
+ *
+ *  \param	policy_id:	Policy ID
+ *
+ *  \return	The mapped group ID
+ *
+ **************************************************************************/
+static u32 bmgr_get_policy_group_association(u8 policy_id)
+{
+	return RD_REG_32(BMGR_POLICY_GROUP_ASSOCIATED_ADDR(BM_RAM_BASE,
+			 policy_id));
+}
+
+/**************************************************************************
+ *! \fn	bmgr_set_policy_pool_mapping
+ **************************************************************************
+ *
+ *  \brief	Maps pool to policy (With priority)
+ *
+ *  \param	policy_id:	Policy ID
+ *  \param	pool_id:	Pool ID
+ *  \param	priority:	0 (highest priority) - 3 scale
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+static s32 bmgr_set_policy_pool_mapping(u8 policy_id, u8 pool_id, u8 priority)
+{
+	u32	value;
+	u32	mask;
+
+	if (priority >= bmgr_policy_pool_priority_max)
+		pr_err("highest priority %d allowed is %d\n",
+		       priority, bmgr_policy_pool_priority_max - 1);
+
+	value = RD_REG_32(BMGR_POLICY_POOLS_MAPPING_ADDR(BM_RAM_BASE,
+							 policy_id));
+
+	// Clear relevant byte
+	mask = 0xFF << (8 * priority);
+	value &= ~mask;
+
+	// Prepare the value to be wriiten
+	value |= (pool_id << (8 * priority));
+
+	WR_REG_32(BMGR_POLICY_POOLS_MAPPING_ADDR(BM_RAM_BASE, policy_id),
+		  value);
+
+	return RC_SUCCESS;
+}
+
+/**************************************************************************
+ *! \fn	bmgr_get_policy_pools_mapping
+ **************************************************************************
+ *
+ *  \brief	Returns the mapped pools to this policy
+ *
+ *  \param	policy_id:	Policy ID
+ *
+ *  \return	The mapped pool ID
+ *
+ **************************************************************************/
+static u32 bmgr_get_policy_pools_mapping(u8 policy_id)
+{
+	return RD_REG_32(BMGR_POLICY_POOLS_MAPPING_ADDR(BM_RAM_BASE,
+			 policy_id));
+}
+
+/**************************************************************************
+ *! \fn	bmgr_set_policy_max_allowed_per_pool
+ **************************************************************************
+ *
+ *  \brief	Sets policy max allowed per pool
+ *
+ *  \param	policy_id:	Policy ID
+ *  \param	pool_id:	Pool ID
+ *  \param	max_allowed:	Max allowed for this pool
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+static s32 bmgr_set_policy_max_allowed_per_pool(u8 policy_id,
+						u8 pool_id,
+						u32 max_allowed)
+{
+	WR_REG_32(BMGR_POLICY_MAX_ALLOWED_PER_POOL_ADDR(BM_RAM_BASE,
+							policy_id,
+							pool_id),
+							max_allowed);
+
+	return RC_SUCCESS;
+}
+
+/**************************************************************************
+ *! \fn	bmgr_get_policy_max_allowed_per_pool
+ **************************************************************************
+ *
+ *  \brief	Returns the max allowed per pool
+ *
+ *  \param	policy_id:	Policy ID
+ *  \param	pool_id:	Pool ID
+ *
+ *  \return	Max allowed for this pool
+ *
+ **************************************************************************/
+static u32 bmgr_get_policy_max_allowed_per_pool(u8 policy_id, u8 pool_id)
+{
+	return RD_REG_32(BMGR_POLICY_MAX_ALLOWED_PER_POOL_ADDR(BM_RAM_BASE,
+							       policy_id,
+							       pool_id));
+}
+
+/**************************************************************************
+ *! \fn	bmgr_get_policy_number_of_allocated_buffers
+ **************************************************************************
+ *
+ *  \brief	Returns the number of allocated buffer in this policy
+ *
+ *  \param	policy_id:	Policy ID
+ *
+ *  \return	Number of allocated buffer in this policy
+ *
+ **************************************************************************/
+static u32 bmgr_get_policy_number_of_allocated_buffers(u8 policy_id)
+{
+	return RD_REG_32(BMGR_POLICY_ALLOC_BUFF_COUNTER_ADDR(BM_RAM_BASE,
+			 policy_id));
+}
+
+/**************************************************************************
+ *! \fn	bmgr_get_policy_num_allocated_per_pool
+ **************************************************************************
+ *
+ *  \brief	Returns the number of allocated buffers per pool in this policy
+ *
+ *  \param	policy_id:	Policy ID
+ *  \param	pool_id:	Pool ID
+ *
+ *  \return	Number of allocated buffers per pool in this policy
+ *
+ **************************************************************************/
+static u32 bmgr_get_policy_num_allocated_per_pool(u8 policy_id,
+						  u8 pool_id)
+{
+	return RD_REG_32(BMGR_POLICY_ALLOC_BUFF_PER_POOL_COUNTER_ADDR(
+			 BM_RAM_BASE, pool_id, policy_id));
+}
+
+s32 bmgr_test_dma(u32 num_bytes)
+{
+	void		*addr = NULL;
+	void		*addr1 = NULL;
+	dma_addr_t	dma;
+	dma_addr_t	dma1;
+
+	addr = dma_alloc_coherent(&this->pdev->dev,
+				   2 * sizeof(u32),
+				   &dma,
+				   GFP_KERNEL | GFP_DMA);
+	if (!addr) {
+		dev_err(&this->pdev->dev, "Could not allocate addr using dmam_alloc_coherent\n");
+		return -ENOMEM;
+	}
+
+	addr1 = dma_alloc_coherent(&this->pdev->dev,
+				    2 * sizeof(u32),
+				    &dma1,
+				    GFP_KERNEL | GFP_DMA);
+	if (!addr1) {
+		dev_err(&this->pdev->dev, "Could not allocate addr1 using dmam_alloc_coherent\n");
+		dma_free_coherent(&this->pdev->dev, 2 * sizeof(u32), addr, dma);
+		return -ENOMEM;
+	}
+
+	WR_REG_32(addr, 0);
+	WR_REG_32((addr + 4), 0);
+	WR_REG_32(addr1, 0xcafecafe);
+	WR_REG_32((addr1 + 4), 0x12345678);
+
+	pr_info("ADDRESSES ======> 0x%x[0x%x] ; 0x%x[0x%x] ; 0x%x[0x%x] ; 0x%x[0x%x]\n",
+		(u32)addr, (u32)dma, (u32)(addr + 4), (u32)(dma + 4),
+		(u32)(addr1), (u32)(dma1), (u32)(addr1 + 4), (u32)(dma1 + 4));
+	pr_info("Before TEST ======> 0x%x ; 0x%x ; 0x%x ; 0x%x\n",
+		RD_REG_32(addr), RD_REG_32(addr + 4),
+		RD_REG_32(addr1), RD_REG_32(addr1 + 4));
+	copy_dma((u32)dma1, (u32)dma, (0x80100000 | (8 * num_bytes)));
+	pr_info("After TEST ======> 0x%x ; 0x%x ; 0x%x ; 0x%x\n",
+		RD_REG_32(addr), RD_REG_32(addr + 4),
+		RD_REG_32(addr1), RD_REG_32(addr1 + 4));
+
+	if ((RD_REG_32(addr) == RD_REG_32(addr1)) &&
+	    (RD_REG_32(addr + 4) == RD_REG_32(addr1 + 4))) {
+		pr_info("\nDMA Test OK\n");
+	} else {
+		pr_info("\nDMA Test Failed\n");
+	}
+
+	dma_free_coherent(&this->pdev->dev, 2 * sizeof(u32), addr1, dma1);
+	dma_free_coherent(&this->pdev->dev, 2 * sizeof(u32), addr, dma);
+
+	return 0;
+}
+
+/**************************************************************************
+ *! \fn	bmgr_pop_buffer
+ **************************************************************************
+ *
+ *  \brief	Pops a buffer from the buffer manager
+ *
+ *  \param	buff_info:	Buffer information
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+s32 bmgr_pop_buffer(struct bmgr_buff_info * const buff_info)
+{
+	u32	address = BMGR_DATAPATH_BASE;
+	u32	index = 0;
+	u32	low = 0;
+	u32	high = 0;
+	void	*addr;
+	dma_addr_t	dma;
+	u16	offset = 0;
+
+	if (!buff_info) {
+		pr_err("bmgr_pop_buffer: buff_info is NULL\n");
+		return -EINVAL;
+	}
+
+	if (buff_info->num_allocs > PP_BMGR_MAX_BURST_IN_POP) {
+		pr_err("bmgr_pop_buffer: num_allocs %d is limited with %d\n",
+		       buff_info->num_allocs, PP_BMGR_MAX_BURST_IN_POP);
+		return -EINVAL;
+	}
+
+	// Write the Single/Burst bit
+	if (buff_info->num_allocs > 1)
+		address |= BIT(16);
+
+	// Write the Policy
+	address |= (buff_info->policy_id << 8);
+
+	addr = dmam_alloc_coherent(&this->pdev->dev,
+				   2 * sizeof(u32) * buff_info->num_allocs,
+				   &dma,
+				   GFP_KERNEL | GFP_DMA);
+	if (!addr) {
+		dev_err(&this->pdev->dev, "Could not allocate using dmam_alloc_coherent\n");
+		return -ENOMEM;
+	}
+
+	copy_dma(address, (u32)dma,
+		 (0x80100000 | (8 * buff_info->num_allocs)));
+
+	for (index = 0; index < buff_info->num_allocs; index++) {
+		low = RD_REG_32(addr + offset);
+		high = RD_REG_32(addr + offset + 4);
+		offset += 8;
+
+		pr_info("POP ======> 0x%x ; 0x%x\n", low, high);
+
+		buff_info->addr_low[index] = low;
+		buff_info->addr_high[index] = high & 0xF;
+		buff_info->pool_id[index] = (high & 0xFF000000) >> 24;
+
+		pr_info("bmgr_pop_buffer: ---> pop buffer from address %p (pool %d, addr low %p, addr high %p)\n",
+			(void *)address, buff_info->pool_id[index],
+			(void *)buff_info->addr_low[index],
+			(void *)buff_info->addr_high[index]);
+
+		if (buff_info->pool_id[index] >= this->driver_db.max_pools) {
+			this->driver_db.policies[buff_info->policy_id].
+				num_allocated_buffers += index;
+			pr_info("Can't pop from policy %d\n",
+				buff_info->policy_id);
+			return -ENOSPC;
+		}
+
+		this->driver_db.pools[buff_info->pool_id[index]].
+			num_allocated_buffers++;
+	}
+
+	this->driver_db.policies[buff_info->policy_id].
+		num_allocated_buffers += buff_info->num_allocs;
+
+	return RC_SUCCESS;
+}
+EXPORT_SYMBOL(bmgr_pop_buffer);
+
+/**************************************************************************
+ *! \fn	bmgr_push_buffer
+ **************************************************************************
+ *
+ *  \brief	Pushes a buffer back to the buffer manager
+ *
+ *  \param	buff_info:	Buffer information
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+s32 bmgr_push_buffer(struct bmgr_buff_info * const buff_info)
+{
+	u32	address = BMGR_DATAPATH_BASE;
+	u32	value = 0;
+	u32	index = 0;
+	u32	ptr2push[2 * PP_BMGR_MAX_BURST_IN_POP]; // 64 bit per allocation
+
+	if (!buff_info) {
+		pr_err("bmgr_push_buffer: buff_info is NULL\n");
+		return -EINVAL;
+	}
+
+	if (buff_info->num_allocs > PP_BMGR_MAX_BURST_IN_POP) {
+		pr_err("bmgr_push_buffer: num_allocs %d is limited with %d\n",
+		       buff_info->num_allocs, PP_BMGR_MAX_BURST_IN_POP);
+		return -EINVAL;
+	}
+
+	// Write the Single/Burst bit
+	if (buff_info->num_allocs > 1)
+		address |= BIT(16);
+
+	// Write the Policy
+	address |= (buff_info->policy_id << 8);
+
+	// write to ddr
+	for (index = 0; index < buff_info->num_allocs; index++) {
+		ptr2push[2*index] = buff_info->addr_low[index];
+		value = (buff_info->addr_high[index] & 0xF) |
+			 ((buff_info->pool_id[index] & 0xFF) << 24);
+		ptr2push[2*index + 1] = value;
+
+		pr_info("bmgr_push_buffer: <--- push buffer to address %p (pool %d, addr low %p, addr high %p, value %d)\n",
+			(void *)address, buff_info->pool_id[index],
+			(void *)buff_info->addr_low[index],
+			(void *)buff_info->addr_high[index], value);
+
+		this->driver_db.pools[buff_info->pool_id[index]].
+			num_deallocated_buffers++;
+	}
+
+	copy_dma((u32)&ptr2push[0], address,
+		 (0x80100000 | (8 * buff_info->num_allocs))/*0x80100008*/);
+
+	this->driver_db.policies[buff_info->policy_id].
+		num_deallocated_buffers += buff_info->num_allocs;
+
+	return RC_SUCCESS;
+}
+EXPORT_SYMBOL(bmgr_push_buffer);
+
+/**************************************************************************
+ *! \fn	bmgr_driver_init
+ **************************************************************************
+ *
+ *  \brief  Initializes the buffer manager driver.
+ *          Must be the first driver's function to be called
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+s32 bmgr_driver_init(void)
+{
+	u16 index;
+	u16 idx2;
+
+	pr_info("Buffer Manager driver is initializing....");
+
+	// @lock
+	bmgr_db_lock();
+
+	bmgr_set_control();
+	bmgr_configure_ocp_master();
+
+	// Reset group reserved buffers
+	for (index = 0; index < this->driver_db.max_groups; index++)
+		bmgr_set_group_reserved_buffers(index, 0);
+
+	// Init RAM
+	for (index = 0; index < this->driver_db.max_policies; index++) {
+		bmgr_set_policy_max_allowed_per_policy(index, 0);
+		bmgr_set_policy_min_guaranteed_per_policy(index, 0);
+		bmgr_set_policy_group_association(index, 0);
+		WR_REG_32(BMGR_POLICY_POOLS_MAPPING_ADDR(BM_RAM_BASE, index),
+			  0);
+		WR_REG_32(BMGR_POLICY_ALLOC_BUFF_COUNTER_ADDR(BM_RAM_BASE,
+							      index), 0);
+		for (idx2 = 0; idx2 < this->driver_db.max_pools; idx2++) {
+			bmgr_set_policy_max_allowed_per_pool(index, idx2, 0);
+			WR_REG_32(BMGR_POLICY_ALLOC_BUFF_PER_POOL_COUNTER_ADDR(
+					BM_RAM_BASE, idx2, index), 0);
+		}
+	}
+
+	// @unlock
+	bmgr_db_unlock();
+
+	pr_info("Done\n");
+
+	return RC_SUCCESS;
+}
+EXPORT_SYMBOL(bmgr_driver_init);
+
+/**************************************************************************
+ *! \fn	bmgr_pool_configure
+ **************************************************************************
+ *
+ *  \brief	Configure a Buffer Manager pool
+ *
+ *  \param	pool_params:	Pool param from user
+ *  \param	pool_id[OUT]:	Pool ID
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+s32 bmgr_pool_configure(const struct bmgr_pool_params * const pool_params,
+			u8 * const pool_id)
+{
+	s32	status = RC_SUCCESS;
+	void	*pointers_table = NULL;
+	u32	index = 0;
+	u32	*temp_pointers_table_ptr = NULL;
+	u64	user_array_ptr;
+	u32	phy_ll_base;
+	u32	val = 0;
+
+	pr_info("Configuring buffer manager pool...");
+
+	// @lock
+	bmgr_db_lock();
+
+	// Validity check
+	status = bmgr_is_pool_params_valid(pool_params);
+	if (status != RC_SUCCESS)
+		goto unlock;
+
+	*pool_id = 0;
+
+	// Find next available slot in pools db
+	for (index = 0; index < this->driver_db.max_pools; index++) {
+		if (this->driver_db.pools[index].is_busy == 0) {
+			*pool_id = index;
+			break;
+		}
+	}
+
+	// If not found (Can be done also using num_pools)
+	if (index == this->driver_db.max_pools) {
+		pr_err("bmgr_pool_configure: pools DB is full!");
+		status = -EIO;
+
+		goto unlock;
+	}
+
+	// Allocate pool_param->pool_num_of_buff * POINTER_SIZE bytes array
+	pointers_table = devm_kzalloc(&this->pdev->dev,
+				      sizeof(u32) * pool_params->num_buffers,
+				      GFP_KERNEL);
+	if (!pointers_table) {
+		pr_err("bmgr_pool_configure: Failed to allocate pointers_table, num_buffers %d",
+		       pool_params->num_buffers);
+		status = -ENOMEM;
+		goto unlock;
+	}
+
+	temp_pointers_table_ptr = (u32 *)pointers_table;
+
+	user_array_ptr = (pool_params->base_addr_low) |
+			((u64)pool_params->base_addr_high << 32);
+
+	for (index = 0; index < pool_params->num_buffers; index++) {
+		u32 temp = user_array_ptr >> 6;
+		// for debugging ...
+		if (index == 0 || index == pool_params->num_buffers - 1)
+			pr_info("bmgr_pool_configure: index %d) writing 0x%x to 0x%x\n",
+				index, temp, (u32)temp_pointers_table_ptr);
+
+		*temp_pointers_table_ptr = user_array_ptr >> 6;
+		temp_pointers_table_ptr++;
+		user_array_ptr += pool_params->size_of_buffer;
+	}
+
+	phy_ll_base = dma_map_single(&this->pdev->dev,
+				     (void *)pointers_table,
+				     (pool_params->num_buffers * 4),
+				     DMA_TO_DEVICE);
+
+	status = bmgr_set_pool_size(*pool_id, pool_params->num_buffers);
+	if (status != RC_SUCCESS)
+		goto free_memory;
+
+	status = bmgr_set_ext_fifo_base_addr_low(*pool_id, phy_ll_base);
+	if (status != RC_SUCCESS)
+		goto free_memory;
+
+	status = bmgr_set_ext_fifo_base_addr_high(*pool_id, 0);
+	if (status != RC_SUCCESS)
+		goto free_memory;
+
+	status = bmgr_set_ext_fifo_occupancy(*pool_id,
+					     pool_params->num_buffers);
+	if (status != RC_SUCCESS)
+		goto free_memory;
+
+	// Verify group is not full
+	val = pool_params->group_id;
+	index = this->driver_db.groups[val].num_pools_in_group;
+	if (index >= PP_BMGR_MAX_POOLS_IN_GROUP) {
+		pr_err("bmgr_pool_configure: Group %d is full. num_pools_in_group %d",
+		       pool_params->group_id, index);
+		status = -EIO;
+
+		goto free_memory;
+	}
+
+	// The group was unused
+	if (index == 0)
+		this->driver_db.num_groups++;
+
+	this->driver_db.groups[pool_params->group_id].pools[index] = *pool_id;
+	this->driver_db.groups[pool_params->group_id].available_buffers +=
+		pool_params->num_buffers;
+	this->driver_db.groups[pool_params->group_id].num_pools_in_group++;
+	// Group's reserved buffers will be updated when configuring the policy
+
+	val = this->driver_db.groups[pool_params->group_id].available_buffers;
+	status = bmgr_set_group_available_buffers(pool_params->group_id, val);
+	if (status != RC_SUCCESS)
+		goto free_memory;
+
+	status = bmgr_set_pcu_fifo_base_address(*pool_id,
+						BMGR_START_PCU_FIFO_SRAM_ADDR +
+						(*pool_id *
+						BMGR_DEFAULT_PCU_FIFO_SIZE));
+	if (status != RC_SUCCESS)
+		goto free_memory;
+
+	status = bmgr_set_pcu_fifo_size(*pool_id, BMGR_DEFAULT_PCU_FIFO_SIZE);
+	if (status != RC_SUCCESS)
+		goto free_memory;
+
+	status = bmgr_set_pcu_fifo_occupancy(*pool_id, 0);
+	if (status != RC_SUCCESS)
+		goto free_memory;
+
+	val = BMGR_DEFAULT_PCU_FIFO_LOW_THRESHOLD;
+	status = bmgr_set_pcu_fifo_prog_empty(*pool_id, val);
+	if (status != RC_SUCCESS)
+		goto free_memory;
+
+	val = BMGR_DEFAULT_PCU_FIFO_HIGH_THRESHOLD;
+	status = bmgr_set_pcu_fifo_prog_full(*pool_id, val);
+	if (status != RC_SUCCESS)
+		goto free_memory;
+
+	val = BMGR_DEFAULT_WATERMARK_LOW_THRESHOLD;
+	status = bmgr_set_pool_watermark_low_threshold(*pool_id, val);
+	if (status != RC_SUCCESS)
+		goto free_memory;
+
+	if (pool_params->flags & POOL_ENABLE_FOR_MIN_GRNT_POLICY_CALC) {
+		status = bmgr_enable_min_guarantee_per_pool(*pool_id, 1);
+		if (status != RC_SUCCESS)
+			goto free_memory;
+	}
+
+	status = bmgr_pool_reset_fifo(*pool_id);
+	if (status != RC_SUCCESS)
+		goto free_memory;
+
+	status = bmgr_pool_enable(*pool_id, 1);
+	if (status != RC_SUCCESS)
+		goto free_memory;
+
+	// Update pool's DB
+	memcpy(&this->driver_db.pools[*pool_id].pool_params,
+	       pool_params, sizeof(struct bmgr_pool_params));
+	this->driver_db.pools[*pool_id].is_busy = 1;
+	this->driver_db.pools[*pool_id].internal_pointers_tables =
+			temp_pointers_table_ptr;
+	this->driver_db.num_pools++;
+
+	bmgr_wait_for_init_completion();
+
+	// @unlock
+	bmgr_db_unlock();
+
+	pr_info("Done");
+
+	// OK
+	return status;
+
+free_memory:
+	// free pointers_table
+		kfree(pointers_table);
+unlock:
+	// @unlock
+	bmgr_db_unlock();
+
+	return status;
+}
+EXPORT_SYMBOL(bmgr_pool_configure);
+
+/**************************************************************************
+ *! \fn	bmgr_policy_configure
+ **************************************************************************
+ *
+ *  \brief	Configure a Buffer Manager policy
+ *
+ *  \param	policy_params:	Policy param from user
+ *  \param	policy_id[OUT]:	Policy ID
+ *
+ *  \return	RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+s32 bmgr_policy_configure(const struct bmgr_policy_params * const policy_params,
+			  u8 * const policy_id)
+{
+	s32	status = RC_SUCCESS;
+	u16	index = 0;
+	u32	id = 0;
+	u32	min = 0;
+	u32	max = 0;
+
+	pr_info("Configuring buffer manager policy...");
+
+	// @lock
+	bmgr_db_lock();
+
+	// Validity check
+	status = bmgr_is_policy_params_valid(policy_params);
+	if (status != RC_SUCCESS)
+		goto unlock;
+
+	*policy_id = 0;
+
+	// Find next available slot in policy db
+	for (index = 0; index < this->driver_db.max_policies; index++) {
+		if (this->driver_db.policies[index].is_busy == 0) {
+			*policy_id = index;
+			break;
+		}
+	}
+
+	// If not found (Can be done also using num_policies)
+	if (index == this->driver_db.max_policies) {
+		pr_err("No free policy!");
+		status = -EIO;
+
+		goto unlock;
+	}
+
+	status = bmgr_set_policy_group_association(*policy_id,
+						   policy_params->group_id);
+	if (status != RC_SUCCESS)
+		goto unlock;
+
+	max = policy_params->max_allowed;
+	status = bmgr_set_policy_max_allowed_per_policy(*policy_id,
+							max);
+	if (status != RC_SUCCESS)
+		goto unlock;
+
+	min = policy_params->min_guaranteed;
+	status = bmgr_set_policy_min_guaranteed_per_policy(*policy_id,
+							   min);
+	if (status != RC_SUCCESS)
+		goto unlock;
+
+	// Set the group's reserved buffers
+	this->driver_db.groups[policy_params->group_id].reserved_buffers =
+		bmgr_get_group_reserved_buffers(policy_params->group_id);
+	this->driver_db.groups[policy_params->group_id].reserved_buffers +=
+			policy_params->min_guaranteed;
+
+	id = this->driver_db.groups[policy_params->group_id].reserved_buffers;
+	status = bmgr_set_group_reserved_buffers(policy_params->group_id, id);
+	if (status != RC_SUCCESS)
+		goto unlock;
+
+	for (index = 0; index < policy_params->num_pools_in_policy; index++) {
+		max = policy_params->pools_in_policy[index].max_allowed;
+		status = bmgr_set_policy_max_allowed_per_pool(*policy_id,
+							      index,
+							      max);
+		if (status != RC_SUCCESS)
+			goto unlock;
+
+		id = policy_params->pools_in_policy[index].pool_id;
+		status = bmgr_set_policy_pool_mapping(*policy_id,
+						      id,
+						      index);
+		if (status != RC_SUCCESS)
+			goto unlock;
+	}
+
+	// Update Policy DB
+	this->driver_db.num_policies++;
+	this->driver_db.policies[*policy_id].is_busy = 1;
+	memcpy(&this->driver_db.policies[*policy_id].policy_params,
+	       policy_params,
+	       sizeof(struct bmgr_policy_params));
+
+	bmgr_wait_for_init_completion();
+
+	// @unlock
+	bmgr_db_unlock();
+
+	pr_info("Done");
+
+	// OK
+	return status;
+
+unlock:
+	// @unlock
+	bmgr_db_unlock();
+
+	// Failure
+	return status;
+}
+EXPORT_SYMBOL(bmgr_policy_configure);
+
+void print_hw_stats(void)
+{
+	u32	counter;
+	u16	max_pools = this->driver_db.num_pools;
+	u16	max_groups = this->driver_db.num_groups;
+	u16	max_policies = this->driver_db.num_policies;
+	u16	idx;
+	u16	idx1;
+
+	counter = bmgr_get_control();
+	pr_info("Control = 0x%x\n", counter);
+
+	counter = RD_REG_32(BMGR_POOL_MIN_GRNT_MASK_REG_ADDR(BM_BASE));
+	pr_info("Pool Min Grant Bit Mask = 0x%x\n", counter);
+
+	counter = RD_REG_32(BMGR_POOL_ENABLE_REG_ADDR(BM_BASE));
+	pr_info("Pool Enable = 0x%x\n", counter);
+
+	counter = RD_REG_32(BMGR_POOL_FIFO_RESET_REG_ADDR(BM_BASE));
+	pr_info("Pool FIFO Reset = 0x%x\n", counter);
+
+	counter = bmgr_get_ocp_burst_size();
+	pr_info("OCP Master Burst Size = 0x%x\n", counter);
+
+	counter = RD_REG_32(BMGR_OCPM_NUM_OF_BURSTS_REG_ADDR(BM_BASE));
+	pr_info("OCP Master Number Of Bursts = 0x%x\n", counter);
+
+	counter = RD_REG_32(BMGR_STATUS_REG_ADDR(BM_BASE));
+	pr_info("Status = 0x%x\n", counter);
+
+	pr_info("Pool size:\n");
+	pr_info("==========\n");
+	for (idx = 0; idx < max_pools; idx++) {
+		counter = bmgr_get_pool_size(idx);
+		pr_info("Pool %d size = 0x%x\n", idx, counter);
+	}
+
+	pr_info("Group available buffers:\n");
+	pr_info("========================\n");
+	for (idx = 0; idx < max_groups; idx++) {
+		counter = bmgr_get_group_available_buffers(idx);
+		pr_info("Group %d available buffers = 0x%x\n", idx, counter);
+	}
+
+	pr_info("Group reserved buffers:\n");
+	pr_info("=======================\n");
+	for (idx = 0; idx < max_groups; idx++) {
+		counter = bmgr_get_group_reserved_buffers(idx);
+		pr_info("Group %d reserved buffers = 0x%x\n", idx, counter);
+	}
+
+	pr_info("PCU FIFO base address:\n");
+	pr_info("======================\n");
+	for (idx = 0; idx < max_pools; idx++) {
+		counter = bmgr_get_pcu_fifo_base_address(idx);
+		pr_info("Pool %d PCU FIFO base address = 0x%x\n",
+			idx, counter);
+	}
+
+	pr_info("PCU FIFO size:\n");
+	pr_info("==============\n");
+	for (idx = 0; idx < max_pools; idx++) {
+		counter = bmgr_get_pcu_fifo_size(idx);
+		pr_info("Pool %d PCU FIFO size = 0x%x\n", idx, counter);
+	}
+
+	pr_info("PCU FIFO occupancy:\n");
+	pr_info("===================\n");
+	for (idx = 0; idx < max_pools; idx++) {
+		counter = bmgr_get_pcu_fifo_occupancy(idx);
+		pr_info("Pool %d PCU FIFO occupancy = 0x%x\n", idx, counter);
+	}
+
+	pr_info("PCU FIFO Prog empty:\n");
+	pr_info("====================\n");
+	for (idx = 0; idx < max_pools; idx++) {
+		counter = bmgr_get_pcu_fifo_prog_empty(idx);
+		pr_info("Pool %d PCU FIFO Prog empty = 0x%x\n", idx, counter);
+	}
+
+	pr_info("PCU FIFO Prog full:\n");
+	pr_info("===================\n");
+	for (idx = 0; idx < max_pools; idx++) {
+		counter = bmgr_get_pcu_fifo_prog_full(idx);
+		pr_info("Pool %d PCU FIFO Prog full = 0x%x\n", idx, counter);
+	}
+
+	pr_info("EXT FIFO Base Addr Low:\n");
+	pr_info("=======================\n");
+	for (idx = 0; idx < max_pools; idx++) {
+		counter = bmgr_get_ext_fifo_base_addr_low(idx);
+		pr_info("Pool %d EXT FIFO Base Addr Low = 0x%x\n",
+			idx, counter);
+	}
+
+	pr_info("EXT FIFO Base Addr High:\n");
+	pr_info("========================\n");
+	for (idx = 0; idx < max_pools; idx++) {
+		counter = bmgr_get_ext_fifo_base_addr_high(idx);
+		pr_info("Pool %d EXT FIFO Base Addr High = 0x%x\n",
+			idx, counter);
+	}
+
+	pr_info("EXT FIFO occupancy:\n");
+	pr_info("===================\n");
+	for (idx = 0; idx < max_pools; idx++) {
+		counter = bmgr_get_ext_fifo_occupancy(idx);
+		pr_info("Pool %d EXT FIFO occupancy = 0x%x\n", idx, counter);
+	}
+
+	pr_info("Pool allocated counter:\n");
+	pr_info("=======================\n");
+	for (idx = 0; idx < max_pools; idx++) {
+		counter = bmgr_get_pool_allocated_counter(idx);
+		pr_info("Pool %d allocated counter = 0x%x\n", idx, counter);
+	}
+
+	pr_info("Pool pop counter:\n");
+	pr_info("=================\n");
+	for (idx = 0; idx < max_pools; idx++) {
+		counter = bmgr_get_pool_pop_counter(idx);
+		pr_info("Pool %d pop counter = 0x%x\n", idx, counter);
+	}
+
+	pr_info("Pool push counter:\n");
+	pr_info("==================\n");
+	for (idx = 0; idx < max_pools; idx++) {
+		counter = bmgr_get_pool_push_counter(idx);
+		pr_info("Pool %d push counter = 0x%x\n", idx, counter);
+	}
+
+	pr_info("Pool DDR Burst write counter:\n");
+	pr_info("=============================\n");
+	for (idx = 0; idx < max_pools; idx++) {
+		counter = bmgr_get_pool_burst_write_counter(idx);
+		pr_info("Pool %d Burst write counter = 0x%x\n", idx, counter);
+	}
+
+	pr_info("Pool DDR Burst read counter:\n");
+	pr_info("============================\n");
+	for (idx = 0; idx < max_pools; idx++) {
+		counter = bmgr_get_pool_burst_read_counter(idx);
+		pr_info("Pool %d Burst read counter = 0x%x\n", idx, counter);
+	}
+
+	pr_info("Pool watermark low threshold:\n");
+	pr_info("=============================\n");
+	for (idx = 0; idx < max_pools; idx++) {
+		counter = bmgr_get_pool_watermark_low_threshold(idx);
+		pr_info("Pool %d watermark low threshold = 0x%x\n",
+			idx, counter);
+	}
+
+	pr_info("Policy NULL counter:\n");
+	pr_info("====================\n");
+	for (idx = 0; idx < max_policies; idx++) {
+		counter = bmgr_get_policy_null_counter(idx);
+		pr_info("Policy %d NULL counter = 0x%x\n", idx, counter);
+	}
+
+	pr_info("Policy Max allowed per policy:\n");
+	pr_info("==============================\n");
+	for (idx = 0; idx < max_policies; idx++) {
+		counter = bmgr_get_policy_max_allowed_per_policy(idx);
+		pr_info("Policy %d Max allowed = 0x%x\n", idx, counter);
+	}
+
+	pr_info("Policy Min guaranteed per policy:\n");
+	pr_info("=================================\n");
+	for (idx = 0; idx < max_policies; idx++) {
+		counter = bmgr_get_policy_min_guaranteed_per_policy(idx);
+		pr_info("Policy %d Min guaranteed = 0x%x\n", idx, counter);
+	}
+
+	pr_info("Policy group association:\n");
+	pr_info("=========================\n");
+	for (idx = 0; idx < max_policies; idx++) {
+		counter = bmgr_get_policy_group_association(idx);
+		pr_info("Policy %d group association = 0x%x\n", idx, counter);
+	}
+
+	pr_info("Policy pools mapping:\n");
+	pr_info("=====================\n");
+	for (idx = 0; idx < max_policies; idx++) {
+		counter = bmgr_get_policy_pools_mapping(idx);
+		pr_info("Policy %d pools mapping = 0x%x\n", idx, counter);
+	}
+
+	pr_info("Policy max allowed per pool:\n");
+	pr_info("============================\n");
+	for (idx = 0; idx < max_policies; idx++) {
+		pr_info("Policy %d:\n", idx);
+		for (idx1 = 0; idx1 < max_pools; idx1++) {
+			counter = bmgr_get_policy_max_allowed_per_pool(idx,
+								       idx1);
+			pr_info("    max allowed per pool with priority %d = 0x%x\n",
+				idx1, counter);
+		}
+	}
+
+	pr_info("Policy num allocated buffers:\n");
+	pr_info("=============================\n");
+	for (idx = 0; idx < max_policies; idx++) {
+		counter = bmgr_get_policy_number_of_allocated_buffers(idx);
+		pr_info("Policy %d num allocated buffers = 0x%x\n",
+			idx, counter);
+	}
+
+	pr_info("Policy num allocated buffers per pool:\n");
+	pr_info("======================================\n");
+	for (idx = 0; idx < max_policies; idx++) {
+		pr_info("Policy %d:\n", idx);
+		for (idx1 = 0; idx1 < max_pools; idx1++) {
+			counter = bmgr_get_policy_num_allocated_per_pool(idx,
+									 idx1);
+			pr_info("    num allocated per pool with priority %d = 0x%x\n",
+				idx1, counter);
+		}
+	}
+}
diff --git a/drivers/net/ethernet/lantiq/ppv4/bm/pp_bm_drv.h b/drivers/net/ethernet/lantiq/ppv4/bm/pp_bm_drv.h
new file mode 100644
index 000000000000..911eb8b07293
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/ppv4/bm/pp_bm_drv.h
@@ -0,0 +1,241 @@
+/*
+ * GPL LICENSE SUMMARY
+ *
+ *  Copyright(c) 2017 Intel Corporation.
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of version 2 of the GNU General Public License as
+ *  published by the Free Software Foundation.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *  The full GNU General Public License is included in this distribution
+ *  in the file called LICENSE.GPL.
+ *
+ *  Contact Information:
+ *  Intel Corporation
+ *  2200 Mission College Blvd.
+ *  Santa Clara, CA  97052
+ */
+
+#ifndef _BMGR_H_
+#define _BMGR_H_
+
+/**********************************
+ **********************************
+ ************ DEFINES *************
+ **********************************
+ **********************************/
+
+/*! \def PP_BMGR_MAX_POOLS
+ *       Max supported pools. Real max defined in the DTS
+ */
+#define PP_BMGR_MAX_POOLS		(16)
+
+/*! \def PP_BMGR_MAX_POOLS_IN_GROUP
+ *       Max pools in group
+ */
+#define PP_BMGR_MAX_POOLS_IN_GROUP	(4)
+
+/*! \def PP_BMGR_MAX_GROUPS
+ *       Max supported groups. Real max defined in the DTS
+ */
+#define PP_BMGR_MAX_GROUPS		(16)
+
+/*! \def PP_BMGR_MAX_POLICIES
+ *       Max supoorted policies. Real max defined in the DTS
+ */
+#define PP_BMGR_MAX_POLICIES		(256)
+
+/*! \def POOL_ENABLE_FOR_MIN_GRNT_POLICY_CALC
+ *       bmgr pools flags (Used in bmgr_pool_params.pool_flags)
+ *       When set pool will be take part in
+ *       policy minimum guaranteed calculation
+ */
+#define POOL_ENABLE_FOR_MIN_GRNT_POLICY_CALC	BIT(0)
+
+/**************************************************************************
+ *! \struct	bmgr_pool_params
+ **************************************************************************
+ *
+ * \brief This structure is used in bmgr_pool_configure API in parameter
+ *
+ **************************************************************************/
+struct bmgr_pool_params {
+	//!< Pool flags
+	u16	flags;
+	//!< Group index which the pool belong to
+	u8	group_id;
+	//!< Amount of buffers in pool
+	u32	num_buffers;
+	//!< Buffer size for this pool (in bytes). Minimum is 64 bytes
+	u32	size_of_buffer;
+	//!< Base address of the pool (low)
+	u32	base_addr_low;
+	//!< Base address of the pool (high)
+	u32	base_addr_high;
+};
+
+/**************************************************************************
+ *! \struct	bmgr_group_params
+ **************************************************************************
+ *
+ * \brief This structure is used for buffer manager database
+ *
+ **************************************************************************/
+struct bmgr_group_params {
+	//!< available buffers in group
+	u32	available_buffers;
+	//!< reserved buffers in this group
+	u32	reserved_buffers;
+	//!< Pools in policy
+	u8	pools[PP_BMGR_MAX_POOLS];
+};
+
+/**************************************************************************
+ *! \struct	bmgr_pool_in_policy_info
+ **************************************************************************
+ *
+ * \brief	This structure is used in policy_params struct and holds
+ *			the information about the pools in policy
+ *
+ **************************************************************************/
+struct bmgr_pool_in_policy_info {
+	//!< Pool id
+	u8	pool_id;
+	//!< Max allowed per pool per policy
+	u32	max_allowed;
+};
+
+/*! \def POLICY_FLAG_RESERVED1
+ *       bmgr policy flags (Used in bmgr_policy_param.policy_flags)
+ */
+#define POLICY_FLAG_RESERVED1	BIT(0)
+
+/**************************************************************************
+ *! \struct	bmgr_policy_params
+ **************************************************************************
+ *
+ * \brief This structure is used in bmgr_policy_configure API in parameter
+ *
+ **************************************************************************/
+struct bmgr_policy_params {
+	//!< Policy flags
+	u16		flags;
+	//!< Group index
+	u8		group_id;
+	//!< Policy maximum allowed
+	u32		max_allowed;
+	//!< Policy minimum guaranteed
+	u32		min_guaranteed;
+	//!< Pools information. Sorted from high priority (index 0) to lowest
+	struct bmgr_pool_in_policy_info
+		pools_in_policy[4/*PP_BMGR_MAX_POOLS_IN_POLICY*/];
+	//!< Number of pools in pools_in_policy
+	u8		num_pools_in_policy;
+};
+
+#define PP_BMGR_MAX_BURST_IN_POP	(32)
+/**************************************************************************
+ *! \struct	bmgr_buff_info
+ **************************************************************************
+ *
+ * \brief This structure is used for allocate and deallocate buffer
+ *
+ **************************************************************************/
+struct bmgr_buff_info {
+	//!< [Out] buffer pointer low
+	u32	addr_low[PP_BMGR_MAX_BURST_IN_POP];
+	//!< [Out] buffer pointer high
+	u32	addr_high[PP_BMGR_MAX_BURST_IN_POP];
+	//!< policy to allocate from
+	u8	policy_id;
+	//!< pool for deallocate buffer back
+	u8	pool_id[PP_BMGR_MAX_BURST_IN_POP];
+	//!< number of pointers to allocate (up to 32 pointers)
+	u8	num_allocs;
+};
+
+/**********************************
+ **********************************
+ *********** PROTOTYPES ***********
+ **********************************
+ **********************************/
+
+/**************************************************************************
+ *! \fn	bmgr_driver_init
+ **************************************************************************
+ *
+ *  \brief	Initializes the buffer manager driver.
+ *			Must be the first driver's function to be called
+ *
+ *  \return	PP_RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+s32 bmgr_driver_init(void);
+
+/**************************************************************************
+ *! \fn	bmgr_pool_configure
+ **************************************************************************
+ *
+ *  \brief	Configure a Buffer Manager pool
+ *
+ *  \param	pool_params:	Pool param from user
+ *  \param	pool_id[OUT]:	Pool ID
+ *
+ *  \return	PP_RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+s32 bmgr_pool_configure(const struct bmgr_pool_params * const pool_params,
+			u8 * const pool_id);
+
+/**************************************************************************
+ *! \fn	bmgr_policy_configure
+ **************************************************************************
+ *
+ *  \brief	Configure a Buffer Manager policy
+ *
+ *  \param	policy_params:	Policy param from user
+ *  \param	policy_id[OUT]:	Policy ID
+ *
+ *  \return	PP_RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+s32 bmgr_policy_configure(
+		const struct bmgr_policy_params * const policy_params,
+		u8 * const policy_id);
+
+/**************************************************************************
+ *! \fn	bmgr_pop_buffer
+ **************************************************************************
+ *
+ *  \brief	Pops a buffer from the buffer manager
+ *
+ *  \param	buff_info:	Buffer information
+ *
+ *  \return	PP_RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+s32 bmgr_pop_buffer(struct bmgr_buff_info * const buff_info);
+
+/**************************************************************************
+ *! \fn	bmgr_push_buffer
+ **************************************************************************
+ *
+ *  \brief	Pushes a buffer back to the buffer manager
+ *
+ *  \param	buff_info:	Buffer information
+ *
+ *  \return	PP_RC_SUCCESS on success, other error code on failure
+ *
+ **************************************************************************/
+s32 bmgr_push_buffer(struct bmgr_buff_info * const buff_info);
+
+#endif /* _BMGR_H_ */
diff --git a/drivers/net/ethernet/lantiq/ppv4/bm/pp_bm_drv_internal.h b/drivers/net/ethernet/lantiq/ppv4/bm/pp_bm_drv_internal.h
new file mode 100644
index 000000000000..e37fa13e9cb6
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/ppv4/bm/pp_bm_drv_internal.h
@@ -0,0 +1,220 @@
+/*
+ * GPL LICENSE SUMMARY
+ *
+ *  Copyright(c) 2017 Intel Corporation.
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of version 2 of the GNU General Public License as
+ *  published by the Free Software Foundation.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *  The full GNU General Public License is included in this distribution
+ *  in the file called LICENSE.GPL.
+ *
+ *  Contact Information:
+ *  Intel Corporation
+ *  2200 Mission College Blvd.
+ *  Santa Clara, CA  97052
+ */
+
+#ifndef _BMGR_DRV_INTERNAL_H_
+#define _BMGR_DRV_INTERNAL_H_
+
+#include <linux/debugfs.h>
+#include "pp_bm_drv.h"
+
+/*! \def BMGR_DRIVER_VERSION */
+#define BMGR_DRIVER_VERSION			"1.0.0"
+
+/*! \def BMGR_MIN_POOL_BUFFER_SIZE
+ *       Minimum buffer size in configured pool
+ */
+#define BMGR_MIN_POOL_BUFFER_SIZE		(64)
+
+/*! \def BMGR_START_PCU_SRAM_ADDR
+ *       Start PCU address in SRAM. Used in confiure pool
+ */
+#define BMGR_START_PCU_FIFO_SRAM_ADDR		(0)
+
+/*! \def BMGR_DEFAULT_PCU_FIFO_SIZE
+ *       PCU fifo size
+ */
+#define BMGR_DEFAULT_PCU_FIFO_SIZE		(0x80)
+
+/*! \def BMGR_DEFAULT_PCU_FIFO_LOW_THRESHOLD
+ *       PCU fifo low threshold
+ */
+#define BMGR_DEFAULT_PCU_FIFO_LOW_THRESHOLD	(1)
+
+/*! \def BMGR_DEFAULT_PCU_FIFO_HIGH_THRESHOLD
+ *       PCU fifo high threshold
+ */
+#define BMGR_DEFAULT_PCU_FIFO_HIGH_THRESHOLD	(0x70)
+
+/*! \def BMGR_DEFAULT_WATERMARK_LOW_THRESHOLD
+ *       Watermark low threshold
+ */
+#define BMGR_DEFAULT_WATERMARK_LOW_THRESHOLD	(0x200)
+
+/**********************************
+ **********************************
+ *********** STRUCTURE ************
+ **********************************
+ **********************************/
+
+/**************************************************************************
+ *! \enum	bmgr_policy_pools_priority_e
+ **************************************************************************
+ *
+ * \brief enum to describe the pool's priority
+ *
+ **************************************************************************/
+enum bmgr_policy_pools_priority_e {
+	bmgr_policy_pool_priority_high,		//!< High priority
+	bmgr_policy_pool_priority_mid_high,	//!< Mid-High priority
+	bmgr_policy_pool_priority_mid_low,	//!< Mid-Low priority
+	bmgr_policy_pool_priority_low,		//!< Low priority
+	bmgr_policy_pool_priority_max		//!< Last priority
+};
+
+/**************************************************************************
+ *! \struct	bmgr_pool_db_entry
+ **************************************************************************
+ *
+ * \brief This structure holds the pool database
+ *
+ **************************************************************************/
+struct bmgr_pool_db_entry {
+	//!< Pool params
+	struct bmgr_pool_params	pool_params;
+	//!< Number of allocated buffers
+	u32						num_allocated_buffers;
+	//!< Number of deallocated buffers
+	u32						num_deallocated_buffers;
+	//!< Is entry in used
+	u8						is_busy;
+	//!< Pointers table to be used in HW
+	void					*internal_pointers_tables;
+};
+
+/**************************************************************************
+ *! \struct	bmgr_group_db_entry
+ **************************************************************************
+ *
+ * \brief This structure holds the group database
+ *
+ **************************************************************************/
+struct bmgr_group_db_entry {
+	//!< available buffers in group
+	u32	available_buffers;
+	//!< reserved buffers in this group
+	u32	reserved_buffers;
+	//!< Pools in policy (if set, pool is part of this group)
+	u8	pools[PP_BMGR_MAX_POOLS_IN_GROUP];
+	//!< Number of pools in group
+	u8	num_pools_in_group;
+};
+
+/**************************************************************************
+ *! \struct	bmgr_policy_db_entry
+ **************************************************************************
+ *
+ * \brief This structure holds the policy database
+ *
+ **************************************************************************/
+struct bmgr_policy_db_entry {
+	//!< Policy params
+	struct bmgr_policy_params	policy_params;
+	//!< Number of allocated buffers
+	u32		num_allocated_buffers;
+	//!< Number of deallocated buffers
+	u32		num_deallocated_buffers;
+	//!< Is entry in used
+	u8		is_busy;
+};
+
+/**************************************************************************
+ *! \struct	bmgr_driver_db
+ **************************************************************************
+ *
+ * \brief This structure holds the buffer manager database
+ *
+ **************************************************************************/
+struct bmgr_driver_db {
+	//!< Pools information
+	struct bmgr_pool_db_entry	pools[PP_BMGR_MAX_POOLS];
+	//!< Groups information
+	struct bmgr_group_db_entry	groups[PP_BMGR_MAX_GROUPS];
+	//!< Policies information
+	struct bmgr_policy_db_entry	policies[PP_BMGR_MAX_POLICIES];
+
+	// general counters
+	//!< Number of active pools
+	u32				num_pools;
+	//!< Number of active groups
+	u32				num_groups;
+	//!< Number of active policies
+	u32				num_policies;
+
+	// per project constants
+	//!< Max number of pools
+	u32				max_pools;
+	//!< Max number of groups
+	u32				max_groups;
+	//!< Max number of policies
+	u32				max_policies;
+
+	//!< spinlock
+	spinlock_t		db_lock;
+};
+
+/**************************************************************************
+ *! \struct	bmgr_driver_db
+ **************************************************************************
+ *
+ * \brief This structure holds the buffer manager database
+ *
+ **************************************************************************/
+struct bmgr_debugfs_info {
+	//!< Debugfs dir
+	struct dentry	*dir;
+	u32		virt2phyoff;
+	u32		ddrvirt2phyoff;
+	u32		fpga_offset;
+};
+
+/**************************************************************************
+ *! \struct	bmgr_driver_private
+ **************************************************************************
+ *
+ * \brief This struct defines the driver's private data
+ *
+ **************************************************************************/
+struct bmgr_driver_private {
+	//!< Platform device pointer
+	struct platform_device		*pdev;
+	//!< Is driver enabled
+	int				enabled;
+	//!< Platform device DB
+	struct bmgr_driver_db		driver_db;
+	//!< Debugfs info
+	struct bmgr_debugfs_info	debugfs_info;
+};
+
+s32 bmgr_test_dma(u32 num_bytes);
+void print_hw_stats(void);
+
+int bm_dbg_dev_init(struct platform_device *pdev);
+void bm_dbg_dev_clean(struct platform_device *pdev);
+int bm_dbg_module_init(void);
+void bm_dbg_module_clean(void);
+
+#endif /* _BMGR_DRV_INTERNAL_H_ */
diff --git a/drivers/net/ethernet/lantiq/ppv4/qos/Makefile b/drivers/net/ethernet/lantiq/ppv4/qos/Makefile
new file mode 100644
index 000000000000..c461b727d261
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/ppv4/qos/Makefile
@@ -0,0 +1,7 @@
+#
+# Makefile for PPv4 QoS driver.
+#
+
+obj-$(CONFIG_LTQ_PPV4_QOS) += pp_qos_drv.o
+pp_qos_drv-y 	:= pp_qos_linux.o pp_qos_main.o pp_qos_utils.o pp_qos_fw.o pp_qos_debugfs.o
+ccflags-y	+= -Iinclude/net -DQOS_CPU_UC_SAME_ENDIANESS
diff --git a/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_common.h b/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_common.h
new file mode 100644
index 000000000000..9931caa7d5df
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_common.h
@@ -0,0 +1,182 @@
+/*
+ * GPL LICENSE SUMMARY
+ *
+ *  Copyright(c) 2017 Intel Corporation.
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of version 2 of the GNU General Public License as
+ *  published by the Free Software Foundation.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *  The full GNU General Public License is included in this distribution
+ *  in the file called LICENSE.GPL.
+ *
+ *  Contact Information:
+ *  Intel Corporation
+ *  2200 Mission College Blvd.
+ *  Santa Clara, CA  97052
+ */
+#ifndef PP_QOS_COMMON_H
+#define PP_QOS_COMMON_H
+
+#ifdef __KERNEL__
+#include <linux/bitops.h>
+#include <linux/types.h>
+#include <linux/list.h>
+#include <linux/device.h>
+#include <linux/string.h>
+#include <linux/vmalloc.h>
+#include <linux/spinlock.h>
+#include <linux/delay.h>
+#include <linux/io.h>
+#include <linux/stringify.h>
+#else
+#include <errno.h>
+#include <stdint.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <unistd.h>
+#include "list.h"
+#endif
+
+/* Logs */
+#ifndef __KERNEL__
+#define BIT(x) ((uint32_t) (1U << (x)))
+#define QOS_LOG_CRIT(format, arg...)	printf(format, ##arg)
+#define QOS_LOG_ERR(format, arg...)	printf(format, ##arg)
+#define QOS_LOG_INFO(format, arg...)	printf(format, ##arg)
+#define QOS_LOG_DEBUG(format, arg...)	printf(format, ##arg)
+#else
+extern struct device *cur_dev;
+#define QOS_LOG_CRIT(format, arg...) \
+do { \
+	if (cur_dev) \
+		dev_crit(cur_dev, format, ##arg); \
+	else \
+		pr_crit(format, ##arg); \
+} while (0)
+#define QOS_LOG_ERR(format, arg...) \
+do { \
+	if (cur_dev) \
+		dev_err(cur_dev, format, ##arg); \
+	else \
+		pr_err(format, ##arg); \
+} while (0)
+#define QOS_LOG_INFO(format, arg...) \
+do { \
+	if (cur_dev) \
+		dev_info(cur_dev, format, ##arg); \
+	else \
+		pr_info(format, ##arg); \
+} while (0)
+#define QOS_LOG_DEBUG(format, arg...) \
+do { \
+	if (cur_dev) \
+		dev_dbg(cur_dev, format, ##arg); \
+	else \
+		pr_debug(format, ##arg); \
+} while (0)
+
+#endif
+
+#ifndef __KERNEL__
+#define max(a, b)		\
+	({ typeof(a) _a = (a);	\
+	typeof(b) _b = (b);	\
+	_a > _b ? _a : _b; })
+
+#define min(a, b)		\
+	({ typeof(a) _a = (a);	\
+	 typeof(b) _b = (b);	\
+	 _a < _b ? _a : _b; })
+#endif
+
+/* Memory allocation */
+#ifdef __KERNEL__
+#define QOS_MALLOC(size)	vmalloc(size)
+#define QOS_FREE(p)		vfree(p)
+#else
+#define QOS_MALLOC(size)	malloc(size)
+#define QOS_FREE(p)		free(p)
+#endif
+
+#define ALLOCATE_DDR_FOR_QM(qdev) allocate_ddr_for_qm(qdev)
+
+/* Locking */
+#ifdef __KERNEL__
+#define LOCK spinlock_t
+#define QOS_LOCK_INIT(qdev)	spin_lock_init(&qdev->lock)
+#define QOS_LOCK(qdev)		spin_lock(&qdev->lock)
+#define QOS_UNLOCK(qdev)	spin_unlock(&qdev->lock)
+#define QOS_SPIN_IS_LOCKED(qdev) spin_is_locked(&qdev->lock)
+#else
+#define LOCK int
+#define QOS_LOCK_INIT(qdev)	(qdev->lock = 0)
+#define QOS_LOCK(qdev)		do {QOS_ASSERT(qdev->lock == 0,\
+				"Lock already taken\n");\
+				qdev->lock++; } while (0)
+#define QOS_UNLOCK(qdev)	do {QOS_ASSERT(qdev->lock == 1,\
+				"Lock not taken\n");\
+				qdev->lock--; } while (0)
+#define QOS_SPIN_IS_LOCKED(qdev) (qdev->lock)
+#endif
+
+/* MMIO */
+#ifdef __KERNEL__
+#else
+#define __iomem
+#define iowrite32(val, addr) (*((uint32_t *)(addr)) = (val))
+#endif
+
+/* TODO not clean
+ * There might be other mapping on platform different
+ * than the FPGA I am currently testing on
+ */
+#ifdef CONFIG_OF
+#define TO_QOS_ADDR(x) (x)
+#define FW_OK_OFFSET (0x25e10000U)
+#else
+#define FW_OK_OFFSET (0x6ffff8U)
+#define PCI_DDR_BAR_START 0xd2000000
+#define TO_QOS_ADDR(x) ((x) - PCI_DDR_BAR_START)
+#endif
+
+/* Sleep */
+#ifdef __KERNEL__
+#define qos_sleep(t) msleep((t))
+#else
+#define qos_sleep(t) usleep((t) * 1000)
+#endif
+
+/* Endianess */
+#ifndef __KERNEL__
+#define cpu_to_le32(x) (x)
+#define le32_to_cpu(x) (x)
+#endif
+
+#ifdef QOS_CPU_UC_SAME_ENDIANESS
+#define qos_u32_to_uc(val)	(val)
+#define qos_u32_from_uc(val)  (val)
+#else
+#define qos_u32_to_uc(val)	(cpu_to_le32((val)))
+#define qos_u32_from_uc(val)	(le32_to_cpu((val)))
+#endif
+
+/* Creating enums and enums strings */
+#ifndef __KERNEL__
+#define __stringify(x)		#x
+#endif
+
+#define GEN_ENUM(enumerator)    enumerator,
+#define GEN_STR(str)		__stringify(str),
+
+#endif /* PP_QOS_COMMON_H */
diff --git a/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_debugfs.c b/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_debugfs.c
new file mode 100644
index 000000000000..eefe96af29d7
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_debugfs.c
@@ -0,0 +1,841 @@
+/*
+ * GPL LICENSE SUMMARY
+ *
+ *  Copyright(c) 2017 Intel Corporation.
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of version 2 of the GNU General Public License as
+ *  published by the Free Software Foundation.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *  The full GNU General Public License is included in this distribution
+ *  in the file called LICENSE.GPL.
+ *
+ *  Contact Information:
+ *  Intel Corporation
+ *  2200 Mission College Blvd.
+ *  Santa Clara, CA  97052
+ */
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/device.h>
+#include <linux/platform_device.h>
+#include <linux/debugfs.h>
+#include "pp_qos_common.h"
+#include "pp_qos_utils.h"
+#include "pp_qos_fw.h"
+#include "pp_qos_kernel.h"
+
+/* TODO is this the common way for driver's not device specific data */
+static struct {
+	struct dentry *dir;
+} dbg_data = {NULL, };
+
+#define PP_QOS_DEBUGFS_DIR "ppv4_qos"
+
+static int pp_qos_dbg_node_show(struct seq_file *s, void *unused)
+{
+	struct platform_device *pdev;
+	struct pp_qos_dev *qdev;
+	struct pp_qos_drv_data *pdata;
+	const char *typename;
+	unsigned int phy;
+	unsigned int id;
+	unsigned int i;
+	static const char *const types[] = {"Port", "Sched", "Queue",
+		"Unknown"};
+	static const char *const yesno[] = {"No", "Yes"};
+	int rc;
+	struct pp_qos_node_info info = {0, };
+
+	pdev = s->private;
+	dev_info(&pdev->dev, "node_show called\n");
+	if (pdev) {
+		pdata = platform_get_drvdata(pdev);
+		qdev = pdata->qdev;
+		if (!qos_device_ready(qdev)) {
+			seq_puts(s, "Device is not ready !!!!\n");
+			return 0;
+		}
+
+		id = pdata->dbg.node;
+		phy = get_phy_from_id(qdev->mapping, id);
+		if (!QOS_PHY_VALID(phy)) {
+			seq_printf(s, "Invalid id %u\n", id);
+			return 0;
+		}
+		rc = pp_qos_get_node_info(qdev, id, &info);
+		if (rc) {
+			seq_printf(s, "Could not get info for node %u!!!!\n",
+					id);
+			return 0;
+		}
+
+		if (info.type >=  PPV4_QOS_NODE_TYPE_PORT &&
+				info.type <= PPV4_QOS_NODE_TYPE_QUEUE)
+			typename = types[info.type];
+		else
+			typename = types[3];
+
+		seq_printf(s, "%u(%u) - %s: internal node(%s)\n",
+				id, phy,
+				typename,
+				yesno[!!info.is_internal]);
+
+		if (info.preds[0].phy != PPV4_QOS_INVALID) {
+			seq_printf(s, "%u(%u)", id, phy);
+			for (i = 0; i < 6; ++i) {
+				if (info.preds[i].phy == PPV4_QOS_INVALID)
+					break;
+				seq_printf(s, " ==> %u(%u)",
+						info.preds[i].id,
+						info.preds[i].phy);
+			}
+			seq_puts(s, "\n");
+		}
+
+		if (info.children[0].phy != PPV4_QOS_INVALID) {
+			for (i = 0; i < 8; ++i) {
+				if (info.children[i].phy == PPV4_QOS_INVALID)
+					break;
+				seq_printf(s, "%u(%u) ",
+						info.children[i].id,
+						info.children[i].phy);
+			}
+			seq_puts(s, "\n");
+		}
+
+		if (info.type == PPV4_QOS_NODE_TYPE_QUEUE)  {
+			seq_printf(s, "Physical queue: %u\n",
+					info.queue_physical_id);
+			seq_printf(s, "Port: %u\n", info.port);
+		}
+
+		seq_printf(s, "Bandwidth: %u Kbps\n", info.bw_limit);
+	}
+	return 0;
+}
+
+static int pp_qos_dbg_node_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, pp_qos_dbg_node_show, inode->i_private);
+}
+
+static const struct file_operations debug_node_fops = {
+	.open = pp_qos_dbg_node_open,
+	.read = seq_read,
+	.release = single_release,
+};
+
+static int pp_qos_dbg_stat_show(struct seq_file *s, void *unused)
+{
+	struct platform_device *pdev;
+	struct pp_qos_dev *qdev;
+	struct pp_qos_drv_data *pdata;
+	struct pp_qos_queue_stat qstat;
+	struct pp_qos_port_stat pstat;
+	struct qos_node *node;
+	unsigned int phy;
+	unsigned int id;
+
+	pdev = s->private;
+	dev_info(&pdev->dev, "node_show called\n");
+	if (pdev) {
+		pdata = platform_get_drvdata(pdev);
+		qdev = pdata->qdev;
+		if (!qos_device_ready(qdev)) {
+			seq_puts(s, "Device is not ready !!!!\n");
+			return 0;
+		}
+
+		id = pdata->dbg.node;
+		phy = get_phy_from_id(qdev->mapping, id);
+		if (!QOS_PHY_VALID(phy)) {
+			seq_printf(s, "Invalid id %u\n", id);
+			return 0;
+		}
+
+		node = get_node_from_phy(qdev->nodes, phy);
+		if (node_used(node)) {
+			seq_printf(s, "%u(%u) - ", id, phy);
+			if (node_queue(node)) {
+				seq_puts(s, "Queue\n");
+				memset(&qstat, 0, sizeof(qstat));
+				if (pp_qos_queue_stat_get(qdev, id, &qstat)
+						== 0) {
+					seq_printf(s, "queue_packets_occupancy:%u\n",
+						qstat.queue_packets_occupancy);
+					seq_printf(s, "queue_bytes_occupancy:%u\n",
+						qstat.queue_bytes_occupancy);
+					seq_printf(s, "total_packets_accepted:%u\n",
+						qstat.total_packets_accepted);
+					seq_printf(s, "total_packets_dropped:%u\n",
+						qstat.total_packets_dropped);
+					seq_printf(
+						s,
+						"total_packets_red_dropped:%u\n",
+						qstat.total_packets_red_dropped
+						);
+					seq_printf(s, "total_bytes_accepted:%llu\n",
+						qstat.total_bytes_accepted);
+					seq_printf(s, "total_bytes_dropped:%llu\n",
+						qstat.total_bytes_dropped);
+				} else {
+					seq_puts(s, "Could not obtained statistics\n");
+				}
+			} else if (node_port(node)) {
+				seq_puts(s, "Port\n");
+				memset(&pstat, 0, sizeof(pstat));
+				if (pp_qos_port_stat_get(qdev, id, &pstat)
+						== 0) {
+					seq_printf(
+						s,
+						"total_green_bytes in port's queues:%u\n",
+						pstat.total_green_bytes);
+					seq_printf(
+						s,
+						"total_yellow_bytes in port's queues:%u\n",
+						pstat.total_yellow_bytes);
+				} else {
+					seq_puts(s, "Could not obtained statistics\n");
+				}
+			} else {
+					seq_puts(s, "Node is not a queue or port, no statistics\n");
+			}
+		} else {
+			seq_printf(s, "Node %u is unused\n", id);
+		}
+	}
+	return 0;
+}
+
+static int pp_qos_dbg_stat_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, pp_qos_dbg_stat_show, inode->i_private);
+}
+
+static const struct file_operations debug_stat_fops = {
+	.open = pp_qos_dbg_stat_open,
+	.read = seq_read,
+	.release = single_release,
+};
+
+static int pp_qos_dbg_gen_show(struct seq_file *s, void *unused)
+{
+	unsigned int i;
+	struct platform_device *pdev;
+	struct pp_qos_drv_data *pdata;
+	const struct qos_node *node;
+	struct pp_qos_dev *qdev;
+	unsigned int ports;
+	unsigned int queues;
+	unsigned int scheds;
+	unsigned int internals;
+	unsigned int used;
+	unsigned int major;
+	unsigned int minor;
+	unsigned int build;
+
+	pdev = s->private;
+	if (pdev) {
+		pdata = platform_get_drvdata(pdev);
+		qdev = pdata->qdev;
+		if (qos_device_ready(qdev)) {
+			ports = 0;
+			scheds = 0;
+			queues = 0;
+			internals = 0;
+			used = 0;
+			node = get_node_from_phy(qdev->nodes, 0);
+			for (i = 0; i < NUM_OF_NODES; ++i) {
+				if (node_used(node)) {
+					++used;
+					if (node_port(node))
+						++ports;
+					else if (node_queue(node))
+						++queues;
+					else if (node_internal(node))
+						++internals;
+					else if (node_sched(node))
+						++scheds;
+				}
+				++node;
+			}
+
+			seq_printf(s, "Driver version: %s\n", PPV4_QOS_DRV_VER);
+			if (pp_qos_get_fw_version(
+						qdev,
+						&major,
+						&minor,
+						&build) == 0)
+				seq_printf(s, "FW version:\tmajor(%u) minor(%u) build(%u)\n",
+						major, minor, build);
+			else
+				seq_puts(s, "Could not obtain FW version\n");
+
+			seq_printf(s, "Used nodes:\t%u\nPorts:\t\t%u\nScheds:\t\t%u\nQueues:\t\t%u\nInternals:\t%u\n",
+					used, ports, scheds, queues, internals);
+		} else {
+			seq_puts(s, "Device is not ready !!!!\n");
+		}
+
+	} else {
+		pr_err("Error, platform device was not found\n");
+	}
+
+	return 0;
+}
+
+#define NUM_QUEUES_ON_QUERY (32U)
+#define NUM_OF_TRIES (20U)
+struct queue_stat_info {
+	uint32_t qid;
+	struct queue_stats_s qstat;
+};
+static int pp_qos_dbg_qstat_show(struct seq_file *s, void *unused)
+{
+	unsigned int i;
+	struct platform_device *pdev;
+	struct pp_qos_drv_data *pdata;
+	struct pp_qos_dev *qdev;
+	struct queue_stat_info *stat;
+	unsigned int tries;
+	uint32_t *dst;
+	unsigned int j;
+	uint32_t val;
+	uint32_t num;
+	volatile uint32_t *pos;
+
+	pdev = s->private;
+	if (!pdev) {
+		seq_puts(s, "Error, platform device was not found\n");
+		return 0;
+	}
+
+	pdata = platform_get_drvdata(pdev);
+	qdev = pdata->qdev;
+
+	if (!qos_device_ready(qdev)) {
+		seq_puts(s, "Device is not ready !!!!\n");
+		return 0;
+	}
+	seq_puts(s, "Queue\t\tQocc(p)\t\tAccept(p)\tDrop(p)\t\tRed dropped(p)\n");
+	dst = (uint32_t *)(qdev->fwcom.cmdbuf);
+	*dst++ = qos_u32_to_uc(
+			UC_QOS_COMMAND_GET_ACTIVE_QUEUES_STATS);
+	pos = dst;
+	*dst++ = qos_u32_to_uc(UC_CMD_FLAG_IMMEDIATE);
+	*dst++ = qos_u32_to_uc(3);
+
+	for (i = 0; i < NUM_OF_QUEUES; i += NUM_QUEUES_ON_QUERY) {
+		*pos = qos_u32_to_uc(UC_CMD_FLAG_IMMEDIATE);
+		dst = (uint32_t *)(qdev->fwcom.cmdbuf) + 3;
+		*dst++ = qos_u32_to_uc(i);
+		*dst++ = qos_u32_to_uc(i + NUM_QUEUES_ON_QUERY - 1);
+		*dst++ = qos_u32_to_uc(qdev->hwconf.fw_stat);
+		signal_uc(qdev);
+		val = qos_u32_from_uc(*pos);
+		tries = 0;
+		while ((
+				val &
+				(UC_CMD_FLAG_UC_DONE |
+				 UC_CMD_FLAG_UC_ERROR))
+				== 0) {
+			qos_sleep(10);
+			tries++;
+			if (tries == NUM_OF_TRIES) {
+				seq_puts(s, "firmware not responding !!!!\n");
+				return 0;
+			}
+			val = qos_u32_from_uc(*pos);
+		}
+		if (val & UC_CMD_FLAG_UC_ERROR) {
+			seq_puts(s, "firmware signaled error !!!!\n");
+			return 0;
+		}
+		stat = (struct queue_stat_info *)(qdev->stat + 4);
+		num =   *((uint32_t *)(qdev->stat));
+		for (j = 0; j < num; ++j) {
+			seq_printf(s, "%u\t\t%u\t\t%u\t\t%u\t\t%u\n",
+					stat->qid,
+					stat->qstat.queue_size_entries,
+					stat->qstat.total_accepts,
+					stat->qstat.total_drops,
+					stat->qstat.total_red_dropped
+				  );
+			++stat;
+		}
+	}
+	return 0;
+}
+
+static int pp_qos_dbg_pstat_show(struct seq_file *s, void *unused)
+{
+	unsigned int i;
+	struct platform_device *pdev;
+	struct pp_qos_drv_data *pdata;
+	struct pp_qos_dev *qdev;
+	struct pp_qos_port_stat statp;
+
+	pdev = s->private;
+	if (!pdev) {
+		seq_puts(s, "Error, platform device was not found\n");
+		return 0;
+	}
+	pdata = platform_get_drvdata(pdev);
+	qdev = pdata->qdev;
+	if (!qos_device_ready(qdev)) {
+		seq_puts(s, "Device is not ready !!!!\n");
+		return 0;
+	}
+
+	seq_puts(s, "Port\t\tGreen Bytes\tYellow Bytes\n");
+	memset(&statp, 0, sizeof(statp));
+	for (i = 0; i <= qdev->max_port; ++i) {
+		create_get_port_stats_cmd(
+				qdev,
+				i,
+				qdev->hwconf.fw_stat,
+				&statp);
+		transmit_cmds(qdev);
+		if (statp.total_green_bytes || statp.total_yellow_bytes)
+			seq_printf(s, "%u\t\t%u\t\t%u\t\t\n",
+					i,
+					statp.total_green_bytes,
+					statp.total_yellow_bytes);
+	}
+	return 0;
+}
+
+static int pp_qos_dbg_gen_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, pp_qos_dbg_gen_show, inode->i_private);
+}
+
+static int pp_qos_dbg_qstat_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, pp_qos_dbg_qstat_show, inode->i_private);
+}
+
+static int pp_qos_dbg_pstat_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, pp_qos_dbg_pstat_show, inode->i_private);
+}
+
+static const struct file_operations debug_gen_fops = {
+	.open = pp_qos_dbg_gen_open,
+	.read = seq_read,
+	.release = single_release,
+};
+
+static const struct file_operations debug_qstat_fops = {
+	.open = pp_qos_dbg_qstat_open,
+	.read = seq_read,
+	.release = single_release,
+};
+
+static const struct file_operations debug_pstat_fops = {
+	.open = pp_qos_dbg_pstat_open,
+	.read = seq_read,
+	.release = single_release,
+};
+
+static int dbg_cmd_open(struct inode *inode, struct file *filep)
+{
+	filep->private_data = inode->i_private;
+	return 0;
+}
+
+#define MAX_CMD_LEN 0x200
+static ssize_t dbg_cmd_write(struct file *fp, const char __user *user_buffer,
+			     size_t cnt, loff_t *pos)
+{
+	int rc;
+	int i;
+	int j;
+	uint8_t cmd[MAX_CMD_LEN];
+	struct platform_device *pdev;
+	struct pp_qos_drv_data *pdata;
+	struct pp_qos_dev *qdev;
+	uint32_t *dst;
+	uint32_t *src;
+	unsigned long res;
+
+	pdev = (struct platform_device *)(fp->private_data);
+	pdata = platform_get_drvdata(pdev);
+	qdev = pdata->qdev;
+
+	pr_info("qos drv address is %p\n", qdev);
+
+	if (cnt > MAX_CMD_LEN) {
+		dev_err(&pdev->dev, "Illegal length %zu\n", cnt);
+		return -EINVAL;
+	}
+
+	rc =  simple_write_to_buffer(cmd, MAX_CMD_LEN, pos, user_buffer, cnt);
+	if (rc < 0) {
+		dev_err(&pdev->dev, "Write failed with %d\n", rc);
+		return rc;
+	}
+
+	dst = (uint32_t *)(qdev->fwcom.cmdbuf);
+	src = (uint32_t *)cmd;
+	dev_info(&pdev->dev, "Writing %d bytes into\n", rc);
+	j = 0;
+	for (i = 0; i < rc; ++i) {
+		if (cmd[i] == '\n') {
+			cmd[i] = 0;
+			kstrtoul(cmd + j, 0, &res);
+			*dst++ = qos_u32_to_uc(res);
+			dev_info(&pdev->dev, "Wrote 0x%08lX\n", res);
+			j = i + 1;
+		}
+	}
+	signal_uc(qdev);
+
+	return rc;
+}
+
+static const struct file_operations debug_cmd_fops = {
+	.open = dbg_cmd_open,
+	.write = dbg_cmd_write,
+};
+
+static void swap_msg(char *msg)
+{
+	unsigned int i;
+	uint32_t *cur;
+
+	cur = (uint32_t *)msg;
+
+	for (i = 0; i < 32; ++i)
+		cur[i] = le32_to_cpu(cur[i]);
+}
+
+static void print_fw_log(struct platform_device *pdev)
+{
+	char		msg[128];
+	unsigned int	num;
+	unsigned int    i;
+	uint32_t	*addr;
+	uint32_t	read;
+	char		*cur;
+	struct device	*dev;
+	struct pp_qos_drv_data *pdata;
+
+	pdata = platform_get_drvdata(pdev);
+	addr = (uint32_t *)(pdata->dbg.fw_logger_addr);
+	num = qos_u32_from_uc(*addr);
+	read = qos_u32_from_uc(addr[1]);
+	dev = &pdev->dev;
+	cur = (char *)(pdata->dbg.fw_logger_addr + 8);
+
+	dev_info(dev, "addr is 0x%08X num of messages is %u, read index is %u",
+		 (unsigned int)(uintptr_t)cur,
+		 num,
+		 read);
+
+	for (i = read; i < num; ++i) {
+		memcpy((char *)msg, (char *)(cur + 128 * i), 128);
+		swap_msg(msg);
+		msg[127] = '\0';
+		dev_info(dev, "[ARC]: %s\n", msg);
+	}
+
+	addr[1] = num;
+}
+
+static int ctrl_set(void *data, u64 val)
+{
+	struct platform_device *pdev;
+	struct pp_qos_drv_data *pdata;
+
+	pdev = data;
+	dev_info(&pdev->dev, "ctrl got %llu", val);
+	pdata = platform_get_drvdata(pdev);
+
+	switch (val) {
+#ifdef PP_QOS_TEST
+	case 0:
+		QOS_LOG_INFO("running basic tests\n");
+		basic_tests();
+		break;
+	case 1:
+		QOS_LOG_INFO("running advance tests\n");
+		advance_tests();
+		break;
+	case 2:
+		QOS_LOG_INFO("running all tests\n");
+		tests();
+		break;
+	case 7:
+		QOS_LOG_INFO("running falcon test\n");
+		falcon_test();
+		break;
+	case 8:
+		QOS_LOG_INFO("running simple test\n");
+		stat_test();
+		break;
+	case 9:
+		QOS_LOG_INFO("running load fw test\n");
+		load_fw_test();
+		break;
+	case 10:
+		QOS_LOG_INFO("running stat test\n");
+		stat_test();
+		break;
+	case 11:
+		QOS_LOG_INFO("running info test\n");
+		info_test();
+		break;
+
+#endif
+	case 14:
+		QOS_LOG_INFO("printing logger\n");
+		print_fw_log(pdev);
+		break;
+
+
+	default:
+		QOS_LOG_INFO("unknown test\n");
+		break;
+	}
+
+	return 0;
+}
+
+static int phy2id_get(void *data, u64 *val)
+{
+	uint16_t id;
+	struct platform_device *pdev;
+	struct pp_qos_drv_data *pdata;
+	struct pp_qos_dev *qdev;
+
+	pdev = data;
+	pdata = platform_get_drvdata(pdev);
+	qdev = pdata->qdev;
+	if (!qdev->initialized) {
+		dev_err(&pdev->dev, "Device is not initialized\n");
+		id =  QOS_INVALID_ID;
+		goto out;
+	}
+
+	id = get_id_from_phy(qdev->mapping, pdata->dbg.node);
+out:
+	*val = id;
+	return 0;
+}
+
+DEFINE_SIMPLE_ATTRIBUTE(dbg_ctrl_fops, NULL, ctrl_set, "%llu\n");
+DEFINE_SIMPLE_ATTRIBUTE(dbg_phy2id_fops, phy2id_get, NULL, "%llu\n");
+
+#define MAX_DIR_NAME 11
+int qos_dbg_dev_init(struct platform_device *pdev)
+{
+
+	struct pp_qos_drv_data *pdata;
+	char   dirname[MAX_DIR_NAME];
+	struct dentry *dent;
+	int err;
+
+	if (!pdev) {
+		dev_err(&pdev->dev, "Invalid platform device\n");
+		return -ENODEV;
+	}
+
+	pdata = platform_get_drvdata(pdev);
+
+	snprintf(dirname, MAX_DIR_NAME, "qos%d", pdata->id);
+	dent = debugfs_create_dir(dirname, dbg_data.dir);
+	if (IS_ERR_OR_NULL(dent)) {
+		err = (int) PTR_ERR(dent);
+		dev_err(&pdev->dev, "debugfs_create_dir failed with %d\n", err);
+		return err;
+	}
+
+	pdata->dbg.dir = dent;
+	dent = debugfs_create_file(
+			"nodeinfo",
+			0400,
+			pdata->dbg.dir,
+			pdev,
+			&debug_node_fops
+			);
+
+	if (IS_ERR_OR_NULL(dent)) {
+		err = (int) PTR_ERR(dent);
+		dev_err(&pdev->dev,
+			"debugfs_create_file failed creating nodeinfo with %d\n",
+			err);
+		goto fail;
+	}
+
+	dent = debugfs_create_file(
+			"stat",
+			0400,
+			pdata->dbg.dir,
+			pdev,
+			&debug_stat_fops
+			);
+
+	if (IS_ERR_OR_NULL(dent)) {
+		err = (int) PTR_ERR(dent);
+		dev_err(&pdev->dev,
+			"debugfs_create_file failed creating stat with %d\n",
+			err);
+		goto fail;
+	}
+
+	dent = debugfs_create_u16("node",
+			0600,
+			pdata->dbg.dir,
+			&pdata->dbg.node);
+	if (IS_ERR_OR_NULL(dent)) {
+		err = (int) PTR_ERR(dent);
+		dev_err(&pdev->dev,
+			"debugfs_create_u16 failed creating nodeinfo with %d\n",
+			err);
+		goto fail;
+	}
+
+	dent = debugfs_create_file(
+			"ctrl",
+			0200,
+			pdata->dbg.dir,
+			pdev,
+			&dbg_ctrl_fops
+			);
+	if (IS_ERR_OR_NULL(dent)) {
+		err = (int) PTR_ERR(dent);
+		dev_err(&pdev->dev,
+			"debugfs_create_file failed creating ctrl with %d\n",
+			err);
+		goto fail;
+	}
+
+	dent = debugfs_create_file(
+			"phy2id",
+			0400,
+			pdata->dbg.dir,
+			pdev,
+			&dbg_phy2id_fops
+			);
+	if (IS_ERR_OR_NULL(dent)) {
+		err = (int) PTR_ERR(dent);
+		dev_err(&pdev->dev,
+			"debugfs_create_file failed creating phy2id with %d\n",
+			err);
+		goto fail;
+	}
+
+	dent = debugfs_create_file(
+			"geninfo",
+			0400,
+			pdata->dbg.dir,
+			pdev,
+			&debug_gen_fops
+			);
+
+	if (IS_ERR_OR_NULL(dent)) {
+		err = (int) PTR_ERR(dent);
+		dev_err(&pdev->dev,
+			"debugfs_create_file failed creating geninfo with %d\n",
+			err);
+		goto fail;
+	}
+
+	dent = debugfs_create_file(
+			"qstat",
+			0400,
+			pdata->dbg.dir,
+			pdev,
+			&debug_qstat_fops
+			);
+
+	if (IS_ERR_OR_NULL(dent)) {
+		err = (int) PTR_ERR(dent);
+		dev_err(&pdev->dev,
+			"debugfs_create_file failed creating qstat with %d\n",
+			err);
+		goto fail;
+	}
+
+	dent = debugfs_create_file(
+			"pstat",
+			0400,
+			pdata->dbg.dir,
+			pdev,
+			&debug_pstat_fops
+			);
+
+	if (IS_ERR_OR_NULL(dent)) {
+		err = (int) PTR_ERR(dent);
+		dev_err(&pdev->dev,
+			"debugfs_create_file failed creating pstat with %d\n",
+			err);
+		goto fail;
+	}
+
+
+	dent = debugfs_create_file(
+			"cmd",
+			0200,
+			pdata->dbg.dir,
+			pdev,
+			&debug_cmd_fops
+			);
+
+	if (IS_ERR_OR_NULL(dent)) {
+		err = (int) PTR_ERR(dent);
+		dev_err(&pdev->dev,
+			"debugfs_create_file failed creating cmds with %d\n",
+			err);
+		goto fail;
+	}
+
+	return 0;
+
+fail:
+	debugfs_remove_recursive(pdata->dbg.dir);
+	return err;
+
+}
+
+void qos_dbg_dev_clean(struct platform_device *pdev)
+{
+	struct pp_qos_drv_data *pdata;
+
+	if (pdev) {
+		pdata = platform_get_drvdata(pdev);
+		if (pdata)
+			debugfs_remove_recursive(pdata->dbg.dir);
+	}
+}
+
+int qos_dbg_module_init(void)
+{
+	int rc;
+	struct dentry *dir;
+
+	dir = debugfs_create_dir(PP_QOS_DEBUGFS_DIR, NULL);
+	if (IS_ERR_OR_NULL(dir)) {
+		rc = (int) PTR_ERR(dir);
+		pr_err("debugfs_create_dir failed with %d\n", rc);
+		return rc;
+	}
+	dbg_data.dir = dir;
+	return 0;
+}
+
+void qos_dbg_module_clean(void)
+{
+	debugfs_remove_recursive(dbg_data.dir);
+}
diff --git a/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_fw.c b/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_fw.c
new file mode 100644
index 000000000000..ee6b4affaad3
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_fw.c
@@ -0,0 +1,2813 @@
+/*
+ * GPL LICENSE SUMMARY
+ *
+ *  Copyright(c) 2017 Intel Corporation.
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of version 2 of the GNU General Public License as
+ *  published by the Free Software Foundation.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *  The full GNU General Public License is included in this distribution
+ *  in the file called LICENSE.GPL.
+ *
+ *  Contact Information:
+ *  Intel Corporation
+ *  2200 Mission College Blvd.
+ *  Santa Clara, CA  97052
+ */
+#include "pp_qos_common.h"
+#include "pp_qos_utils.h"
+#include "pp_qos_uc_wrapper.h"
+#include "pp_qos_fw.h"
+
+
+#ifdef __LP64__
+#define GET_ADDRESS_HIGH(addr) ((((uintptr_t)(addr)) >> 32) & 0xFFFFFFFF)
+#else
+#define GET_ADDRESS_HIGH(addr) (0)
+#endif
+
+#ifndef PP_QOS_DISABLE_CMDQ
+
+#define CMD_FLAGS_WRAP_SUSPEND_RESUME 1
+#define CMD_FLAGS_WRAP_PARENT_SUSPEND_RESUME 2
+#define CMD_FLAGS_POST_PROCESS	4
+
+
+#define FW_CMDS(OP) \
+	OP(CMD_TYPE_INIT_LOGGER)		\
+	OP(CMD_TYPE_INIT_QOS)			\
+	OP(CMD_TYPE_MOVE)			\
+	OP(CMD_TYPE_ADD_PORT)			\
+	OP(CMD_TYPE_SET_PORT)			\
+	OP(CMD_TYPE_REMOVE_PORT)		\
+	OP(CMD_TYPE_ADD_SCHED)			\
+	OP(CMD_TYPE_SET_SCHED)			\
+	OP(CMD_TYPE_REMOVE_SCHED)		\
+	OP(CMD_TYPE_ADD_QUEUE)			\
+	OP(CMD_TYPE_SET_QUEUE)			\
+	OP(CMD_TYPE_REMOVE_QUEUE)		\
+	OP(CMD_TYPE_UPDATE_PREDECESSORS)	\
+	OP(CMD_TYPE_PARENT_CHANGE)		\
+	OP(CMD_TYPE_REMOVE_NODE)		\
+	OP(CMD_TYPE_GET_QUEUE_STATS)		\
+	OP(CMD_TYPE_GET_PORT_STATS)		\
+	OP(CMD_TYPE_ADD_SHARED_GROUP)		\
+	OP(CMD_TYPE_PUSH_DESC)			\
+	OP(CMD_TYPE_GET_NODE_INFO)		\
+	OP(CMD_TYPE_REMOVE_SHARED_GROUP)	\
+	OP(CMD_TYPE_SET_SHARED_GROUP)		\
+	OP(CMD_TYPE_FLUSH_QUEUE)		\
+	OP(CMD_TYPE_GET_NUM_USED_NODES)		\
+	OP(CMD_TYPE_INTERNAL)
+
+enum cmd_type {
+	FW_CMDS(GEN_ENUM)
+};
+
+static const char *const cmd_str[] = {
+	FW_CMDS(GEN_STR)
+};
+
+static void update_moved_nodes(
+		struct pp_qos_dev *qdev,
+		unsigned int src,
+		unsigned int dst);
+
+struct ppv4_qos_fw_hdr {
+	uint32_t major;
+	uint32_t minor;
+	uint32_t build;
+};
+
+struct ppv4_qos_fw_sec {
+	uint32_t dst;
+	uint32_t size;
+};
+
+#define FW_OK_SIGN (0xCAFECAFEU)
+#define FW_DDR_LOWEST (0x400000U)
+
+static void copy_section(void *_dst, const void *_src, unsigned int size)
+{
+	unsigned int i;
+	const uint32_t *src;
+	uint32_t *dst;
+
+	src = (uint32_t *)_src;
+	dst = (uint32_t *)_dst;
+
+	for (i = size; i > 0; i -= 4)
+		*dst++ = cpu_to_le32(*src++);
+}
+
+/*
+ * This function loads the firmware.
+ * The firmware is built from a header which holds the major, minor
+ * and build numbers.
+ * Following the header are sections. Each section is composed of
+ * header which holds the memory destination where this section's
+ * data should be copied and the size of this section in bytes.
+ * After the header comes section's data which is a stream of uint32
+ * words.
+ * The memory destination on section header designates offset relative
+ * to either ddr (a.k.a external) or qos (a.k.a) spaces. Offsets higher
+ * than FW_DDR_LOWEST refer to ddr space.
+ *
+ * Firmware is little endian.
+ *
+ * When firmware runs it writes 0xCAFECAFE to offset FW_OK_OFFSET of ddr
+ * space.
+ */
+int do_load_firmware(
+		struct pp_qos_dev *qdev,
+		const struct ppv4_qos_fw *fw,
+		void *ddr_base,
+		void *qos_base,
+		void *data)
+{
+	size_t size;
+	struct ppv4_qos_fw_hdr *hdr;
+	const uint8_t *cur;
+	const uint8_t *last;
+	void *dst;
+	struct ppv4_qos_fw_sec *sec;
+	uint32_t val;
+
+	size = fw->size;
+	hdr = (struct ppv4_qos_fw_hdr *)(fw->data);
+	hdr->major = le32_to_cpu(hdr->major);
+	hdr->minor = le32_to_cpu(hdr->minor);
+	hdr->build = le32_to_cpu(hdr->build);
+	QOS_LOG_INFO("Firmware size(%zu) major(%u) minor(%u) build(%u)\n",
+			size,
+			hdr->major,
+			hdr->minor,
+			hdr->build);
+
+	if (hdr->major != UC_VERSION_MAJOR || hdr->minor != UC_VERSION_MINOR) {
+		QOS_LOG_ERR("mismatch major %u or minor %u\n",
+				UC_VERSION_MAJOR, UC_VERSION_MINOR);
+		return -EINVAL;
+	}
+
+	qdev->fwver.major = hdr->major;
+	qdev->fwver.minor = hdr->minor;
+	qdev->fwver.build = hdr->build;
+
+	last = fw->data + size - 1;
+	cur = (uint8_t *)(hdr + 1);
+	while (cur < last) {
+		sec = (struct ppv4_qos_fw_sec *)cur;
+		sec->dst = le32_to_cpu(sec->dst);
+		sec->size = le32_to_cpu(sec->size);
+		cur = (uint8_t *)(sec + 1);
+
+		if (sec->dst >= FW_DDR_LOWEST)
+			dst = ddr_base + sec->dst;
+		else
+			dst = qos_base + sec->dst;
+
+		QOS_LOG_DEBUG("Copying %u bytes (0x%08X) <-- (0x%08X)\n",
+				sec->size,
+				(unsigned int)(uintptr_t)(dst),
+				(unsigned int)(uintptr_t)(cur));
+
+		copy_section(dst, cur, sec->size);
+		cur += sec->size;
+	}
+
+	wake_uc(data);
+	QOS_LOG_DEBUG("waked fw\n");
+	qos_sleep(10);
+	val = *((uint32_t *)(qdev->fwcom.cmdbuf));
+	if (val != FW_OK_SIGN) {
+		QOS_LOG_ERR("FW OK value is 0x%08X, instead got 0x%08X\n",
+				FW_OK_SIGN, val);
+		return  -ENODEV;
+	}
+	QOS_LOG_INFO("FW is running :)\n");
+	*((uint32_t *)(qdev->fwcom.cmdbuf)) = 0;
+	return 0;
+}
+
+/******************************************************************************/
+/*                         Driver commands structures	                      */
+/******************************************************************************/
+struct cmd {
+	unsigned int id;
+	unsigned int fw_id;
+	unsigned int flags;
+	enum cmd_type  type;
+	size_t len;
+	uint32_t *pos;
+};
+
+struct cmd_internal {
+	struct cmd base;
+};
+
+struct cmd_init_logger {
+	struct cmd base;
+	unsigned int	addr;
+	int mode;
+	int level;
+	unsigned int num_of_msgs;
+};
+
+struct cmd_init_qos {
+	struct cmd base;
+	unsigned int qm_ddr_start;
+	unsigned int qm_num_pages;
+	unsigned int wred_total_avail_resources;
+	unsigned int wred_prioritize_pop;
+	unsigned int wred_avg_q_size_p;
+	unsigned int wred_max_q_size;
+	unsigned int num_of_ports;
+};
+
+struct cmd_move {
+	struct cmd base;
+	int      node_type;
+	uint16_t src;
+	uint16_t dst;
+	unsigned int rlm;
+	uint16_t dst_port;
+	uint16_t preds[6];
+};
+
+struct cmd_remove_node {
+	struct cmd base;
+	unsigned int phy;
+	unsigned int data; /* rlm in queue, otherwise irrlevant */
+};
+
+struct cmd_update_preds {
+	struct cmd base;
+	int node_type;
+	uint16_t preds[6];
+	unsigned int phy;
+	unsigned int rlm;
+};
+
+struct port_properties {
+	struct pp_qos_common_node_properties common;
+	struct pp_qos_parent_node_properties parent;
+	void *ring_addr;
+	size_t ring_size;
+	uint8_t  packet_credit_enable;
+	unsigned int credit;
+	int	     disable;
+};
+
+struct cmd_add_port {
+	struct cmd base;
+	unsigned int phy;
+	struct port_properties prop;
+};
+
+struct cmd_set_port {
+	struct cmd base;
+	unsigned int phy;
+	struct port_properties prop;
+	uint32_t modified;
+};
+
+struct sched_properties {
+	struct pp_qos_common_node_properties common;
+	struct pp_qos_parent_node_properties parent;
+	struct pp_qos_child_node_properties  child;
+};
+
+struct cmd_add_sched {
+	struct cmd base;
+	unsigned int phy;
+	unsigned int parent;
+	uint16_t preds[6];
+	struct sched_properties prop;
+};
+
+struct cmd_set_sched {
+	struct cmd base;
+	unsigned int phy;
+	unsigned int parent;
+	struct sched_properties prop;
+	uint32_t modified;
+};
+
+struct queue_properties {
+	struct pp_qos_common_node_properties common;
+	struct pp_qos_child_node_properties  child;
+	uint8_t  blocked;
+	uint8_t  wred_enable;
+	uint8_t  fixed_drop_prob_enable;
+	unsigned int max_burst;
+	unsigned int queue_wred_min_avg_green;
+	unsigned int queue_wred_max_avg_green;
+	unsigned int queue_wred_slope_green;
+	unsigned int queue_wred_min_avg_yellow;
+	unsigned int queue_wred_max_avg_yellow;
+	unsigned int queue_wred_slope_yellow;
+	unsigned int queue_wred_min_guaranteed;
+	unsigned int queue_wred_max_allowed;
+	unsigned int queue_wred_fixed_drop_prob_green;
+	unsigned int queue_wred_fixed_drop_prob_yellow;
+	unsigned int      rlm;
+};
+
+struct cmd_add_queue {
+	struct cmd base;
+	unsigned int phy;
+	unsigned int parent;
+	unsigned int port;
+	uint16_t preds[6];
+	struct queue_properties prop;
+};
+
+struct cmd_set_queue {
+	struct cmd base;
+	unsigned int phy;
+	struct queue_properties prop;
+	uint32_t modified;
+};
+
+struct cmd_flush_queue {
+	struct cmd base;
+	unsigned int rlm;
+};
+
+struct cmd_parent_change {
+	struct cmd base;
+	unsigned int phy;
+	int type;
+	int arbitration;
+	int first;
+	unsigned int num;
+};
+
+struct cmd_get_queue_stats {
+	struct cmd base;
+	unsigned int phy;
+	unsigned int rlm;
+	unsigned int addr;
+	struct pp_qos_queue_stat *stat;
+};
+
+struct cmd_get_port_stats {
+	struct cmd base;
+	unsigned int phy;
+	unsigned int addr;
+	struct pp_qos_port_stat *stat;
+};
+
+struct cmd_push_desc {
+	struct cmd base;
+	unsigned int queue;
+	unsigned int size;
+	unsigned int color;
+	unsigned int addr;
+};
+
+struct cmd_get_node_info {
+	struct cmd base;
+	unsigned int phy;
+	unsigned int addr;
+	struct pp_qos_node_info *info;
+};
+
+struct stub_cmd {
+	struct cmd cmd;
+	uint8_t data;
+};
+
+struct cmd_set_shared_group {
+	struct cmd base;
+	unsigned int id;
+	unsigned int limit;
+};
+
+struct cmd_remove_shared_group {
+	struct cmd base;
+	unsigned int id;
+};
+
+struct cmd_get_num_used_nodes {
+	struct cmd base;
+	unsigned int addr;
+	uint32_t *num;
+};
+
+union driver_cmd {
+	struct cmd	 cmd;
+	struct stub_cmd  stub;
+	struct cmd_init_logger init_logger;
+	struct cmd_init_qos  init_qos;
+	struct cmd_move  move;
+	struct cmd_update_preds update_preds;
+	struct cmd_remove_node remove_node;
+	struct cmd_add_port add_port;
+	struct cmd_set_port set_port;
+	struct cmd_add_sched add_sched;
+	struct cmd_set_sched set_sched;
+	struct cmd_add_queue add_queue;
+	struct cmd_set_queue set_queue;
+	struct cmd_parent_change parent_change;
+	struct cmd_get_queue_stats queue_stats;
+	struct cmd_get_port_stats port_stats;
+	struct cmd_set_shared_group set_shared_group;
+	struct cmd_remove_shared_group remove_shared_group;
+	struct cmd_push_desc	pushd;
+	struct cmd_get_node_info node_info;
+	struct cmd_flush_queue flush_queue;
+	struct cmd_get_num_used_nodes num_used;
+	struct cmd_internal	internal;
+};
+
+/******************************************************************************/
+/*                         Driver functions                                   */
+/******************************************************************************/
+
+/*
+ * Following functions creates commands in driver fromat to be stored at
+ * drivers queues before sending to firmware
+ */
+
+/*
+ * Extract ancestors of node from driver's DB
+ */
+static void fill_preds(
+		const struct pp_nodes *nodes,
+		unsigned int phy,
+		uint16_t *preds,
+		size_t size)
+{
+	unsigned int i;
+	const struct qos_node *node;
+
+	i = 0;
+	memset(preds, 0x0, size * sizeof(uint16_t));
+	node = get_const_node_from_phy(nodes, phy);
+	while (node_child(node) && (i < size)) {
+		preds[i] = node->child_prop.parent_phy;
+		node = get_const_node_from_phy(nodes,
+				node->child_prop.parent_phy);
+		i++;
+	}
+}
+
+static void cmd_init(
+		const struct pp_qos_dev *qdev,
+		struct cmd *cmd,
+		enum cmd_type type,
+		size_t len,
+		unsigned int flags)
+{
+	cmd->flags = flags;
+	cmd->type = type;
+	cmd->len = len;
+	cmd->id = qdev->drvcmds.cmd_id;
+	cmd->fw_id = qdev->drvcmds.cmd_fw_id;
+}
+
+/* TODO - make less hard code */
+void create_init_logger_cmd(struct pp_qos_dev *qdev)
+{
+	struct cmd_init_logger cmd;
+
+	cmd_init(qdev, &(cmd.base), CMD_TYPE_INIT_LOGGER, sizeof(cmd), 0);
+	cmd.addr = qdev->hwconf.fw_logger_start;
+	cmd.mode = UC_LOGGER_MODE_WRITE_HOST_MEM;
+	cmd.level = UC_LOGGER_LEVEL_INFO;
+	cmd.num_of_msgs = PPV4_QOS_LOGGER_BUF_SIZE / PPV4_QOS_LOGGER_MSG_SIZE;
+	QOS_LOG_DEBUG("cmd %u:%u CMD_TYPE_INIT_LOGGER\n",
+			qdev->drvcmds.cmd_id,
+			qdev->drvcmds.cmd_fw_id);
+	cmd_queue_put(qdev->drvcmds.cmdq, (uint8_t *)&cmd, sizeof(cmd));
+	qdev->drvcmds.cmd_fw_id++;
+}
+
+void create_init_qos_cmd(struct pp_qos_dev *qdev)
+{
+	struct cmd_init_qos cmd;
+
+	if (PP_QOS_DEVICE_IS_ASSERT(qdev))
+		return;
+
+	cmd_init(qdev, &(cmd.base), CMD_TYPE_INIT_QOS, sizeof(cmd), 0);
+	cmd.qm_ddr_start = qdev->hwconf.qm_ddr_start;
+	cmd.qm_num_pages = qdev->hwconf.qm_num_pages;
+	cmd.wred_total_avail_resources =
+		qdev->hwconf.wred_total_avail_resources;
+	cmd.wred_prioritize_pop = qdev->hwconf.wred_prioritize_pop;
+	cmd.wred_avg_q_size_p = qdev->hwconf.wred_const_p;
+	cmd.wred_max_q_size = qdev->hwconf.wred_max_q_size;
+	cmd.num_of_ports = qdev->max_port + 1;
+	QOS_LOG_DEBUG("cmd %u:%u CMD_TYPE_INIT_QOS\n",
+			qdev->drvcmds.cmd_id,
+			qdev->drvcmds.cmd_fw_id);
+	cmd_queue_put(qdev->drvcmds.cmdq, (uint8_t *)&cmd, sizeof(cmd));
+	qdev->drvcmds.cmd_fw_id++;
+}
+
+void create_move_cmd(
+		struct pp_qos_dev *qdev,
+		uint16_t dst,
+		uint16_t src,
+		uint16_t dst_port)
+{
+	struct cmd_move cmd;
+	const struct qos_node *node;
+
+	if (PP_QOS_DEVICE_IS_ASSERT(qdev))
+		return;
+
+	node = get_const_node_from_phy(qdev->nodes, dst);
+
+	cmd_init(qdev, &(cmd.base), CMD_TYPE_MOVE, sizeof(cmd), 0);
+	cmd.src = src;
+	cmd.dst = dst;
+	cmd.dst_port = dst_port;
+	cmd.node_type = node->type;
+	if (node->type == TYPE_QUEUE)
+		cmd.rlm = node->data.queue.rlm;
+	else
+		cmd.rlm = -1;
+
+	fill_preds(qdev->nodes, dst, cmd.preds, 6);
+	QOS_LOG_DEBUG("cmd %u:%u CMD_TYPE_MOVE %u-->%u type:%d, rlm:%d, port:%u\n",
+			qdev->drvcmds.cmd_id,
+			qdev->drvcmds.cmd_fw_id,
+			src,
+			dst,
+			node->type,
+			cmd.rlm,
+			dst_port);
+
+	update_moved_nodes(qdev, src, dst);
+	cmd_queue_put(qdev->drvcmds.cmdq, (uint8_t *)&cmd, sizeof(cmd));
+	qdev->drvcmds.cmd_fw_id++;
+}
+
+void create_remove_node_cmd(
+		struct pp_qos_dev *qdev,
+		enum node_type ntype,
+		unsigned int phy,
+		unsigned int data)
+{
+	struct cmd_remove_node cmd;
+	enum cmd_type ctype;
+
+	if (PP_QOS_DEVICE_IS_ASSERT(qdev))
+		return;
+
+	switch (ntype) {
+	case TYPE_PORT:
+		ctype = CMD_TYPE_REMOVE_PORT;
+		break;
+	case TYPE_SCHED:
+		ctype = CMD_TYPE_REMOVE_SCHED;
+		break;
+	case TYPE_QUEUE:
+		ctype = CMD_TYPE_REMOVE_QUEUE;
+		break;
+	case TYPE_UNKNOWN:
+		QOS_ASSERT(0, "Unexpected unknow type\n");
+		ctype = CMD_TYPE_REMOVE_NODE;
+		break;
+	default:
+		QOS_LOG_ERR("illegal node type %d\n", ntype);
+		return;
+	}
+
+	cmd_init(qdev, &(cmd.base), ctype, sizeof(cmd), 0);
+	cmd.phy = phy;
+	cmd.data = data;
+
+	QOS_LOG_DEBUG("cmd %u:%u %s %u rlm %u\n",
+			qdev->drvcmds.cmd_id,
+			qdev->drvcmds.cmd_fw_id,
+			cmd_str[ctype], phy, data);
+	cmd_queue_put(qdev->drvcmds.cmdq, &cmd, sizeof(cmd));
+	if (ctype != CMD_TYPE_REMOVE_PORT)
+		add_suspend_port(qdev, get_port(qdev->nodes, phy));
+	qdev->drvcmds.cmd_fw_id++;
+}
+
+void create_update_preds_cmd(struct pp_qos_dev *qdev, unsigned int phy)
+{
+	const struct qos_node *node;
+	struct cmd_update_preds cmd;
+
+	if (PP_QOS_DEVICE_IS_ASSERT(qdev))
+		return;
+
+	memset(&cmd, 0, sizeof(cmd));
+	cmd_init(
+			qdev,
+			&(cmd.base),
+			CMD_TYPE_UPDATE_PREDECESSORS,
+			sizeof(cmd),
+			0);
+	cmd.phy = phy;
+	fill_preds(qdev->nodes, phy, cmd.preds, 6);
+	node = get_const_node_from_phy(qdev->nodes, phy);
+	cmd.node_type = node->type;
+	cmd.rlm = node->data.queue.rlm;
+
+	QOS_LOG_DEBUG(
+			"cmd %u:%u CMD_TYPE_UPDATE_PREDECESSORS %u:%u-->%u-->%u-->%u-->%u-->%u\n",
+			qdev->drvcmds.cmd_id,
+			qdev->drvcmds.cmd_fw_id,
+			phy,
+			cmd.preds[0], cmd.preds[1], cmd.preds[2],
+			cmd.preds[3], cmd.preds[4], cmd.preds[5]);
+	cmd_queue_put(qdev->drvcmds.cmdq, &cmd, sizeof(cmd));
+	qdev->drvcmds.cmd_fw_id++;
+}
+
+static void set_cmd_port_properties(
+		struct port_properties *prop,
+		const struct pp_qos_port_conf *conf)
+{
+	prop->common = conf->common_prop;
+	prop->parent = conf->port_parent_prop;
+	prop->packet_credit_enable = !!conf->packet_credit_enable;
+	prop->ring_addr = conf->ring_address;
+	prop->ring_size = conf->ring_size;
+	prop->credit = conf->credit;
+	prop->disable = !!conf->disable;
+}
+
+static void create_add_port_cmd(
+		struct pp_qos_dev *qdev,
+		const struct pp_qos_port_conf *conf,
+		unsigned int phy)
+{
+	struct cmd_add_port cmd;
+
+	if (PP_QOS_DEVICE_IS_ASSERT(qdev))
+		return;
+
+	memset(&cmd, 0, sizeof(cmd));
+	cmd_init(qdev, &(cmd.base), CMD_TYPE_ADD_PORT, sizeof(cmd), 0);
+	cmd.phy = phy;
+	set_cmd_port_properties(&cmd.prop, conf);
+
+	QOS_LOG_DEBUG("cmd %u:%u CMD_TYPE_ADD_PORT %u\n",
+			qdev->drvcmds.cmd_id,
+			qdev->drvcmds.cmd_fw_id,
+			phy);
+	cmd_queue_put(qdev->drvcmds.cmdq, &cmd, sizeof(cmd));
+	qdev->drvcmds.cmd_fw_id++;
+}
+
+
+static void _create_set_port_cmd(
+		struct pp_qos_dev *qdev,
+		const struct pp_qos_port_conf *conf,
+		unsigned int phy,
+		uint32_t modified,
+		struct cmd_queue *q,
+		uint32_t *pos)
+{
+	struct cmd_set_port cmd;
+
+	if (PP_QOS_DEVICE_IS_ASSERT(qdev))
+		return;
+
+	if (QOS_BITS_IS_SET(modified, QOS_MODIFIED_NODE_TYPE)) {
+		create_add_port_cmd(qdev, conf, phy);
+	} else {
+		memset(&cmd, 0, sizeof(cmd));
+		cmd_init(qdev, &(cmd.base), CMD_TYPE_SET_PORT, sizeof(cmd), 0);
+		cmd.phy = phy;
+		set_cmd_port_properties(&cmd.prop, conf);
+		cmd.modified = modified;
+		cmd.base.pos = pos;
+		cmd_queue_put(q, &cmd, sizeof(cmd));
+		qdev->drvcmds.cmd_fw_id++;
+	}
+}
+
+void create_set_port_cmd(
+		struct pp_qos_dev *qdev,
+		const struct pp_qos_port_conf *conf,
+		unsigned int phy,
+		uint32_t modified)
+{
+	if (PP_QOS_DEVICE_IS_ASSERT(qdev))
+		return;
+
+	_create_set_port_cmd(qdev, conf, phy, modified,
+			qdev->drvcmds.cmdq, NULL);
+	if (!QOS_BITS_IS_SET(modified, QOS_MODIFIED_NODE_TYPE))
+		QOS_LOG_DEBUG("cmd %u:%u CMD_TYPE_SET_PORT %u\n",
+				qdev->drvcmds.cmd_id,
+				qdev->drvcmds.cmd_fw_id,
+				phy);
+
+}
+
+static void set_cmd_sched_properties(
+		struct sched_properties *prop,
+		const struct pp_qos_sched_conf *conf)
+{
+	prop->common = conf->common_prop;
+	prop->parent = conf->sched_parent_prop;
+	prop->child = conf->sched_child_prop;
+}
+
+static void create_add_sched_cmd(
+		struct pp_qos_dev *qdev,
+		const struct pp_qos_sched_conf *conf,
+		unsigned int phy,
+		unsigned int parent)
+{
+	struct cmd_add_sched cmd;
+
+	if (PP_QOS_DEVICE_IS_ASSERT(qdev))
+		return;
+
+	memset(&cmd, 0, sizeof(cmd));
+	cmd_init(qdev, &(cmd.base), CMD_TYPE_ADD_SCHED, sizeof(cmd), 0);
+	cmd.phy = phy;
+	cmd.parent = parent;
+	fill_preds(qdev->nodes, phy, cmd.preds, 6);
+	set_cmd_sched_properties(&cmd.prop, conf);
+
+	QOS_LOG_DEBUG("cmd %u:%u CMD_TYPE_ADD_SCHED %u\n",
+			qdev->drvcmds.cmd_id,
+			qdev->drvcmds.cmd_fw_id, phy);
+	cmd_queue_put(qdev->drvcmds.cmdq, &cmd, sizeof(cmd));
+	add_suspend_port(qdev, get_port(qdev->nodes, phy));
+	qdev->drvcmds.cmd_fw_id++;
+}
+
+static void _create_set_sched_cmd(
+		struct pp_qos_dev *qdev,
+		const struct pp_qos_sched_conf *conf,
+		unsigned int phy,
+		unsigned int parent,
+		uint32_t modified)
+{
+	struct cmd_set_sched cmd;
+
+	if (PP_QOS_DEVICE_IS_ASSERT(qdev))
+		return;
+
+	if (QOS_BITS_IS_SET(modified, QOS_MODIFIED_NODE_TYPE)) {
+		create_add_sched_cmd(qdev, conf, phy, parent);
+	} else {
+		memset(&cmd, 0, sizeof(cmd));
+		cmd_init(qdev, &(cmd.base), CMD_TYPE_SET_SCHED, sizeof(cmd), 0);
+		cmd.phy = phy;
+		set_cmd_sched_properties(&cmd.prop, conf);
+		cmd.modified = modified;
+		cmd_queue_put(qdev->drvcmds.cmdq, &cmd, sizeof(cmd));
+		qdev->drvcmds.cmd_fw_id++;
+	}
+}
+
+void create_set_sched_cmd(
+		struct pp_qos_dev *qdev,
+		const struct pp_qos_sched_conf *conf,
+		unsigned int phy,
+		unsigned int parent,
+		uint32_t modified)
+{
+	if (PP_QOS_DEVICE_IS_ASSERT(qdev))
+		return;
+
+	_create_set_sched_cmd(qdev, conf, phy, parent, modified);
+
+	if (!QOS_BITS_IS_SET(modified, QOS_MODIFIED_NODE_TYPE))
+		QOS_LOG_DEBUG("cmd %u:%u CMD_TYPE_SET_SCHED %u\n",
+				qdev->drvcmds.cmd_id,
+				qdev->drvcmds.cmd_fw_id, phy);
+}
+
+static void set_cmd_queue_properties(
+		struct queue_properties *prop,
+		const struct pp_qos_queue_conf *conf,
+		unsigned int rlm)
+{
+	prop->common = conf->common_prop;
+	prop->child = conf->queue_child_prop;
+	prop->blocked = !!conf->blocked;
+	prop->wred_enable = !!conf->wred_enable;
+	prop->fixed_drop_prob_enable = !!conf->wred_fixed_drop_prob_enable;
+	prop->max_burst =  conf->max_burst;
+	prop->queue_wred_min_avg_green = conf->queue_wred_min_avg_green;
+	prop->queue_wred_max_avg_green = conf->queue_wred_max_avg_green;
+	prop->queue_wred_slope_green = conf->queue_wred_slope_green;
+	prop->queue_wred_min_avg_yellow = conf->queue_wred_min_avg_yellow;
+	prop->queue_wred_max_avg_yellow = conf->queue_wred_max_avg_yellow;
+	prop->queue_wred_slope_yellow = conf->queue_wred_slope_yellow;
+	prop->queue_wred_min_guaranteed = conf->queue_wred_min_guaranteed;
+	prop->queue_wred_max_allowed = conf->queue_wred_max_allowed;
+	prop->queue_wred_fixed_drop_prob_green =
+		conf->queue_wred_fixed_drop_prob_green;
+	prop->queue_wred_fixed_drop_prob_yellow =
+		conf->queue_wred_fixed_drop_prob_yellow;
+	prop->rlm = rlm;
+}
+
+static void create_add_queue_cmd(
+		struct pp_qos_dev *qdev,
+		const struct pp_qos_queue_conf *conf,
+		unsigned int phy,
+		unsigned int parent,
+		unsigned int rlm)
+{
+	struct cmd_add_queue cmd;
+
+	if (PP_QOS_DEVICE_IS_ASSERT(qdev))
+		return;
+
+	memset(&cmd, 0, sizeof(cmd));
+	cmd_init(
+			qdev,
+			&(cmd.base),
+			CMD_TYPE_ADD_QUEUE,
+			sizeof(cmd),
+			CMD_FLAGS_WRAP_PARENT_SUSPEND_RESUME);
+	cmd.phy = phy;
+	cmd.parent = parent;
+
+	cmd.port = get_port(qdev->nodes, phy);
+
+	fill_preds(qdev->nodes, phy, cmd.preds, 6);
+	set_cmd_queue_properties(&cmd.prop, conf, rlm);
+
+	QOS_LOG_DEBUG("cmd %u:%u CMD_TYPE_ADD_QUEUE %u\n",
+			qdev->drvcmds.cmd_id,
+			qdev->drvcmds.cmd_fw_id,
+			phy);
+	cmd_queue_put(qdev->drvcmds.cmdq, &cmd, sizeof(cmd));
+	add_suspend_port(qdev, cmd.port);
+	qdev->drvcmds.cmd_fw_id++;
+
+}
+
+static void _create_set_queue_cmd(
+		struct pp_qos_dev *qdev,
+		const struct pp_qos_queue_conf *conf,
+		unsigned int phy,
+		unsigned int parent,
+		unsigned int rlm,
+		uint32_t modified)
+{
+	struct cmd_set_queue cmd;
+
+	if (PP_QOS_DEVICE_IS_ASSERT(qdev))
+		return;
+
+	if (QOS_BITS_IS_SET(modified, QOS_MODIFIED_NODE_TYPE)) {
+		create_add_queue_cmd(qdev, conf, phy, parent, rlm);
+	} else {
+		memset(&cmd, 0, sizeof(cmd));
+		cmd_init(
+				qdev,
+				&(cmd.base),
+				CMD_TYPE_SET_QUEUE,
+				sizeof(cmd),
+				CMD_FLAGS_WRAP_SUSPEND_RESUME |
+				CMD_FLAGS_WRAP_PARENT_SUSPEND_RESUME);
+		cmd.phy = phy;
+		set_cmd_queue_properties(&cmd.prop, conf, rlm);
+		cmd.modified = modified;
+		cmd_queue_put(qdev->drvcmds.cmdq, &cmd, sizeof(cmd));
+		qdev->drvcmds.cmd_fw_id++;
+	}
+}
+
+void create_set_queue_cmd(
+		struct pp_qos_dev *qdev,
+		const struct pp_qos_queue_conf *conf,
+		unsigned int phy,
+		unsigned int parent,
+		unsigned int rlm,
+		uint32_t modified)
+{
+	if (PP_QOS_DEVICE_IS_ASSERT(qdev))
+		return;
+
+	_create_set_queue_cmd(qdev, conf, phy, parent, rlm, modified);
+	if (!QOS_BITS_IS_SET(modified, QOS_MODIFIED_NODE_TYPE))
+		QOS_LOG_DEBUG("cmd %u:%u CMD_TYPE_SET_QUEUE %u\n",
+				qdev->drvcmds.cmd_id,
+				qdev->drvcmds.cmd_fw_id, phy);
+}
+
+void create_flush_queue_cmd(struct pp_qos_dev *qdev, unsigned int rlm)
+{
+	struct cmd_flush_queue cmd;
+
+	if (PP_QOS_DEVICE_IS_ASSERT(qdev))
+		return;
+
+	cmd_init(qdev, &(cmd.base), CMD_TYPE_FLUSH_QUEUE, sizeof(cmd), 0);
+	cmd.rlm = rlm;
+	QOS_LOG_DEBUG("cmd %u:%u CMD_TYPE_FLUSH_QUEUE %u\n",
+			qdev->drvcmds.cmd_id,
+			qdev->drvcmds.cmd_fw_id,
+			rlm);
+	cmd_queue_put(qdev->drvcmds.cmdq, &cmd, sizeof(cmd));
+	qdev->drvcmds.cmd_fw_id++;
+}
+
+void create_parent_change_cmd(struct pp_qos_dev *qdev, unsigned int phy)
+{
+	struct cmd_parent_change cmd;
+	const struct qos_node *node;
+
+	if (PP_QOS_DEVICE_IS_ASSERT(qdev))
+		return;
+
+	node = get_const_node_from_phy(qdev->nodes, phy);
+	memset(&cmd, 0, sizeof(cmd));
+	cmd_init(qdev, &(cmd.base), CMD_TYPE_PARENT_CHANGE, sizeof(cmd), 0);
+	cmd.phy = phy;
+	cmd.type = node->type;
+	cmd.arbitration = node->parent_prop.arbitration;
+	cmd.first = node->parent_prop.first_child_phy;
+	cmd.num = node->parent_prop.num_of_children;
+	QOS_LOG_DEBUG("cmd %u:%u CMD_TYPE_PARENT_CHANGE %u first:%u num:%d\n",
+			qdev->drvcmds.cmd_id,
+			qdev->drvcmds.cmd_fw_id,
+			phy,
+			cmd.first,
+			cmd.num);
+	cmd_queue_put(qdev->drvcmds.cmdq, &cmd, sizeof(cmd));
+	qdev->drvcmds.cmd_fw_id++;
+}
+
+void create_get_port_stats_cmd(
+		struct pp_qos_dev *qdev,
+		unsigned int phy,
+		unsigned int addr,
+		struct pp_qos_port_stat *pstat)
+{
+	struct cmd_get_port_stats cmd;
+
+	if (PP_QOS_DEVICE_IS_ASSERT(qdev))
+		return;
+
+	memset(&cmd, 0, sizeof(cmd));
+	cmd_init(
+			qdev,
+			&(cmd.base),
+			CMD_TYPE_GET_PORT_STATS,
+			sizeof(cmd),
+			CMD_FLAGS_POST_PROCESS);
+	cmd.phy = phy;
+	cmd.addr =  addr;
+	cmd.stat = pstat;
+	QOS_LOG_DEBUG("cmd %u:%u CMD_TYPE_GET_PORT_STATS %u\n",
+			qdev->drvcmds.cmd_id,
+			qdev->drvcmds.cmd_fw_id,
+			phy);
+	cmd_queue_put(qdev->drvcmds.cmdq, &cmd, sizeof(cmd));
+	qdev->drvcmds.cmd_fw_id++;
+}
+
+void create_get_queue_stats_cmd(
+		struct pp_qos_dev *qdev,
+		unsigned int phy,
+		unsigned int rlm,
+		unsigned int addr,
+		struct pp_qos_queue_stat *qstat)
+{
+	struct cmd_get_queue_stats cmd;
+
+	if (PP_QOS_DEVICE_IS_ASSERT(qdev))
+		return;
+
+	memset(&cmd, 0, sizeof(cmd));
+	cmd_init(
+			qdev,
+			&(cmd.base),
+			CMD_TYPE_GET_QUEUE_STATS,
+			sizeof(cmd),
+			CMD_FLAGS_POST_PROCESS);
+	cmd.phy = phy;
+	cmd.rlm = rlm;
+	cmd.addr =  addr;
+	cmd.stat = qstat;
+	QOS_LOG_DEBUG("cmd %u:%u CMD_TYPE_GET_QUEUE_STATS %u\n",
+			qdev->drvcmds.cmd_id,
+			qdev->drvcmds.cmd_fw_id,
+			phy);
+	cmd_queue_put(qdev->drvcmds.cmdq, &cmd, sizeof(cmd));
+	qdev->drvcmds.cmd_fw_id++;
+}
+
+void create_get_node_info_cmd(
+		struct pp_qos_dev *qdev,
+		unsigned int phy,
+		unsigned int addr,
+		struct pp_qos_node_info *info)
+{
+	struct cmd_get_node_info cmd;
+
+	if (PP_QOS_DEVICE_IS_ASSERT(qdev))
+		return;
+
+	memset(&cmd, 0, sizeof(cmd));
+	cmd_init(
+			qdev,
+			&(cmd.base),
+			CMD_TYPE_GET_NODE_INFO,
+			sizeof(cmd),
+			CMD_FLAGS_POST_PROCESS);
+	cmd.phy = phy;
+	cmd.addr =  addr;
+	cmd.info = info;
+	QOS_LOG_DEBUG("cmd %u:%u CMD_TYPE_GET_NODE_INFO %u\n",
+			qdev->drvcmds.cmd_id,
+			qdev->drvcmds.cmd_fw_id,
+			phy);
+	cmd_queue_put(qdev->drvcmds.cmdq, &cmd, sizeof(cmd));
+	qdev->drvcmds.cmd_fw_id++;
+}
+
+void create_num_used_nodes_cmd(
+		struct pp_qos_dev *qdev,
+		unsigned int addr,
+		uint32_t *num)
+{
+	struct cmd_get_num_used_nodes cmd;
+
+	if (PP_QOS_DEVICE_IS_ASSERT(qdev))
+		return;
+
+	memset(&cmd, 0, sizeof(cmd));
+	cmd_init(
+			qdev,
+			&(cmd.base),
+			CMD_TYPE_GET_NUM_USED_NODES,
+			sizeof(cmd),
+			CMD_FLAGS_POST_PROCESS);
+	cmd.addr =  addr;
+	cmd.num = num;
+	QOS_LOG_DEBUG("cmd %u:%u CMD_TYPE_GET_NUM_USED_NODES\n",
+			qdev->drvcmds.cmd_id,
+			qdev->drvcmds.cmd_fw_id);
+	cmd_queue_put(qdev->drvcmds.cmdq, &cmd, sizeof(cmd));
+	qdev->drvcmds.cmd_fw_id++;
+}
+
+static void _create_set_shared_group_cmd(struct pp_qos_dev *qdev,
+		enum cmd_type ctype,
+		unsigned int id,
+		unsigned int limit)
+{
+	struct cmd_set_shared_group cmd;
+
+	if (PP_QOS_DEVICE_IS_ASSERT(qdev))
+		return;
+
+	cmd_init(qdev, &(cmd.base), ctype, sizeof(cmd), 0);
+	cmd.id = id;
+	cmd.limit = limit;
+	QOS_LOG("cmd %u:%u %s id %u limit %u\n",
+		qdev->drvcmds.cmd_id,
+		qdev->drvcmds.cmd_fw_id,
+		cmd_str[ctype],
+		id, limit);
+	cmd_queue_put(qdev->drvcmds.cmdq, &cmd, sizeof(cmd));
+	qdev->drvcmds.cmd_fw_id++;
+}
+
+void create_add_shared_group_cmd(struct pp_qos_dev *qdev,
+		unsigned int id,
+		unsigned int limit)
+{
+	_create_set_shared_group_cmd(qdev, CMD_TYPE_ADD_SHARED_GROUP,
+			id, limit);
+}
+
+void create_set_shared_group_cmd(struct pp_qos_dev *qdev,
+		unsigned int id,
+		unsigned int limit)
+{
+	_create_set_shared_group_cmd(qdev, CMD_TYPE_SET_SHARED_GROUP,
+			id, limit);
+}
+
+void create_remove_shared_group_cmd(struct pp_qos_dev *qdev,
+		unsigned int id)
+{
+	struct cmd_remove_shared_group cmd;
+
+	if (PP_QOS_DEVICE_IS_ASSERT(qdev))
+		return;
+
+	cmd_init(
+			qdev,
+			&(cmd.base),
+			CMD_TYPE_REMOVE_SHARED_GROUP,
+			sizeof(cmd), 0);
+	cmd.id = id;
+	QOS_LOG_DEBUG("cmd %u:%u CMD_TYPE_REMOVE_SHARED_GROUP id %u\n",
+			qdev->drvcmds.cmd_id,
+			qdev->drvcmds.cmd_fw_id,
+			id);
+	cmd_queue_put(qdev->drvcmds.cmdq, &cmd, sizeof(cmd));
+	qdev->drvcmds.cmd_fw_id++;
+}
+
+void create_push_desc_cmd(struct pp_qos_dev *qdev, unsigned int queue,
+		unsigned int size, unsigned int color, unsigned int addr)
+{
+	struct cmd_push_desc cmd;
+
+	if (PP_QOS_DEVICE_IS_ASSERT(qdev))
+		return;
+
+	cmd_init(qdev, &(cmd.base), CMD_TYPE_PUSH_DESC, sizeof(cmd), 0);
+	cmd.queue = queue;
+	cmd.size = size;
+	cmd.color = color;
+	cmd.addr = addr;
+	cmd_queue_put(qdev->drvcmds.cmdq, &cmd, sizeof(cmd));
+	qdev->drvcmds.cmd_fw_id++;
+}
+
+/******************************************************************************/
+/*                                 FW CMDS                                    */
+/******************************************************************************/
+
+struct fw_set_common {
+	uint32_t valid;
+	int suspend;
+	unsigned int bw_limit;
+	unsigned int shared_bw_group;
+};
+
+struct fw_set_parent {
+	uint32_t valid;
+	int best_effort_enable;
+	uint16_t first;
+	uint16_t last;
+	uint16_t first_wrr;
+};
+
+struct fw_set_child {
+	uint32_t valid;
+	uint32_t bw_share;
+	uint16_t preds[6];
+};
+
+struct fw_set_port {
+	uint32_t valid;
+	void	 *ring_addr;
+	size_t	 ring_size;
+	int active;
+};
+
+struct fw_set_sched {
+	uint32_t valid;
+};
+
+struct fw_set_queue {
+	uint32_t valid;
+	unsigned int rlm;
+	int active;
+	uint32_t disable;
+	unsigned int queue_wred_min_avg_green;
+	unsigned int queue_wred_max_avg_green;
+	unsigned int queue_wred_slope_green;
+	unsigned int queue_wred_min_avg_yellow;
+	unsigned int queue_wred_max_avg_yellow;
+	unsigned int queue_wred_slope_yellow;
+	unsigned int queue_wred_min_guaranteed;
+	unsigned int queue_wred_max_allowed;
+	unsigned int queue_wred_fixed_drop_prob_green;
+	unsigned int queue_wred_fixed_drop_prob_yellow;
+};
+
+struct fw_internal {
+	struct fw_set_common common;
+	struct fw_set_parent parent;
+	struct fw_set_child  child;
+	union {
+		struct fw_set_port port;
+		struct fw_set_sched sched;
+		struct fw_set_queue queue;
+	} type_data;
+	unsigned int suspend_port_index;
+	unsigned int suspend_ports[QOS_MAX_PORTS];
+	unsigned int moved_node_index;
+	struct move_info {
+		unsigned int phy;
+	} moved_nodes[2 * MAX_MOVING_NODES];
+	unsigned int	pushed;
+	int		ongoing;
+};
+
+/******************************************************************************/
+/*                         FW write functions                                 */
+/******************************************************************************/
+
+void add_suspend_port(struct pp_qos_dev *qdev, unsigned int port)
+{
+	struct fw_internal *internals;
+	unsigned int i;
+	struct qos_node *node;
+
+	node = get_node_from_phy(qdev->nodes, port);
+	QOS_ASSERT(node_port(node), "Node %u is not a port\n", port);
+	internals = qdev->fwbuf;
+	for (i = 0; i <  internals->suspend_port_index; ++i)
+		if (internals->suspend_ports[i] == port)
+			return;
+	QOS_ASSERT(internals->suspend_port_index <= qdev->max_port, "Suspend ports buffer is full\n");
+	internals->suspend_ports[internals->suspend_port_index] = port;
+	++(internals->suspend_port_index);
+}
+
+/*
+ * Maintains a list of destinations for nodes that were moved.
+ * This list is used to instruct firmware to resume suspend this nodes,
+ * to workaround HW inability to maintain the work available signal when node
+ * are moved.
+ * The logic is as follow:
+ * If the src of the new moved node is in that list - node is removed from list
+ * If dst of new moved node is not in that list - node is added to the list
+ */
+static void update_moved_nodes(
+		struct pp_qos_dev *qdev,
+		unsigned int src,
+		unsigned int dst)
+{
+	unsigned int i;
+	unsigned int j;
+	struct fw_internal *internals;
+	struct move_info *info;
+	int found;
+
+	internals = qdev->fwbuf;
+	j = internals->moved_node_index;
+	found = 0;
+
+	for (i = 0; i < j; ++i) {
+		info = internals->moved_nodes + i;
+		if (src == info->phy) {
+			info->phy = internals->moved_nodes[j - 1].phy;
+			--j;
+		}
+		if (dst == info->phy)
+			found = 1;
+	}
+
+	internals->moved_node_index = j;
+	QOS_ASSERT(internals->moved_node_index < 2 * MAX_MOVING_NODES,
+			"Moved ports buffer is full\n");
+	if (!found) {
+		internals->moved_nodes[j].phy = dst;
+		++(internals->moved_node_index);
+	}
+}
+
+
+int init_fwdata_internals(struct pp_qos_dev *qdev)
+{
+	qdev->fwbuf = QOS_MALLOC(sizeof(struct fw_internal));
+	if (qdev->fwbuf) {
+		memset(qdev->fwbuf, 0, sizeof(struct fw_internal));
+		return 0;
+	}
+	return -EBUSY;
+}
+
+void clean_fwdata_internals(struct pp_qos_dev *qdev)
+{
+	if (qdev->fwbuf)
+		QOS_FREE(qdev->fwbuf);
+}
+
+/*
+ * Following functions translate driver commands to firmware
+ * commands
+ */
+static uint32_t *fw_write_init_logger_cmd(
+		uint32_t *buf,
+		const struct cmd_init_logger *cmd,
+		uint32_t flags)
+{
+	*buf++ = qos_u32_to_uc(UC_QOS_COMMAND_INIT_UC_LOGGER);
+	*buf++ = qos_u32_to_uc(flags);
+	*buf++ = qos_u32_to_uc(4);
+	*buf++ = qos_u32_to_uc((uintptr_t)cmd->addr & 0xFFFFFFFF);
+	*buf++ = qos_u32_to_uc(cmd->mode);
+	*buf++ = qos_u32_to_uc(cmd->level);
+	*buf++ = qos_u32_to_uc(cmd->num_of_msgs);
+	return buf;
+}
+
+static uint32_t *fw_write_init_qos_cmd(
+		uint32_t *buf,
+		const struct cmd_init_qos *cmd,
+		uint32_t flags)
+{
+	*buf++ = qos_u32_to_uc(UC_QOS_COMMAND_INIT_QOS);
+	*buf++ = qos_u32_to_uc(flags);
+	*buf++ = qos_u32_to_uc(7);
+	*buf++ = qos_u32_to_uc(cmd->qm_ddr_start & 0xFFFFFFFF);
+	*buf++ = qos_u32_to_uc(cmd->qm_num_pages);
+	*buf++ = qos_u32_to_uc(cmd->wred_total_avail_resources);
+	*buf++ = qos_u32_to_uc(cmd->wred_prioritize_pop);
+	*buf++ = qos_u32_to_uc(cmd->wred_avg_q_size_p);
+	*buf++ = qos_u32_to_uc(cmd->wred_max_q_size);
+	*buf++ = qos_u32_to_uc(cmd->num_of_ports);
+	return buf;
+}
+
+static uint32_t *fw_write_add_port_cmd(
+		uint32_t *buf,
+		const struct cmd_add_port *cmd,
+		uint32_t flags)
+{
+	*buf++ = qos_u32_to_uc(UC_QOS_COMMAND_ADD_PORT);
+	*buf++ = qos_u32_to_uc(flags);
+	*buf++ = qos_u32_to_uc(13);
+	*buf++ = qos_u32_to_uc(cmd->phy);
+	*buf++ = qos_u32_to_uc(!cmd->prop.disable);
+	*buf++ = qos_u32_to_uc(0);
+	*buf++ = qos_u32_to_uc(0);
+	*buf++ = qos_u32_to_uc(cmd->prop.common.bandwidth_limit);
+	*buf++ = qos_u32_to_uc(!!cmd->prop.parent.best_effort_enable);
+	*buf++ = qos_u32_to_uc(0);
+	*buf++ = qos_u32_to_uc(cmd->prop.common.shared_bandwidth_group);
+	*buf++ = qos_u32_to_uc(cmd->prop.packet_credit_enable);
+	*buf++ = qos_u32_to_uc(cmd->prop.ring_size);
+	*buf++ = qos_u32_to_uc(GET_ADDRESS_HIGH(cmd->prop.ring_addr));
+	*buf++ = qos_u32_to_uc(((uintptr_t)cmd->prop.ring_addr) & 0xFFFFFFFF);
+	*buf++ = qos_u32_to_uc(cmd->prop.credit);
+	return buf;
+}
+
+static uint32_t *fw_write_set_port_cmd(
+		uint32_t *buf,
+		unsigned int phy,
+		uint32_t flags,
+		const struct fw_set_common *common,
+		struct fw_set_parent *parent,
+		const struct fw_set_port *port)
+{
+	*buf++ = qos_u32_to_uc(UC_QOS_COMMAND_SET_PORT);
+	*buf++ = qos_u32_to_uc(flags);
+	*buf++ = qos_u32_to_uc(14);
+	*buf++ = qos_u32_to_uc(phy);
+	*buf++ = qos_u32_to_uc(common->valid | parent->valid | port->valid);
+	*buf++ = qos_u32_to_uc(common->suspend);
+	if (parent->first > parent->last) {
+		parent->first = 0;
+		parent->last = 0;
+	}
+	*buf++ = qos_u32_to_uc(parent->first);
+	*buf++ = qos_u32_to_uc(parent->last);
+	*buf++ = qos_u32_to_uc(common->bw_limit);
+	*buf++ = qos_u32_to_uc(parent->best_effort_enable);
+	*buf++ = qos_u32_to_uc(parent->first_wrr);
+	*buf++ = qos_u32_to_uc(common->shared_bw_group);
+	*buf++ = qos_u32_to_uc(port->valid);
+	*buf++ = qos_u32_to_uc(port->ring_size);
+	*buf++ = qos_u32_to_uc(GET_ADDRESS_HIGH(port->ring_addr));
+	*buf++ = qos_u32_to_uc(((uintptr_t)port->ring_addr) & 0xFFFFFFFF);
+	*buf++ = qos_u32_to_uc(port->active);
+	return buf;
+}
+
+static uint32_t *fw_write_add_sched_cmd(
+		uint32_t *buf,
+		const struct cmd_add_sched *cmd,
+		uint32_t flags)
+{
+	unsigned int i;
+
+	*buf++ = qos_u32_to_uc(UC_QOS_COMMAND_ADD_SCHEDULER);
+	*buf++ = qos_u32_to_uc(flags);
+	*buf++ = qos_u32_to_uc(13);
+	*buf++ = qos_u32_to_uc(cmd->phy);
+	*buf++ = qos_u32_to_uc(0);
+	*buf++ = qos_u32_to_uc(0);
+	*buf++ = qos_u32_to_uc(cmd->prop.common.bandwidth_limit);
+	*buf++ = qos_u32_to_uc(!!cmd->prop.parent.best_effort_enable);
+	*buf++ = qos_u32_to_uc(0);
+	*buf++ = qos_u32_to_uc(cmd->prop.common.shared_bandwidth_group);
+	for (i = 0; i < 6; ++i)
+		*buf++ = qos_u32_to_uc(cmd->preds[i]);
+
+	return buf;
+}
+
+static uint32_t *fw_write_set_sched_cmd(
+		uint32_t *buf,
+		unsigned int phy,
+		uint32_t flags,
+		const struct fw_set_common *common,
+		struct fw_set_parent *parent,
+		const struct fw_set_child *child,
+		const struct fw_set_sched *sched)
+{
+	unsigned int i;
+
+	*buf++ = qos_u32_to_uc(UC_QOS_COMMAND_SET_SCHEDULER);
+	*buf++ = qos_u32_to_uc(flags);
+	*buf++ = qos_u32_to_uc(15);
+	*buf++ = qos_u32_to_uc(phy);
+	*buf++ = qos_u32_to_uc(common->valid | parent->valid |
+			child->valid | sched->valid);
+	*buf++ = qos_u32_to_uc(common->suspend);
+	if (parent->first > parent->last) {
+		parent->first = 0;
+		parent->last = 0;
+	}
+	*buf++ = qos_u32_to_uc(parent->first);
+	*buf++ = qos_u32_to_uc(parent->last);
+	*buf++ = qos_u32_to_uc(common->bw_limit);
+	*buf++ = qos_u32_to_uc(parent->best_effort_enable);
+	*buf++ = qos_u32_to_uc(parent->first_wrr);
+	*buf++ = qos_u32_to_uc(common->shared_bw_group);
+
+	for (i = 0; i < 6; ++i)
+		*buf++ = qos_u32_to_uc(child->preds[i]);
+
+	return buf;
+}
+
+static uint32_t *fw_write_add_queue_cmd(
+		uint32_t *buf,
+		const struct cmd_add_queue *cmd,
+		uint32_t flags)
+{
+	unsigned int i;
+	uint32_t disable;
+
+	*buf++ = qos_u32_to_uc(UC_QOS_COMMAND_ADD_QUEUE);
+	*buf++ = qos_u32_to_uc(flags);
+	*buf++ = qos_u32_to_uc(23);
+	*buf++ = qos_u32_to_uc(cmd->phy);
+	*buf++ = qos_u32_to_uc(cmd->port);
+	*buf++ = qos_u32_to_uc(cmd->prop.rlm);
+	*buf++ = qos_u32_to_uc(cmd->prop.common.bandwidth_limit);
+	*buf++ = qos_u32_to_uc(cmd->prop.common.shared_bandwidth_group);
+	for (i = 0; i < 6; ++i)
+		*buf++ = qos_u32_to_uc(cmd->preds[i]);
+	*buf++ = qos_u32_to_uc(!cmd->prop.blocked);
+
+	disable = 0;
+	if (!cmd->prop.wred_enable)
+		QOS_BITS_SET(disable, 1);
+	if (cmd->prop.fixed_drop_prob_enable)
+		QOS_BITS_SET(disable, 8);
+	*buf++ = qos_u32_to_uc(disable);
+
+	*buf++ = qos_u32_to_uc(cmd->prop.queue_wred_fixed_drop_prob_green);
+	*buf++ = qos_u32_to_uc(cmd->prop.queue_wred_fixed_drop_prob_yellow);
+	*buf++ = qos_u32_to_uc(cmd->prop.queue_wred_min_avg_yellow);
+	*buf++ = qos_u32_to_uc(cmd->prop.queue_wred_max_avg_yellow);
+	*buf++ = qos_u32_to_uc(cmd->prop.queue_wred_slope_yellow);
+	*buf++ = qos_u32_to_uc(cmd->prop.queue_wred_min_avg_green);
+	*buf++ = qos_u32_to_uc(cmd->prop.queue_wred_max_avg_green);
+	*buf++ = qos_u32_to_uc(cmd->prop.queue_wred_slope_green);
+	*buf++ = qos_u32_to_uc(cmd->prop.queue_wred_min_guaranteed);
+	if (cmd->prop.blocked)
+		*buf++ = qos_u32_to_uc(0);
+	else
+		*buf++ = qos_u32_to_uc(cmd->prop.queue_wred_max_allowed);
+	return buf;
+}
+
+static uint32_t *fw_write_set_queue_cmd(
+		uint32_t *buf,
+		unsigned int phy,
+		uint32_t flags,
+		const struct fw_set_common *common,
+		const struct fw_set_child *child,
+		const struct fw_set_queue *queue)
+{
+	unsigned int i;
+
+	*buf++ = qos_u32_to_uc(UC_QOS_COMMAND_SET_QUEUE);
+	*buf++ = qos_u32_to_uc(flags);
+	*buf++ = qos_u32_to_uc(25);
+	*buf++ = qos_u32_to_uc(phy);
+	*buf++ = qos_u32_to_uc(queue->rlm);
+	*buf++ = qos_u32_to_uc(common->valid | child->valid);
+	*buf++ = qos_u32_to_uc(common->suspend);
+	*buf++ = qos_u32_to_uc(common->bw_limit);
+	*buf++ = qos_u32_to_uc(common->shared_bw_group);
+	for (i = 0; i < 6; ++i)
+		*buf++ = qos_u32_to_uc(child->preds[i]);
+	*buf++ = qos_u32_to_uc(queue->valid);
+	*buf++ = qos_u32_to_uc(queue->active);
+	*buf++ = qos_u32_to_uc(queue->disable);
+
+	*buf++ = qos_u32_to_uc(queue->queue_wred_fixed_drop_prob_green);
+	*buf++ = qos_u32_to_uc(queue->queue_wred_fixed_drop_prob_yellow);
+	*buf++ = qos_u32_to_uc(queue->queue_wred_min_avg_yellow);
+	*buf++ = qos_u32_to_uc(queue->queue_wred_max_avg_yellow);
+	*buf++ = qos_u32_to_uc(queue->queue_wred_slope_yellow);
+	*buf++ = qos_u32_to_uc(queue->queue_wred_min_avg_green);
+	*buf++ = qos_u32_to_uc(queue->queue_wred_max_avg_green);
+	*buf++ = qos_u32_to_uc(queue->queue_wred_slope_green);
+	*buf++ = qos_u32_to_uc(queue->queue_wred_min_guaranteed);
+	*buf++ = qos_u32_to_uc(queue->queue_wred_max_allowed);
+
+	return buf;
+}
+
+static uint32_t *fw_write_flush_queue_cmd(
+		uint32_t *buf,
+		const struct cmd_flush_queue *cmd,
+		uint32_t flags)
+{
+	*buf++ = qos_u32_to_uc(UC_QOS_COMMAND_FLUSH_QUEUE);
+	*buf++ = qos_u32_to_uc(flags);
+	*buf++ = qos_u32_to_uc(1);
+	*buf++ = qos_u32_to_uc(cmd->rlm);
+	return buf;
+}
+
+static uint32_t *fw_write_move_sched_cmd(
+		uint32_t *buf,
+		const struct cmd_move *cmd,
+		uint32_t flags)
+{
+	unsigned int i;
+
+	*buf++ = qos_u32_to_uc(UC_QOS_COMMAND_MOVE_SCHEDULER);
+	*buf++ = qos_u32_to_uc(flags);
+	*buf++ = qos_u32_to_uc(8);
+	*buf++ = qos_u32_to_uc(cmd->src);
+	*buf++ = qos_u32_to_uc(cmd->dst);
+
+	for (i = 0; i < 6; ++i)
+		*buf++ = qos_u32_to_uc(cmd->preds[i]);
+
+	return buf;
+}
+
+static uint32_t *fw_write_move_queue_cmd(
+		uint32_t *buf,
+		const struct cmd_move *cmd,
+		uint32_t flags)
+{
+	unsigned int i;
+
+	*buf++ = qos_u32_to_uc(UC_QOS_COMMAND_MOVE_QUEUE);
+	*buf++ = qos_u32_to_uc(flags);
+	*buf++ = qos_u32_to_uc(10);
+	*buf++ = qos_u32_to_uc(cmd->src);
+	*buf++ = qos_u32_to_uc(cmd->dst);
+	*buf++ = qos_u32_to_uc(cmd->dst_port);
+	*buf++ = qos_u32_to_uc(cmd->rlm);
+
+	for (i = 0; i < 6; ++i)
+		*buf++ = qos_u32_to_uc(cmd->preds[i]);
+
+	return buf;
+}
+
+static uint32_t *fw_write_remove_queue_cmd(
+		uint32_t *buf,
+		const struct cmd_remove_node *cmd,
+		uint32_t flags)
+{
+	*buf++ = qos_u32_to_uc(UC_QOS_COMMAND_REMOVE_QUEUE);
+	*buf++ = qos_u32_to_uc(flags);
+	*buf++ = qos_u32_to_uc(2);
+	*buf++ = qos_u32_to_uc(cmd->phy);
+	*buf++ = qos_u32_to_uc(cmd->data);
+	return buf;
+}
+
+static uint32_t *fw_write_remove_sched_cmd(
+		uint32_t *buf,
+		const struct cmd_remove_node *cmd,
+		uint32_t flags)
+{
+	*buf++ = qos_u32_to_uc(UC_QOS_COMMAND_REMOVE_SCHEDULER);
+	*buf++ = qos_u32_to_uc(flags);
+	*buf++ = qos_u32_to_uc(1);
+	*buf++ = qos_u32_to_uc(cmd->phy);
+	return buf;
+}
+
+static uint32_t *fw_write_remove_port_cmd(
+		uint32_t *buf,
+		const struct cmd_remove_node *cmd,
+		uint32_t flags)
+{
+	*buf++ = qos_u32_to_uc(UC_QOS_COMMAND_REMOVE_PORT);
+	*buf++ = qos_u32_to_uc(flags);
+	*buf++ = qos_u32_to_uc(1);
+	*buf++ = qos_u32_to_uc(cmd->phy);
+	return buf;
+}
+
+static uint32_t *fw_write_get_queue_stats(
+		uint32_t *buf,
+		const struct cmd_get_queue_stats *cmd,
+		uint32_t flags)
+{
+	uint32_t reset;
+
+	*buf++ = qos_u32_to_uc(UC_QOS_COMMAND_GET_QUEUE_STATS);
+	*buf++ = qos_u32_to_uc(flags);
+	*buf++ = qos_u32_to_uc(4);
+	*buf++ = qos_u32_to_uc(cmd->phy);
+	*buf++ = qos_u32_to_uc(cmd->rlm);
+	*buf++ = qos_u32_to_uc((uintptr_t)cmd->addr & 0xFFFFFFFF);
+	if (cmd->stat->reset)
+		reset = QUEUE_STATS_CLEAR_Q_AVG_SIZE_BYTES |
+			QUEUE_STATS_CLEAR_DROP_P_YELLOW |
+			QUEUE_STATS_CLEAR_DROP_P_GREEN |
+			QUEUE_STATS_CLEAR_TOTAL_BYTES_ADDED |
+			QUEUE_STATS_CLEAR_TOTAL_ACCEPTS |
+			QUEUE_STATS_CLEAR_TOTAL_DROPS |
+			QUEUE_STATS_CLEAR_TOTAL_DROPPED_BYTES |
+			QUEUE_STATS_CLEAR_TOTAL_RED_DROPS;
+	else
+		reset = QUEUE_STATS_CLEAR_NONE;
+	*buf++ = qos_u32_to_uc(reset);
+
+	return buf;
+}
+
+static uint32_t *fw_write_get_port_stats(
+		uint32_t *buf,
+		const struct cmd_get_port_stats *cmd,
+		uint32_t flags)
+{
+	uint32_t reset;
+
+	*buf++ = qos_u32_to_uc(UC_QOS_COMMAND_GET_PORT_STATS);
+	*buf++ = qos_u32_to_uc(flags);
+	*buf++ = qos_u32_to_uc(3);
+	*buf++ = qos_u32_to_uc(cmd->phy);
+	*buf++ = qos_u32_to_uc((uintptr_t)cmd->addr & 0xFFFFFFFF);
+	if (cmd->stat->reset)
+		reset = PORT_STATS_CLEAR_ALL;
+	else
+		reset = PORT_STATS_CLEAR_NONE;
+	*buf++ = qos_u32_to_uc(reset);
+	return buf;
+}
+
+static uint32_t *fw_write_get_system_info(
+		uint32_t *buf,
+		const struct cmd_get_num_used_nodes *cmd,
+		uint32_t flags)
+{
+	*buf++ = qos_u32_to_uc(UC_QOS_COMMAND_GET_SYSTEM_STATS);
+	*buf++ = qos_u32_to_uc(flags);
+	*buf++ = qos_u32_to_uc(1);
+	*buf++ = qos_u32_to_uc((uintptr_t)cmd->addr & 0xFFFFFFFF);
+	return buf;
+}
+
+static uint32_t *fw_write_set_shared_group(
+		uint32_t *buf,
+		enum cmd_type ctype,
+		const struct cmd_set_shared_group *cmd,
+		uint32_t flags)
+{
+	uint32_t uc_cmd;
+
+	if (ctype == CMD_TYPE_ADD_SHARED_GROUP)
+		uc_cmd = UC_QOS_COMMAND_ADD_SHARED_BW_LIMIT_GROUP;
+	else
+		uc_cmd = UC_QOS_COMMAND_SET_SHARED_BW_LIMIT_GROUP;
+
+	*buf++ = qos_u32_to_uc(uc_cmd);
+	*buf++ = qos_u32_to_uc(flags);
+	*buf++ = qos_u32_to_uc(2);
+	*buf++ = qos_u32_to_uc(cmd->id);
+	*buf++ = qos_u32_to_uc(cmd->limit);
+	return buf;
+}
+
+static uint32_t *fw_write_remove_shared_group(uint32_t *buf,
+		const struct cmd_remove_shared_group *cmd,
+		uint32_t flags)
+{
+	*buf++ = qos_u32_to_uc(UC_QOS_COMMAND_REMOVE_SHARED_BW_LIMIT_GROUP);
+	*buf++ = qos_u32_to_uc(flags);
+	*buf++ = qos_u32_to_uc(1);
+	*buf++ = qos_u32_to_uc(cmd->id);
+	return buf;
+}
+
+static uint32_t *fw_write_push_desc(
+		uint32_t *buf,
+		const struct cmd_push_desc *cmd,
+		uint32_t flags)
+{
+	*buf++ = qos_u32_to_uc(UC_QOS_COMMAND_DEBUG_PUSH_DESC);
+	*buf++ = qos_u32_to_uc(flags);
+	*buf++ = qos_u32_to_uc(6);
+	*buf++ = qos_u32_to_uc(cmd->queue);
+	*buf++ = qos_u32_to_uc(cmd->size);
+	*buf++ = qos_u32_to_uc(cmd->color);
+	*buf++ = qos_u32_to_uc(cmd->addr);
+	*buf++ = qos_u32_to_uc(0);
+	*buf++ = qos_u32_to_uc(0);
+	return buf;
+}
+
+static uint32_t *fw_write_get_node_info(
+		uint32_t *buf,
+		const struct cmd_get_node_info *cmd,
+		uint32_t flags)
+{
+	*buf++ = qos_u32_to_uc(UC_QOS_COMMAND_GET_NODE_INFO);
+	*buf++ = qos_u32_to_uc(flags);
+	*buf++ = qos_u32_to_uc(2);
+	*buf++ = qos_u32_to_uc(cmd->phy);
+	*buf++ = qos_u32_to_uc(cmd->addr);
+	return buf;
+}
+/******************************************************************************/
+/*                                FW wrappers                                 */
+/******************************************************************************/
+
+static void set_common(
+		const struct pp_qos_common_node_properties *conf,
+		struct fw_set_common *common, uint32_t modified)
+{
+	uint32_t valid;
+
+	valid = 0;
+
+	if (QOS_BITS_IS_SET(modified, QOS_MODIFIED_BANDWIDTH_LIMIT)) {
+		QOS_BITS_SET(valid, TSCD_NODE_CONF_BW_LIMIT);
+		common->bw_limit = conf->bandwidth_limit;
+	}
+
+	if (QOS_BITS_IS_SET(modified, QOS_MODIFIED_SHARED_GROUP_ID)) {
+		QOS_BITS_SET(valid, TSCD_NODE_CONF_SHARED_BWL_GROUP);
+		common->shared_bw_group = conf->shared_bandwidth_group;
+	}
+	common->valid = valid;
+}
+
+static void set_parent(
+		const struct pp_qos_parent_node_properties *conf,
+		struct fw_set_parent *parent,
+		uint32_t modified)
+{
+	uint32_t valid;
+
+	valid = 0;
+	if (QOS_BITS_IS_SET(modified, QOS_MODIFIED_BEST_EFFORT)) {
+		QOS_BITS_SET(valid, TSCD_NODE_CONF_BEST_EFFORT_ENABLE);
+		parent->best_effort_enable = conf->best_effort_enable;
+	}
+	parent->valid = valid;
+}
+
+static void set_child(
+		const struct pp_qos_child_node_properties *conf,
+		struct fw_set_child *child,
+		uint32_t modified)
+{
+	uint32_t valid;
+
+	valid = 0;
+	if (QOS_BITS_IS_SET(modified, QOS_MODIFIED_VIRT_BW_SHARE)) {
+		QOS_BITS_SET(valid, TSCD_NODE_CONF_SHARED_BWL_GROUP);
+		child->bw_share = conf->bandwidth_share;
+	}
+	child->valid = valid;
+}
+
+static uint32_t *restart_node(
+		struct pp_qos_dev *qdev,
+		unsigned int phy,
+		uint32_t *_cur,
+		struct cmd_internal *cmd)
+{
+	struct qos_node *node;
+	struct fw_set_common common;
+	struct fw_set_parent parent;
+	struct fw_set_child child;
+	struct fw_set_sched sched;
+	struct fw_set_queue queue;
+	uint32_t *cur;
+
+	cur = _cur;
+	common.valid = TSCD_NODE_CONF_SUSPEND_RESUME;
+	parent.valid = 0;
+	child.valid = 0;
+	sched.valid = 0;
+	queue.valid = 0;
+	common.suspend = 1;
+
+	node = get_node_from_phy(qdev->nodes, phy);
+	QOS_ASSERT(!node_port(node), "Can't restart port %u\n", phy);
+
+	if (node_sched(node)) {
+		QOS_LOG_DEBUG("CMD_INTERNAL_RESTART_SCHED: %u\n", phy);
+		cmd->base.pos = cur;
+		cur = fw_write_set_sched_cmd(
+				cur,
+				phy,
+				0,
+				&common,
+				&parent,
+				&child,
+				&sched);
+
+		cmd_queue_put(
+				qdev->drvcmds.pendq,
+				cmd,
+				cmd->base.len);
+
+		common.suspend = 0;
+		cmd->base.pos = cur;
+		cur = fw_write_set_sched_cmd(
+				cur,
+				phy,
+				0,
+				&common,
+				&parent,
+				&child,
+				&sched);
+		cmd_queue_put(
+				qdev->drvcmds.pendq,
+				cmd,
+				cmd->base.len);
+	} else if (node_queue(node)) {
+		cmd->base.pos = cur;
+		queue.rlm = node->data.queue.rlm;
+		QOS_LOG_DEBUG("CMD_INTERNAL_RESTART_QUEUE: %u\n", phy);
+		cur = fw_write_set_queue_cmd(
+				cur,
+				phy,
+				0,
+				&common,
+				&child,
+				&queue);
+		cmd_queue_put(
+				qdev->drvcmds.pendq,
+				cmd,
+				cmd->base.len);
+
+		common.suspend = 0;
+		cmd->base.pos = cur;
+		cur = fw_write_set_queue_cmd(
+				cur,
+				phy,
+				0,
+				&common,
+				&child,
+				&queue);
+		cmd_queue_put(
+				qdev->drvcmds.pendq,
+				cmd,
+				cmd->base.len);
+	}
+
+	return cur;
+}
+
+#if 0
+static void set_port_specific(
+		const struct port_properties *conf,
+		struct fw_set_port *port,
+		uint32_t modified)
+{
+	uint32_t valid;
+
+	valid = 0;
+	if (QOS_BITS_IS_SET(modified, QOS_MODIFIED_RING_ADDRESS)) {
+		QOS_BITS_SET(valid,
+				PORT_CONF_RING_ADDRESS_HIGH |
+				PORT_CONF_RING_ADDRESS_LOW);
+		port->ring_addr = conf->ring_addr;
+	}
+	if (QOS_BITS_IS_SET(modified, QOS_MODIFIED_RING_SIZE)) {
+		QOS_BITS_SET(valid, PORT_CONF_RING_SIZE);
+		port->ring_size = conf->ring_size;
+	}
+	port->valid = valid;
+}
+#endif
+
+static uint32_t *set_port_cmd_wrapper(
+		struct fw_internal *fwdata,
+		uint32_t *buf,
+		const struct cmd_set_port *cmd,
+		uint32_t flags)
+{
+	uint32_t modified;
+
+	modified = cmd->modified;
+	set_common(&cmd->prop.common, &fwdata->common, modified);
+	set_parent(&cmd->prop.parent, &fwdata->parent, modified);
+	fwdata->type_data.port.valid = 0;
+	if (QOS_BITS_IS_SET(modified, QOS_MODIFIED_DISABLE)) {
+		QOS_BITS_SET(fwdata->type_data.port.valid, PORT_CONF_ACTIVE);
+		fwdata->type_data.port.active = !!cmd->prop.disable;
+	}
+
+	if ((fwdata->common.valid | fwdata->parent.valid |
+				fwdata->type_data.port.valid) == 0) {
+		QOS_LOG_DEBUG("IGNORING EMPTY CMD_TYPE_SET_PORT\n");
+		return buf;
+	}
+
+	return fw_write_set_port_cmd(
+			buf,
+			cmd->phy,
+			flags,
+			&fwdata->common,
+			&fwdata->parent,
+			&fwdata->type_data.port);
+}
+
+static uint32_t *set_sched_cmd_wrapper(
+		struct fw_internal *fwdata,
+		uint32_t *buf,
+		const struct cmd_set_sched *cmd,
+		uint32_t flags)
+{
+	uint32_t modified;
+
+	modified = cmd->modified;
+	set_common(&cmd->prop.common, &fwdata->common, modified);
+	set_parent(&cmd->prop.parent, &fwdata->parent, modified);
+	set_child(&cmd->prop.child, &fwdata->child, modified);
+	fwdata->type_data.sched.valid = 0;
+
+	if ((fwdata->common.valid |
+				fwdata->parent.valid |
+				fwdata->child.valid) == 0) {
+		QOS_LOG_DEBUG("IGNORING EMPTY CMD_TYPE_SET_SCHED\n");
+		return buf;
+	}
+
+	return fw_write_set_sched_cmd(
+			buf,
+			cmd->phy,
+			flags,
+			&fwdata->common,
+			&fwdata->parent,
+			&fwdata->child,
+			&fwdata->type_data.sched);
+}
+
+/*
+ * Topologic changes like first/last child change or change of predecessors
+ * will not be manifested through this path. They will be manifested through
+ * CMD_TYPE_PARENT_CHANGE and CMD_TYPE_UPDATE_PREDECESSORS driver commands
+ * which will call fw_write_set_node_cmd.
+ * So the only think that needs to use fw_write_set_node_cmd in this path is
+ * modify of suspend/resume
+ *
+ */
+static uint32_t *set_queue_cmd_wrapper(
+		struct fw_internal *fwdata,
+		uint32_t *buf,
+		const struct cmd_set_queue *cmd,
+		uint32_t flags)
+{
+	uint32_t modified;
+	uint32_t valid;
+	uint32_t disable;
+	struct fw_set_queue *queue;
+
+	queue = &fwdata->type_data.queue;
+	modified = cmd->modified;
+	set_common(&cmd->prop.common, &fwdata->common, modified);
+	set_child(&cmd->prop.child, &fwdata->child, modified);
+
+	valid = 0;
+	queue->rlm = cmd->prop.rlm;
+
+	if (QOS_BITS_IS_SET(modified, QOS_MODIFIED_WRED_MAX_ALLOWED)) {
+		QOS_BITS_SET(valid, WRED_QUEUE_CONF_MAX_ALLOWED);
+		queue->queue_wred_max_allowed =
+			cmd->prop.queue_wred_max_allowed;
+	}
+
+	if (QOS_BITS_IS_SET(modified, QOS_MODIFIED_BLOCKED)) {
+		QOS_BITS_SET(valid, WRED_QUEUE_CONF_ACTIVE_Q);
+		QOS_BITS_SET(valid, WRED_QUEUE_CONF_MAX_ALLOWED);
+		queue->active = !cmd->prop.blocked;
+		if (queue->active)
+			queue->queue_wred_max_allowed =
+				cmd->prop.queue_wred_max_allowed;
+		else
+			queue->queue_wred_max_allowed = 0;
+
+	}
+
+	disable = 0;
+	if (QOS_BITS_IS_SET(modified, QOS_MODIFIED_WRED_ENABLE)) {
+		QOS_BITS_SET(valid, WRED_QUEUE_CONF_DISABLE);
+		if (!cmd->prop.wred_enable)
+			QOS_BITS_SET(disable, BIT(0));
+	}
+	if (QOS_BITS_IS_SET(
+				modified,
+				QOS_MODIFIED_WRED_FIXED_DROP_PROB_ENABLE)) {
+		QOS_BITS_SET(valid, WRED_QUEUE_CONF_DISABLE);
+		if (cmd->prop.fixed_drop_prob_enable)
+			QOS_BITS_SET(disable, BIT(3));
+	}
+	queue->disable = disable;
+
+	if (QOS_BITS_IS_SET(modified, QOS_MODIFIED_WRED_FIXED_GREEN_PROB)) {
+		QOS_BITS_SET(valid, WRED_QUEUE_CONF_FIXED_GREEN_DROP_P);
+		queue->queue_wred_fixed_drop_prob_green =
+			cmd->prop.queue_wred_fixed_drop_prob_green;
+	}
+
+	if (QOS_BITS_IS_SET(modified, QOS_MODIFIED_WRED_FIXED_YELLOW_PROB)) {
+		QOS_BITS_SET(valid, WRED_QUEUE_CONF_FIXED_YELLOW_DROP_P);
+		queue->queue_wred_fixed_drop_prob_yellow =
+			cmd->prop.queue_wred_fixed_drop_prob_yellow;
+	}
+
+	if (QOS_BITS_IS_SET(modified, QOS_MODIFIED_WRED_MIN_YELLOW)) {
+		QOS_BITS_SET(valid, WRED_QUEUE_CONF_MIN_AVG_YELLOW);
+		queue->queue_wred_min_avg_yellow =
+			cmd->prop.queue_wred_min_avg_yellow;
+	}
+
+	if (QOS_BITS_IS_SET(modified, QOS_MODIFIED_WRED_MAX_YELLOW)) {
+		QOS_BITS_SET(valid, WRED_QUEUE_CONF_MAX_AVG_YELLOW);
+		queue->queue_wred_max_avg_yellow =
+			cmd->prop.queue_wred_max_avg_yellow;
+	}
+
+	if (QOS_BITS_IS_SET(modified, QOS_MODIFIED_WRED_SLOPE_YELLOW)) {
+		QOS_BITS_SET(valid, WRED_QUEUE_CONF_SLOPE_YELLOW);
+		queue->queue_wred_slope_yellow =
+			cmd->prop.queue_wred_slope_yellow;
+	}
+
+	if (QOS_BITS_IS_SET(modified, QOS_MODIFIED_WRED_MIN_GREEN)) {
+		QOS_BITS_SET(valid, WRED_QUEUE_CONF_MIN_AVG_GREEN);
+		queue->queue_wred_min_avg_green =
+			cmd->prop.queue_wred_min_avg_green;
+	}
+
+	if (QOS_BITS_IS_SET(modified, QOS_MODIFIED_WRED_MAX_GREEN)) {
+		QOS_BITS_SET(valid, WRED_QUEUE_CONF_MAX_AVG_GREEN);
+		queue->queue_wred_max_avg_green =
+			cmd->prop.queue_wred_max_avg_green;
+	}
+
+	if (QOS_BITS_IS_SET(modified, QOS_MODIFIED_WRED_SLOPE_GREEN)) {
+		QOS_BITS_SET(valid, WRED_QUEUE_CONF_SLOPE_GREEN);
+		queue->queue_wred_slope_green =
+			cmd->prop.queue_wred_slope_green;
+	}
+
+	if (QOS_BITS_IS_SET(modified, QOS_MODIFIED_WRED_MIN_GUARANTEED)) {
+		QOS_BITS_SET(valid, WRED_QUEUE_CONF_MIN_GUARANTEED);
+		queue->queue_wred_min_guaranteed =
+			cmd->prop.queue_wred_min_guaranteed;
+	}
+
+	if ((valid | fwdata->common.valid | fwdata->child.valid) == 0) {
+		QOS_LOG_DEBUG("IGNORING EMPTY CMD_TYPE_SET_QUEUE\n");
+		return buf;
+	}
+
+	queue->valid = valid;
+	return fw_write_set_queue_cmd(
+			buf,
+			cmd->phy,
+			flags,
+			&fwdata->common,
+			&fwdata->child,
+			queue);
+}
+
+static uint32_t *parent_change_cmd_wrapper(
+		struct fw_internal *fwdata,
+		uint32_t *buf,
+		const struct cmd_parent_change *cmd,
+		uint32_t flags)
+{
+	fwdata->parent.valid =
+		TSCD_NODE_CONF_FIRST_CHILD |
+		TSCD_NODE_CONF_LAST_CHILD |
+		TSCD_NODE_CONF_FIRST_WRR_NODE;
+	fwdata->parent.first = cmd->first;
+	fwdata->parent.last = cmd->first + cmd->num - 1;
+	if (cmd->arbitration == PP_QOS_ARBITRATION_WSP)
+		fwdata->parent.first_wrr = 0;
+	else
+		fwdata->parent.first_wrr = cmd->first;
+
+	fwdata->common.valid = 0;
+	if (cmd->type == TYPE_PORT) {
+		fwdata->type_data.port.valid  = 0;
+		return fw_write_set_port_cmd(
+				buf,
+				cmd->phy,
+				flags,
+				&fwdata->common,
+				&fwdata->parent,
+				&fwdata->type_data.port);
+	} else {
+		fwdata->child.valid = 0;
+		fwdata->type_data.sched.valid = 0;
+		return fw_write_set_sched_cmd(
+				buf,
+				cmd->phy,
+				flags,
+				&fwdata->common,
+				&fwdata->parent,
+				&fwdata->child,
+				&fwdata->type_data.sched);
+	}
+}
+
+static uint32_t *update_preds_cmd_wrapper(
+		struct fw_internal *fwdata,
+		uint32_t *buf,
+		const struct cmd_update_preds *cmd,
+		uint32_t flags)
+{
+	unsigned int i;
+
+	fwdata->common.valid = 0;
+	fwdata->child.valid = TSCD_NODE_CONF_PREDECESSOR_0 |
+		TSCD_NODE_CONF_PREDECESSOR_1 |
+		TSCD_NODE_CONF_PREDECESSOR_2 |
+		TSCD_NODE_CONF_PREDECESSOR_3 |
+		TSCD_NODE_CONF_PREDECESSOR_4 |
+		TSCD_NODE_CONF_PREDECESSOR_5;
+	for (i = 0; i < 6; ++i)
+		fwdata->child.preds[i] = cmd->preds[i];
+
+	if (cmd->node_type == TYPE_SCHED) {
+		fwdata->type_data.sched.valid  = 0;
+		fwdata->parent.valid = 0;
+		return fw_write_set_sched_cmd(
+				buf,
+				cmd->phy,
+				flags,
+				&fwdata->common,
+				&fwdata->parent,
+				&fwdata->child,
+				&fwdata->type_data.sched);
+	} else {
+		fwdata->type_data.queue.valid = 0;
+		fwdata->type_data.queue.rlm = cmd->rlm;
+		return fw_write_set_queue_cmd(
+				buf,
+				cmd->phy,
+				flags,
+				&fwdata->common,
+				&fwdata->child,
+				&fwdata->type_data.queue);
+	}
+}
+
+/*
+ * Signal firmware to read and executes commands from cmd
+ * buffer.
+ * This is done by using the mailbox bundle.
+ * Refer to driver's design document for further information
+ * Since this is the only signal that is sent from driver to firmware
+ * the value of DRV_SIGNAL is insignificant
+ */
+#define DRV_SIGNAL (2U)
+void signal_uc(struct pp_qos_dev *qdev)
+{
+	if (PP_QOS_DEVICE_IS_ASSERT(qdev))
+		return;
+#ifndef NO_FW
+	iowrite32(DRV_SIGNAL, qdev->fwcom.mbx_to_uc + 4);
+	QOS_LOG_DEBUG("signal uc was called\n");
+#endif
+}
+
+/******************************************************************************/
+/*                                Engine                                      */
+/******************************************************************************/
+
+/*
+ * Translation to user format and sanity checks on node info
+ * obtained from firmware
+ */
+static void post_process_get_node_info(
+		struct pp_qos_dev *qdev,
+		struct cmd_get_node_info *cmd)
+{
+	struct pp_qos_node_info *info;
+	struct hw_node_info_s *fw_info;
+	struct qos_node *node;
+	struct qos_node *child;
+	uint16_t preds[6];
+	uint32_t *fw_preds;
+	unsigned int first;
+	unsigned int phy;
+	unsigned int id;
+	unsigned int i;
+	unsigned int num;
+	unsigned int port;
+	int reached_port;
+
+	port = PPV4_QOS_INVALID;
+	node = get_node_from_phy(qdev->nodes, cmd->phy);
+	QOS_ASSERT(node_used(node), "Node %u is not used\n", cmd->phy);
+	fw_info = (struct hw_node_info_s *)(qdev->stat);
+	info = cmd->info;
+	memset(info, 0xFF, sizeof(*info));
+
+	QOS_ASSERT(node->type != TYPE_UNKNOWN,
+			"Node %u has unknown type\n",
+			cmd->phy);
+	info->type = node->type - 1;
+	info->is_internal = node_internal(node);
+
+	if (node_parent(node)) {
+		first = fw_info->first_child;
+		if (first == 0) {
+			QOS_ASSERT(fw_info->last_child == 0,
+					"HW reports first child 0 but last child is %u\n",
+					fw_info->last_child);
+			first = QOS_INVALID_PHY;
+			num = 0;
+		} else {
+			num = fw_info->last_child - first + 1;
+			child = get_node_from_phy(qdev->nodes, first);
+		}
+		QOS_ASSERT(node->parent_prop.num_of_children == num,
+				"Driver has %u as the number of children of node %u, while HW has %u\n",
+				node->parent_prop.num_of_children,
+				cmd->phy,
+				num);
+
+		QOS_ASSERT(num == 0 ||
+				node->parent_prop.first_child_phy == first,
+				"Driver has %u as the phy of node's %u first child, while HW has %u\n",
+				node->parent_prop.first_child_phy,
+				cmd->phy,
+				first);
+
+		for (i = 0; i < num; ++i) {
+			phy = i + first;
+			id = get_id_from_phy(qdev->mapping, phy);
+			QOS_ASSERT(QOS_ID_VALID(id),
+					"Child of %u with phy %u has no valid id\n",
+					cmd->phy, phy);
+			QOS_ASSERT(node_used(child),
+					"Child node with phy %u and id %u is not used\n",
+					phy,
+					id);
+			info->children[i].phy = phy;
+			info->children[i].id = id;
+			++child;
+		}
+	}
+
+	if (!node_port(node)) {
+		fill_preds(qdev->nodes, cmd->phy, preds, 6);
+		fw_preds = &(fw_info->predecessor0);
+		reached_port = 0;
+		for (i = 0; i < 6; ++i) {
+			QOS_ASSERT(preds[i] == *fw_preds,
+					"Driver has %u as the %u predecessor of node %u, while HW has %u\n",
+					preds[i],
+					i,
+					cmd->phy,
+					*fw_preds);
+
+			if (!reached_port) {
+				info->preds[i].phy = preds[i];
+				id = get_id_from_phy(qdev->mapping, preds[i]);
+				QOS_ASSERT(QOS_ID_VALID(id),
+						"Pred with phy %u has no valid id\n",
+						preds[i]);
+				info->preds[i].id = id;
+				if (preds[i] <= qdev->max_port) {
+					reached_port = 1;
+					port = preds[i];
+				}
+			} else {
+				info->preds[i].phy = PPV4_QOS_INVALID;
+			}
+			++fw_preds;
+		}
+	}
+
+	if (node_queue(node)) {
+		QOS_ASSERT(node->data.queue.rlm == fw_info->queue_physical_id,
+				"Node %u physical queue is %u according to driver and %u according to HW\n",
+				cmd->phy, node->data.queue.rlm,
+				fw_info->queue_physical_id);
+		QOS_ASSERT(port == fw_info->queue_port,
+				"Driver has %u as %u port, while HW has %u\n",
+				port,
+				cmd->phy,
+				fw_info->queue_physical_id);
+
+		info->queue_physical_id = fw_info->queue_physical_id;
+		info->port = fw_info->queue_port;
+	}
+
+	QOS_ASSERT(fw_info->bw_limit == node->bandwidth_limit,
+			"Driver has %u as node's %u bandwidth limit, while HW has %u\n",
+			node->bandwidth_limit,
+			cmd->phy,
+			fw_info->bw_limit);
+
+	info->bw_limit = fw_info->bw_limit;
+}
+
+/*
+ * Commands that are marked with POST_PROCESS reach
+ * here for further processing before return to client
+ */
+static void post_process(struct pp_qos_dev *qdev, union driver_cmd *dcmd)
+{
+	enum cmd_type  type;
+	struct pp_qos_queue_stat *qstat;
+	struct pp_qos_port_stat *pstat;
+	struct queue_stats_s *fw_qstat;
+	struct port_stats_s *fw_pstat;
+	struct system_stats_s *fw_sys_stat;
+
+	if (PP_QOS_DEVICE_IS_ASSERT(qdev))
+		return;
+
+	type = dcmd->cmd.type;
+	switch (type) {
+	case CMD_TYPE_GET_QUEUE_STATS:
+		fw_qstat = (struct queue_stats_s *)(qdev->stat);
+		qstat = dcmd->queue_stats.stat;
+		qstat->queue_packets_occupancy = fw_qstat->
+			qmgr_num_queue_entries;
+		qstat->queue_bytes_occupancy = fw_qstat->queue_size_bytes;
+		qstat->total_packets_accepted = fw_qstat->total_accepts;
+		qstat->total_packets_dropped = fw_qstat->total_drops;
+		qstat->total_packets_red_dropped = fw_qstat->total_red_dropped;
+		qstat->total_bytes_accepted =
+			(((uint64_t)fw_qstat->total_bytes_added_high) << 32)
+			| fw_qstat->total_bytes_added_low;
+		qstat->total_bytes_dropped =
+			(((uint64_t)fw_qstat->total_dropped_bytes_high) << 32)
+			| fw_qstat->total_dropped_bytes_low;
+		break;
+
+	case CMD_TYPE_GET_PORT_STATS:
+		fw_pstat = (struct port_stats_s *)(qdev->stat);
+		pstat = dcmd->port_stats.stat;
+		pstat->total_green_bytes = fw_pstat->total_green_bytes;
+		pstat->total_yellow_bytes = fw_pstat->total_yellow_bytes;
+		break;
+
+	case CMD_TYPE_GET_NUM_USED_NODES:
+		fw_sys_stat = (struct system_stats_s *)qdev->stat;
+		*(dcmd->num_used.num) = fw_sys_stat->tscd_num_of_used_nodes;
+		break;
+
+	case CMD_TYPE_GET_NODE_INFO:
+		post_process_get_node_info(qdev, &dcmd->node_info);
+		break;
+
+	default:
+		QOS_ASSERT(0, "Unexpected cmd %d for post process\n", type);
+		return;
+
+	}
+}
+
+#define MAX_FW_CMD_SIZE 120U
+#define MS_SLEEP_BETWEEN_POLL 10U
+#define NUM_OF_POLLS	500U
+
+/*
+ * Go over all commands on pending queue until cmd id
+ * is changed or queue is empty
+ * (refer to driver design document to learn more about cmd id).
+ * On current implmentation it is expected that pending queue contain
+ * firmware commands for a single client command, therfore queue should
+ * become empty before cmd id is changed.
+ *
+ * For each command wait until firmware signals
+ * completion before continue to next command.
+ * Completion status for each command is polled NUM_OF_POLLS
+ * times. And the function sleeps between pools.
+ * If command have not completed after all that polls -
+ * function asserts.
+ *
+ */
+void check_completion(struct pp_qos_dev *qdev)
+{
+	union driver_cmd dcmd;
+
+	volatile uint32_t *pos;
+	uint32_t	val;
+	size_t len;
+	unsigned int idcur;
+	int rc;
+	unsigned int i;
+	unsigned int popped;
+	struct fw_internal *internals;
+
+	if (PP_QOS_DEVICE_IS_ASSERT(qdev))
+		return;
+
+	popped = 0;
+	idcur = 0;
+	while (cmd_queue_peek(
+			      qdev->drvcmds.pendq,
+			      &dcmd.cmd,
+			      sizeof(struct cmd)) == 0) {
+		pos = dcmd.stub.cmd.pos;
+		len = dcmd.cmd.len;
+		++pos;
+#ifndef NO_FW
+		i = 0;
+		val = qos_u32_from_uc(*pos);
+		while ((val &
+			(UC_CMD_FLAG_UC_DONE | UC_CMD_FLAG_UC_ERROR)) == 0) {
+			qos_sleep(MS_SLEEP_BETWEEN_POLL);
+			val = qos_u32_from_uc(*pos);
+			++i;
+			if (i == NUM_OF_POLLS) {
+				QOS_ASSERT(
+					0,
+					"FW is not responding, polling offset 0x%04tX for cmd type %s\n",
+					(void *)pos -
+					(void *)(qdev->fwcom.cmdbuf),
+					cmd_str[dcmd.cmd.type]);
+				return;
+			}
+		}
+		if (val & UC_CMD_FLAG_UC_ERROR) {
+			QOS_ASSERT(0,
+				   "FW signaled error, polling offset 0x%04tX, cmd type %s\n",
+				   (void *)pos - (void *)(qdev->fwcom.cmdbuf),
+				   cmd_str[dcmd.cmd.type]);
+			return;
+		}
+#endif
+		rc = cmd_queue_get(qdev->drvcmds.pendq, &dcmd.stub, len);
+		QOS_ASSERT(rc == 0,
+			   "Command queue does not contain a full command\n");
+		if (dcmd.cmd.flags & CMD_FLAGS_POST_PROCESS)
+			post_process(qdev, &dcmd);
+		++popped;
+	}
+
+	internals = qdev->fwbuf;
+	QOS_ASSERT(popped == internals->pushed,
+		   "Expected to pop %u msgs from pending queue but popped %u\n",
+		   internals->pushed, popped);
+	QOS_ASSERT(cmd_queue_is_empty(qdev->drvcmds.pendq),
+		   "Driver's pending queue is not empty\n");
+	internals->pushed = 0;
+	qdev->drvcmds.cmd_fw_id = 0;
+}
+
+#define FW_CMD_BUFFER_DCCM_START 0xF0006000
+
+
+/*
+ * Take all commands from driver cmd queue, translate them to
+ * firmware format and put them on firmware queue.
+ * When finish signal firmware.
+ */
+void enqueue_cmds(struct pp_qos_dev *qdev)
+{
+	size_t len;
+	int rc;
+	uint32_t *cur;
+	uint32_t *prev;
+	uint32_t *start;
+	size_t remain;
+	uint32_t flags;
+	union driver_cmd dcmd;
+	struct cmd_internal	cmd_internal;
+	struct fw_internal *internals;
+	unsigned int pushed;
+	unsigned int i;
+	struct fw_set_common common;
+	struct fw_set_parent parent;
+	struct fw_set_port port;
+
+	if (PP_QOS_DEVICE_IS_ASSERT(qdev))
+		return;
+
+	start = qdev->fwcom.cmdbuf;
+	remain = qdev->fwcom.cmdbuf_sz;
+
+	pushed = 0;
+	cur = start;
+	prev = start;
+	*cur++ = qos_u32_to_uc(UC_QOS_COMMAND_MULTIPLE_COMMANDS);
+	*cur++ = qos_u32_to_uc(UC_CMD_FLAG_IMMEDIATE);
+	*cur++ = qos_u32_to_uc(1);
+	*cur = qos_u32_to_uc(((uintptr_t)(cur) - (uintptr_t)start +
+				4 + FW_CMD_BUFFER_DCCM_START) & 0xFFFFFFFF);
+	++cur;
+
+	internals = qdev->fwbuf;
+	remain -= 16;
+	flags = UC_CMD_FLAG_IMMEDIATE;
+
+	cmd_init(
+			qdev,
+			&(cmd_internal.base),
+			CMD_TYPE_INTERNAL,
+			sizeof(cmd_internal), 0);
+	common.valid = TSCD_NODE_CONF_SUSPEND_RESUME;
+	parent.valid = 0;
+	port.valid = 0;
+
+	if (!internals->ongoing) {
+		common.suspend = 1;
+		for (i = 0; i < internals->suspend_port_index; ++i) {
+			prev = cur;
+			QOS_LOG_DEBUG("CMD_INTERNAL_SUSPEND_PORT port: %u\n",
+					internals->suspend_ports[i]);
+
+			cur = fw_write_set_port_cmd(
+					prev,
+					internals->suspend_ports[i],
+					flags,
+					&common,
+					&parent,
+					&port);
+
+			if (cur != prev) {
+				cmd_internal.base.pos = prev;
+				cmd_queue_put(
+						qdev->drvcmds.pendq,
+						&cmd_internal,
+						cmd_internal.base.len);
+				++pushed;
+				remain -= (uintptr_t)cur - (uintptr_t)prev;
+			}
+		}
+		if (pushed)
+			internals->ongoing = 1;
+	}
+
+	while ((remain >= MAX_FW_CMD_SIZE) &&
+			cmd_queue_peek(
+				qdev->drvcmds.cmdq,
+				&dcmd.cmd,
+				sizeof(struct cmd)) == 0) {
+		len = dcmd.cmd.len;
+		rc = cmd_queue_get(qdev->drvcmds.cmdq, &dcmd.stub, len);
+		QOS_ASSERT(rc == 0,
+				"Command queue does not contain a full command\n");
+		prev = cur;
+		dcmd.stub.cmd.pos = cur;
+		switch (dcmd.cmd.type) {
+		case CMD_TYPE_INIT_LOGGER:
+			cur = fw_write_init_logger_cmd(
+					prev,
+					&dcmd.init_logger,
+					flags);
+			break;
+
+		case CMD_TYPE_INIT_QOS:
+			cur = fw_write_init_qos_cmd(
+					prev,
+					&dcmd.init_qos,
+					flags);
+			break;
+
+		case CMD_TYPE_MOVE:
+			if (dcmd.move.node_type == TYPE_QUEUE)
+				cur = fw_write_move_queue_cmd(
+						prev,
+						&dcmd.move,
+						flags);
+			else
+				cur = fw_write_move_sched_cmd(
+						prev,
+						&dcmd.move,
+						flags);
+			break;
+
+		case CMD_TYPE_PARENT_CHANGE:
+			cur = parent_change_cmd_wrapper(
+					internals,
+					prev,
+					&dcmd.parent_change,
+					flags);
+			break;
+
+		case CMD_TYPE_UPDATE_PREDECESSORS:
+			cur = update_preds_cmd_wrapper(
+					internals,
+					prev,
+					&dcmd.update_preds,
+					flags);
+			break;
+
+		case CMD_TYPE_ADD_PORT:
+			cur = fw_write_add_port_cmd(
+					prev,
+					&dcmd.add_port,
+					flags);
+			break;
+
+		case CMD_TYPE_SET_PORT:
+			cur = set_port_cmd_wrapper(
+					internals,
+					prev,
+					&dcmd.set_port,
+					flags);
+			break;
+
+		case CMD_TYPE_REMOVE_PORT:
+			cur = fw_write_remove_port_cmd(
+					prev,
+					&dcmd.remove_node,
+					flags);
+			break;
+
+		case CMD_TYPE_ADD_SCHED:
+			cur = fw_write_add_sched_cmd(
+					prev,
+					&dcmd.add_sched,
+					flags);
+			break;
+
+		case CMD_TYPE_ADD_QUEUE:
+			cur = fw_write_add_queue_cmd(
+					prev,
+					&dcmd.add_queue,
+					flags);
+			break;
+
+		case CMD_TYPE_REMOVE_QUEUE:
+			cur = fw_write_remove_queue_cmd(
+					prev,
+					&dcmd.remove_node,
+					flags);
+			break;
+
+		case CMD_TYPE_REMOVE_SCHED:
+			cur = fw_write_remove_sched_cmd(
+					prev,
+					&dcmd.remove_node,
+					flags);
+			break;
+
+		case CMD_TYPE_SET_SCHED:
+			cur = set_sched_cmd_wrapper(
+					internals,
+					prev,
+					&dcmd.set_sched,
+					flags);
+			break;
+
+		case CMD_TYPE_SET_QUEUE:
+			cur = set_queue_cmd_wrapper(
+					internals,
+					prev,
+					&dcmd.set_queue,
+					flags);
+			break;
+
+		case CMD_TYPE_REMOVE_NODE:
+			QOS_ASSERT(0,
+					"Did not expect CMD_TYPE_REMOVE_NODE\n");
+			break;
+
+		case CMD_TYPE_GET_QUEUE_STATS:
+			cur = fw_write_get_queue_stats(
+					prev, &dcmd.queue_stats, flags);
+			break;
+
+		case CMD_TYPE_GET_PORT_STATS:
+			cur = fw_write_get_port_stats(
+					prev, &dcmd.port_stats, flags);
+			break;
+
+		case CMD_TYPE_GET_NUM_USED_NODES:
+			cur = fw_write_get_system_info(
+					prev, &dcmd.num_used, flags);
+			break;
+
+		case CMD_TYPE_ADD_SHARED_GROUP:
+		case CMD_TYPE_SET_SHARED_GROUP:
+			cur = fw_write_set_shared_group(
+					prev,
+					dcmd.cmd.type,
+					&dcmd.set_shared_group,
+					flags);
+			break;
+
+		case CMD_TYPE_REMOVE_SHARED_GROUP:
+			cur = fw_write_remove_shared_group(
+					prev,
+					&dcmd.remove_shared_group,
+					flags);
+			break;
+		case CMD_TYPE_PUSH_DESC:
+			cur = fw_write_push_desc(
+					prev,
+					&dcmd.pushd,
+					flags);
+			break;
+
+		case CMD_TYPE_GET_NODE_INFO:
+			cur = fw_write_get_node_info(
+					prev,
+					&dcmd.node_info,
+					flags);
+			break;
+
+		case CMD_TYPE_FLUSH_QUEUE:
+			cur = fw_write_flush_queue_cmd(
+					prev,
+					&dcmd.flush_queue,
+					flags);
+			break;
+
+		default:
+			QOS_ASSERT(0, "Unexpected msg type %d\n",
+					dcmd.cmd.type);
+			return;
+		}
+		if (cur != prev) {
+			cmd_queue_put(
+					qdev->drvcmds.pendq,
+					&dcmd.stub,
+					dcmd.stub.cmd.len);
+			++pushed;
+			remain -= (uintptr_t)cur - (uintptr_t)prev;
+		}
+	}
+
+	if (cmd_queue_is_empty(qdev->drvcmds.cmdq)) {
+		for (i = 0; i < internals->moved_node_index; ++i) {
+			prev = cur;
+			cur = restart_node(qdev,
+					internals->moved_nodes[i].phy,
+					cur,
+					&cmd_internal);
+			if (cur != prev) {
+				pushed += 2;
+				remain -= (uintptr_t)cur - (uintptr_t)prev;
+			}
+		}
+
+		common.suspend = 0;
+		for (i = 0; i < internals->suspend_port_index; ++i) {
+			prev = cur;
+			QOS_LOG_DEBUG("CMD_INTERNAL_RESUME_PORT port: %u\n",
+					internals->suspend_ports[i]);
+
+			cur = fw_write_set_port_cmd(
+					prev,
+					internals->suspend_ports[i],
+					flags,
+					&common,
+					&parent,
+					&port);
+
+			if (cur != prev) {
+				cmd_internal.base.pos = prev;
+				cmd_queue_put(
+						qdev->drvcmds.pendq,
+						&cmd_internal,
+						cmd_internal.base.len);
+				++pushed;
+				remain -= (uintptr_t)cur - (uintptr_t)prev;
+			}
+		}
+		internals->suspend_port_index = 0;
+		internals->moved_node_index = 0;
+		internals->ongoing = 0;
+	}
+
+	if (pushed) {
+		internals->pushed = pushed;
+		prev += 1;
+		flags = qos_u32_from_uc(*prev);
+		QOS_BITS_SET(flags, UC_CMD_FLAG_MULTIPLE_COMMAND_LAST);
+		*prev = qos_u32_to_uc(flags);
+		signal_uc(qdev);
+	}
+}
+#endif
diff --git a/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_fw.h b/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_fw.h
new file mode 100644
index 000000000000..bd1b97fd5e82
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_fw.h
@@ -0,0 +1,167 @@
+/*
+ * GPL LICENSE SUMMARY
+ *
+ *  Copyright(c) 2017 Intel Corporation.
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of version 2 of the GNU General Public License as
+ *  published by the Free Software Foundation.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *  The full GNU General Public License is included in this distribution
+ *  in the file called LICENSE.GPL.
+ *
+ *  Contact Information:
+ *  Intel Corporation
+ *  2200 Mission College Blvd.
+ *  Santa Clara, CA  97052
+ */
+#ifndef PP_QOS_FW_H
+#define PP_QOS_FW_H
+
+#include "pp_qos_utils.h"
+#include "pp_qos_common.h"
+#include "pp_qos_uc_wrapper.h"
+
+struct ppv4_qos_fw {
+	size_t size;
+	const uint8_t *data;
+};
+
+int load_firmware(struct pp_qos_dev *qdev, const char *name);
+
+#ifndef PP_QOS_DISABLE_CMDQ
+void create_move_cmd(struct pp_qos_dev *qdev, uint16_t dst, uint16_t src,
+		     uint16_t dst_port);
+void create_set_port_cmd(struct pp_qos_dev *qdev,
+			 const struct pp_qos_port_conf *conf, unsigned int phy,
+			 uint32_t modified);
+void create_set_queue_cmd(struct pp_qos_dev *qdev,
+			  const struct pp_qos_queue_conf *conf,
+			  unsigned int phy,
+			  unsigned int parent,
+			  unsigned int rlm, uint32_t modified);
+void create_set_sched_cmd(struct pp_qos_dev *qdev,
+			  const struct pp_qos_sched_conf *conf,
+			  unsigned int phy,
+			  unsigned int parent, uint32_t modified);
+void create_remove_node_cmd(struct pp_qos_dev *qdev, enum node_type type,
+			    unsigned int phy, unsigned int data);
+void create_parent_change_cmd(struct pp_qos_dev *qdev, unsigned int phy);
+void create_update_preds_cmd(struct pp_qos_dev *qdev, unsigned int phy);
+void create_init_qos_cmd(struct pp_qos_dev *qdev);
+void enqueue_cmds(struct pp_qos_dev *qdev);
+void check_completion(struct pp_qos_dev *qdev);
+void create_get_queue_stats_cmd(
+		struct pp_qos_dev *qdev,
+		unsigned int phy,
+		unsigned int rlm,
+		unsigned int addr,
+		struct pp_qos_queue_stat *qstat);
+void create_get_port_stats_cmd(
+		struct pp_qos_dev *qdev,
+		unsigned int phy,
+		unsigned int addr,
+		struct pp_qos_port_stat *pstat);
+int init_fwdata_internals(struct pp_qos_dev *qdev);
+void clean_fwdata_internals(struct pp_qos_dev *qdev);
+void create_init_logger_cmd(struct pp_qos_dev *qdev);
+void create_add_shared_group_cmd(struct pp_qos_dev *qdev,
+		unsigned int id,
+		unsigned int limit);
+void create_remove_shared_group_cmd(struct pp_qos_dev *qdev,
+		unsigned int id);
+
+void create_set_shared_group_cmd(struct pp_qos_dev *qdev,
+		unsigned int id,
+		unsigned int limit);
+
+void create_push_desc_cmd(struct pp_qos_dev *qdev, unsigned int queue,
+		unsigned int size, unsigned int color, unsigned int addr);
+
+void create_get_node_info_cmd(
+		struct pp_qos_dev *qdev,
+		unsigned int phy,
+		unsigned int addr,
+		struct pp_qos_node_info *info);
+
+void create_flush_queue_cmd(struct pp_qos_dev *qdev, unsigned int rlm);
+int do_load_firmware(
+		struct pp_qos_dev *qdev,
+		const struct ppv4_qos_fw *fw,
+		void *ddr_base,
+		void *ivt_base,
+		void *data);
+
+void add_suspend_port(struct pp_qos_dev *qdev, unsigned int port);
+void signal_uc(struct pp_qos_dev *qdev);
+void create_num_used_nodes_cmd(struct pp_qos_dev *qdev,
+		unsigned int addr,
+		uint32_t *num);
+#elif defined(PRINT_CREATE_CMD)
+#define create_move_cmd(qdev, dst, src, dst_port)\
+	QOS_LOG_DEBUG("MOVE: %u ==> %u\n", src, dst)
+#define create_set_port_cmd(qdev, conf, phy, modified)\
+	QOS_LOG_DEBUG("SET PORT: %u\n", phy)
+#define create_set_sched_cmd(qdev, conf, phy, parent, modified)\
+	QOS_LOG_DEBUG("SET SCH: %u\n", phy)
+#define create_set_queue_cmd(qdev, conf, phy, parent, rlm, modified)\
+	QOS_LOG_DEBUG("SET QUEUE: %u\n", phy)
+#define create_remove_node_cmd(qdev, type, phy, data)\
+	QOS_LOG_DEBUG("REMOVE: %u(%u)\n", phy, type)
+#define create_parent_change_cmd(qdev, phy)\
+	QOS_LOG_DEBUG("PARENT_CHANGE: %u\n", phy)
+#define create_update_preds_cmd(qdev, phy)\
+	QOS_LOG_DEBUG("UPDATE_PREDS: %u\n", phy)
+#define create_get_queue_stats_cmd(qdev, phy, rlm, addr, qstat)
+#define create_init_qos_cmd(qdev)
+#define enqueue_cmds(qdev)
+#define check_completion(qdev)
+#define init_fwdata_internals(qdev) 0
+#define clean_fwdata_internals(qdev)
+#define create_init_logger_cmd(qdev)
+#define create_add_shared_group_cmd(qdev, id, limit)
+#define create_set_shared_group_cmd(qdev, id, limit)
+#define create_remove_shared_group_cmd(qdev, id)
+#define create_get_queue_stats_cmd(qdev, phy, rlm, addr, qstat)
+#define create_get_port_stats_cmd(qdev, phy, addr, pstat)
+#define create_get_node_info_cmd(qdev, phy, addr, info)
+#define create_push_desc_cmd(qdev, queue, size, color, addr)
+#define create_num_used_nodes_cmd(qdev, addr, num)
+#define add_suspend_port(qdev, port)
+#define create_flush_queue_cmd(qdev, rlm)
+#else
+#define create_move_cmd(qdev, dst, src, dst_port)
+#define create_set_port_cmd(qdev, conf, phy, modified)
+#define create_set_sched_cmd(qdev, conf, phy, parent, modified)
+#define create_set_queue_cmd(qdev, conf, phy, parent, rlm, modified)
+#define create_remove_node_cmd(qdev, type, phy, data)
+#define create_parent_change_cmd(qdev, phy)
+#define create_update_preds_cmd(qdev, phy)
+#define create_get_queue_stats_cmd(qdev, phy, rlm, addr, qstat)
+#define create_init_qos_cmd(qdev)
+#define enqueue_cmds(qdev)
+#define check_completion(qdev)
+#define init_fwdata_internals(qdev) 0
+#define clean_fwdata_internals(qdev)
+#define create_init_logger_cmd(qdev)
+#define create_add_shared_group_cmd(qdev, id, limit)
+#define create_set_shared_group_cmd(qdev, id, limit)
+#define create_remove_shared_group_cmd(qdev, id)
+#define create_get_queue_stats_cmd(qdev, phy, rlm, addr, qstat)
+#define create_get_port_stats_cmd(qdev, phy, addr, pstat)
+#define create_get_node_info_cmd(qdev, phy, addr, info)
+#define create_push_desc_cmd(qdev, queue, size, color, addr)
+#define create_flush_queue_cmd(qdev, rlm)
+#define create_num_used_nodes_cmd(qdev, addr, num)
+#define add_suspend_port(qdev, port)
+#endif
+#endif
diff --git a/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_kernel.h b/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_kernel.h
new file mode 100644
index 000000000000..fe1d38b81036
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_kernel.h
@@ -0,0 +1,75 @@
+/*
+ * GPL LICENSE SUMMARY
+ *
+ *  Copyright(c) 2017 Intel Corporation.
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of version 2 of the GNU General Public License as
+ *  published by the Free Software Foundation.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *  The full GNU General Public License is included in this distribution
+ *  in the file called LICENSE.GPL.
+ *
+ *  Contact Information:
+ *  Intel Corporation
+ *  2200 Mission College Blvd.
+ *  Santa Clara, CA  97052
+ */
+#ifndef _PP_QOS_KERNEL_H
+#define _PP_QOS_KERNEL_H
+
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/platform_device.h>
+
+int qos_dbg_dev_init(struct platform_device *pdev);
+void qos_dbg_dev_clean(struct platform_device *pdev);
+int qos_dbg_module_init(void);
+void qos_dbg_module_clean(void);
+
+struct pp_qos_dbg_data {
+	struct dentry *dir;
+	uint16_t	node;
+	void		*fw_logger_addr;
+};
+
+struct pp_qos_drv_data {
+	int	id;
+	struct pp_qos_dev *qdev;
+	struct pp_qos_dbg_data dbg;
+	void *ddr;
+	void *dccm;
+	void *ivt;
+	void __iomem *wakeuc;
+	resource_size_t ddr_phy_start;
+
+};
+
+#ifndef __BIG_ENDIAN
+#define ENDIANESS_MATCH(endianess) (endianess == 0)
+#else
+#define ENDIANESS_MATCH(endianess) (endianess != 0)
+#endif
+
+#ifdef CONFIG_OF
+#define PPV4_QOS_MANUAL_ADD(rc)
+#define PPV4_QOS_MANUAL_REMOVE()
+#else
+#define PPV4_QOS_MANUAL_ADD(rc)		\
+do {					\
+	if (rc == 0)			\
+		rc = add_qos_dev();	\
+} while (0)
+#define PPV4_QOS_MANUAL_REMOVE() remove_qos_dev()
+#endif
+
+#endif
diff --git a/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_linux.c b/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_linux.c
new file mode 100644
index 000000000000..8e84e927005b
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_linux.c
@@ -0,0 +1,569 @@
+/*
+ * GPL LICENSE SUMMARY
+ *
+ *  Copyright(c) 2017 Intel Corporation.
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of version 2 of the GNU General Public License as
+ *  published by the Free Software Foundation.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *  The full GNU General Public License is included in this distribution
+ *  in the file called LICENSE.GPL.
+ *
+ *  Contact Information:
+ *  Intel Corporation
+ *  2200 Mission College Blvd.
+ *  Santa Clara, CA  97052
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/ctype.h>
+#include <linux/errno.h>
+#include <linux/device.h>
+#include <linux/of_device.h>
+#include <linux/platform_device.h>
+#include <linux/io.h>
+#include <linux/firmware.h>
+#include <linux/of_address.h>
+#include <linux/dma-mapping.h>
+#include "pp_qos_kernel.h"
+#include "pp_qos_common.h"
+#include "pp_qos_utils.h"
+#include "pp_qos_fw.h"
+
+/*
+ * TODO - this obstruct multi instances but helps debug
+ * at this phase
+ */
+struct device *cur_dev;
+
+#define PPV4_QOS_CMD_BUF_SIZE (0x1000)
+#define PPV4_QOS_CMD_BUF_OFFSET (0x6000U)
+#define MAX_CMD_SIZE 150
+
+void stop_run(void)
+{
+	struct pp_qos_drv_data *pdata;
+
+	if (cur_dev) {
+		dev_crit(cur_dev, "!!!!! Qos driver in unstable mode !!!!!\n");
+		pdata = dev_get_drvdata(cur_dev);
+		QOS_BITS_SET(pdata->qdev->flags, PP_QOS_FLAGS_ASSERT);
+	}
+}
+
+void wake_uc(void *data)
+{
+	struct pp_qos_drv_data *pdata;
+
+	pdata = (struct pp_qos_drv_data *)data;
+	iowrite32(4, pdata->wakeuc);
+}
+
+int load_firmware(struct pp_qos_dev *qdev, const char *name)
+{
+	int err;
+	const struct firmware *firmware;
+	struct ppv4_qos_fw fw;
+	struct pp_qos_drv_data *pdata;
+	struct device *dev;
+	struct platform_device *pdev;
+
+	pdev = (struct platform_device *)(qdev->pdev);
+	dev = &pdev->dev;
+	pdata = platform_get_drvdata(pdev);
+	err = request_firmware(&firmware, name, dev);
+	if (err < 0) {
+		dev_err(dev, "Failed loading firmware ret is %d\n", err);
+		return err;
+	}
+
+	fw.size = firmware->size;
+	fw.data = firmware->data;
+	err = do_load_firmware(qdev,
+			&fw,
+			pdata->ddr - pdata->ddr_phy_start,
+			pdata->ivt - PPV4_QOS_IVT_START,
+			pdata);
+
+	release_firmware(firmware);
+	return err;
+}
+
+static const struct of_device_id pp_qos_match[] = {
+	{ .compatible = "intel,falconmx-ppv4qos" },
+	{},
+};
+MODULE_DEVICE_TABLE(of, pp_qos_match);
+
+static struct resource *get_resource(
+		struct platform_device *pdev,
+		const char *name,
+		unsigned int type,
+		size_t size)
+{
+	struct resource *r;
+	size_t sz;
+	struct device *dev;
+
+	dev = &pdev->dev;
+
+	r = platform_get_resource_byname(pdev, type, name);
+	if (!r) {
+		dev_err(dev, "Could not get %s resource\n", name);
+		return NULL;
+	}
+
+	sz = resource_size(r);
+	if (size && sz != size) {
+		dev_err(dev, "Illegal size %zu for %s resource expected %zu\n",
+				sz, name, size);
+		return NULL;
+	}
+
+	return r;
+}
+
+static void print_resource(
+		struct device *dev,
+		const char *name,
+		struct resource *r)
+{
+
+	dev_info(dev, "%s memory resource: start(0x%08zX), size(%zu)\n",
+			name,
+			(size_t)(uintptr_t)r->start,
+			(size_t)(uintptr_t)resource_size(r));
+}
+
+static void *map_mem_resource(
+		struct platform_device *pdev,
+		const char *name,
+		unsigned int type,
+		unsigned int size)
+{
+	struct resource *r;
+	void *addr;
+	struct device *dev;
+	int err;
+
+	dev = &pdev->dev;
+
+	r = get_resource(
+			pdev,
+			name,
+			type,
+			size);
+	if (!r)
+		return NULL;
+
+	print_resource(dev, name, r);
+	addr = devm_memremap(dev, r->start, resource_size(r), MEMREMAP_WT);
+	if (IS_ERR_OR_NULL(addr)) {
+		err = (int) PTR_ERR(addr);
+		dev_err(dev,
+			"devm_memremap on resource %s failed with %d\n",
+			name,
+			err);
+		return NULL;
+	}
+	dev_info(dev, "%s memory mapped to %p\n", name, addr);
+
+	return addr;
+}
+
+static void __iomem *map_reg_resource(
+		struct platform_device *pdev,
+		const char *name,
+		unsigned int type,
+		unsigned int size)
+{
+	struct resource *r;
+	void __iomem *addr;
+
+	r = get_resource(
+			pdev,
+			name,
+			type,
+			size);
+
+	if (!r)
+		return NULL;
+
+	print_resource(&pdev->dev, name, r);
+
+	addr = devm_ioremap_resource(&pdev->dev, r);
+	if (!addr)
+		dev_err(&pdev->dev, "Map of resource %s failed\n", name);
+	else
+		dev_info(&pdev->dev, "%s register mapped to %p\n", name, addr);
+
+	return addr;
+}
+
+static int pp_qos_get_resources(
+		struct platform_device *pdev,
+		struct qos_dev_init_info *info)
+{
+	int err;
+	struct pp_qos_drv_data *pdata;
+	struct device *dev;
+	void __iomem  *regaddr;
+	void *memaddr;
+
+	dev = &pdev->dev;
+	pdata = platform_get_drvdata(pdev);
+
+	/* Cmd buffer and dccm */
+	memaddr = map_mem_resource(pdev, "dccm", IORESOURCE_MEM, 0);
+	if (!memaddr)
+		return -ENODEV;
+	pdata->dccm = memaddr;
+	info->fwcom.cmdbuf = memaddr + PPV4_QOS_CMD_BUF_OFFSET;
+	info->fwcom.cmdbuf_sz = PPV4_QOS_CMD_BUF_SIZE;
+
+	memaddr = map_mem_resource(pdev, "ivt", IORESOURCE_MEM, 0);
+	if (!memaddr)
+		return -ENODEV;
+	pdata->ivt = memaddr;
+
+	/* Mbx from uc */
+	regaddr = map_reg_resource(pdev, "mbx-from-uc", IORESOURCE_MEM, 24);
+	if (!regaddr)
+		return -ENODEV;
+	info->fwcom.mbx_from_uc = regaddr;
+
+	/* Mbx to uc */
+	regaddr = map_reg_resource(pdev, "mbx-to-uc", IORESOURCE_MEM, 24);
+	if (!regaddr)
+		return -ENODEV;
+	info->fwcom.mbx_to_uc = regaddr;
+
+	regaddr = map_reg_resource(pdev, "wake-uc", IORESOURCE_MEM, 4);
+	if (!regaddr)
+		return -ENODEV;
+	pdata->wakeuc = regaddr;
+
+	err = platform_get_irq(pdev, 0);
+	if (err < 0) {
+		dev_err(dev,
+			"Could not retrieve irqline from platform err is %d\n",
+			err);
+		return err;
+	}
+	info->fwcom.irqline = err;
+	dev_info(dev, "irq is %d\n", err);
+
+	return 0;
+}
+
+#ifdef CONFIG_OF
+int allocate_ddr_for_qm(struct pp_qos_dev *qdev)
+{
+	void *addr;
+	dma_addr_t dma;
+	struct device *dev;
+	unsigned int size;
+
+	dev = &((struct platform_device *)qdev->pdev)->dev;
+	size = qdev->hwconf.wred_total_avail_resources * PPV4_QOS_DESC_SIZE;
+
+	addr = dmam_alloc_coherent(dev, size, &dma, GFP_KERNEL);
+	if (addr == NULL) {
+		dev_err(dev,
+			"Could not allocate %u bytes for queue manager\n",
+			size);
+		return -ENOMEM;
+	}
+
+	qdev->hwconf.qm_ddr_start = (unsigned int)(dma);
+	qdev->hwconf.qm_num_pages =
+		qdev->hwconf.wred_total_avail_resources / PPV4_QOS_DESC_IN_PAGE;
+
+	return 0;
+}
+#else
+int allocate_ddr_for_qm(struct pp_qos_dev *qdev)
+{
+	struct ppv4_qos_platform_data *psrc;
+	struct device *dev;
+
+	dev = &((struct platform_device *)qdev->pdev)->dev;
+	psrc = (struct ppv4_qos_platform_data *)dev_get_platdata(dev);
+	if (!psrc) {
+		dev_err(dev, "Device contain no platform data\n");
+		return -ENODEV;
+	}
+
+	qdev->hwconf.qm_num_pages =
+		qdev->hwconf.wred_total_avail_resources / PPV4_QOS_DESC_IN_PAGE;
+
+	qdev->hwconf.qm_ddr_start = psrc->qm_ddr_start;
+	return 0;
+}
+#endif
+
+
+static int pp_qos_config_from_of_node(
+		struct platform_device *pdev,
+		struct ppv4_qos_platform_data *pdata,
+		struct pp_qos_drv_data *pdrvdata)
+{
+	int err;
+	uint32_t val;
+	struct resource r;
+	struct device_node *node;
+	struct device *dev;
+	void *addr;
+	dma_addr_t dma;
+
+	dev = &pdev->dev;
+	node = pdev->dev.of_node;
+	dev_info(&pdev->dev, "Using device tree info to init platform data\n");
+	err = of_alias_get_id(node, "qos");
+	if (err < 0) {
+		dev_err(dev, "failed to get alias id, errno %d\n", err);
+		return err;
+	}
+	pdata->id = err;
+
+	err = of_property_read_u32(node, "intel,qos-max-port", &val);
+	if (err) {
+		dev_err(dev,
+			"Could not get max port from DT, error is %d\n",
+			err);
+		return -ENODEV;
+	}
+	pdata->max_port = val;
+
+	err = of_property_read_u32(
+			node,
+			"intel,wred-prioritize",
+			&val);
+	if (err) {
+		dev_err(dev,
+			"Could not get wred pop prioritize from DT, error is %d\n",
+			err);
+		return -ENODEV;
+	}
+	pdata->wred_prioritize_pop = val;
+
+	/* Get reserved memory region */
+	node = of_parse_phandle(node, "memory-region", 0);
+	if (!node) {
+		dev_err(dev, "No memory-region specified\n");
+		return -ENODEV;
+	}
+	err = of_address_to_resource(node, 0, &r);
+	if (err) {
+		dev_err(dev, "No memory address assigned to the region\n");
+		return err;
+	}
+
+	print_resource(dev, "ddr", &r);
+	pdrvdata->ddr_phy_start = r.start;
+	pdrvdata->ddr = devm_memremap(
+			dev,
+			r.start,
+			resource_size(&r),
+			MEMREMAP_WT);
+	if (IS_ERR_OR_NULL(pdrvdata->ddr)) {
+		err = (int) PTR_ERR(pdrvdata->ddr);
+		dev_err(dev, "devm_memremap failed mapping ddr with %d\n", err);
+		return err;
+	}
+	dev_info(dev, "DDR memory mapped to %p\n", pdrvdata->ddr);
+
+	addr = dmam_alloc_coherent(
+			dev,
+			PPV4_QOS_LOGGER_BUF_SIZE + PPV4_QOS_STAT_SIZE,
+			&dma,
+			GFP_KERNEL);
+	if (addr == NULL) {
+		dev_err(dev,
+				"Could not allocate %u bytes for logger buffer\n",
+				PPV4_QOS_LOGGER_BUF_SIZE);
+		return -ENOMEM;
+	}
+	pdata->fw_logger_start = (unsigned int)(dma);
+	pdata->fw_stat = pdata->fw_logger_start + PPV4_QOS_LOGGER_BUF_SIZE;
+	pdrvdata->dbg.fw_logger_addr = addr;
+
+	dev_info(dev, "Dma allocated %u bytes for fw logger, bus address is 0x%08X, virtual addr is %p\n",
+		 PPV4_QOS_LOGGER_BUF_SIZE,
+		 pdata->fw_logger_start,
+		 pdrvdata->dbg.fw_logger_addr);
+
+	return 0;
+}
+
+static int pp_qos_config_from_platform_data(
+		struct platform_device *pdev,
+		struct ppv4_qos_platform_data *pdata,
+		struct pp_qos_drv_data *pdrvdata
+		)
+{
+	struct ppv4_qos_platform_data *psrc;
+	void *memaddr;
+
+	dev_info(&pdev->dev, "Using platform info to init platform data\n");
+	psrc = (struct ppv4_qos_platform_data *)dev_get_platdata(&pdev->dev);
+	if (!psrc) {
+		dev_err(&pdev->dev, "Device contain no platform data\n");
+		return -ENODEV;
+	}
+	pdata->id = psrc->id;
+	pdata->max_port = psrc->max_port;
+	pdata->wred_prioritize_pop = psrc->wred_prioritize_pop;
+
+	/* DDR */
+	pdrvdata->ddr_phy_start = 0;
+	memaddr = map_mem_resource(pdev, "ddr", IORESOURCE_MEM, 0);
+	if (!memaddr)
+		return -ENODEV;
+	pdrvdata->ddr = memaddr;
+
+	pdata->qm_ddr_start = psrc->qm_ddr_start;
+	pdata->qm_num_pages = psrc->qm_num_pages;
+	pdata->fw_logger_start = psrc->fw_logger_start;
+	pdata->fw_stat = pdata->fw_logger_start + PPV4_QOS_LOGGER_BUF_SIZE;
+	pdrvdata->dbg.fw_logger_addr = pdrvdata->ddr + pdata->fw_logger_start;
+	return 0;
+}
+
+static int pp_qos_probe(struct platform_device *pdev)
+{
+	const struct device_node *of_node = pdev->dev.of_node;
+	struct qos_dev_init_info init_info = {};
+	const struct of_device_id *match;
+	struct pp_qos_dev *qdev;
+	int err;
+	struct pp_qos_drv_data *pdata;
+	struct device *dev;
+
+	dev = &pdev->dev;
+	dev_info(dev, "Probing...\n");
+
+	/* there is no devm_vmalloc so using dev_kzalloc */
+	pdata = devm_kzalloc(dev, sizeof(*pdata), GFP_KERNEL);
+	if (!pdata)
+		return -ENOMEM;
+
+	platform_set_drvdata(pdev, pdata);
+
+	match = of_match_device(of_match_ptr(pp_qos_match), dev);
+	if (match && of_node) {
+		err = pp_qos_config_from_of_node(
+				pdev,
+				&init_info.pl_data,
+				pdata);
+		if (err)
+			return err;
+	} else if (dev_get_platdata(dev)) {
+		err = pp_qos_config_from_platform_data(
+				pdev,
+				&init_info.pl_data,
+				pdata);
+		if (err)
+			return err;
+	} else {
+		dev_err(dev, "Could not get qos configuration\n");
+		return -ENODEV;
+	}
+
+	dev_info(dev, "id(%d), max_port(%u), pop_prioritize(%u)\n",
+			init_info.pl_data.id,
+			init_info.pl_data.max_port,
+			init_info.pl_data.wred_prioritize_pop
+			);
+
+	pdata->id = init_info.pl_data.id;
+	err = pp_qos_get_resources(pdev, &init_info);
+	if (err)
+		return err;
+
+	qdev = create_qos_dev_desc(&init_info);
+	if (!qdev)
+		return -ENODEV;
+	qdev->pdev = pdev;
+	pdata->qdev = qdev;
+	qdev->stat = pdata->dbg.fw_logger_addr + PPV4_QOS_LOGGER_BUF_SIZE;
+
+	err = qos_dbg_dev_init(pdev);
+	if (err)
+		goto fail;
+
+	dev_info(dev, "Probe completed\n");
+	cur_dev = dev;
+	return 0;
+
+fail:
+	_qos_clean(qdev);
+	return err;
+}
+
+static int pp_qos_remove(struct platform_device *pdev)
+{
+	struct pp_qos_drv_data *pdata;
+
+	dev_err(&pdev->dev, "Removing...\n");
+	pdata = platform_get_drvdata(pdev);
+	remove_qos_instance(pdata->id);
+	qos_dbg_dev_clean(pdev);
+	/* TODO !!! How to cleanup? */
+	return 0;
+}
+
+static struct platform_driver pp_qos_driver = {
+	.probe = pp_qos_probe,
+	.remove = pp_qos_remove,
+	.driver = {
+		.name = "pp-qos",
+		.owner = THIS_MODULE,
+#ifdef CONFIG_OF
+		.of_match_table = pp_qos_match,
+#endif
+	},
+};
+
+static int __init pp_qos_mod_init(void)
+{
+	int rc;
+
+	qos_module_init();
+	qos_dbg_module_init();
+	rc = platform_driver_register(&pp_qos_driver);
+	PPV4_QOS_MANUAL_ADD(rc);
+	pr_info("pp_qos_driver init, status %d\n", rc);
+	return rc;
+}
+
+static void __exit pp_qos_mod_exit(void)
+{
+	PPV4_QOS_MANUAL_REMOVE();
+	platform_driver_unregister(&pp_qos_driver);
+
+	qos_dbg_module_clean();
+	pr_info("pp_qos_driver exit\n");
+}
+
+//device_initcall(pp_qos_mod_init);
+// TODO ask Thomas
+arch_initcall(pp_qos_mod_init);
+module_exit(pp_qos_mod_exit);
+
+MODULE_VERSION(PPV4_QOS_DRV_VER);
+MODULE_AUTHOR("Intel CHD");
+MODULE_DESCRIPTION("PPv4 QoS driver");
+MODULE_LICENSE("GPL v2");
+MODULE_FIRMWARE(FIRMWARE_FILE);
diff --git a/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_main.c b/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_main.c
new file mode 100644
index 000000000000..8f07e2bac90f
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_main.c
@@ -0,0 +1,2082 @@
+/*
+ * GPL LICENSE SUMMARY
+ *
+ *  Copyright(c) 2017 Intel Corporation.
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of version 2 of the GNU General Public License as
+ *  published by the Free Software Foundation.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *  The full GNU General Public License is included in this distribution
+ *  in the file called LICENSE.GPL.
+ *
+ *  Contact Information:
+ *  Intel Corporation
+ *  2200 Mission College Blvd.
+ *  Santa Clara, CA  97052
+ */
+#include "pp_qos_common.h"
+#include "pp_qos_utils.h"
+#include "pp_qos_fw.h"
+#include "pp_qos_drv.h"
+
+#define PP_QOS_ENTER_FUNC()
+#define PP_QOS_EXIT_FUNC()
+
+void update_cmd_id(struct driver_cmds *drvcmds)
+{
+	drvcmds->cmd_id++;
+	drvcmds->cmd_fw_id = 0;
+}
+
+void transmit_cmds(struct pp_qos_dev *qdev)
+{
+
+	while (!cmd_queue_is_empty(qdev->drvcmds.cmdq)) {
+		enqueue_cmds(qdev);
+		check_completion(qdev);
+	}
+}
+
+static struct pp_qos_dev *qos_devs[MAX_QOS_INSTANCES];
+
+/******************************************************************************/
+/*                                      PORTS                                 */
+/******************************************************************************/
+static int set_port_specific_prop(struct pp_qos_dev *qdev,
+				  struct qos_node *node,
+				  const struct pp_qos_port_conf *conf,
+				  uint32_t *modified)
+{
+	if (node->data.port.ring_address != conf->ring_address) {
+		node->data.port.ring_address = conf->ring_address;
+		QOS_BITS_SET(*modified, QOS_MODIFIED_RING_ADDRESS);
+	}
+
+	if (node->data.port.ring_size != conf->ring_size) {
+		node->data.port.ring_size = conf->ring_size;
+		QOS_BITS_SET(*modified, QOS_MODIFIED_RING_SIZE);
+	}
+
+	if (!!(conf->packet_credit_enable) !=
+			!!QOS_BITS_IS_SET(node->flags,
+				QOS_NODE_FLAGS_PORT_PACKET_CREDIT_ENABLE)) {
+		QOS_BITS_TOGGLE(node->flags,
+				QOS_NODE_FLAGS_PORT_PACKET_CREDIT_ENABLE);
+		QOS_BITS_SET(*modified, QOS_MODIFIED_BEST_EFFORT);
+	}
+
+	if (node->data.port.credit != conf->credit) {
+		node->data.port.credit = conf->credit;
+		QOS_BITS_SET(*modified, QOS_MODIFIED_CREDIT);
+	}
+
+	if (!!(node->data.port.disable) != !!(conf->disable)) {
+		node->data.port.disable = !!(conf->disable);
+		QOS_BITS_SET(*modified, QOS_MODIFIED_DISABLE);
+	}
+	return 0;
+}
+
+STATIC_UNLESS_TEST
+int port_cfg_valid(
+		const struct pp_qos_dev *qdev,
+		const struct qos_node *node,
+		const struct qos_node *orig_node)
+{
+	if (!node_port(node)) {
+		QOS_LOG_ERR("Node is not a port\n");
+		return 0;
+	}
+
+	if (node->data.port.ring_address == NULL ||
+			node->data.port.ring_size == 0) {
+		QOS_LOG_ERR("Invalid ring address %p or ring size %zu\n",
+				node->data.port.ring_address,
+				node->data.port.ring_size);
+		return 0;
+	}
+
+	return node_cfg_valid(qdev, node, orig_node, QOS_INVALID_PHY);
+}
+
+static
+struct qos_node *get_conform_node(const struct pp_qos_dev *qdev,
+		unsigned int id,
+		int (*conform)(const struct qos_node *node))
+{
+	struct qos_node *node;
+	unsigned int phy;
+
+	phy = get_phy_from_id(qdev->mapping, id);
+	if (!QOS_PHY_VALID(phy)) {
+		QOS_LOG_ERR("Illegal id %u\n", id);
+		return NULL;
+	}
+	node = get_node_from_phy(qdev->nodes, phy);
+	if (conform && !conform(node)) {
+		QOS_LOG_ERR("Illegal id %u\n", id);
+		return NULL;
+	}
+
+	return node;
+}
+
+/******************************************************************************/
+/*                                 API                                        */
+/******************************************************************************/
+static int _pp_qos_port_set(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		const struct pp_qos_port_conf *conf);
+static int
+_pp_qos_port_conf_get(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		struct pp_qos_port_conf *conf);
+
+void pp_qos_port_conf_set_default(struct pp_qos_port_conf *conf)
+{
+	memset(conf, 0, sizeof(struct pp_qos_port_conf));
+	conf->common_prop.bandwidth_limit = QOS_NO_BANDWIDTH_LIMIT;
+	conf->common_prop.shared_bandwidth_group =
+		QOS_NO_SHARED_BANDWIDTH_GROUP;
+	conf->port_parent_prop.arbitration = PP_QOS_ARBITRATION_WSP;
+	conf->packet_credit_enable = 1;
+}
+
+int pp_qos_port_allocate(
+		struct pp_qos_dev *qdev,
+		unsigned int physical_id,
+		unsigned int *_id)
+{
+	unsigned int phy;
+	int rc;
+	unsigned int id;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+
+	if (physical_id == ALLOC_PORT_ID) {
+		phy = pp_pool_get(qdev->portsphys);
+		if (!QOS_PHY_VALID(phy)) {
+			rc = -EBUSY;
+			goto out;
+		}
+	} else if (physical_id > qdev->max_port ||
+			!qdev->reserved_ports[physical_id]) {
+		QOS_LOG(
+				"requested physical id %u is not reserved or out of range\n",
+				physical_id);
+		rc = -EINVAL;
+		goto out;
+	} else if (node_used(get_const_node_from_phy(qdev->nodes,
+					physical_id))) {
+		QOS_LOG("port %u already used\n", physical_id);
+		rc = -EBUSY;
+		goto out;
+	} else {
+		phy = physical_id;
+	}
+
+	node_init(qdev, get_node_from_phy(qdev->nodes, phy), 1, 1, 0);
+	nodes_modify_used_status(qdev, phy, 1, 1);
+
+	id = pp_pool_get(qdev->ids);
+	QOS_ASSERT(QOS_ID_VALID(id), "got illegal id %u\n", id);
+	map_id_phy(qdev->mapping, id, phy);
+	*_id = id;
+	rc = 0;
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+int pp_qos_port_disable(struct pp_qos_dev *qdev, unsigned int id)
+{
+	int rc;
+	struct pp_qos_port_conf conf;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+
+	rc = _pp_qos_port_conf_get(qdev, id, &conf);
+	if (rc)
+		goto out;
+	conf.disable = 1;
+	rc =  _pp_qos_port_set(qdev, id, &conf);
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+
+}
+
+int pp_qos_port_enable(struct pp_qos_dev *qdev, unsigned int id)
+{
+	int rc;
+	struct pp_qos_port_conf conf;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+
+	rc = _pp_qos_port_conf_get(qdev, id, &conf);
+	if (rc)
+		goto out;
+	conf.disable = 0;
+	rc =  _pp_qos_port_set(qdev, id, &conf);
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+
+}
+
+int pp_qos_port_block(struct pp_qos_dev *qdev, unsigned int id)
+{
+	int rc;
+	const struct qos_node *node;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+
+	node = get_conform_node(qdev, id, node_port);
+	if (!node) {
+		rc = -EINVAL;
+		goto out;
+	}
+	rc = tree_modify_blocked_status(
+			qdev,
+			get_phy_from_id(qdev->mapping, id), 1);
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+
+}
+
+int pp_qos_port_unblock(struct pp_qos_dev *qdev, unsigned int id)
+{
+	int rc;
+	const struct qos_node *node;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+
+	node = get_conform_node(qdev, id, node_port);
+	if (!node) {
+		rc = -EINVAL;
+		goto out;
+	}
+	rc = tree_modify_blocked_status(
+			qdev,
+			get_phy_from_id(qdev->mapping, id), 0);
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+
+}
+
+int pp_qos_port_flush(struct pp_qos_dev *qdev, unsigned int id)
+{
+	int rc;
+	const struct qos_node *node;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+	node = get_conform_node(qdev, id, node_port);
+	if (!node) {
+		rc = -EINVAL;
+		goto out;
+	}
+	rc = tree_flush(qdev, get_phy_from_id(qdev->mapping, id));
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+int pp_qos_port_remove(struct pp_qos_dev *qdev, unsigned int id)
+{
+	int rc;
+	const struct qos_node *node;
+	unsigned int phy;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+	node = get_conform_node(qdev, id, node_port);
+	if (!node) {
+		rc = -EINVAL;
+		goto out;
+	}
+	phy = get_phy_from_node(qdev->nodes, node);
+	rc = tree_remove(qdev, phy);
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+static int _pp_qos_port_conf_get(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		struct pp_qos_port_conf *conf)
+{
+	const struct qos_node *node;
+
+	if (conf == NULL)
+		return -EINVAL;
+
+	node = get_conform_node(qdev, id, node_port);
+	if (!node)
+		return -EINVAL;
+
+	conf->ring_address = node->data.port.ring_address;
+	conf->ring_size = node->data.port.ring_size;
+	conf->packet_credit_enable = !!QOS_BITS_IS_SET(node->flags,
+			QOS_NODE_FLAGS_PORT_PACKET_CREDIT_ENABLE);
+	conf->credit = node->data.port.credit;
+	conf->disable = node->data.port.disable;
+	return  get_node_prop(
+			qdev,
+			node,
+			&conf->common_prop,
+			&conf->port_parent_prop,
+			NULL);
+}
+
+int pp_qos_port_conf_get(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		struct pp_qos_port_conf *conf)
+{
+	int rc;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+
+	rc = _pp_qos_port_conf_get(qdev, id, conf);
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+int pp_qos_port_info_get(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		struct pp_qos_port_info *info)
+{
+	int rc;
+	const struct qos_node *node;
+	unsigned int phy;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+
+	if (info == NULL) {
+		rc = -EINVAL;
+		goto out;
+	}
+	node = get_conform_node(qdev, id, node_port);
+	if (!node) {
+		rc = -EINVAL;
+		goto out;
+	}
+
+	phy = get_phy_from_node(qdev->nodes, node);
+	info->physical_id = phy;
+	get_node_queues(qdev, phy, NULL, 0, &info->num_of_queues);
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+	rc = 0;
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+int pp_qos_port_get_queues(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		uint16_t *queue_ids,
+		unsigned int size,
+		unsigned int *queues_num)
+{
+	int rc;
+	unsigned int phy;
+	const struct qos_node *node;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+
+	node = get_conform_node(qdev, id, node_port);
+	if (!node) {
+		rc = -EINVAL;
+		goto out;
+	}
+
+	phy = get_phy_from_node(qdev->nodes, node);
+	get_node_queues(qdev, phy, queue_ids, size, queues_num);
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+	rc = 0;
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+static int _pp_qos_port_set(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		const struct pp_qos_port_conf *conf)
+{
+	int rc;
+	uint32_t modified;
+	struct qos_node node;
+	struct qos_node *nodep;
+	uint16_t phy;
+
+	modified = 0;
+
+	nodep = get_conform_node(qdev, id, NULL);
+	if (!nodep)
+		return -EINVAL;
+	memcpy(&node, nodep, sizeof(node));
+
+	if (node.type != TYPE_PORT) {
+		node.type = TYPE_PORT;
+		QOS_BITS_SET(modified, QOS_MODIFIED_NODE_TYPE);
+	}
+
+	rc = set_node_prop(
+			qdev,
+			&node,
+			&conf->common_prop,
+			&conf->port_parent_prop,
+			NULL,
+			&modified);
+	if (rc)
+		return rc;
+
+	rc = set_port_specific_prop(qdev, &node, conf, &modified);
+	if (rc)
+		return rc;
+
+	if (!port_cfg_valid(qdev, &node, nodep))
+		return -EINVAL;
+
+	memcpy(nodep, &node, sizeof(struct qos_node));
+
+	if (modified) {
+		phy = get_phy_from_node(qdev->nodes, nodep);
+		create_set_port_cmd(qdev, conf, phy, modified);
+	}
+
+	return 0;
+}
+
+int pp_qos_port_set(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		const struct pp_qos_port_conf *conf)
+{
+	int rc;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+
+	rc = _pp_qos_port_set(qdev, id, conf);
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+/**
+ * pp_qos_port_stat_get() - Get port's statistics
+ * @qos_dev: handle to qos device instance obtained previously from qos_dev_init
+ * @id:	  queue's id obtained from queue_allocate
+ * @stat: pointer to struct to be filled with queue's statistics
+ *
+ * Return: 0 on success
+ */
+int pp_qos_port_stat_get(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		struct pp_qos_port_stat *stat)
+{
+	int rc;
+	struct qos_node *node;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+
+	node = get_conform_node(qdev, id, node_port);
+	if (!node) {
+		rc = -EINVAL;
+		goto out;
+	}
+
+	create_get_port_stats_cmd(
+			qdev,
+			get_phy_from_node(qdev->nodes, node),
+			qdev->hwconf.fw_stat,
+			stat);
+
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+
+	rc = 0;
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+
+/******************************************************************************/
+/*                                     QUEUES                                 */
+/******************************************************************************/
+static int _pp_qos_queue_set(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		const struct pp_qos_queue_conf *conf);
+static int _pp_qos_queue_conf_get(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		struct pp_qos_queue_conf *conf);
+
+static void node_queue_init(struct pp_qos_dev *qdev, struct qos_node *node)
+{
+	node_init(qdev, node, 1, 0, 1);
+	memset(&(node->data.queue), 0x0, sizeof(struct _queue));
+	node->type = TYPE_QUEUE;
+}
+
+static int set_queue_specific_prop(struct pp_qos_dev *qdev,
+				  struct qos_node *node,
+				  const struct pp_qos_queue_conf *conf,
+				  uint32_t *modified)
+{
+	if (node->data.queue.max_burst != conf->max_burst) {
+		node->data.queue.max_burst = conf->max_burst;
+		QOS_BITS_SET(*modified, QOS_MODIFIED_MAX_BURST);
+	}
+
+	if (node->data.queue.green_min != conf->queue_wred_min_avg_green) {
+		node->data.queue.green_min = conf->queue_wred_min_avg_green;
+		QOS_BITS_SET(*modified, QOS_MODIFIED_WRED_MIN_GREEN);
+	}
+
+	if (node->data.queue.green_max != conf->queue_wred_max_avg_green) {
+		node->data.queue.green_max = conf->queue_wred_max_avg_green;
+		QOS_BITS_SET(*modified, QOS_MODIFIED_WRED_MAX_GREEN);
+	}
+
+	if (node->data.queue.green_slope != conf->queue_wred_slope_green) {
+		node->data.queue.green_slope = conf->queue_wred_slope_green;
+		QOS_BITS_SET(*modified, QOS_MODIFIED_WRED_SLOPE_GREEN);
+	}
+
+	if (node->data.queue.yellow_min != conf->queue_wred_min_avg_yellow) {
+		node->data.queue.yellow_min = conf->queue_wred_min_avg_yellow;
+		QOS_BITS_SET(*modified, QOS_MODIFIED_WRED_MIN_YELLOW);
+	}
+
+	if (node->data.queue.yellow_max != conf->queue_wred_max_avg_yellow) {
+		node->data.queue.yellow_max = conf->queue_wred_max_avg_yellow;
+		QOS_BITS_SET(*modified, QOS_MODIFIED_WRED_MAX_YELLOW);
+	}
+
+	if (node->data.queue.yellow_slope != conf->queue_wred_slope_yellow) {
+		node->data.queue.yellow_slope = conf->queue_wred_slope_yellow;
+		QOS_BITS_SET(*modified, QOS_MODIFIED_WRED_SLOPE_YELLOW);
+	}
+
+	if (node->data.queue.max_burst != conf->max_burst) {
+		node->data.queue.max_burst = conf->max_burst;
+		QOS_BITS_SET(*modified, QOS_MODIFIED_MAX_BURST);
+	}
+
+	if (node->data.queue.max_allowed != conf->queue_wred_max_allowed) {
+		node->data.queue.max_allowed = conf->queue_wred_max_allowed;
+		QOS_BITS_SET(*modified, QOS_MODIFIED_WRED_MAX_ALLOWED);
+	}
+
+	if (node->data.queue.min_guaranteed !=
+			conf->queue_wred_min_guaranteed) {
+		node->data.queue.min_guaranteed =
+			conf->queue_wred_min_guaranteed;
+		QOS_BITS_SET(*modified, QOS_MODIFIED_WRED_MIN_GUARANTEED);
+	}
+
+	if (!!(conf->wred_enable) !=
+			!!QOS_BITS_IS_SET(node->flags,
+				QOS_NODE_FLAGS_QUEUE_WRED_ENABLE)) {
+		QOS_BITS_TOGGLE(node->flags, QOS_NODE_FLAGS_QUEUE_WRED_ENABLE);
+		QOS_BITS_SET(*modified, QOS_MODIFIED_WRED_ENABLE);
+	}
+
+	if (!!(conf->wred_fixed_drop_prob_enable) !=
+			!!QOS_BITS_IS_SET(node->flags,
+			QOS_NODE_FLAGS_QUEUE_WRED_FIXED_DROP_PROB_ENABLE)) {
+		QOS_BITS_TOGGLE(node->flags,
+			QOS_NODE_FLAGS_QUEUE_WRED_FIXED_DROP_PROB_ENABLE);
+		QOS_BITS_SET(*modified,
+			QOS_MODIFIED_WRED_FIXED_DROP_PROB_ENABLE);
+	}
+
+	if (node->data.queue.fixed_green_prob !=
+			conf->queue_wred_fixed_drop_prob_green) {
+		node->data.queue.fixed_green_prob =
+			conf->queue_wred_fixed_drop_prob_green;
+		QOS_BITS_SET(*modified, QOS_MODIFIED_WRED_FIXED_GREEN_PROB);
+	}
+
+	if (node->data.queue.fixed_yellow_prob !=
+			conf->queue_wred_fixed_drop_prob_yellow) {
+		node->data.queue.fixed_yellow_prob =
+			conf->queue_wred_fixed_drop_prob_yellow;
+		QOS_BITS_SET(*modified, QOS_MODIFIED_WRED_FIXED_YELLOW_PROB);
+	}
+
+	if (!!(conf->blocked) !=
+			!!QOS_BITS_IS_SET(node->flags,
+				QOS_NODE_FLAGS_QUEUE_BLOCKED)) {
+		QOS_BITS_TOGGLE(node->flags, QOS_NODE_FLAGS_QUEUE_BLOCKED);
+		QOS_BITS_SET(*modified, QOS_MODIFIED_BLOCKED);
+	}
+	return 0;
+}
+
+STATIC_UNLESS_TEST int queue_cfg_valid(
+		const struct pp_qos_dev *qdev,
+		const struct qos_node *node,
+		const struct qos_node *orig_node,
+		unsigned int prev_virt_parent_phy)
+{
+	if (!node_queue(node)) {
+		QOS_LOG("Node is not a queue\n");
+		return 0;
+	}
+
+	return node_cfg_valid(qdev, node, orig_node, prev_virt_parent_phy);
+}
+
+static int check_queue_conf_validity(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		const struct pp_qos_queue_conf *conf,
+		struct qos_node *node,
+		uint32_t *modified)
+{
+	unsigned int phy;
+	int rc;
+	struct qos_node *nodep;
+
+	/*
+	 * The phy of the current first ancesstor which is not an
+	 * internal scheduler QOS_INVALID_PHY if it is a new queue
+	 */
+	unsigned int prev_virt_parent_phy;
+
+	/* Check if node id is valid */
+	phy = get_phy_from_id(qdev->mapping, id);
+	if (!QOS_PHY_VALID(phy)) {
+		if (!QOS_PHY_UNKNOWN(phy)) {
+			QOS_LOG_ERR("Id %u is not a node\n", id);
+			return -EINVAL;
+		}
+		nodep = NULL;
+
+		/* New queue which has id, but no phy was allocated for it */
+		node_queue_init(qdev, node);
+		node->data.queue.rlm = pp_pool_get(qdev->rlms);
+		if (node->data.queue.rlm == QOS_INVALID_RLM) {
+			QOS_LOG_ERR("Can't get free rlm\n");
+			return  -EBUSY;
+		}
+		QOS_BITS_SET(node->flags, QOS_NODE_FLAGS_USED);
+		QOS_BITS_SET(*modified,
+			     QOS_MODIFIED_NODE_TYPE |
+			     QOS_MODIFIED_BANDWIDTH_LIMIT |
+			     QOS_MODIFIED_VIRT_BW_SHARE |
+			     QOS_MODIFIED_PARENT |
+			     QOS_MODIFIED_MAX_BURST |
+			     QOS_MODIFIED_BLOCKED |
+			     QOS_MODIFIED_WRED_ENABLE |
+			     QOS_MODIFIED_WRED_FIXED_DROP_PROB_ENABLE |
+			     QOS_MODIFIED_WRED_FIXED_GREEN_PROB |
+			     QOS_MODIFIED_WRED_FIXED_YELLOW_PROB |
+			     QOS_MODIFIED_WRED_MIN_GREEN |
+			     QOS_MODIFIED_WRED_MAX_GREEN |
+			     QOS_MODIFIED_WRED_SLOPE_GREEN |
+			     QOS_MODIFIED_WRED_MIN_YELLOW |
+			     QOS_MODIFIED_WRED_MAX_YELLOW |
+			     QOS_MODIFIED_WRED_SLOPE_YELLOW |
+			     QOS_MODIFIED_WRED_MAX_ALLOWED |
+			     QOS_MODIFIED_WRED_MIN_GUARANTEED |
+			     QOS_MODIFIED_RLM);
+		prev_virt_parent_phy = QOS_INVALID_PHY;
+	} else {
+		nodep = get_node_from_phy(qdev->nodes, phy);
+		if (!node_queue(nodep)) {
+			QOS_LOG("Id %u is not a queue\n", id);
+			rc = -EINVAL;
+		}
+		memcpy(node, nodep, sizeof(struct qos_node));
+		prev_virt_parent_phy = get_virtual_parent_phy(
+				qdev->nodes,
+				node);
+	}
+
+	/*
+	 * Before calling set_node_prop:
+	 * If new node
+	 *	node.child.parent.phy = INVALID_PHY
+	 *	prev_virt_parent_phy = INVALID_PHY
+	 * Else
+	 *	node.child.parent.phy = actual parent
+	 *	prev_virt_parent_phy = virtual parent
+	 *	(i.e. first non internal ancestor)
+	 *
+	 * When returning:
+	 * If new node
+	 *     node.child.parent.phy == virtual parent phy
+	 * Else
+	 *     If map_id_to_phy(conf.parent) != prev_virt_parent_phy
+	 *		node.child.parent.phy = map_id_to_phy(conf.parent)
+	 *     Else
+	 *		node.child.parent.phy = actual parent (no change)
+	 *
+	 */
+	rc = set_node_prop(
+			qdev,
+			node,
+			&conf->common_prop,
+			NULL,
+			&conf->queue_child_prop,
+			modified);
+	if (rc)
+		return rc;
+
+	rc = set_queue_specific_prop(qdev, node, conf, modified);
+	if (rc)
+		return rc;
+
+	if (!queue_cfg_valid(qdev, node, nodep, prev_virt_parent_phy))
+		return -EINVAL;
+
+	return 0;
+}
+
+void pp_qos_queue_conf_set_default(struct pp_qos_queue_conf *conf)
+{
+	memset(conf, 0, sizeof(struct pp_qos_queue_conf));
+	conf->common_prop.bandwidth_limit = QOS_NO_BANDWIDTH_LIMIT;
+	conf->common_prop.shared_bandwidth_group =
+		QOS_NO_SHARED_BANDWIDTH_GROUP;
+}
+
+int pp_qos_queue_allocate(struct pp_qos_dev *qdev, unsigned int *id)
+{
+	int rc;
+	uint16_t _id;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+	_id = pp_pool_get(qdev->ids);
+	QOS_ASSERT(QOS_ID_VALID(_id), "got illegal id %u\n", _id);
+	*id = _id;
+	map_id_reserved(qdev->mapping, _id);
+	rc = 0;
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+static int _pp_qos_queue_conf_get(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		struct pp_qos_queue_conf *conf)
+{
+	const struct qos_node *node;
+
+	if (conf == NULL)
+		return -EINVAL;
+
+	node = get_conform_node(qdev, id, node_queue);
+	if (!node)
+		return -EINVAL;
+
+	conf->max_burst = node->data.queue.max_burst;
+	conf->queue_wred_min_avg_green = node->data.queue.green_min;
+	conf->queue_wred_max_avg_green = node->data.queue.green_max;
+	conf->queue_wred_slope_green = node->data.queue.green_slope;
+	conf->queue_wred_min_avg_yellow = node->data.queue.yellow_min;
+	conf->queue_wred_max_avg_yellow = node->data.queue.yellow_max;
+	conf->queue_wred_slope_yellow = node->data.queue.yellow_slope;
+	conf->queue_wred_max_allowed = node->data.queue.max_allowed;
+	conf->queue_wred_min_guaranteed = node->data.queue.min_guaranteed;
+	conf->queue_wred_fixed_drop_prob_green =
+		node->data.queue.fixed_green_prob;
+	conf->queue_wred_fixed_drop_prob_yellow =
+		node->data.queue.fixed_yellow_prob;
+	conf->wred_enable =
+		!!QOS_BITS_IS_SET(node->flags,
+				QOS_NODE_FLAGS_QUEUE_WRED_ENABLE);
+	conf->wred_fixed_drop_prob_enable =
+		!!QOS_BITS_IS_SET(node->flags,
+			QOS_NODE_FLAGS_QUEUE_WRED_FIXED_DROP_PROB_ENABLE);
+	conf->blocked =
+		!!QOS_BITS_IS_SET(node->flags,
+				QOS_NODE_FLAGS_QUEUE_BLOCKED);
+	return get_node_prop(
+			qdev,
+			node,
+			&conf->common_prop,
+			NULL,
+			&conf->queue_child_prop);
+}
+
+int pp_qos_queue_conf_get(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		struct pp_qos_queue_conf *conf)
+{
+	int rc;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+	rc = _pp_qos_queue_conf_get(qdev, id, conf);
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+static int _pp_qos_queue_remove(struct pp_qos_dev *qdev, int id)
+{
+	struct qos_node *node;
+	int rc;
+
+	node = get_conform_node(qdev, id, node_queue);
+	if (!node)
+		return -EINVAL;
+
+	rc  = node_remove(qdev, node);
+	return rc;
+}
+
+/**
+ * pp_qos_queue_remove() -	Remove a queue
+ * @qos_dev: handle to qos device instance obtained previously from qos_dev_init
+ * @id:   queue's id obtained from queue_allocate
+ *
+ * Note: client should make sure that queue is empty and
+ * that new packets are not enqueued, by calling
+ *       pp_qos_queue_disable and pp_qos_queue_flush
+ *
+ * Return: 0 on success
+ */
+int pp_qos_queue_remove(struct pp_qos_dev *qdev, unsigned int id)
+{
+	int rc;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+	rc = _pp_qos_queue_remove(qdev, id);
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+
+/**
+ * pp_qos_queue_set() - Set queue configuration
+ * @qos_dev: handle to qos device instance obtained previously from qos_dev_init
+ * @id:	  queue's id obtained from queue_allocate
+ * @conf: new configuration for the queue
+ *
+ * Return: 0 on success
+ */
+static int _pp_qos_queue_set(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		const struct pp_qos_queue_conf *conf)
+{
+	int rc;
+	unsigned int phy;
+	unsigned int prev_phy;
+	struct qos_node *parent;
+	struct qos_node node;
+	struct qos_node *nodep;
+	uint32_t modified;
+	int parent_changed;
+	unsigned int dst_port;
+
+	modified = 0;
+	nodep = NULL;
+
+	rc = check_queue_conf_validity(qdev, id, conf, &node, &modified);
+	parent_changed = QOS_BITS_IS_SET(modified, QOS_MODIFIED_PARENT);
+	if (rc)
+		goto out;
+
+	if (parent_changed) {
+		parent = get_node_from_phy(
+				qdev->nodes,
+				node.child_prop.parent_phy);
+		phy = phy_alloc_by_parent(
+				qdev,
+				parent,
+				conf->queue_child_prop.priority);
+		if (!QOS_PHY_VALID(phy)) {
+			rc = -EINVAL;
+			goto out;
+		}
+		nodep = get_node_from_phy(qdev->nodes, phy);
+		node.child_prop.parent_phy = nodep->child_prop.parent_phy;
+		memcpy(nodep, &node, sizeof(struct qos_node));
+
+		/* If this is not a new queue - delete previous node */
+		if (!QOS_BITS_IS_SET(modified, QOS_MODIFIED_NODE_TYPE)) {
+			prev_phy = get_phy_from_id(qdev->mapping, id);
+			add_suspend_port(qdev, get_port(qdev->nodes, prev_phy));
+			dst_port = get_port(qdev->nodes, phy);
+			create_move_cmd(qdev, phy, prev_phy, dst_port);
+			map_invalidate_id(qdev->mapping, id);
+			map_id_phy(qdev->mapping, id, phy);
+			node_phy_delete(qdev, prev_phy);
+			phy = get_phy_from_id(qdev->mapping, id);
+			nodep = get_node_from_phy(qdev->nodes, phy);
+			if (phy != prev_phy)
+				create_update_preds_cmd(qdev, phy);
+		} else {
+			map_id_phy(qdev->mapping, id, phy);
+		}
+
+		parent = get_node_from_phy(
+				qdev->nodes,
+				nodep->child_prop.parent_phy);
+		if (nodep->child_prop.virt_bw_share && node_internal(parent))
+			update_internal_bandwidth(qdev, parent);
+	} else {
+		phy = get_phy_from_id(qdev->mapping, id);
+		nodep = get_node_from_phy(qdev->nodes, phy);
+		parent = get_node_from_phy(
+				qdev->nodes,
+				nodep->child_prop.parent_phy);
+
+		/* Child of WSP changes priority i.e. position */
+		if (parent->parent_prop.arbitration == PP_QOS_ARBITRATION_WSP) {
+			update_children_position(
+					qdev,
+					nodep,
+					parent,
+					min(conf->queue_child_prop.priority,
+					(unsigned int)parent->
+					parent_prop.num_of_children),
+					&node);
+
+
+		}  else {
+			memcpy(nodep, &node, sizeof(struct qos_node));
+		}
+	}
+
+	if (modified != QOS_MODIFIED_PARENT)
+		create_set_queue_cmd(
+				qdev,
+				conf,
+				phy,
+				nodep->child_prop.parent_phy,
+				nodep->data.queue.rlm,
+				modified);
+out:
+	if (rc) {
+		if (parent_changed) {
+			if (QOS_BITS_IS_SET(modified, QOS_MODIFIED_RLM))
+				release_rlm(qdev->rlms, node.data.queue.rlm);
+		}
+	}
+	return rc;
+}
+
+int pp_qos_queue_set(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		const struct pp_qos_queue_conf *conf)
+{
+	int rc;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+
+	rc = _pp_qos_queue_set(qdev, id, conf);
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+int _pp_qos_queue_block(struct pp_qos_dev *qdev, unsigned int id)
+{
+	int rc;
+	struct pp_qos_queue_conf conf;
+
+	rc = _pp_qos_queue_conf_get(qdev, id, &conf);
+	if (rc)
+		return rc;
+	conf.blocked = 1;
+	return _pp_qos_queue_set(qdev, id, &conf);
+}
+
+/**
+ * pp_qos_queue_block() - All new packets enqueue to this queue will be dropped
+ * @qos_dev: handle to qos device instance obtained previously from qos_dev_init
+ * @id:   queue's id obtained from queue_allocate
+ *
+ * Note	- already enqueued descriptors will be transmitted
+ *
+ * Return: 0 on success
+ */
+int pp_qos_queue_block(struct pp_qos_dev *qdev, unsigned int id)
+{
+	int rc;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+	rc = _pp_qos_queue_block(qdev, id);
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+int _pp_qos_queue_unblock(struct pp_qos_dev *qdev, unsigned int id)
+{
+	int rc;
+	struct pp_qos_queue_conf conf;
+
+	rc = _pp_qos_queue_conf_get(qdev, id, &conf);
+	if (rc)
+		return rc;
+
+	conf.blocked = 0;
+	return _pp_qos_queue_set(qdev, id, &conf);
+}
+
+/**
+ * pp_qos_queue_unblock() - Unblock enqueuing of new packets
+ * @qos_dev: handle to qos device instance obtained previously from qos_dev_init
+ * @id:	  queue's id obtained from queue_allocate
+ *
+ * Return: 0 on success
+ */
+int pp_qos_queue_unblock(struct pp_qos_dev *qdev, unsigned int id)
+{
+	int rc;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+	rc = _pp_qos_queue_unblock(qdev, id);
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+
+int _pp_qos_queue_flush(struct pp_qos_dev *qdev, unsigned int id)
+{
+	struct qos_node *node;
+
+	node = get_conform_node(qdev, id, node_queue);
+	if (!node)
+		return -EINVAL;
+	return node_flush(qdev, node);
+}
+
+/**
+ * pp_qos_queue_flush() - Drop all enqueued packets
+ * @qos_dev: handle to qos device instance obtained previously from qos_dev_init
+ * @id:	  queue's id obtained from queue_allocate
+ *
+ * Return: 0 on success
+ */
+int pp_qos_queue_flush(struct pp_qos_dev *qdev, unsigned int id)
+{
+	int rc;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+	rc = _pp_qos_queue_flush(qdev, id);
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+
+/**
+ * pp_qos_queue_info_get() - Get information about queue
+ * @qos_dev: handle to qos device instance obtained previously from qos_dev_init
+ * @id:	  queue's id obtained from queue_allocate
+ * @info: pointer to struct to be filled with queue's info
+ *
+ * Return: 0 on success
+ */
+int pp_qos_queue_info_get(struct pp_qos_dev *qdev, unsigned int id,
+			  struct pp_qos_queue_info *info)
+{
+	int rc;
+	struct qos_node *node;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+
+	if (info == NULL) {
+		rc = -EINVAL;
+		goto out;
+	}
+
+	node = get_conform_node(qdev, id, node_queue);
+
+	if (!node) {
+		rc = -EINVAL;
+		goto out;
+	}
+
+	info->physical_id = node->data.queue.rlm;
+	info->port_id = get_port(
+			qdev->nodes,
+			get_phy_from_node(qdev->nodes, node));
+	rc = 0;
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+/**
+ * pp_qos_queue_stat_get() - Get queue's statistics
+ * @qos_dev: handle to qos device instance obtained previously from qos_dev_init
+ * @id:	  queue's id obtained from queue_allocate
+ * @stat: pointer to struct to be filled with queue's statistics
+ *
+ * Return: 0 on success
+ */
+int pp_qos_queue_stat_get(struct pp_qos_dev *qdev, unsigned int id,
+			  struct pp_qos_queue_stat *stat)
+{
+	int rc;
+	struct qos_node *node;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+
+	node = get_conform_node(qdev, id, node_queue);
+	if (!node) {
+		rc = -EINVAL;
+		goto out;
+	}
+	create_get_queue_stats_cmd(
+			qdev,
+			get_phy_from_node(qdev->nodes, node),
+			node->data.queue.rlm,
+			qdev->hwconf.fw_stat,
+			stat);
+
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+
+	rc = 0;
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+/******************************************************************************/
+/*                                 Schedulers                                 */
+/******************************************************************************/
+static int _pp_qos_sched_set(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		const struct pp_qos_sched_conf *conf);
+static int _pp_qos_sched_conf_get(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		struct pp_qos_sched_conf *conf);
+
+static void node_sched_init(struct pp_qos_dev *qdev, struct qos_node *node)
+{
+	node_init(qdev, node, 1, 1, 1);
+	node->type = TYPE_SCHED;
+}
+
+STATIC_UNLESS_TEST int sched_cfg_valid(
+		const struct pp_qos_dev *qdev,
+		const struct qos_node *node,
+		const struct qos_node *orig_node,
+		unsigned int prev_virt_parent_phy)
+{
+	if (!node_sched(node)) {
+		QOS_LOG_ERR("Node is not a sched\n");
+		return 0;
+	}
+
+	return node_cfg_valid(qdev, node, orig_node, prev_virt_parent_phy);
+}
+
+static int check_sched_conf_validity(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		const struct pp_qos_sched_conf *conf,
+		struct qos_node *node,
+		uint32_t *modified)
+{
+	unsigned int phy;
+	int rc;
+	struct qos_node *nodep;
+	unsigned int prev_virt_parent_phy_phy;
+
+	/* Check if node id is valid */
+	phy = get_phy_from_id(qdev->mapping, id);
+	if (!QOS_PHY_VALID(phy)) {
+		if (!QOS_PHY_UNKNOWN(phy)) {
+			QOS_LOG_ERR("Id %u is not a node\n", id);
+			return -EINVAL;
+		}
+
+		/* New sched which has id, but no phy was allocated for it */
+		node_sched_init(qdev, node);
+		QOS_BITS_SET(node->flags, QOS_NODE_FLAGS_USED);
+		QOS_BITS_SET(*modified,
+			     QOS_MODIFIED_NODE_TYPE |
+			     QOS_MODIFIED_SHARED_GROUP_ID |
+			     QOS_MODIFIED_BANDWIDTH_LIMIT |
+			     QOS_MODIFIED_VIRT_BW_SHARE |
+			     QOS_MODIFIED_PARENT |
+			     QOS_MODIFIED_ARBITRATION |
+			     QOS_MODIFIED_BEST_EFFORT);
+
+		nodep = NULL;
+		prev_virt_parent_phy_phy = QOS_INVALID_PHY;
+	} else {
+		nodep = get_node_from_phy(qdev->nodes, phy);
+		if (!node_sched(nodep)) {
+			QOS_LOG_ERR("Id %u is not a sched\n", id);
+			rc = -EINVAL;
+		}
+		memcpy(node, nodep, sizeof(struct qos_node));
+		prev_virt_parent_phy_phy = get_virtual_parent_phy(
+				qdev->nodes,
+				node);
+	}
+
+	rc = set_node_prop(qdev,
+			node,
+			&conf->common_prop,
+			&conf->sched_parent_prop,
+			&conf->sched_child_prop,
+			modified);
+	if (rc)
+		return rc;
+
+	if (!sched_cfg_valid(qdev, node, nodep, prev_virt_parent_phy_phy))
+		return -EINVAL;
+
+	return 0;
+}
+
+void pp_qos_sched_conf_set_default(struct pp_qos_sched_conf *conf)
+{
+	memset(conf, 0, sizeof(struct pp_qos_sched_conf));
+	conf->common_prop.bandwidth_limit = QOS_NO_BANDWIDTH_LIMIT;
+	conf->common_prop.shared_bandwidth_group =
+		QOS_NO_SHARED_BANDWIDTH_GROUP;
+	conf->sched_parent_prop.arbitration = PP_QOS_ARBITRATION_WRR;
+}
+
+int pp_qos_sched_allocate(struct pp_qos_dev *qdev, unsigned int *id)
+{
+	uint16_t _id;
+	int rc;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+	_id = pp_pool_get(qdev->ids);
+	QOS_ASSERT(QOS_ID_VALID(_id), "got illegal id %u\n", _id);
+	*id = _id;
+	map_id_reserved(qdev->mapping, _id);
+	rc = 0;
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+int pp_qos_sched_remove(struct pp_qos_dev *qdev, unsigned int id)
+{
+	struct qos_node *node;
+	int rc;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+	node = get_conform_node(qdev, id, node_sched);
+	if (!node)  {
+		rc = -EINVAL;
+		goto out;
+	}
+
+	rc = tree_remove(qdev, get_phy_from_node(qdev->nodes, node));
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+static int _pp_qos_sched_set(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		const struct pp_qos_sched_conf *conf)
+{
+	int rc;
+	unsigned int phy;
+	unsigned int prev_phy;
+	struct qos_node *parent;
+	struct qos_node node;
+	struct qos_node *nodep;
+	struct qos_node *oldnode;
+	uint32_t modified;
+	unsigned int dst_port;
+
+	modified = 0;
+
+	rc = check_sched_conf_validity(qdev, id, conf, &node, &modified);
+	if (rc)
+		return rc;
+
+	if (QOS_BITS_IS_SET(modified, QOS_MODIFIED_PARENT)) {
+		parent = get_node_from_phy(
+				qdev->nodes,
+				node.child_prop.parent_phy);
+		phy = phy_alloc_by_parent(
+				qdev,
+				parent,
+				conf->sched_child_prop.priority);
+		if (!QOS_PHY_VALID(phy))
+			return -EINVAL;
+
+		/*
+		 * nodep's parent_phy holds value of actual parent, node's
+		 * parent_phy holds phy of virtual parent
+		 */
+		nodep = get_node_from_phy(qdev->nodes, phy);
+		node.child_prop.parent_phy = nodep->child_prop.parent_phy;
+		memcpy(nodep, &node, sizeof(struct qos_node));
+
+		/* If this is not a new sched - delete previous node */
+		if (!QOS_BITS_IS_SET(modified, QOS_MODIFIED_NODE_TYPE)) {
+			prev_phy = get_phy_from_id(qdev->mapping, id);
+			add_suspend_port(qdev, get_port(qdev->nodes, prev_phy));
+			dst_port = get_port(qdev->nodes, phy);
+			create_move_cmd(qdev, phy, prev_phy, dst_port);
+			node_update_children(qdev, prev_phy, phy);
+			map_invalidate_id(qdev->mapping, id);
+			map_id_phy(qdev->mapping, id, phy);
+			oldnode = get_node_from_phy(qdev->nodes, prev_phy);
+			nodep->parent_prop.first_child_phy =
+				oldnode->parent_prop.first_child_phy;
+			nodep->parent_prop.num_of_children =
+				oldnode->parent_prop.num_of_children;
+			oldnode->parent_prop.first_child_phy = QOS_INVALID_PHY;
+			oldnode->parent_prop.num_of_children = 0;
+			node_phy_delete(qdev, prev_phy);
+			phy = get_phy_from_id(qdev->mapping, id);
+			nodep = get_node_from_phy(qdev->nodes, phy);
+			tree_update_predecessors(qdev, phy);
+		} else {
+			map_id_phy(qdev->mapping, id, phy);
+		}
+
+		parent = get_node_from_phy(
+				qdev->nodes,
+				nodep->child_prop.parent_phy);
+		if (nodep->child_prop.virt_bw_share && node_internal(parent))
+			update_internal_bandwidth(qdev, parent);
+	} else {
+		phy = get_phy_from_id(qdev->mapping, id);
+		nodep = get_node_from_phy(qdev->nodes, phy);
+		parent = get_node_from_phy(
+				qdev->nodes,
+				nodep->child_prop.parent_phy);
+
+		/* Child of WSP changes priority i.e. position */
+		if (parent->parent_prop.arbitration == PP_QOS_ARBITRATION_WSP) {
+			update_children_position(
+					qdev,
+					nodep,
+					parent,
+					min(conf->sched_child_prop.priority,
+					(unsigned int)parent->
+					parent_prop.num_of_children),
+					&node);
+
+
+		}  else {
+			memcpy(nodep, &node, sizeof(struct qos_node));
+		}
+	}
+
+	if (modified != QOS_MODIFIED_PARENT)
+		create_set_sched_cmd(
+				qdev,
+				conf,
+				phy,
+				nodep->child_prop.parent_phy,
+				modified);
+	return 0;
+}
+
+int pp_qos_sched_set(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		const struct pp_qos_sched_conf *conf)
+{
+	int rc;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+	rc = _pp_qos_sched_set(qdev, id, conf);
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+static int _pp_qos_sched_conf_get(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		struct pp_qos_sched_conf *conf)
+{
+	const struct qos_node *node;
+
+	if (conf == NULL)
+		return -EINVAL;
+
+	node = get_conform_node(qdev, id, node_sched);
+	if (!node)
+		return -EINVAL;
+
+	return get_node_prop(
+			qdev,
+			node,
+			&conf->common_prop,
+			&conf->sched_parent_prop,
+			&conf->sched_child_prop);
+}
+
+int pp_qos_sched_conf_get(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		struct pp_qos_sched_conf *conf)
+{
+	int rc;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+	rc = _pp_qos_sched_conf_get(qdev, id, conf);
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+int pp_qos_sched_info_get(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		struct pp_qos_sched_info *info)
+{
+	int rc;
+	struct qos_node *node;
+	unsigned int phy;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+
+	if (info == NULL) {
+		rc = -EINVAL;
+		goto out;
+	}
+
+	node = get_conform_node(qdev, id, node_sched);
+
+	if (!node) {
+		rc = -EINVAL;
+		goto out;
+	}
+	phy = get_phy_from_node(qdev->nodes, node);
+	info->port_id = get_port(
+			qdev->nodes,
+			get_phy_from_node(qdev->nodes, node));
+	get_node_queues(qdev, phy, NULL, 0, &info->num_of_queues);
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+	rc = 0;
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+int pp_qos_sched_get_queues(struct pp_qos_dev *qdev, unsigned int id,
+			    uint16_t *queue_ids, unsigned int size,
+			    unsigned int *queues_num)
+{
+	int rc;
+	unsigned int phy;
+	const struct qos_node *node;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+
+	node = get_conform_node(qdev, id, node_sched);
+	if (!node) {
+		rc = -EINVAL;
+		goto out;
+	}
+
+	phy = get_phy_from_node(qdev->nodes, node);
+	get_node_queues(qdev, phy, queue_ids, size, queues_num);
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+	rc = 0;
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+int pp_qos_get_fw_version(
+		struct pp_qos_dev *qdev,
+		unsigned int *major,
+		unsigned int *minor,
+		unsigned int *build)
+{
+	int rc;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+
+	*major = qdev->fwver.major;
+	*minor = qdev->fwver.minor;
+	*build = qdev->fwver.build;
+	rc = 0;
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+/******************************************************************************/
+/*                                 Node                                       */
+/******************************************************************************/
+int pp_qos_get_node_info(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		struct pp_qos_node_info *info)
+{
+	int rc;
+	struct qos_node *node;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+
+	node = get_conform_node(qdev, id, node_used);
+	if (!node) {
+		rc = -EINVAL;
+		goto out;
+	}
+
+	create_get_node_info_cmd(
+			qdev,
+			get_phy_from_node(qdev->nodes, node),
+			qdev->hwconf.fw_stat,
+			info);
+
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+
+	rc = 0;
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+
+}
+
+/******************************************************************************/
+/*                                 SHARED_LIMIT                               */
+/******************************************************************************/
+static int _pp_qos_shared_limit_group_get_members(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		uint16_t *members,
+		unsigned int size,
+		unsigned int *members_num);
+
+int pp_qos_shared_limit_group_add(
+		struct pp_qos_dev *qdev,
+		unsigned int limit,
+		unsigned int *id)
+{
+	unsigned int i;
+	int rc;
+	struct shared_bandwidth_group *grp;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+
+	grp = qdev->groups + 1;
+	for (i = 1; i <= QOS_MAX_SHARED_BANDWIDTH_GROUP; ++grp, ++i) {
+		if (!grp->used)
+			break;
+	}
+
+	if (grp->used) {
+		QOS_LOG_ERR("No free shared groups\n");
+		rc = -EBUSY;
+		goto out;
+	}
+
+	grp->used = 1;
+	grp->limit = limit;
+	*id = i;
+
+	create_add_shared_group_cmd(qdev, i, limit);
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+	rc = 0;
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+int pp_qos_shared_limit_group_remove(struct pp_qos_dev *qdev, unsigned int id)
+{
+	int rc;
+	unsigned int num;
+	uint16_t tmp;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+	if (QOS_BW_GRP_VALID(id) && (qdev->groups[id].used)) {
+		_pp_qos_shared_limit_group_get_members(
+				qdev, id, &tmp, 1, &num);
+		if (num) {
+			QOS_LOG_ERR(
+					"Shared group %u has members can't remove\n",
+					id);
+			rc = -EBUSY;
+			goto out;
+		}
+		qdev->groups[id].used = 0;
+		create_remove_shared_group_cmd(qdev, id);
+		update_cmd_id(&qdev->drvcmds);
+		transmit_cmds(qdev);
+		rc = 0;
+	} else {
+		QOS_LOG_ERR("Shared group %u is not used\n", id);
+		rc = -EINVAL;
+	}
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+int pp_qos_shared_limit_group_modify(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		unsigned int limit)
+{
+	int rc;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (QOS_BW_GRP_VALID(id) && (qdev->groups[id].used)) {
+		if (qdev->groups[id].limit != limit) {
+			create_set_shared_group_cmd(qdev, id, limit);
+			update_cmd_id(&qdev->drvcmds);
+			transmit_cmds(qdev);
+		}
+		rc = 0;
+	} else {
+		QOS_LOG_ERR("Shared group %u is not used\n", id);
+		rc = -EINVAL;
+	}
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+static int _pp_qos_shared_limit_group_get_members(
+		struct pp_qos_dev *qdev, unsigned int id,
+		uint16_t *members,
+		unsigned int size,
+		unsigned int *members_num)
+{
+	unsigned int phy;
+	unsigned int total;
+	unsigned int tmp;
+	const struct qos_node *node;
+
+	total = 0;
+	for (phy = 0; (phy <= qdev->max_port) && (size > 0); ++phy) {
+		node = get_const_node_from_phy(qdev->nodes, phy);
+		if (node_used(node)) {
+			get_bw_grp_members_under_node(
+					qdev,
+					id,
+					phy,
+					members,
+					size,
+					&tmp);
+			total += tmp;
+			members += tmp;
+			if (tmp < size)
+				size -= tmp;
+			else
+				size = 0;
+		}
+	}
+
+	*members_num = total;
+	return 0;
+}
+
+int pp_qos_shared_limit_group_get_members(
+		struct pp_qos_dev *qdev,
+		unsigned int id,
+		uint16_t *members,
+		unsigned int size,
+		unsigned int *members_num)
+{
+	int rc;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (!qos_device_ready(qdev)) {
+		rc = -EINVAL;
+		goto out;
+	}
+	rc = _pp_qos_shared_limit_group_get_members(
+			qdev, id, members, size, members_num);
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+out:
+	QOS_UNLOCK(qdev);
+	return rc;
+}
+
+struct pp_qos_dev *pp_qos_dev_open(unsigned int id)
+{
+	if (id < MAX_QOS_INSTANCES)
+		return qos_devs[id];
+
+	QOS_LOG_ERR("Illegal qos instance %u\n", id);
+	return NULL;
+}
+
+int pp_qos_dev_init(struct pp_qos_dev *qdev, struct pp_qos_init_param *conf)
+{
+	int rc;
+
+	QOS_LOCK(qdev);
+	PP_QOS_ENTER_FUNC();
+	if (qdev->initialized) {
+		QOS_LOG_ERR(
+				"Device already initialized, can't initialize again\n");
+		rc = -EINVAL;
+		goto out;
+	}
+	if ((qdev->max_port + 1) & 7) {
+		QOS_LOG_ERR("Given max port %u is not last node of an octet\n",
+				qdev->max_port);
+		rc = -EINVAL;
+		goto out;
+	}
+	qdev->portsphys = free_ports_phys_init(
+			qdev->reserved_ports,
+			qdev->max_port,
+			conf->reserved_ports,
+			QOS_MAX_PORTS);
+
+	qdev->hwconf.wred_total_avail_resources =
+		conf->wred_total_avail_resources;
+
+	rc = ALLOCATE_DDR_FOR_QM(qdev);
+	if (rc) {
+		QOS_LOG_ERR("Could not allocate %u bytes for queue manager\n",
+				qdev->hwconf.wred_total_avail_resources *
+				PPV4_QOS_DESC_SIZE);
+		goto out;
+	}
+
+	QOS_LOG_INFO("wred total resources\t%u\n",
+			qdev->hwconf.wred_total_avail_resources);
+	QOS_LOG_INFO("qm_ddr_start\t\t0x%08X\n", qdev->hwconf.qm_ddr_start);
+	QOS_LOG_INFO("qm_num_of_pages\t%u\n", qdev->hwconf.qm_num_pages);
+
+	if (conf->wred_p_const > 1023) {
+		QOS_LOG_ERR("wred_p_const should be not greater than 1023\n");
+		rc = -EINVAL;
+		goto out;
+	}
+
+	qdev->hwconf.wred_const_p = conf->wred_p_const;
+	qdev->hwconf.wred_max_q_size = conf->wred_max_q_size;
+
+	QOS_LOG_INFO("wred p const\t\t%u\n", qdev->hwconf.wred_const_p);
+	QOS_LOG_INFO("wred max q size\t%u\n", qdev->hwconf.wred_max_q_size);
+
+	rc = load_firmware(qdev, FIRMWARE_FILE);
+	if (rc)
+		goto out;
+
+	create_init_logger_cmd(qdev);
+	create_init_qos_cmd(qdev);
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+
+	qdev->initialized = 1;
+	rc = 0;
+out:
+	QOS_UNLOCK(qdev);
+	QOS_LOG_DEBUG("pp_qos_dev_init completed with %d\n", rc);
+	return rc;
+}
+
+
+struct pp_qos_dev *create_qos_dev_desc(struct qos_dev_init_info *initinfo)
+{
+	struct pp_qos_dev *qdev;
+	int rc;
+	unsigned int id;
+
+	id = initinfo->pl_data.id;
+	if (id >= MAX_QOS_INSTANCES) {
+		QOS_LOG_ERR("Illegal qos instance %u\n", id);
+		return NULL;
+	}
+
+	if (qos_devs[id]) {
+		QOS_LOG_ERR("qos instance %u already exists\n", id);
+		return NULL;
+	}
+
+	qdev = _qos_init(initinfo->pl_data.max_port);
+	if (qdev) {
+		qdev->hwconf.wred_prioritize_pop =
+			initinfo->pl_data.wred_prioritize_pop;
+		qdev->hwconf.fw_logger_start =
+			initinfo->pl_data.fw_logger_start;
+		qdev->hwconf.fw_stat = initinfo->pl_data.fw_stat;
+		memcpy(&qdev->fwcom, &initinfo->fwcom, sizeof(struct fw_com));
+		rc = init_fwdata_internals(qdev);
+		if (rc)
+			goto err;
+		qos_devs[id] = qdev;
+		QOS_LOG_INFO("Initialized qos instance\nmax_port:\t\t%u\n",
+				qdev->max_port);
+		QOS_LOG_INFO("fw_logger_start:\t0x%08X\n",
+				qdev->hwconf.fw_logger_start);
+		QOS_LOG_INFO("fw_stat:\t\t0x%08X\n",
+				qdev->hwconf.fw_stat);
+		QOS_LOG_INFO("cmdbuf:\t\t0x%08X\ncmdbuf size:\t\t%zu\n",
+				(unsigned int)(uintptr_t)qdev->fwcom.cmdbuf,
+				qdev->fwcom.cmdbuf_sz);
+	} else {
+		QOS_LOG_CRIT("Failed creating qos instance %u\n", id);
+	}
+
+	return qdev;
+err:
+	_qos_clean(qdev);
+	QOS_LOG_CRIT("Failed creating qos instance %u\n", id);
+	return NULL;
+}
+
+void remove_qos_instance(unsigned int id)
+{
+	if (id >= MAX_QOS_INSTANCES) {
+		QOS_LOG_ERR("Illegal qos instance %u\n", id);
+		return;
+	}
+
+	if (qos_devs[id] == NULL) {
+		QOS_LOG_ERR("qos instance %u not exists\n", id);
+		return;
+	}
+
+	_qos_clean(qos_devs[id]);
+	qos_devs[id] = NULL;
+	QOS_LOG_INFO("removed qos instance %u\n", id);
+}
+
+void qos_module_init(void)
+{
+	unsigned int i;
+
+	for (i = 0 ; i < MAX_QOS_INSTANCES; ++i)
+		qos_devs[i] = NULL;
+
+	QOS_LOG_INFO("qos_module_init completed\n");
+}
diff --git a/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_uc_defs.h b/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_uc_defs.h
new file mode 100644
index 000000000000..a5ef4f6e0676
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_uc_defs.h
@@ -0,0 +1,522 @@
+/*
+ * GPL LICENSE SUMMARY
+ *
+ *  Copyright(c) 2017 Intel Corporation.
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of version 2 of the GNU General Public License as
+ *  published by the Free Software Foundation.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *  The full GNU General Public License is included in this distribution
+ *  in the file called LICENSE.GPL.
+ *
+ *  Contact Information:
+ *  Intel Corporation
+ *  2200 Mission College Blvd.
+ *  Santa Clara, CA  97052
+ */
+/*
+ * uc_host_defs.h
+ *
+ *  Created on: Dec 17, 2017
+ *      Author: obakshe
+ */
+
+#ifndef SRC_UC_HOST_DEFS_H_
+#define SRC_UC_HOST_DEFS_H_
+
+// UC version
+#define UC_VERSION_MAJOR	(1)
+#define UC_VERSION_MINOR	(0)
+#define UC_VERSION_BUILD	(4)
+
+/**************************************************************************
+ *! @enum	UC_STATUS
+ **************************************************************************
+ *
+ * @brief UC general status enum
+ *
+ **************************************************************************/
+enum uc_status {
+	//!< Status OK
+	UC_STATUS_OK,
+
+	//!< General failure
+	UC_STATUS_GENERAL_FAILURE,
+
+	//!< Invalid user input
+	UC_STATUS_INVALID_INPUT,
+};
+
+/**************************************************************************
+ *! @enum	UC_LOGGER_LEVEL
+ **************************************************************************
+ *
+ * @brief UC Logger level enum
+ *
+ **************************************************************************/
+enum uc_logger_level {
+	//!< FATAL error occurred. SW will probably fail to proceed
+	UC_LOGGER_LEVEL_FATAL,
+
+	//!< General ERROR occurred.
+	UC_LOGGER_LEVEL_ERROR,
+
+	//!< WARNING
+	UC_LOGGER_LEVEL_WARNING,
+
+	//!< Information print to the user
+	UC_LOGGER_LEVEL_INFO,
+
+	//!< Debug purposes level
+	UC_LOGGER_LEVEL_DEBUG,
+};
+
+/**************************************************************************
+ *! @enum	UC_LOGGER_MODE
+ **************************************************************************
+ *
+ * @brief UC Logger operation mode
+ *
+ **************************************************************************/
+enum uc_logger_mode {
+	//!< Logger is disabled
+	UC_LOGGER_MODE_NONE,
+
+	//!< Messages are written to the standard output
+	UC_LOGGER_MODE_STDOUT,
+
+	//!< Local file. N/A
+//	UC_LOGGER_MODE_LOCAL_FILE,
+
+	//!< Messages are written to the host allocated memory
+	UC_LOGGER_MODE_WRITE_HOST_MEM,
+};
+
+/**************************************************************************
+ *! \enum	TSCD_NODE_CONF
+ **************************************************************************
+ *
+ * \brief TSCD node configuration valid bits. Used in modify existing node
+ *
+ **************************************************************************/
+enum tscd_node_conf {
+	//!< None
+	TSCD_NODE_CONF_NONE					=	0x0000,
+
+	//!< Suspend/Resume node
+	TSCD_NODE_CONF_SUSPEND_RESUME		=	0x0001,
+
+	//!< first child (Not relevant for queue)
+	TSCD_NODE_CONF_FIRST_CHILD			=	0x0002,
+
+	//!< last child (Not relevant for queue)
+	TSCD_NODE_CONF_LAST_CHILD			=	0x0004,
+
+	//!< 0 - BW Limit disabled >0 - define BW
+	TSCD_NODE_CONF_BW_LIMIT				=	0x0008,
+
+	//!< Best Effort enable
+	TSCD_NODE_CONF_BEST_EFFORT_ENABLE	=	0x0010,
+
+	//!< First Weighted-Round-Robin node (Not relevant for queue)
+	TSCD_NODE_CONF_FIRST_WRR_NODE		=	0x0020,
+
+	//!< Update predecessor 0 (Not relevant for port)
+	TSCD_NODE_CONF_PREDECESSOR_0		=	0x0040,
+
+	//!< Update predecessor 1 (Not relevant for port)
+	TSCD_NODE_CONF_PREDECESSOR_1		=	0x0080,
+
+	//!< Update predecessor 2 (Not relevant for port)
+	TSCD_NODE_CONF_PREDECESSOR_2		=	0x0100,
+
+	//!< Update predecessor 3 (Not relevant for port)
+	TSCD_NODE_CONF_PREDECESSOR_3		=	0x0200,
+
+	//!< Update predecessor 4 (Not relevant for port)
+	TSCD_NODE_CONF_PREDECESSOR_4		=	0x0400,
+
+	//!< Update predecessor 5 (Not relevant for port)
+	TSCD_NODE_CONF_PREDECESSOR_5		=	0x0800,
+
+	//!< Shared BW limit group (0: no shared BW limit, 1-511: group ID)
+	TSCD_NODE_CONF_SHARED_BWL_GROUP		=	0x1000,
+
+	//!< All flags are set
+	TSCD_NODE_CONF_ALL					=	0xFFFF
+};
+
+/**************************************************************************
+ *! \enum	WRED_QUEUE_CONF
+ **************************************************************************
+ *
+ * \brief WRED queue configuration valid bits. Used in modify existing queue
+ *
+ **************************************************************************/
+enum wred_queue_conf {
+	//!< None
+	WRED_QUEUE_CONF_NONE				=	0x0000,
+
+	//!< Q is active
+	WRED_QUEUE_CONF_ACTIVE_Q			=	0x0001,
+
+	//!< Disable flags valid
+	WRED_QUEUE_CONF_DISABLE				=	0x0002,
+
+	//!< Use fixed green drop probability
+	WRED_QUEUE_CONF_FIXED_GREEN_DROP_P	=	0x0004,
+
+	//!< Use fixed yellow drop probability
+	WRED_QUEUE_CONF_FIXED_YELLOW_DROP_P	=	0x0008,
+
+	//!< Min average yellow
+	WRED_QUEUE_CONF_MIN_AVG_YELLOW		=	0x0010,
+
+	//!< Max average yellow
+	WRED_QUEUE_CONF_MAX_AVG_YELLOW		=	0x0020,
+
+	//!< Slope yellow
+	WRED_QUEUE_CONF_SLOPE_YELLOW		=	0x0040,
+
+	//!< INTERNAL CONFIGURATION. SHOULD NOT BE SET BY HOST
+	WRED_QUEUE_CONF_SHIFT_AVG_YELLOW	=	0x0080,
+
+	//!< Min average green
+	WRED_QUEUE_CONF_MIN_AVG_GREEN		=	0x0100,
+
+	//!< Max average green
+	WRED_QUEUE_CONF_MAX_AVG_GREEN		=	0x0200,
+
+	//!< Slope green
+	WRED_QUEUE_CONF_SLOPE_GREEN			=	0x0400,
+
+	//!< INTERNAL CONFIGURATION. SHOULD NOT BE SET BY HOST
+	WRED_QUEUE_CONF_SHIFT_AVG_GREEN		=	0x0800,
+
+	//!< Min guaranteed
+	WRED_QUEUE_CONF_MIN_GUARANTEED		=	0x1000,
+
+	//!< max allowed
+	WRED_QUEUE_CONF_MAX_ALLOWED			=	0x2000,
+
+	//!< All flags are set
+	WRED_QUEUE_CONF_ALL					=	0xFFFF
+};
+
+/**************************************************************************
+ *! \enum	PORT_CONF
+ **************************************************************************
+ *
+ * \brief Port configuration valid bits. Used in modify existing port
+ *
+ **************************************************************************/
+enum port_conf {
+	//!< None
+	PORT_CONF_NONE					=	0x0000,
+
+	//!< Ring Size
+	PORT_CONF_RING_SIZE				=	0x0001,
+
+	//!< Ring address high
+	PORT_CONF_RING_ADDRESS_HIGH		=	0x0002,
+
+	//!< Ring address low
+	PORT_CONF_RING_ADDRESS_LOW		=	0x0004,
+
+	//!< Enable port
+	PORT_CONF_ACTIVE				=	0x0008,
+
+	//!< All flags are set
+	PORT_CONF_ALL					=	0xFFFF
+};
+
+/**************************************************************************
+ *! \struct	port_stats_s
+ **************************************************************************
+ *
+ * \brief Port stats
+ *
+ **************************************************************************/
+struct port_stats_s {
+	u32	total_green_bytes;
+	u32	total_yellow_bytes;
+
+	// Following stats can not be reset
+	u32	debug_back_pressure_status;
+};
+
+/**************************************************************************
+ *! \struct	hw_node_info_s
+ **************************************************************************
+ *
+ * \brief HW node info
+ *
+ **************************************************************************/
+struct hw_node_info_s {
+	u32	first_child;
+	u32	last_child;
+	u32	is_suspended;
+	u32	bw_limit;
+	u32	predecessor0;
+	u32	predecessor1;
+	u32	predecessor2;
+	u32	predecessor3;
+	u32	predecessor4;
+	u32	predecessor5;
+	u32	queue_physical_id;
+	u32	queue_port;
+};
+
+/**************************************************************************
+ *! \enum	PORT_STATS_CLEAR_FLAGS
+ **************************************************************************
+ *
+ * \brief	port stats clear flags.
+ *			Used in get port stats command to set which stats
+ *			will be reset after read
+ *
+ **************************************************************************/
+enum port_stats_clear_flags {
+	//!< None
+	PORT_STATS_CLEAR_NONE					=	0x0000,
+
+	//!< Clear port total green bytes stats
+	PORT_STATS_CLEAR_TOTAL_GREEN_BYTES		=	0x0001,
+
+	//!< Clear port total yellow bytes stats
+	PORT_STATS_CLEAR_TOTAL_YELLOW_BYTES		=	0x0002,
+
+	//!< All above stats will be cleared
+	PORT_STATS_CLEAR_ALL					=	0xFFFF,
+};
+
+/**************************************************************************
+ *! \struct	queue_stats_s
+ **************************************************************************
+ *
+ * \brief Queue stats
+ *
+ **************************************************************************/
+struct queue_stats_s {
+	u32	queue_size_bytes;
+	u32	queue_average_size_bytes;
+	u32	queue_size_entries;
+	u32	drop_p_yellow;
+	u32	drop_p_green;
+	u32	total_bytes_added_low;
+	u32	total_bytes_added_high;
+	u32	total_accepts;
+	u32	total_drops;
+	u32	total_dropped_bytes_low;
+	u32	total_dropped_bytes_high;
+	u32	total_red_dropped;
+
+	// Following stats can not be reset
+	u32	qmgr_num_queue_entries;
+	u32	qmgr_null_pop_queue_counter;
+	u32	qmgr_empty_pop_queue_counter;
+	u32	qmgr_null_push_queue_counter;
+};
+
+/**************************************************************************
+ *! \enum	QUEUE_STATS_CLEAR_FLAGS
+ **************************************************************************
+ *
+ * \brief	queue stats clear flags.
+ *			Used in get queue stats command to set which stats
+ *			will be reset after read
+ *
+ **************************************************************************/
+enum queue_stats_clear_flags {
+	//!< None
+	QUEUE_STATS_CLEAR_NONE					=	0x0000,
+
+	//!< Clear queue size bytes stats
+	QUEUE_STATS_CLEAR_Q_SIZE_BYTES			=	0x0001,
+
+	//!< Clear queue average size bytes stats
+	QUEUE_STATS_CLEAR_Q_AVG_SIZE_BYTES		=	0x0002,
+
+	//!< Clear queue size entries stats
+	QUEUE_STATS_CLEAR_Q_SIZE_ENTRIES		=	0x0004,
+
+	//!< Clear drop probability yellow stats
+	QUEUE_STATS_CLEAR_DROP_P_YELLOW			=	0x0008,
+
+	//!< Clear drop probability green stats
+	QUEUE_STATS_CLEAR_DROP_P_GREEN			=	0x0010,
+
+	//!< Clear total bytes added stats
+	QUEUE_STATS_CLEAR_TOTAL_BYTES_ADDED		=	0x0020,
+
+	//!< Clear total accepts stats
+	QUEUE_STATS_CLEAR_TOTAL_ACCEPTS			=	0x0040,
+
+	//!< Clear total drops stats
+	QUEUE_STATS_CLEAR_TOTAL_DROPS			=	0x0080,
+
+	//!< Clear total dropped bytes stats
+	QUEUE_STATS_CLEAR_TOTAL_DROPPED_BYTES	=	0x0100,
+
+	//!< Clear total RED drops stats
+	QUEUE_STATS_CLEAR_TOTAL_RED_DROPS		=	0x0200,
+
+	//!< All above stats will be cleared
+	QUEUE_STATS_CLEAR_ALL					=	0xFFFF,
+};
+
+/**************************************************************************
+ *! \struct	system_stats_s
+ **************************************************************************
+ *
+ * \brief system stats
+ *
+ **************************************************************************/
+struct system_stats_s {
+	u32	qmgr_cache_free_pages_counter;
+	u32	qmgr_sm_current_state;
+	u32	qmgr_cmd_machine_busy;
+	u32	qmgr_cmd_machine_pop_busy;
+	u32	qmgr_null_pop_counter;
+	u32	qmgr_empty_pop_counter;
+	u32	qmgr_null_push_counter;
+	u32	qmgr_ddr_stop_push_low_threshold;
+	u32	qmgr_fifo_error_register;
+	u32	qmgr_ocp_error_register;
+	u32	qmgr_cmd_machine_sm_current_state_0;
+	u32	qmgr_cmd_machine_sm_current_state_1;
+	u32	qmgr_cmd_machine_sm_current_state_2;
+	u32	qmgr_cmd_machine_sm_current_state_3;
+	u32	qmgr_cmd_machine_sm_current_state_4;
+	u32	qmgr_cmd_machine_sm_current_state_5;
+	u32	qmgr_cmd_machine_sm_current_state_6;
+	u32	qmgr_cmd_machine_sm_current_state_7;
+	u32	qmgr_cmd_machine_sm_current_state_8;
+	u32	qmgr_cmd_machine_sm_current_state_9;
+	u32	qmgr_cmd_machine_sm_current_state_10;
+	u32	qmgr_cmd_machine_sm_current_state_11;
+	u32	qmgr_cmd_machine_sm_current_state_12;
+	u32	qmgr_cmd_machine_sm_current_state_13;
+	u32	qmgr_cmd_machine_sm_current_state_14;
+	u32	qmgr_cmd_machine_sm_current_state_15;
+	u32	qmgr_cmd_machine_queue_0;
+	u32	qmgr_cmd_machine_queue_1;
+	u32	qmgr_cmd_machine_queue_2;
+	u32	qmgr_cmd_machine_queue_3;
+	u32	qmgr_cmd_machine_queue_4;
+	u32	qmgr_cmd_machine_queue_5;
+	u32	qmgr_cmd_machine_queue_6;
+	u32	qmgr_cmd_machine_queue_7;
+	u32	qmgr_cmd_machine_queue_8;
+	u32	qmgr_cmd_machine_queue_9;
+	u32	qmgr_cmd_machine_queue_10;
+	u32	qmgr_cmd_machine_queue_11;
+	u32	qmgr_cmd_machine_queue_12;
+	u32	qmgr_cmd_machine_queue_13;
+	u32	qmgr_cmd_machine_queue_14;
+	u32	qmgr_cmd_machine_queue_15;
+
+	u32	tscd_num_of_used_nodes;
+};
+
+/**************************************************************************
+ *! @enum	UC_QOS_COMMAND
+ **************************************************************************
+ *
+ * @brief UC QOS command enum. Must be synced with the Host definition
+ *
+ **************************************************************************/
+enum uc_qos_command {
+	UC_QOS_COMMAND_GET_FW_VERSION,
+	UC_QOS_COMMAND_MULTIPLE_COMMANDS,
+	UC_QOS_COMMAND_INIT_UC_LOGGER,
+	UC_QOS_COMMAND_INIT_QOS,
+	UC_QOS_COMMAND_ADD_PORT,
+	UC_QOS_COMMAND_REMOVE_PORT,
+	UC_QOS_COMMAND_ADD_SCHEDULER,
+	UC_QOS_COMMAND_REMOVE_SCHEDULER,
+	UC_QOS_COMMAND_ADD_QUEUE,
+	UC_QOS_COMMAND_REMOVE_QUEUE,
+	UC_QOS_COMMAND_FLUSH_QUEUE,
+	UC_QOS_COMMAND_SET_PORT,
+	UC_QOS_COMMAND_SET_SCHEDULER,
+	UC_QOS_COMMAND_SET_QUEUE,
+	UC_QOS_COMMAND_MOVE_SCHEDULER,
+	UC_QOS_COMMAND_MOVE_QUEUE,
+	UC_QOS_COMMAND_GET_PORT_STATS,
+	UC_QOS_COMMAND_GET_QUEUE_STATS,
+	UC_QOS_COMMAND_GET_SYSTEM_STATS,
+	UC_QOS_COMMAND_ADD_SHARED_BW_LIMIT_GROUP,
+	UC_QOS_COMMAND_REMOVE_SHARED_BW_LIMIT_GROUP,
+	UC_QOS_COMMAND_SET_SHARED_BW_LIMIT_GROUP,
+	UC_QOS_COMMAND_GET_NODE_INFO,
+	UC_QOS_COMMAND_DEBUG_READ_NODE,
+	UC_QOS_COMMAND_DEBUG_PUSH_DESC,
+	UC_QOS_COMMAND_DEBUG_ADD_CREDIT_TO_PORT,
+	UC_QOS_COMMAND_GET_ACTIVE_QUEUES_STATS,
+};
+
+/**************************************************************************
+ *! @struct	uc_qos_cmd_s
+ **************************************************************************
+ *
+ * @brief UC commands.
+ * This structure defines the Host <-->UC interface
+ *
+ **************************************************************************/
+struct uc_qos_cmd_s {
+	//!< Type of command (UC_QOS_COMMAND)
+	u32			type;
+
+	//!< Commands flags
+	u32			flags;
+#define	UC_CMD_FLAG_IMMEDIATE				BIT(0)
+#define	UC_CMD_FLAG_BATCH_FIRST				BIT(1)
+#define	UC_CMD_FLAG_BATCH_LAST				BIT(2)
+#define	UC_CMD_FLAG_MULTIPLE_COMMAND_LAST	BIT(3)
+#define	UC_CMD_FLAG_UC_DONE					BIT(4)
+#define	UC_CMD_FLAG_UC_ERROR				BIT(5)
+
+	//!< Number of 32bit parameters available for this command.
+	// must be synced between the host and uc!
+	u32			num_params;
+
+	u32			param0;
+	u32			param1;
+	u32			param2;
+	u32			param3;
+	u32			param4;
+	u32			param5;
+	u32			param6;
+	u32			param7;
+	u32			param8;
+	u32			param9;
+	u32			param10;
+	u32			param11;
+	u32			param12;
+	u32			param13;
+	u32			param14;
+	u32			param15;
+	u32			param16;
+	u32			param17;
+	u32			param18;
+	u32			param19;
+	u32			param20;
+	u32			param21;
+	u32			param22;
+	u32			param23;
+	u32			param24;
+	u32			param25;
+};
+
+#endif /* SRC_UC_HOST_DEFS_H_ */
diff --git a/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_uc_wrapper.h b/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_uc_wrapper.h
new file mode 100644
index 000000000000..24c375248649
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_uc_wrapper.h
@@ -0,0 +1,34 @@
+/*
+ * GPL LICENSE SUMMARY
+ *
+ *  Copyright(c) 2017 Intel Corporation.
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of version 2 of the GNU General Public License as
+ *  published by the Free Software Foundation.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *  The full GNU General Public License is included in this distribution
+ *  in the file called LICENSE.GPL.
+ *
+ *  Contact Information:
+ *  Intel Corporation
+ *  2200 Mission College Blvd.
+ *  Santa Clara, CA  97052
+ */
+#ifndef PP_QOS_UC_H
+#define PP_QOS_UC_H
+
+typedef uint32_t u32;
+
+#include "pp_qos_uc_defs.h"
+
+#endif
diff --git a/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_utils.c b/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_utils.c
new file mode 100644
index 000000000000..b61c80696b5a
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_utils.c
@@ -0,0 +1,2627 @@
+
+/*
+ * GPL LICENSE SUMMARY
+ *
+ *  Copyright(c) 2017 Intel Corporation.
+ * *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of version 2 of the GNU General Public License as
+ *  published by the Free Software Foundation.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *  The full GNU General Public License is included in this distribution
+ *  in the file called LICENSE.GPL.
+ *
+ *  Contact Information:
+ *  Intel Corporation
+ *  2200 Mission College Blvd.
+ *  Santa Clara, CA  97052
+ */
+#include "pp_qos_common.h"
+#include "pp_qos_fw.h"
+#include "pp_qos_utils.h"
+
+//#define VERY_VERBOSE_TESTS
+#ifdef VERY_VERBOSE_TESTS
+
+#define ISSUE_BW_CMD(phy, numinator, denominator) do {  \
+if (denominator)					\
+QOS_LOG("Issue bw change for phy %u: %u : %u\n", phy, numinator, denominator);\
+} while (0)
+
+#else
+#define ISSUE_BW_CMD(phy, numinator, denominator)
+
+#endif
+
+#define QOS_INVALID_PRIORITY   0xF
+
+/******************************************************************************/
+/*                                 OCTETS                                     */
+/******************************************************************************/
+struct octet_info {
+	struct list_head list;
+	uint8_t usage;
+};
+
+struct pp_octets {
+	unsigned int last_port_octet;
+	struct octet_info info[NUM_OF_OCTETS];
+	struct list_head lists[9];
+};
+
+static unsigned int octet_get_use_count(const struct pp_octets *octets,
+					unsigned int octet)
+{
+	return octets->info[octet].usage;
+}
+
+unsigned int octet_get_with_at_least_free_entries(struct pp_octets *octets,
+						  unsigned int count)
+{
+	unsigned int id;
+	struct list_head *list;
+
+	for (list = octets->lists + 8 - count; list >= octets->lists; --list)  {
+		if (!list_empty(list)) {
+			id = list_entry(list->next, struct octet_info, list) -
+				octets->info;
+			list_del_init(list->next);
+			return id;
+		}
+	}
+
+	return QOS_INVALID_OCTET;
+}
+
+unsigned int  octet_get_least_free_entries(struct pp_octets *octets)
+{
+	return octet_get_with_at_least_free_entries(octets, 1);
+}
+
+static void octet_set_use_count(struct pp_octets *octets,
+				unsigned int octet,
+				unsigned int usage)
+{
+	struct list_head *list;
+
+	QOS_ASSERT(QOS_OCTET_VALID(octet), "Illegal octet %u\n", octet);
+	QOS_ASSERT(usage <= 8, "Illegal usage %u\n", usage);
+	octets->info[octet].usage = usage;
+	if (octet > octets->last_port_octet) {
+		list = octets->lists + usage;
+		list_move_tail(&(octets->info[octet].list), list);
+	}
+}
+
+STATIC_UNLESS_TEST struct pp_octets *octets_init(unsigned int last_port_octet)
+{
+	unsigned int i;
+	struct octet_info *info;
+	struct pp_octets *octets;
+
+	octets = QOS_MALLOC(sizeof(struct pp_octets));
+	if (octets) {
+		for (i = 0; i < 9; ++i)
+			INIT_LIST_HEAD(octets->lists + i);
+
+		info = octets->info;
+		for (i = 0; i < NUM_OF_OCTETS; ++i) {
+			info->usage = 0;
+			if (i > last_port_octet)
+				list_add_tail(&info->list, octets->lists);
+			else
+				INIT_LIST_HEAD(&info->list);
+
+			++info;
+		}
+		octets->last_port_octet = last_port_octet;
+	}
+	return octets;
+}
+
+STATIC_UNLESS_TEST void octets_clean(struct pp_octets *octets)
+{
+	if (octets)
+		QOS_FREE(octets);
+}
+
+#ifdef PP_QOS_TEST
+STATIC_UNLESS_TEST void debug_verify_octet_usage(struct pp_octets *octets,
+						 unsigned int octet,
+						 unsigned int usage)
+{
+	struct list_head *list;
+	struct list_head *pos;
+	unsigned int tmp;
+	int found;
+
+	tmp = octet_get_use_count(octets, octet);
+	QOS_ASSERT(tmp == usage, "Octet %u count is %u and not %u\n", octet,
+		   tmp, usage);
+
+	list = &(octets->info[octet].list);
+	if (octet > octets->last_port_octet) {
+		found = 0;
+		for (pos = list->next; pos != list; pos = pos->next) {
+			if (pos >= octets->lists && pos <= octets->lists + 8) {
+				tmp = (pos - octets->lists);
+				QOS_ASSERT(tmp == usage,
+					   "Octet is on list %u but should be on %u\n",
+					   tmp, usage);
+				found = 1;
+				break;
+			}
+		}
+		QOS_ASSERT(found, "Did not find list of octet %u\n", octet);
+	} else {
+		QOS_ASSERT(list_empty(list),
+			"Octet %u is on a list though its a port octet\n",
+			octet);
+	}
+}
+#endif
+
+/******************************************************************************/
+/*                        Ids, Rlms and Port phys pools                       */
+/******************************************************************************/
+struct pp_pool {
+	unsigned int capacity;
+	unsigned int top;
+	uint16_t invalid;
+	uint16_t data[1];
+};
+
+static struct pp_pool *pp_pool_init(unsigned int capacity, uint16_t invalid)
+{
+	struct pp_pool *tmp;
+
+	tmp = QOS_MALLOC(sizeof(struct pp_pool) +
+			(capacity - 1) * sizeof(uint16_t));
+	if (tmp) {
+		tmp->capacity = capacity;
+		tmp->top = 0;
+		tmp->invalid = invalid;
+	}
+	return tmp;
+}
+
+uint16_t pp_pool_get(struct pp_pool *pool)
+{
+	uint16_t tmp;
+	unsigned int top;
+
+	top = pool->top;
+	if (top > 0) {
+		--top;
+		tmp = pool->data[top];
+		pool->top = top;
+		return tmp;
+	}
+	return pool->invalid;
+}
+
+int pp_pool_put(struct pp_pool *pool, uint16_t data)
+{
+	unsigned int top;
+
+	top = pool->top;
+	if (top < pool->capacity) {
+		pool->data[top] = data;
+		pool->top = top + 1;
+		return 0;
+	}
+	return -1;
+}
+
+static void pp_pool_clean(struct pp_pool *pool)
+{
+	if (pool)
+		QOS_FREE(pool);
+}
+
+static struct pp_pool *free_id_init(void)
+{
+	unsigned int i;
+	struct pp_pool *tmp;
+
+	tmp  = pp_pool_init(NUM_OF_NODES, QOS_INVALID_ID);
+	if (tmp)
+		for (i = NUM_OF_NODES; i > 0; --i)
+			pp_pool_put(tmp, i - 1);
+
+	return tmp;
+}
+
+static struct pp_pool *free_rlm_init(void)
+{
+	unsigned int i;
+	struct pp_pool *tmp;
+
+	tmp  = pp_pool_init(NUM_OF_QUEUES, QOS_INVALID_ID);
+	if (tmp)
+		for (i = NUM_OF_QUEUES; i > 0; --i)
+			pp_pool_put(tmp, i - 1);
+
+	return tmp;
+}
+
+
+struct pp_pool *free_ports_phys_init(unsigned int *reserved,
+				     unsigned int max_port,
+				     const unsigned int *reserved_ports,
+				     unsigned int size)
+{
+	unsigned int i;
+	unsigned int res;
+	unsigned int tmp;
+	struct pp_pool *pool;
+
+	res = 0;
+	tmp = min(size, max_port + 1);
+	memcpy(reserved, reserved_ports, tmp * sizeof(unsigned int));
+
+	for (i = 0; i < tmp; ++i)
+		if (reserved[i] == 1)
+			++res;
+
+	tmp = max_port + 1 - res;
+	pool  = pp_pool_init(tmp, QOS_INVALID_PHY);
+	if (pool)
+		for (i = max_port + 1; i > 0; --i)
+			if (reserved[i - 1] == 0)
+				pp_pool_put(pool, i - 1);
+	return pool;
+}
+
+/******************************************************************************/
+/*                                 Queue                                      */
+/******************************************************************************/
+struct q_item {
+	struct list_head list;
+	uint16_t data;
+};
+
+struct pp_queue {
+	unsigned int capacity;
+	struct list_head used;
+	struct list_head free;
+	struct q_item items[1];
+};
+
+uint16_t pp_queue_dequeue(struct pp_queue *queue)
+{
+	struct q_item *item;
+	int rc;
+
+	if (!list_empty(&queue->used)) {
+		item = list_entry(queue->used.next, struct q_item, list);
+		rc = item->data;
+		list_move_tail(&item->list, &queue->free);
+	} else {
+		QOS_LOG("queue is empty\n");
+		rc = QOS_INVALID_ID;
+	}
+	return rc;
+}
+
+int pp_queue_enqueue(struct pp_queue *queue, uint16_t data)
+{
+	struct q_item *item;
+
+	if (!list_empty(&queue->free)) {
+		item = list_entry(queue->free.next, struct q_item, list);
+		item->data = data;
+		list_move_tail(&item->list, &queue->used);
+		return 0;
+	}
+	QOS_LOG("queue is full\n");
+	return -1;
+}
+
+static int queue_is_reset(struct pp_queue *queue)
+{
+	unsigned int i;
+	struct list_head *pos;
+
+	if (!list_empty(&queue->used))
+		return 0;
+
+	i = 0;
+	list_for_each(pos, &queue->free) ++i;
+
+	return (i == queue->capacity);
+}
+
+void pp_queue_reset(struct pp_queue *queue)
+{
+	list_splice_tail_init(&queue->used, &queue->free);
+	QOS_ASSERT(queue_is_reset(queue), "Queue is not reset\n");
+}
+
+static void pp_queue_clean(struct pp_queue *queue)
+{
+	if (queue)
+		QOS_FREE(queue);
+}
+
+static struct pp_queue *pp_queue_init(unsigned int capacity)
+{
+	struct pp_queue *queue;
+	unsigned int i;
+
+	queue = QOS_MALLOC(sizeof(struct pp_queue) +
+			   sizeof(struct q_item) * (capacity - 1));
+	if (queue) {
+		queue->capacity = capacity;
+		INIT_LIST_HEAD(&(queue->free));
+		INIT_LIST_HEAD(&(queue->used));
+		for (i = 0; i < queue->capacity; ++i)
+			list_add_tail(&(queue->items[i].list), &(queue->free));
+	}
+	return queue;
+}
+
+/******************************************************************************/
+/*                             Cmd Queue                                      */
+/******************************************************************************/
+#define CMD_QUEUE_SIZE 16384LU
+struct cmd_queue {
+	size_t read;  /* next index to read */
+	size_t write; /* next free index to write */
+	size_t count; /* how many bytes are in */
+	uint8_t data[CMD_QUEUE_SIZE];
+};
+
+static struct cmd_queue *cmd_queue_init(void)
+{
+	struct cmd_queue *q;
+
+	q = QOS_MALLOC(sizeof(struct cmd_queue));
+	if (q) {
+		q->read = 0;
+		q->write = 0;
+		q->count = 0;
+	}
+	return q;
+}
+
+static void cmd_queue_clean(struct cmd_queue *q)
+{
+	if (q)
+		QOS_FREE(q);
+}
+
+int cmd_queue_put(struct cmd_queue *q, void *_cmd, size_t size)
+{
+	unsigned int toend;
+	uint8_t *cmd;
+
+	cmd = _cmd;
+
+	if (CMD_QUEUE_SIZE - q->count < size) {
+		QOS_LOG_ERR("%lu bytes free, can't accommodate %zu bytes\n",
+				CMD_QUEUE_SIZE - q->count, size);
+		return -1;
+	}
+
+	toend = CMD_QUEUE_SIZE - q->write;
+	if (toend >= size) {
+		memcpy(q->data + q->write, cmd, size);
+		q->write += size;
+	} else {
+		memcpy(q->data + q->write, cmd, toend);
+		memcpy(q->data, cmd + toend, size - toend);
+		q->write = size - toend;
+	}
+
+	q->count += size;
+
+	return 0;
+}
+
+static int cmd_queue_read(struct cmd_queue *q,
+		void *_cmd,
+		size_t size,
+		int remove)
+{
+	unsigned int toend;
+	uint8_t *cmd;
+
+	cmd = _cmd;
+
+	if (q->count < size) {
+		QOS_LOG_DEBUG("has only %zu bytes, can't retrieve %zu bytes\n",
+				q->count, size);
+		return -1;
+	}
+
+	toend = CMD_QUEUE_SIZE - q->read;
+	if (toend >= size) {
+		memcpy(cmd, q->data + q->read, size);
+		if (remove)
+			q->read += size;
+	} else {
+		memcpy(cmd, q->data + q->read, toend);
+		memcpy(cmd + toend, q->data, size - toend);
+		if (remove)
+			q->read = size - toend;
+	}
+
+	if (remove)
+		q->count -= size;
+
+	return 0;
+}
+
+int cmd_queue_get(struct cmd_queue *q, void *_cmd, size_t size)
+{
+	return cmd_queue_read(q, _cmd, size, 1);
+}
+
+int cmd_queue_peek(struct cmd_queue *q, void *_cmd, size_t size)
+{
+	return cmd_queue_read(q, _cmd, size, 0);
+}
+
+int cmd_queue_is_empty(struct cmd_queue *q)
+{
+	return (q->count == 0);
+}
+
+/******************************************************************************/
+/*                                 Nodes                                      */
+/******************************************************************************/
+struct pp_nodes {
+	struct qos_node nodes[NUM_OF_NODES];
+};
+
+static struct pp_nodes *pp_nodes_init(void)
+{
+	struct pp_nodes *nodes;
+
+	nodes = QOS_MALLOC(sizeof(struct pp_nodes));
+	if (nodes)
+		memset(nodes, 0, sizeof(struct pp_nodes));
+
+	return nodes;
+}
+
+unsigned int get_phy_from_node(const struct pp_nodes *nodes,
+		const struct qos_node *node)
+{
+	QOS_ASSERT(node >= nodes->nodes && node <= nodes->nodes + NUM_OF_NODES,
+		   "invalid node pointer\n");
+	return node - nodes->nodes;
+}
+
+struct qos_node *get_node_from_phy(struct pp_nodes *nodes, unsigned int phy)
+{
+	QOS_ASSERT(QOS_PHY_VALID(phy), "invalid phy %u\n", phy);
+	return nodes->nodes + phy;
+}
+
+const struct qos_node *get_const_node_from_phy(const struct pp_nodes *nodes,
+					       unsigned int phy)
+{
+	QOS_ASSERT(QOS_PHY_VALID(phy), "invalid phy %u\n", phy);
+	return nodes->nodes + phy;
+}
+
+static void pp_nodes_clean(struct pp_nodes *nodes)
+{
+	if (nodes)
+		QOS_FREE(nodes);
+}
+
+static unsigned int get_child_node_order(
+		struct pp_nodes *nodes,
+		const struct qos_node *node)
+{
+	unsigned int phy;
+	const struct qos_node *parent_node;
+
+	QOS_ASSERT(node_child(node), "Node is not a child\n");
+	phy = get_phy_from_node(nodes, node);
+	parent_node = get_const_node_from_phy(nodes,
+			node->child_prop.parent_phy);
+	return (phy - parent_node->parent_prop.first_child_phy);
+}
+
+/******************************************************************************/
+/*                                 Mapping                                    */
+/******************************************************************************/
+struct pp_mapping {
+	uint16_t id2phy[NUM_OF_NODES];
+	uint16_t phy2id[NUM_OF_NODES];
+};
+
+unsigned int get_id_from_phy(const struct pp_mapping *map, unsigned int phy)
+{
+	QOS_ASSERT(QOS_PHY_VALID(phy), "invalid phy %u\n", phy);
+	return map->phy2id[phy];
+}
+
+unsigned int get_phy_from_id(const struct pp_mapping *map, unsigned int id)
+{
+	QOS_ASSERT(QOS_ID_VALID(id), "invalid id %u\n", id);
+	return map->id2phy[id];
+}
+
+void map_id_phy(struct pp_mapping *map, unsigned int id, unsigned int phy)
+{
+	QOS_ASSERT(QOS_ID_VALID(id), "invalid id %u\n", id);
+	QOS_ASSERT(QOS_PHY_VALID(phy), "invalid phy %u\n", phy);
+	map->id2phy[id] = phy;
+	map->phy2id[phy] = id;
+}
+
+void map_id_reserved(struct pp_mapping *map, unsigned int id)
+{
+	QOS_ASSERT(QOS_ID_VALID(id), "invalid id %u\n", id);
+	map->id2phy[id] = QOS_UNKNOWN_PHY;
+}
+
+static struct pp_mapping *pp_mapping_init(void)
+{
+	unsigned int i;
+	struct pp_mapping *map;
+
+	map = QOS_MALLOC(sizeof(struct pp_mapping));
+	if (map) {
+		for (i = 0; i < NUM_OF_NODES; ++i) {
+			map->id2phy[i] = QOS_INVALID_PHY;
+			map->phy2id[i] = QOS_INVALID_ID;
+		}
+	}
+	return map;
+}
+
+static void pp_mapping_clean(struct pp_mapping *map)
+{
+	if (map)
+		QOS_FREE(map);
+}
+
+void map_invalidate_id(struct pp_mapping *map, unsigned int id)
+{
+	unsigned int phy;
+
+	QOS_ASSERT(QOS_ID_VALID(id), "invalid id %u\n", id);
+	phy = get_phy_from_id(map, id);
+	QOS_ASSERT(QOS_PHY_VALID(phy), "invalid phy is mapped to id %u\n", id);
+
+	map->id2phy[id] = QOS_INVALID_PHY;
+	map->phy2id[phy] = QOS_INVALID_ID;
+}
+
+/******************************************************************************/
+/*                                 Moving                                     */
+/******************************************************************************/
+/*
+ * Return 1 if used status of all count nodes starting from first_phy equals
+ * status
+ */
+static int is_all_nodes_used_status(const struct pp_qos_dev *qdev,
+		unsigned int first_phy, unsigned int count, unsigned int status)
+{
+	const struct qos_node *cur;
+	unsigned int used;
+
+	cur = get_const_node_from_phy(qdev->nodes, first_phy);
+	for (; count; --count) {
+		used = !!QOS_BITS_IS_SET(cur->flags, QOS_NODE_FLAGS_USED);
+		if (used != status)
+			return 0;
+		cur++;
+	}
+	return 1;
+}
+
+/* is child_phy a child of parent_phy */
+static int node_child_of(struct pp_nodes *nodes,
+		unsigned int child_phy, unsigned int parent_phy)
+{
+	const struct qos_node *parent;
+
+	parent = get_const_node_from_phy(nodes, parent_phy);
+
+	return (child_phy >= parent->parent_prop.first_child_phy &&
+		child_phy <= parent->parent_prop.first_child_phy +
+		parent->parent_prop.num_of_children - 1);
+}
+
+/**
+ * node_update_children_internal() - Update children's parent
+ *
+ * @nodes:     original nodes (qdev->nodes)
+ * @tmp_nodes: a window (buffer) holding a copy of some nodes from
+ *	       the original nodes
+ * @first:     The phy that the first node of tmp_nodes
+ *             represents
+ * @count:     How many children should be updated on tmp_nodes
+ * @phy:       original phy of parent
+ * @new_phy:   new phy of parent
+ *
+ * Does not create update predecessor cmds.
+ * The first count children of parent are updated on tmp_nodes
+ * the rest on nodes
+ */
+static void node_update_children_internal(
+				   struct pp_nodes *nodes,
+				   struct qos_node *tmp_nodes,
+				   unsigned int first,
+				   unsigned int count,
+				   unsigned int phy,
+				   unsigned int new_phy)
+
+{
+	unsigned int last;
+	unsigned int cur;
+	struct qos_node *child;
+	struct qos_node *parent;
+
+	parent = get_node_from_phy(nodes, phy);
+	cur = parent->parent_prop.first_child_phy;
+	last = cur + parent->parent_prop.num_of_children - 1;
+	for (; cur <= last; ++cur) {
+		if (cur >= first && cur < first + count)
+			child = tmp_nodes + cur - first;
+		else
+			child = get_node_from_phy(nodes, cur);
+		child->child_prop.parent_phy = new_phy;
+	}
+}
+
+/*
+ * Update the children of a parent which is moving from phy to new_phy
+ * Children parent_phy's is updated
+ */
+void node_update_children(
+			  struct pp_qos_dev *qdev,
+			  unsigned int phy,
+			  unsigned int new_phy)
+{
+	unsigned int num;
+	struct qos_node *child;
+	struct qos_node *parent;
+
+	parent = get_node_from_phy(qdev->nodes, phy);
+	child = get_node_from_phy(
+			qdev->nodes,
+			parent->parent_prop.first_child_phy);
+	num =  parent->parent_prop.num_of_children;
+	for (; num; --num)  {
+		child->child_prop.parent_phy = new_phy;
+		++child;
+	}
+}
+
+
+/*
+ * Update parent's first child phy and num of children, given that count nodes
+ * starting from src_phy are moving to dst_phy
+ *
+ * Note number of children is calculated as last_child - first_child + 1
+ */
+static int node_update_parent(struct pp_qos_dev *qdev,
+		struct qos_node *tmpnodes, unsigned int src_first,
+		unsigned int size, unsigned int src_phy, unsigned int dst_phy,
+		unsigned int count)
+{
+	unsigned int first;
+	unsigned int last;
+	unsigned int num;
+	unsigned int parent_phy;
+	unsigned int moving;
+	struct qos_node *parent;
+	struct qos_node *child;
+	struct pp_nodes *nodes;
+
+	nodes = qdev->nodes;
+	child = get_node_from_phy(nodes, src_phy);
+	parent_phy = child->child_prop.parent_phy;
+
+	if (parent_phy >= src_first && parent_phy < src_first + size)
+		parent = tmpnodes + parent_phy - src_first;
+	else
+		parent = get_node_from_phy(nodes, parent_phy);
+
+	QOS_ASSERT(node_parent(parent),
+			"Node %u suppose to be a parent but is not\n",
+			get_phy_from_node(nodes, parent));
+
+	QOS_ASSERT(node_child_of(nodes, src_phy, parent_phy),
+			"%u is not child of %u\n",
+			src_phy, parent_phy);
+
+	first = parent->parent_prop.first_child_phy;
+	num = parent->parent_prop.num_of_children;
+	last = first + num - 1;
+	QOS_ASSERT(same_octet(first, last),
+			"source nodes %u and %u are not in the same octet\n",
+			first, last);
+
+	/* Number of children going to move */
+	moving = min(count, num - (src_phy - first));
+	QOS_ASSERT(same_octet(dst_phy, dst_phy + moving - 1),
+			"destination nodes %u and %u are not in the same octet\n",
+			dst_phy, dst_phy + moving - 1);
+
+	if (moving == num) { /* all children are moving */
+		parent->parent_prop.first_child_phy = dst_phy;
+	} else {
+		QOS_ASSERT(same_octet(dst_phy, src_phy),
+				"src phy %u and dst phy %u are not in same octet\n",
+				src_phy, dst_phy);
+		if (first != src_phy) /* first is not moving */
+			first = min(first, dst_phy);
+		else   /* first is moving */
+			first = min(first + moving, dst_phy);
+
+		if (last != src_phy + moving - 1)  /* last is not moving */
+			last = max(last, dst_phy + moving - 1);
+		else  /* last is moving */
+			last = max(last - moving, dst_phy + moving - 1);
+
+		QOS_ASSERT(same_octet(first, last),
+				"%u and %u are in different octets\n",
+				first, last);
+		parent->parent_prop.first_child_phy = first;
+		parent->parent_prop.num_of_children = last - first + 1;
+	}
+
+	return moving;
+}
+
+/*
+ * Given count nodes starting at src_phy [src_phy..src_phy + count - 1],
+ * that are going to move to [dst_phy..dst_phy + count - 1]:
+ *
+ * Update id<->phy mapping for each node that is moved
+ *
+ * For each node that is a parent node
+ *   Update parent_phy of each of its children
+ *
+ * For each group of children that have the same parent
+ *   Update parent's first_child_phy and num of children
+ *
+ */
+static void nodes_update_stake_holders(struct pp_qos_dev *qdev,
+		unsigned int src_phy, unsigned int dst_phy, unsigned int count)
+{
+	unsigned int i;
+	unsigned int tmp;
+	struct qos_node *next_updated_parent;
+	struct qos_node *node;
+	struct pp_mapping *map;
+	struct qos_node tmp_nodes[MAX_MOVING_NODES];
+	unsigned int tmp_map[MAX_MOVING_NODES];
+	unsigned int updated_parents[MAX_MOVING_NODES];
+	unsigned int index;
+
+	QOS_ASSERT(count < MAX_MOVING_NODES,
+			"Trying to update %u nodes but the maximum is %u\n",
+			count, MAX_MOVING_NODES);
+	map = qdev->mapping;
+	index = 0;
+
+	memcpy(&tmp_nodes, qdev->nodes->nodes + src_phy,
+			sizeof(struct qos_node) * count);
+
+	/*
+	 * Invalidate current mapping, update children and parents of
+	 * moved nodes
+	 */
+	node = get_node_from_phy(qdev->nodes, src_phy);
+	next_updated_parent = node;
+	for (i = src_phy; i < src_phy + count; ++i) {
+		tmp = get_id_from_phy(map, i);
+		tmp_map[i - src_phy] = tmp;
+		map_invalidate_id(map, tmp);
+		if (node_parent(node))
+			node_update_children_internal(
+					qdev->nodes,
+					tmp_nodes,
+					src_phy,
+					count,
+					i,
+					dst_phy + i - src_phy);
+
+		if (node_child(node) && (node >= next_updated_parent)) {
+			next_updated_parent =  node +
+				node_update_parent(qdev, tmp_nodes,
+						src_phy, count,
+						i, dst_phy + i - src_phy,
+						count - (i - src_phy));
+			updated_parents[index++] = node->child_prop.parent_phy;
+		}
+
+		node++;
+	}
+
+	/* map all previous invalidated ids to their new phy */
+	for (i = dst_phy; i < dst_phy + count; ++i) {
+		tmp = tmp_map[i - dst_phy];
+		QOS_ASSERT(QOS_ID_VALID(tmp), "Invalid id %u on phy %u\n",
+				tmp, i);
+		map_id_phy(map, tmp, i);
+	}
+
+	memcpy(qdev->nodes->nodes + src_phy, tmp_nodes,
+	       sizeof(struct qos_node) * count);
+
+	for (i = 0; i < index; ++i)
+		create_parent_change_cmd(qdev, updated_parents[i]);
+}
+
+/*
+ * For small arrays this is probably quicker than
+ * the heap sort kernel is using
+ */
+static unsigned int remove_duplicates(unsigned int *data, unsigned int size)
+{
+	unsigned int i;
+	unsigned int j;
+
+	QOS_ASSERT(size <= 2 * MAX_MOVING_NODES,
+			"This function intends for array with size up to %d\n",
+			2 * MAX_MOVING_NODES);
+
+	for (i = 0; i < size; ++i) {
+		for (j = i + 1; j < size; ++j) {
+			if (data[i] == data[j]) {
+				--size;
+				data[j] = data[size];
+				--j;
+			}
+		}
+	}
+
+	return size;
+}
+
+/*
+ * Given a group A of count nodes starting at start, find a group B which
+ * satisfies:
+ *
+ * - Each element in B is a direct parent of an element from A
+ * - B has no duplicates
+ * - If (external)
+ *	- Each element in B does not belong to A
+ *	- Every element on A is a decendant of some element on B
+ *   Else
+ *	- Each element in B belongs to A
+ *	- The parent of each element is B does not belong to A
+ *	- Every element on A is either on B or has ancestor on B
+ *
+ *   B is stored on data and the function returns the size of B
+ *
+ *   Return number of elements on B
+ */
+static unsigned int parents_span(struct pp_nodes *nodes, unsigned int external,
+		unsigned int start, unsigned int count,
+		unsigned int *data, unsigned int size)
+{
+	unsigned int i;
+	unsigned int parent_phy;
+	unsigned int cur_phy;
+	const struct qos_node *cur;
+
+	QOS_ASSERT(size >= count,
+			"Array size %u is smaller than number of moved nodes %u\n",
+			size, count);
+	i = 0;
+	cur = get_const_node_from_phy(nodes, start);
+	for (cur_phy = start; cur_phy < start + count; ++cur_phy) {
+		QOS_ASSERT((cur->type == TYPE_UNKNOWN) ||
+				node_child(cur), "Node is not a child\n");
+		parent_phy = cur->child_prop.parent_phy;
+		/* parent not moving */
+		if ((parent_phy < start) ||
+				(parent_phy >= start + count)) {
+			if (external)
+				data[i] = parent_phy;
+			else
+				data[i] = cur_phy;
+			++i;
+		}
+		++cur;
+	}
+
+	return remove_duplicates(data, i);
+}
+
+
+/*
+ * If the src and dst region don't overlap then we can use
+ * either strategy of "move last src first" or "move first src first"
+ * If they overlap and the move is forward (src < dst) we will use
+ * "move last src first".
+ * If they overlap and the move is backward (src > dst) we will use
+ * "move last src first"
+ */
+static void create_multiple_move_cmds(struct pp_qos_dev *qdev,
+		unsigned int dst_phy,
+		unsigned int src_phy,
+		unsigned int count)
+{
+	unsigned int dst_port;
+	unsigned int i;
+	unsigned int dst;
+	unsigned int src;
+	unsigned int delta;
+
+	if (src_phy < dst_phy) {
+		src = src_phy + count - 1;
+		dst = dst_phy + count - 1;
+		delta = ~0;
+	} else {
+		src = src_phy;
+		dst = dst_phy;
+		delta = 1;
+	}
+
+	for (i = 0; i < count; ++i) {
+		dst_port = get_port(qdev->nodes, dst);
+		create_move_cmd(qdev, dst, src, dst_port);
+		src += delta;
+		dst += delta;
+	}
+}
+
+/**
+ * nodes_move() - Move nodes
+ *
+ * Implement the core functionality of moving nodes to different phy.
+ * This involves updating the stake holders nodes e.g. parents and children
+ * of the moved nodes, and updating the id<->phy mapping
+ *
+ * @qdev:
+ * @dst_phy: Dest phy of first node
+ * @src_phy: Source phy of first node
+ * @count: Number of nodes to move
+ *
+ * Notes:
+ * - All src nodes must be used, all dst nodes which don't overlap src
+ *   nodes must be unused
+ * - Number of moving nodes should be smaller than 8
+ * - If not all children of parent are moving then the move
+ *   must be within octet boundries - e.g. a shift
+ *
+ */
+STATIC_UNLESS_TEST void nodes_move(struct pp_qos_dev *qdev,
+		unsigned int dst_phy,
+		unsigned int src_phy,
+		unsigned int count)
+{
+	unsigned int i;
+	unsigned int j;
+#if 0
+	unsigned int suspend_size;
+	unsigned int suspend[MAX_MOVING_NODES];
+#endif
+	unsigned int ances_size;
+	unsigned int ancestors[MAX_MOVING_NODES];
+	unsigned int ports[MAX_MOVING_NODES * 2];
+#if 0
+	const struct qos_node *node;
+#endif
+
+	QOS_ASSERT(src_phy != dst_phy,
+			"src and dst are the same %u\n",
+			src_phy);
+	QOS_ASSERT(count < MAX_MOVING_NODES, "Can't move %u nodes, max is %u\n",
+			count, MAX_MOVING_NODES);
+	if (count) {
+	#if 0
+		suspend_size = parents_span(qdev->nodes, 1,
+				src_phy, count, suspend, MAX_MOVING_NODES);
+		for (i = 0; i < suspend_size; ++i)
+			create_suspend_cmd(qdev, suspend[i]);
+	#endif
+
+		j = 0;
+		for (i = src_phy; i < src_phy + count; ++i) {
+			ports[j] = get_port(qdev->nodes, i);
+			j++;
+		}
+
+		nodes_update_stake_holders(qdev, src_phy, dst_phy, count);
+		nodes_modify_used_status(qdev, src_phy, count, 0);
+		QOS_ASSERT(is_all_nodes_used_status(qdev, dst_phy, count, 0),
+			   "Some of the destination nodes are in used\n");
+		memmove(get_node_from_phy(qdev->nodes, dst_phy),
+				get_node_from_phy(qdev->nodes, src_phy),
+				count * sizeof(struct qos_node));
+		nodes_modify_used_status(qdev, dst_phy, count, 1);
+
+		for (i = dst_phy; i < dst_phy + count; ++i) {
+			ports[j] = get_port(qdev->nodes, i);
+			j++;
+		}
+		QOS_ASSERT(j <= 2 * MAX_MOVING_NODES, "Suspended ports buffer was filled\n");
+		i = remove_duplicates(ports, j);
+		for (j = 0; j < i; ++j)
+			add_suspend_port(qdev, ports[j]);
+
+		create_multiple_move_cmds(qdev, dst_phy, src_phy, count);
+
+#if 0
+		/* FW will delete the src node on a move command */
+		/* Issue delete node cmd to each node */
+		node = get_const_node_from_phy(qdev->nodes, src_phy);
+		for (i = 0; i < count; ++i) {
+			if (!node_used(node))
+				create_remove_node_cmd(qdev,
+						TYPE_UNKNOWN,
+						i + src_phy, 0);
+			++node;
+		}
+#endif
+
+		/* Issue update preds each node whose ancestor moved */
+		ances_size = parents_span(qdev->nodes, 0, dst_phy, count,
+				ancestors, MAX_MOVING_NODES);
+		for (i = 0; i < ances_size; ++i)
+			tree_update_predecessors(qdev, ancestors[i]);
+	#if 0
+		for (i = 0; i < suspend_size; ++i)
+			create_resume_cmd(qdev, suspend[i]);
+	#endif
+	}
+}
+
+/******************************************************************************/
+/*                                 Generic                                    */
+/******************************************************************************/
+
+struct qos_node *octet_get_min_sibling_group(
+		const struct pp_qos_dev *qdev,
+		unsigned int octet,
+		const struct qos_node *special_parent,
+		unsigned int *num_of_children)
+{
+	unsigned int min_size;
+	unsigned int phy;
+	unsigned int last_phy;
+	unsigned int num_children;
+	const struct qos_node *cur;
+	struct qos_node *cur_parent;
+	struct qos_node *min_parent;
+
+	min_size = 9;
+	min_parent = NULL;
+
+	phy = octet * 8;
+	last_phy = phy + octet_get_use_count(qdev->octets, octet);
+	while (phy < last_phy) {
+		cur = get_const_node_from_phy(qdev->nodes, phy);
+		cur_parent = get_node_from_phy(qdev->nodes,
+				cur->child_prop.parent_phy);
+		num_children = cur_parent->parent_prop.num_of_children;
+		if ((cur_parent != special_parent) &&
+				(num_children < min_size)) {
+			min_size = num_children;
+			min_parent = cur_parent;
+		} else if ((cur_parent == special_parent) &&
+				(num_children + 1 < min_size)) {
+			min_size = num_children + 1;
+			min_parent = cur_parent;
+		}
+		phy += cur_parent->parent_prop.num_of_children;
+	}
+	*num_of_children = min_size;
+	return min_parent;
+}
+
+static void update_octet_usage(struct pp_octets *octets, unsigned int phy,
+		unsigned int delta, unsigned int status)
+{
+	unsigned int octet;
+	unsigned int usage;
+
+	octet = octet_of_phy(phy);
+	usage = octet_get_use_count(octets, octet);
+	if (status == 0)
+		usage -= delta;
+	else
+		usage += delta;
+	octet_set_use_count(octets, octet, usage);
+}
+
+/*
+ * Set used status of count nodes starting from first_phy to status
+ * Verify that current used status is different from the value to be set
+ */
+void nodes_modify_used_status(struct pp_qos_dev *qdev, unsigned int first_phy,
+		unsigned int count, unsigned int status)
+{
+	struct qos_node *cur;
+	unsigned int phy;
+	unsigned int delta;
+
+	QOS_ASSERT(is_all_nodes_used_status(qdev, first_phy, count, !status),
+		   "some nodes used status is already %u\n", status);
+	delta = 0;
+	for (phy = first_phy; phy < first_phy + count; ++phy) {
+		cur = get_node_from_phy(qdev->nodes, phy);
+		QOS_BITS_TOGGLE(cur->flags, QOS_NODE_FLAGS_USED);
+		++delta;
+		if (octet_phy_offset(phy) == 7) {
+			update_octet_usage(qdev->octets, phy, delta, status);
+			delta = 0;
+		}
+	}
+
+	if (delta)
+		update_octet_usage(qdev->octets, phy - 1, delta, status);
+}
+
+
+/**
+ * octet_nodes_shift() - Shift nodes whithin octet
+ * @qdev:
+ * @first_phy: Phy of first node to be shifted
+ * @count: Number of nodes to shift
+ * @shift: Number of places to shift Negative indicates shift left,
+ *	   positive shift right
+ */
+STATIC_UNLESS_TEST void octet_nodes_shift(struct pp_qos_dev *qdev,
+		unsigned int first_phy,
+		unsigned int count,
+		int shift)
+{
+	if (count != 0 && shift != 0) {
+		QOS_ASSERT(same_octet(first_phy, first_phy + count - 1) &&
+			   same_octet(first_phy, first_phy + shift) &&
+			   same_octet(first_phy, first_phy + shift + count - 1)
+			   ,
+			   "shift %d places of %u nodes from %u crosses octet boundaries\n",
+			   shift, count, first_phy);
+		nodes_move(qdev, first_phy + shift, first_phy, count);
+	}
+}
+
+/*
+ *
+ */
+void node_phy_delete(struct pp_qos_dev *qdev, unsigned int phy)
+{
+	unsigned int usage;
+	unsigned int reminder;
+	struct qos_node *node;
+	struct qos_node *parent;
+	unsigned int id;
+	unsigned int parent_phy;
+	unsigned int update_bw;
+
+	update_bw = 0;
+	usage = octet_get_use_count(qdev->octets, octet_of_phy(phy));
+	node = get_node_from_phy(qdev->nodes, phy);
+
+	QOS_ASSERT(!node_parent(node) || node->parent_prop.num_of_children == 0,
+			"Try to delete node %u that has children\n", phy);
+
+	if (node_child(node)) {
+		parent_phy = node->child_prop.parent_phy;
+		parent = get_node_from_phy(qdev->nodes, parent_phy);
+		if (node->child_prop.virt_bw_share && node_internal(parent)) {
+			update_bw = 1;
+			id = get_id_from_phy(qdev->mapping, parent_phy);
+		}
+
+		/*
+		 * If this is the last child of a parent than shift
+		 * operation will not update the parent, since its children
+		 * are not moving - we have to update it manually
+		 */
+		if (phy == parent->parent_prop.first_child_phy +
+				parent->parent_prop.num_of_children - 1) {
+			--parent->parent_prop.num_of_children;
+			create_parent_change_cmd(qdev, parent_phy);
+		}
+	}
+	nodes_modify_used_status(qdev, phy, 1, 0);
+	reminder = usage - (octet_phy_offset(phy) + 1);
+	if (octet_of_phy(phy) > octet_of_phy(qdev->max_port))
+		octet_nodes_shift(qdev, phy + 1, reminder, -1);
+
+	if (update_bw) {
+		phy = get_phy_from_id(qdev->mapping, id);
+		parent = get_node_from_phy(qdev->nodes, phy);
+		update_internal_bandwidth(qdev, parent);
+	}
+}
+
+void release_rlm(struct pp_pool *rlms, unsigned int rlm)
+{
+	pp_pool_put(rlms, rlm);
+}
+
+static void release_node_id(struct pp_qos_dev *qdev, unsigned int phy)
+{
+	unsigned int id;
+
+	id = get_id_from_phy(qdev->mapping, phy);
+	QOS_LOG_DEBUG("Deleting id %u phy %u\n", id, phy);
+	QOS_ASSERT(QOS_ID_VALID(id), "Invalid id for phy %u\n", phy);
+	map_invalidate_id(qdev->mapping, id);
+	pp_pool_put(qdev->ids, id);
+}
+
+int node_remove(struct pp_qos_dev *qdev, struct qos_node *node)
+{
+	unsigned int phy;
+	enum node_type type;
+	unsigned int rlm;
+
+	phy = get_phy_from_node(qdev->nodes, node);
+	type = node->type;
+	QOS_ASSERT(type != TYPE_UNKNOWN, "Unknown node type of phy %u\n",
+			phy);
+
+	if (type == TYPE_QUEUE)
+		rlm  = node->data.queue.rlm;
+	else
+		rlm = 0;
+	create_remove_node_cmd(qdev, type, phy, rlm);
+	release_node_id(qdev, phy);
+	if (node_queue(node))
+		release_rlm(qdev->rlms, node->data.queue.rlm);
+
+	node_phy_delete(qdev, get_phy_from_node(qdev->nodes, node));
+	return 0;
+}
+
+int node_flush(struct pp_qos_dev *qdev, struct qos_node *node)
+{
+	unsigned int phy;
+	unsigned int id;
+	unsigned int rlm;
+
+	phy = get_phy_from_node(qdev->nodes, node);
+	id = get_id_from_phy(qdev->mapping, phy);
+	QOS_ASSERT(QOS_ID_VALID(id), "Invalid id for phy %u\n", phy);
+	if (node->type != TYPE_QUEUE) {
+		QOS_LOG_ERR("Node %u(%u) is not a queue can't flushed\n",
+				id, phy);
+		return -EINVAL;
+	}
+	rlm  = node->data.queue.rlm;
+	create_flush_queue_cmd(qdev, rlm);
+	return 0;
+}
+
+STATIC_UNLESS_TEST unsigned int get_children_bandwidth_share(
+		const struct pp_qos_dev *qdev,
+		const struct qos_node *parent);
+void update_internal_bandwidth(const struct pp_qos_dev *qdev,
+		struct qos_node *parent)
+{
+	unsigned int share;
+	const struct qos_node *child;
+	struct qos_node *tmp;
+	unsigned int cnt;
+	unsigned int phy;
+	struct qos_node *internals[10];
+	unsigned int index;
+	unsigned int parent_share;
+
+	tmp = parent;
+	index = 0;
+	do {
+		QOS_ASSERT(node_internal(tmp), "Node is not internal\n");
+		share = get_children_bandwidth_share(qdev, tmp);
+		tmp->child_prop.virt_bw_share = share;
+		internals[index++] = tmp;
+		phy = tmp->child_prop.parent_phy;
+		tmp = get_node_from_phy(qdev->nodes, phy);
+	} while (node_internal(tmp));
+
+	--index;
+	tmp = internals[index];
+	ISSUE_BW_CMD(get_phy_from_node(qdev->nodes, tmp),
+			tmp->child_prop.virt_bw_share, 1U);
+	do {
+		QOS_ASSERT(node_internal(tmp), "Node is not internal\n");
+		parent_share = tmp->child_prop.virt_bw_share;
+		//TODO remove when parent share is used and compiler
+		//does not shout
+		parent_share = parent_share;
+
+		child = get_const_node_from_phy(qdev->nodes,
+				tmp->parent_prop.first_child_phy);
+		cnt = tmp->parent_prop.num_of_children;
+		for (; cnt; --cnt) {
+			QOS_ASSERT(node_child(child), "Node is not a child\n");
+			if (child->child_prop.virt_bw_share)
+				ISSUE_BW_CMD(
+					get_phy_from_node(
+						qdev->nodes,
+						child),
+					child->child_prop.virt_bw_share *
+					10000U,
+					parent_share * 100);
+			++child;
+		}
+
+		if (index > 0) {
+			--index;
+			tmp = internals[index];
+		} else {
+			break;
+		}
+	} while (1);
+}
+
+/**
+ * link_with_parent() - Link a new node to a parent
+ * @qdev:
+ * @phy:	The phy of the node
+ * @parent_id:  Id of a parent to linked with
+ *
+ * Note node is marked as used
+ */
+static void link_with_parent(
+		struct pp_qos_dev *qdev,
+		unsigned int phy,
+		unsigned int parent_id)
+{
+	struct qos_node *node;
+	struct qos_node *parent;
+	unsigned int first;
+	unsigned int last;
+	unsigned int parent_phy;
+	unsigned int num;
+
+	parent_phy = get_phy_from_id(qdev->mapping, parent_id);
+	parent = get_node_from_phy(qdev->nodes, parent_phy);
+	node = get_node_from_phy(qdev->nodes, phy);
+	node->child_prop.parent_phy = parent_phy;
+
+	num = parent->parent_prop.num_of_children;
+	first = parent->parent_prop.first_child_phy;
+	last = first + num - 1;
+
+	if (num == 0 || phy < first)
+		first = phy;
+
+	if (num == 0 || phy > last)
+		last = phy;
+
+	parent->parent_prop.num_of_children = last - first + 1;
+	parent->parent_prop.first_child_phy = first;
+	nodes_modify_used_status(qdev, phy, 1, 1);
+	create_parent_change_cmd(qdev, parent_phy);
+
+}
+
+struct pp_qos_dev *_qos_init(unsigned int max_port)
+{
+	struct pp_qos_dev *qdev;
+	unsigned int i;
+
+	qdev = QOS_MALLOC(sizeof(struct pp_qos_dev));
+	if (qdev) {
+		memset(qdev, 0, sizeof(struct pp_qos_dev));
+		qdev->max_port = max_port;
+
+		qdev->octets = octets_init(octet_of_phy(max_port));
+		if (qdev->octets == NULL)
+			goto fail;
+
+		qdev->nodes = pp_nodes_init();
+		if (qdev->nodes == NULL)
+			goto fail;
+
+		qdev->ids = free_id_init();
+		if (qdev->ids == NULL)
+			goto fail;
+
+		qdev->rlms = free_rlm_init();
+		if (qdev->rlms == NULL)
+			goto fail;
+
+		qdev->mapping = pp_mapping_init();
+		if (qdev->mapping == NULL)
+			goto fail;
+
+		qdev->queue = pp_queue_init(1024);
+		if (qdev->queue == NULL)
+			goto fail;
+
+		qdev->drvcmds.cmdq = cmd_queue_init();
+		if (qdev->drvcmds.cmdq == NULL)
+			goto fail;
+
+		qdev->drvcmds.pendq = cmd_queue_init();
+		if (qdev->drvcmds.pendq == NULL)
+			goto fail;
+
+		for (i = 0; i <= QOS_MAX_SHARED_BANDWIDTH_GROUP; ++i)
+			qdev->groups[i].used = 0;
+		QOS_LOCK_INIT(qdev);
+	}
+	return qdev;
+fail:
+	_qos_clean(qdev);
+	return NULL;
+}
+
+void _qos_clean(struct pp_qos_dev *qdev)
+{
+	if (qdev) {
+		pp_pool_clean(qdev->portsphys);
+		clean_fwdata_internals(qdev);
+		cmd_queue_clean(qdev->drvcmds.pendq);
+		cmd_queue_clean(qdev->drvcmds.cmdq);
+		pp_queue_clean(qdev->queue);
+		pp_mapping_clean(qdev->mapping);
+		pp_pool_clean(qdev->rlms);
+		pp_pool_clean(qdev->ids);
+		pp_nodes_clean(qdev->nodes);
+		octets_clean(qdev->octets);
+		QOS_FREE(qdev);
+	}
+}
+
+
+/******************************************************************************/
+/*                           Tree traversal                                   */
+/******************************************************************************/
+/**
+ * post_order_travers_tree() - Traverse subtree post order - children first
+ * followed by parents and apply operation on each conformed node
+ *
+ * @qdev:
+ * @root:	Phy of sub tree's root node
+ * @conform:    Function to see if the node we reach is conform
+ * @cdata:	Data to supply to conform
+ * @operation:  Operation to apply upon each conformed node
+ * @odata:      Data to supply to operation
+ * Return:	Sum of the returned values on all operations applied at the
+ * subtree
+ *
+ * Note - though operation is allowed to modify qdev, it is important that it
+ * won't do operation that modifies tree topology (e.g. move/shift/delete/add
+ * node), since this will mess the iteration process
+ */
+static int post_order_travers_tree(struct pp_qos_dev *qdev,
+				    int root,
+				    int (*conform)(
+					    const struct pp_qos_dev *qdev,
+					    const struct qos_node *node,
+					    void *cdata),
+				    void *cdata,
+				    int (*operation)(
+					    struct pp_qos_dev *qdev,
+					    const struct qos_node *node,
+					    void *odata),
+				    void *odata)
+{
+	unsigned int i;
+	unsigned int phy;
+	unsigned int total;
+	struct qos_node *node;
+
+	total = 0;
+	node = get_node_from_phy(qdev->nodes, root);
+	QOS_ASSERT(node_used(node), "Unused node\n");
+
+	if (node_parent(node)) {
+		for (i = 0; i < node->parent_prop.num_of_children; ++i) {
+			phy = node->parent_prop.first_child_phy + i;
+			total += post_order_travers_tree(qdev, phy, conform,
+					cdata, operation, odata);
+		}
+	}
+	if (conform(qdev, node, cdata))
+		total += operation(qdev, node, odata);
+
+	return total;
+}
+
+struct ids_container_metadata {
+	unsigned int next;
+	uint16_t *ids;
+	unsigned int size;
+};
+
+static int update_ids_container(struct pp_qos_dev *qdev,
+		const struct qos_node *node, void *data)
+{
+	struct ids_container_metadata *ids;
+	unsigned int phy;
+	uint16_t id;
+
+	ids = (struct ids_container_metadata *)data;
+	phy = get_phy_from_node(qdev->nodes, node);
+	id = get_id_from_phy(qdev->mapping, phy);
+
+	if (ids->next < ids->size) {
+		ids->ids[ids->next] = id;
+		ids->next++;
+	}
+
+	return 1;
+}
+
+static int node_queue_wrapper(const struct pp_qos_dev *qdev,
+		const struct qos_node *node, void *data)
+{
+	return node_used(node) && (node->type == TYPE_QUEUE);
+}
+
+void get_node_queues(struct pp_qos_dev *qdev,
+		unsigned int phy, uint16_t *queue_ids,
+		unsigned int size, unsigned int *queues_num)
+{
+	struct ids_container_metadata data = {0, queue_ids, size};
+
+	if (queue_ids == NULL)
+		data.size = 0;
+	*queues_num = post_order_travers_tree(qdev, phy,
+			node_queue_wrapper, NULL,
+			update_ids_container, &data);
+}
+
+static int node_in_grp(const struct pp_qos_dev *qdev,
+		const struct qos_node *node, void *data)
+{
+	return node_used(node) &&
+		(node->shared_bandwidth_group == (uintptr_t)data);
+}
+
+void get_bw_grp_members_under_node(struct pp_qos_dev *qdev, unsigned int id,
+		unsigned int phy, uint16_t *ids,
+		unsigned int size, unsigned int *ids_num)
+{
+	struct ids_container_metadata data = {0, ids, size};
+
+	if (ids == NULL)
+		data.size = 0;
+	*ids_num = post_order_travers_tree(qdev, phy, node_in_grp,
+			(void *)(uintptr_t)id, update_ids_container, &data);
+}
+
+static int update_predecessors(struct pp_qos_dev *qdev,
+		const struct qos_node *node, void *data)
+{
+	create_update_preds_cmd(qdev, get_phy_from_node(qdev->nodes, node));
+	return 1;
+}
+
+static int node_child_wrapper(const struct pp_qos_dev *qdev,
+		const struct qos_node *node, void *data)
+{
+	return node_child(node);
+}
+
+void tree_update_predecessors(struct pp_qos_dev *qdev, unsigned int phy)
+{
+	post_order_travers_tree(qdev, phy, node_child_wrapper,
+			NULL, update_predecessors, NULL);
+}
+
+#if 0
+static int node_virtual_child_of(const struct pp_qos_dev *qdev,
+		const struct qos_node *node, void *data)
+{
+	return node_child(node) && !node_internal(node) &&
+		((uintptr_t)data == get_virtual_parent_phy(qdev->nodes, node));
+}
+
+static int add_bandwidth_share(struct pp_qos_dev *qdev,
+		const struct qos_node *node,
+		void *data)
+{
+	QOS_ASSERT(node_child(node), "Node is not a child\n");
+	return node->child_prop.virt_bw_share;
+}
+
+
+/*
+ * Return the sum of bandwidth share of all desendants
+ * whose virtual parent is parent
+ * Return 0 if parent is internal
+ */
+STATIC_UNLESS_TEST unsigned int get_virt_children_bandwidth_share(
+		const struct pp_qos_dev *qdev,
+		const struct qos_node *parent)
+{
+	int phy;
+	int total;
+	struct pp_qos_dev *_qdev;
+
+	total = 0;
+	if (node_parent(parent) && !node_internal(parent)) {
+		_qdev = (struct pp_qos_dev *)qdev;
+		phy = get_phy_from_node(qdev->nodes, parent);
+		total = post_order_travers_tree(_qdev, phy,
+				node_virtual_child_of,
+				(void *)(uintptr_t)phy,
+				add_bandwidth_share, NULL);
+	}
+	return total;
+}
+#endif
+
+/*
+ * Return the sum of bandwidth share of all direct children of parent
+ */
+STATIC_UNLESS_TEST unsigned int get_children_bandwidth_share(
+		const struct pp_qos_dev *qdev,
+		const struct qos_node *parent)
+{
+	unsigned int total;
+	unsigned int num;
+	const struct qos_node *child;
+
+	total = 0;
+	if (node_parent(parent)) {
+		num = parent->parent_prop.num_of_children;
+		if (num > 0)
+			child = get_const_node_from_phy(qdev->nodes,
+					parent->parent_prop.first_child_phy);
+		for (; num; --num) {
+			total += child->child_prop.virt_bw_share;
+			++child;
+		}
+	}
+	return total;
+}
+
+static int node_remove_wrapper(
+		struct pp_qos_dev *qdev,
+		const struct qos_node *node, void *data)
+{
+	uint16_t phy;
+	uint16_t id;
+	int rc;
+
+	phy = get_phy_from_node(qdev->nodes, node);
+	id = get_id_from_phy(qdev->mapping, phy);
+	rc = pp_queue_enqueue(qdev->queue, id);
+	QOS_ASSERT(rc == 0, "Could not enqueue\n");
+	return rc;
+}
+
+static int node_flush_wrapper(
+		struct pp_qos_dev *qdev,
+		const struct qos_node *node, void *data)
+{
+	uint16_t phy;
+	uint16_t id;
+	int rc;
+
+	phy = get_phy_from_node(qdev->nodes, node);
+	id = get_id_from_phy(qdev->mapping, phy);
+	rc = _pp_qos_queue_flush(qdev, id);
+	QOS_ASSERT(rc == 0, "Could not enqueue\n");
+	return rc;
+}
+
+static int node_modify_blocked_status(
+		struct pp_qos_dev *qdev,
+		const struct qos_node *node, void *data)
+{
+	uint16_t phy;
+	uint16_t id;
+	unsigned int status;
+	int rc;
+
+	status = (unsigned int)(uintptr_t)data;
+	phy = get_phy_from_node(qdev->nodes, node);
+	id = get_id_from_phy(qdev->mapping, phy);
+	if (status)
+		rc = _pp_qos_queue_block(qdev, id);
+	else
+		rc = _pp_qos_queue_unblock(qdev, id);
+	QOS_ASSERT(rc == 0, "Could not block queue\n");
+	return rc;
+}
+
+static int node_used_wrapper(
+		const struct pp_qos_dev *qdev,
+		const struct qos_node *node, void *data)
+{
+	return node_used(node);
+}
+
+int tree_remove(struct pp_qos_dev *qdev, unsigned int phy)
+{
+	struct qos_node *node;
+	unsigned int id;
+	unsigned int tmp;
+	int rc;
+
+	pp_queue_reset(qdev->queue);
+	rc = post_order_travers_tree(qdev, phy, node_used_wrapper, NULL,
+			node_remove_wrapper, NULL);
+	if (rc) {
+		QOS_LOG("Error while trying to delete subtree whose root is %u\n",
+				phy);
+		return -EBUSY;
+	}
+	id = pp_queue_dequeue(qdev->queue);
+	while (QOS_ID_VALID(id)) {
+		tmp = get_phy_from_id(qdev->mapping, id);
+		node =	get_node_from_phy(qdev->nodes, tmp);
+		node_remove(qdev, node);
+		id = pp_queue_dequeue(qdev->queue);
+	}
+
+	return 0;
+}
+
+int tree_flush(struct pp_qos_dev *qdev, unsigned int phy)
+{
+	int rc;
+
+	rc = post_order_travers_tree(qdev, phy, node_queue_wrapper,
+			NULL, node_flush_wrapper, NULL);
+	if (rc) {
+		QOS_LOG("Unexpected error while flush subtree root is %u\n",
+				phy);
+		return -EBUSY;
+	}
+	return 0;
+}
+
+int tree_modify_blocked_status(
+		struct pp_qos_dev *qdev,
+		unsigned int phy,
+		unsigned int status)
+{
+	int rc;
+
+	rc = post_order_travers_tree(qdev,
+			phy, node_queue_wrapper,
+			NULL, node_modify_blocked_status,
+			(void *)(uintptr_t)status);
+	if (rc) {
+		QOS_LOG_ERR("Error when change blocked status to %u on tree with root %u\n",
+				status, phy);
+		rc =  -EBUSY;
+	}
+	return rc;
+}
+
+/******************************************************************************/
+/*                           Configuration                                    */
+/******************************************************************************/
+
+#ifdef MAY_USE_IN_THE_FUTURE
+static int shared_group_defined(
+		const struct pp_qos_dev *qdev,
+		unsigned int group_id)
+{
+	QOS_ASSERT(group_id <= QOS_MAX_SHARED_BANDWIDTH_GROUP,
+			"illegal shared group %u\n",
+		   group_id);
+	return (qdev->groups[group_id].limit != QOS_NO_BANDWIDTH_LIMIT);
+}
+#endif
+
+static int common_cfg_valid(
+		const struct pp_qos_dev *qdev,
+		const struct qos_node *node)
+{
+	unsigned int shared;
+
+	if (!(node->bandwidth_limit == QOS_NO_BANDWIDTH_LIMIT ||
+				node->bandwidth_limit <=
+				QOS_MAX_BANDWIDTH_LIMIT)) {
+		QOS_LOG("Invalid bandwidth limit %u\n", node->bandwidth_limit);
+		return 0;
+	}
+
+	shared = node->shared_bandwidth_group;
+	if (!(
+				(shared == QOS_NO_SHARED_BANDWIDTH_GROUP) ||
+				(shared <= QOS_MAX_SHARED_BANDWIDTH_GROUP &&
+				qdev->groups[shared].used)
+	     )) {
+		QOS_LOG("Invalid shared bandwidth group %u\n",
+				node->shared_bandwidth_group);
+		return 0;
+	}
+	return 1;
+}
+
+/*
+ * If new node
+ *     node.child.parent.phy == virtual parent phy
+ * Else
+ *     node.child.parent.phy is either virtual parent phy (if user is changing
+ *     parent) or actual parent (if user is not changing parent)
+ */
+static int child_cfg_valid(
+		const struct pp_qos_dev *qdev,
+		const struct qos_node *node,
+		const struct qos_node *orig_node,
+		unsigned int prev_virt_parent_phy)
+{
+	const struct qos_node *parent;
+	unsigned int parent_phy;
+	unsigned int bw;
+	unsigned int cur_virt_parent_phy;
+
+	QOS_ASSERT(node_child(node), "node is not a child\n");
+	parent_phy = node->child_prop.parent_phy;
+	if (parent_phy > NUM_OF_NODES - 1) {
+		QOS_LOG("Illegal parent %u\n", parent_phy);
+		return 0;
+	}
+
+	parent =  get_const_node_from_phy(qdev->nodes, parent_phy);
+	if (!node_parent(parent)) {
+		QOS_LOG("Node's parent %u is not a parent\n", parent_phy);
+		return 0;
+	}
+
+	/* Find current virtual parent and bandwidth of its direct children */
+	if (!node_internal(parent))
+		cur_virt_parent_phy = parent_phy;
+	else
+		cur_virt_parent_phy = get_virtual_parent_phy(
+				qdev->nodes,
+				parent);
+
+	bw = get_children_bandwidth_share(qdev,
+			get_node_from_phy(qdev->nodes, cur_virt_parent_phy));
+
+	/*
+	 * If child does not changing its virtual parent than need
+	 * to subtract its bandwidth share, so it will not calculated twice
+	 */
+	if (cur_virt_parent_phy == prev_virt_parent_phy)
+		bw -= node->child_prop.virt_bw_share;
+
+	if (bw + node->child_prop.virt_bw_share  > 100) {
+		QOS_LOG_ERR("Parent already gave %u bandwidth, can't add %u\n",
+			bw, node->child_prop.virt_bw_share);
+		return 0;
+	}
+
+	return 1;
+}
+
+static int parent_cfg_valid(
+		const struct pp_qos_dev *qdev,
+		const struct qos_node *node)
+{
+	unsigned int first;
+	unsigned int num;
+
+	QOS_ASSERT(node_parent(node), "node is not a parent\n");
+	if (node->parent_prop.num_of_children > 8) {
+		QOS_LOG_ERR("node has %u children but max allowed is 8\n",
+				node->parent_prop.num_of_children);
+		return 0;
+	}
+	first = node->parent_prop.first_child_phy;
+	num = node->parent_prop.num_of_children;
+	if ((num > 0) &&
+	    (first <= qdev->max_port || first > NUM_OF_NODES - 1)) {
+		QOS_LOG_ERR("node has %u children but first child %u is illegal\n",
+			node->parent_prop.num_of_children, first);
+		return 0;
+	}
+
+	return 1;
+}
+
+int node_cfg_valid(
+		const struct pp_qos_dev *qdev,
+		const struct qos_node *node,
+		const struct qos_node *orig_node,
+		unsigned int prev_virt_parent_phy)
+{
+	if (!common_cfg_valid(qdev, node))
+		return 0;
+	if (node_parent(node) && !parent_cfg_valid(qdev, node))
+		return 0;
+	if (node_child(node) &&
+			!child_cfg_valid(qdev, node,
+				orig_node, prev_virt_parent_phy))
+		return 0;
+	return 1;
+}
+
+unsigned int get_virtual_parent_phy(
+		const struct pp_nodes *nodes,
+		const struct qos_node *child)
+{
+	const struct qos_node *tmp;
+	unsigned int phy;
+
+	tmp = child;
+	do {
+		QOS_ASSERT(node_child(tmp), "Node is not a child\n");
+		phy = tmp->child_prop.parent_phy;
+		tmp = get_const_node_from_phy(nodes, phy);
+	} while (node_internal(tmp));
+	return phy;
+}
+
+int get_node_prop(const struct pp_qos_dev *qdev,
+		  const struct qos_node *node,
+		  struct pp_qos_common_node_properties *common,
+		  struct pp_qos_parent_node_properties *parent,
+		  struct pp_qos_child_node_properties *child)
+{
+
+	QOS_ASSERT(node_used(node), "Node is not used\n");
+
+	if (common) {
+		common->bandwidth_limit = node->bandwidth_limit;
+		common->shared_bandwidth_group = node->shared_bandwidth_group;
+	}
+
+	if (parent) {
+		parent->arbitration = node->parent_prop.arbitration;
+		parent->best_effort_enable =
+			!!QOS_BITS_IS_SET(node->flags,
+					QOS_NODE_FLAGS_PARENT_BEST_EFFORT_ENABLE
+					);
+	}
+
+	if (child) {
+		/*
+		 * Internal schedulers are tranparent to clients. Clients see
+		 * only virtual parent - the first parent in hierarchy which is
+		 * not internal scheduler
+		 */
+		child->parent = get_id_from_phy(qdev->mapping,
+				get_virtual_parent_phy(qdev->nodes, node));
+		child->priority = get_child_node_order(qdev->nodes, node);
+		child->bandwidth_share = node->child_prop.virt_bw_share;
+	}
+
+	return 0;
+}
+
+static int set_common(struct pp_qos_dev *qdev,
+		      struct qos_node *node,
+		      const struct pp_qos_common_node_properties *common,
+		      uint32_t *modified)
+{
+	unsigned int grp_id;
+
+	if (node->bandwidth_limit != common->bandwidth_limit) {
+		node->bandwidth_limit = common->bandwidth_limit;
+		QOS_BITS_SET(*modified, QOS_MODIFIED_BANDWIDTH_LIMIT);
+	}
+
+	grp_id = common->shared_bandwidth_group;
+	if (grp_id != node->shared_bandwidth_group) {
+		node->shared_bandwidth_group = grp_id;
+		QOS_BITS_SET(*modified, QOS_MODIFIED_SHARED_GROUP_ID);
+	}
+	return 0;
+}
+
+static int set_parent(struct pp_qos_dev *qdev,
+		      struct qos_node *node,
+		      const struct pp_qos_parent_node_properties *parent,
+		      uint32_t *modified)
+{
+	if (node->parent_prop.arbitration != parent->arbitration) {
+		node->parent_prop.arbitration = parent->arbitration;
+		QOS_BITS_SET(*modified, QOS_MODIFIED_ARBITRATION);
+	}
+
+	if (!!(parent->best_effort_enable) !=
+			!!QOS_BITS_IS_SET(node->flags,
+				QOS_NODE_FLAGS_PARENT_BEST_EFFORT_ENABLE)) {
+		QOS_BITS_TOGGLE(node->flags,
+				QOS_NODE_FLAGS_PARENT_BEST_EFFORT_ENABLE);
+		QOS_BITS_SET(*modified, QOS_MODIFIED_BEST_EFFORT);
+	}
+	return 0;
+}
+
+
+static int set_child(struct pp_qos_dev *qdev,
+		     struct qos_node *node,
+		     const struct pp_qos_child_node_properties *child,
+		     uint32_t *modified)
+{
+	unsigned int conf_parent_phy;
+	unsigned int virt_parent_phy;
+
+	/* Equals to virtual parent phy since client is not aware of internal
+	 * schedulers, they are transparent to him
+	 */
+	conf_parent_phy = get_phy_from_id(qdev->mapping, child->parent);
+	virt_parent_phy = node->child_prop.parent_phy;
+
+	if (QOS_PHY_VALID(virt_parent_phy))
+		virt_parent_phy = get_virtual_parent_phy(qdev->nodes, node);
+
+	if (virt_parent_phy != conf_parent_phy) {
+		node->child_prop.parent_phy = conf_parent_phy;
+		QOS_BITS_SET(*modified, QOS_MODIFIED_PARENT);
+	}
+
+	if (node->child_prop.virt_bw_share != child->bandwidth_share) {
+		node->child_prop.virt_bw_share = child->bandwidth_share;
+		QOS_BITS_SET(*modified, QOS_MODIFIED_VIRT_BW_SHARE);
+	}
+
+	return 0;
+}
+
+int set_node_prop(struct pp_qos_dev *qdev,
+		  struct qos_node *node,
+		  const struct pp_qos_common_node_properties *common,
+		  const struct pp_qos_parent_node_properties *parent,
+		  const struct pp_qos_child_node_properties *child,
+		  uint32_t *modified)
+{
+	int rc;
+
+	if (common) {
+		rc = set_common(qdev, node, common, modified);
+		if (rc)
+			return rc;
+	}
+	if (parent) {
+		rc = set_parent(qdev, node, parent, modified);
+		if (rc)
+			return rc;
+	}
+	if (child)
+		return set_child(qdev, node, child, modified);
+
+	return 0;
+}
+
+static void node_parent_init(struct qos_node *node)
+{
+	node->parent_prop.first_child_phy = QOS_INVALID_PHY;
+	node->parent_prop.num_of_children = 0;
+	node->parent_prop.arbitration = PP_QOS_ARBITRATION_WRR;
+}
+
+static void node_child_init(struct qos_node *node)
+{
+	node->child_prop.parent_phy = QOS_INVALID_PHY;
+	node->child_prop.virt_bw_share = 0;
+}
+
+static void node_common_init(struct qos_node *node)
+{
+	node->flags = 0;
+	node->bandwidth_limit = QOS_NO_BANDWIDTH_LIMIT;
+	node->shared_bandwidth_group = QOS_NO_SHARED_BANDWIDTH_GROUP;
+}
+
+void node_init(const struct pp_qos_dev *qdev,
+	       struct qos_node *node,
+	       unsigned int common,
+	       unsigned int parent,
+	       unsigned int child)
+{
+	memset(node, 0, sizeof(struct qos_node));
+
+	node->type = TYPE_UNKNOWN;
+	if (common)
+		node_common_init(node);
+
+	if (parent)
+		node_parent_init(node);
+
+	if (child)
+		node_child_init(node);
+}
+
+
+/******************************************************************************/
+/*                              ALLOC PHY                                     */
+/******************************************************************************/
+
+
+/**
+ * Return phy where a child whose priority is child_priority should be placed.
+ * If parent is WRR than priority is ignored and the new child will
+ * be given a phy after the current last child
+ *
+ * Assumption: parent's children are held on a non full octet
+ */
+static unsigned int calculate_new_child_location(
+		struct pp_qos_dev *qdev,
+		struct qos_node *parent,
+		unsigned int child_priority)
+{
+	unsigned int phy;
+
+	QOS_ASSERT(node_parent(parent), "node is not a parent\n");
+	if (parent->parent_prop.arbitration == PP_QOS_ARBITRATION_WRR)
+		phy = parent->parent_prop.first_child_phy +
+			parent->parent_prop.num_of_children;
+	else
+		phy = parent->parent_prop.first_child_phy +
+			min(
+					child_priority,
+					(unsigned int)parent->
+					parent_prop.num_of_children);
+	return phy;
+}
+
+/**
+ * children_on_non_full_octet() - Allocate a phy for a new child whose siblings
+ * are held on a non full octet.
+ * @qdev:
+ * @parent:
+ * @usage:	    Number of used nodes on children's octet
+ * @child_priority: New child priority (relevant only for WSP parent)
+ * Return:	    New allocated phy
+ */
+static unsigned int children_on_non_full_octet(
+		struct pp_qos_dev *qdev,
+		struct qos_node *parent,
+		unsigned int usage,
+		unsigned int child_priority)
+{
+	unsigned int phy;
+
+	phy = calculate_new_child_location(qdev, parent, child_priority);
+	octet_nodes_shift(qdev, phy, usage - octet_phy_offset(phy), 1);
+	return phy;
+}
+
+static unsigned int children_on_non_full_octet_wrapper(
+		struct pp_qos_dev *qdev,
+		struct qos_node *parent,
+		unsigned int child_priority)
+{
+	unsigned int usage;
+	unsigned int octet;
+
+	QOS_ASSERT(node_parent(parent),
+			"Node %u is not a parent\n",
+			get_phy_from_node(qdev->nodes, parent));
+	octet = octet_of_phy(parent->parent_prop.first_child_phy);
+	usage = octet_get_use_count(qdev->octets, octet);
+	QOS_ASSERT(usage < 8, "Octet is full\n");
+	return children_on_non_full_octet(qdev, parent, usage, child_priority);
+}
+
+/**
+ * has_less_than_8_children_on_full_octet() - Allocate a phy for a new child
+ * are held on a full octet.
+ * @qdev:
+ * @parent:
+ * @children_octet:
+ * @child_priority:
+ * Return: New allocated phy
+ */
+static unsigned int has_less_than_8_children_on_full_octet(
+		struct pp_qos_dev *qdev,
+		struct qos_node *parent,
+		uint8_t children_octet,
+		unsigned int child_priority)
+{
+	struct qos_node *min_parent;
+	unsigned int octet;
+	unsigned int phy;
+	unsigned int num_of_required_entries;
+	unsigned int num_of_nodes_to_move;
+	unsigned int dst_phy;
+	unsigned int src_phy;
+	unsigned int parent_id;
+
+	parent_id = get_id_from_phy(qdev->mapping,
+			get_phy_from_node(qdev->nodes, parent));
+	QOS_ASSERT(octet_get_use_count(qdev->octets, children_octet) == 8,
+			"Octet %d is not full\n", children_octet);
+	min_parent =  octet_get_min_sibling_group(qdev, children_octet,
+			parent, &num_of_required_entries);
+	QOS_ASSERT(min_parent != NULL, "Can't find min_parent for octet %d\n",
+			children_octet);
+	octet = octet_get_with_at_least_free_entries(
+			qdev->octets,
+			num_of_required_entries);
+	if (!QOS_OCTET_VALID(octet)) {
+		QOS_LOG("could not find free octet\n");
+		return QOS_INVALID_PHY;
+	}
+
+	if (parent == min_parent)
+		num_of_nodes_to_move = num_of_required_entries - 1;
+	else
+		num_of_nodes_to_move = num_of_required_entries;
+
+	/* move children of min_parent to new octet */
+	dst_phy = 8 * octet + octet_get_use_count(qdev->octets, octet);
+	src_phy =  min_parent->parent_prop.first_child_phy;
+	nodes_move(qdev, dst_phy, src_phy, num_of_nodes_to_move);
+
+
+	/*
+	 * shift original octet if necessary i.e. if last moved
+	 * node was not last node on octet
+	 */
+	src_phy += num_of_nodes_to_move;
+	if (octet_phy_offset(src_phy - 1) != 7)
+		octet_nodes_shift(qdev, src_phy,
+				8 - octet_phy_offset(src_phy),
+				-num_of_nodes_to_move);
+
+	//min_parent->parent_prop.first_child_phy = dst_phy;
+
+	parent = get_node_from_phy(qdev->nodes,
+			get_phy_from_id(qdev->mapping, parent_id));
+	phy = children_on_non_full_octet_wrapper(qdev, parent, child_priority);
+	return phy;
+}
+
+/**
+ * phy_alloc_parent_has_less_than_8_children() - Allocate a phy
+ * for a new child whose parent has less than 8 children
+ * @qdev:
+ * @parent:
+ * @child_priority:
+ * Return:
+ */
+static unsigned int phy_alloc_parent_has_less_than_8_children(
+		struct pp_qos_dev *qdev,
+		struct qos_node *parent,
+		unsigned int child_priority)
+{
+	unsigned int phy;
+	unsigned int octet;
+	unsigned int usage;
+	unsigned int parent_id;
+
+	QOS_ASSERT(node_parent(parent) &&
+			(parent->parent_prop.num_of_children < 8),
+		   "Node %u is not a parent with less than 8 children\n",
+		   get_phy_from_node(qdev->nodes, parent));
+
+	parent_id = get_id_from_phy(qdev->mapping,
+			get_phy_from_node(qdev->nodes, parent));
+	if (parent->parent_prop.num_of_children == 0) {
+		octet = octet_get_with_at_least_free_entries(qdev->octets, 1);
+		if (!QOS_OCTET_VALID(octet)) {
+			QOS_LOG("could not find free octet\n");
+			return QOS_INVALID_PHY;
+		}
+		phy = octet * 8 + octet_get_use_count(qdev->octets, octet);
+	} else {
+		octet = octet_of_phy(parent->parent_prop.first_child_phy);
+		usage = octet_get_use_count(qdev->octets, octet);
+		if (usage < 8)
+			phy = children_on_non_full_octet(
+					qdev,
+					parent,
+					usage,
+					child_priority);
+		else
+			phy = has_less_than_8_children_on_full_octet(
+					qdev,
+					parent,
+					octet,
+					child_priority);
+	}
+	if (QOS_PHY_VALID(phy))
+		link_with_parent(qdev, phy, parent_id);
+	return phy;
+}
+
+/**
+ * create_internal_scheduler_on_node() - Create internal scheduler
+ * The original node is moved to another octet (which has at least 2 free slots
+ * and can accommodate the original node and a new child) and becomes a child of
+ * the created internal scheduler
+ * @qdev:
+ * @node: The node where the internal scheduler should be placed
+ * Return:
+ */
+static unsigned int create_internal_scheduler_on_node(
+		struct pp_qos_dev *qdev,
+		struct qos_node *node)
+{
+	struct qos_node *new_node;
+	unsigned int octet;
+	unsigned int new_phy;
+	unsigned int phy;
+	unsigned int id;
+	uint32_t modified;
+	struct pp_qos_sched_conf conf;
+
+	octet = octet_get_with_at_least_free_entries(qdev->octets, 2);
+	if (!QOS_OCTET_VALID(octet)) {
+		QOS_LOG("could not find free octet\n");
+		return  QOS_INVALID_PHY;
+	}
+
+	/*
+	 * The assertion on nodes_move especially the one that states
+	 * that if a child is moved to another octet than its siblings must
+	 * move also, prevents using move in this clone scenario
+	 */
+
+	/* find a place for a new node */
+	new_phy = octet * 8 + octet_get_use_count(qdev->octets, octet);
+	QOS_ASSERT(QOS_PHY_VALID(new_phy + 1),
+			"%u is not a valid phy\n", new_phy + 1);
+	new_node = get_node_from_phy(qdev->nodes, new_phy);
+	QOS_ASSERT(!node_used(new_node), "Node is used\n");
+	nodes_modify_used_status(qdev, new_phy, 1, 1);
+
+	/* update children to point to new node */
+	phy = get_phy_from_node(qdev->nodes, node);
+	id = get_id_from_phy(qdev->mapping, phy);
+	map_id_phy(qdev->mapping, id, new_phy);
+	if (node_parent(node))
+		node_update_children(qdev,
+				get_phy_from_node(qdev->nodes, node),
+				new_phy);
+	memcpy(new_node, node, sizeof(struct qos_node));
+	new_node->child_prop.parent_phy = phy;
+	create_move_cmd(qdev, new_phy, phy, get_port(qdev->nodes, new_phy));
+
+	/* virtual bw share and parent_phy remain */
+	node->flags = 0;
+	QOS_BITS_SET(node->flags,
+			QOS_NODE_FLAGS_USED | QOS_NODE_FLAGS_INTERNAL);
+	node->type = TYPE_SCHED;
+	node->parent_prop.arbitration = PP_QOS_ARBITRATION_WRR;
+	node->parent_prop.num_of_children = 1;
+	node->parent_prop.first_child_phy = new_phy;
+	id = pp_pool_get(qdev->ids);
+	QOS_ASSERT(QOS_ID_VALID(id), "Got invalid id\n");
+	map_id_phy(qdev->mapping, id, phy);
+
+	get_node_prop(qdev, node, &conf.common_prop,
+			&conf.sched_parent_prop,
+			&conf.sched_child_prop);
+	modified = 0;
+	QOS_BITS_SET(modified,
+			QOS_MODIFIED_NODE_TYPE |
+			QOS_MODIFIED_BANDWIDTH_LIMIT |
+			QOS_MODIFIED_SHARED_GROUP_ID |
+			QOS_MODIFIED_VIRT_BW_SHARE |
+			QOS_MODIFIED_PARENT | QOS_MODIFIED_ARBITRATION |
+			QOS_MODIFIED_BEST_EFFORT);
+
+	create_set_sched_cmd(qdev, &conf, phy,
+			node->child_prop.parent_phy, modified);
+
+	/* The new node will be put right after the moved node */
+	++new_phy;
+	link_with_parent(qdev, new_phy, id);
+
+	tree_update_predecessors(qdev, phy);
+
+	return new_phy;
+}
+
+/**
+ * phy_alloc_for_new_child_parent_has_8_children() - Allocate a phy for a new
+ * child whose parent has 8 children
+ * @qdev:
+ * @parent:
+ * Return:
+ */
+static unsigned int phy_alloc_for_new_child_parent_has_8_children(
+		struct pp_qos_dev *qdev,
+		struct qos_node *parent)
+{
+	unsigned int first_phy;
+	struct qos_node *child;
+	struct qos_node *last_child;
+	unsigned int phy;
+	unsigned int parent_phy;
+	unsigned int i;
+
+	phy = QOS_INVALID_PHY;
+
+	QOS_ASSERT(node_parent(parent) &&
+		   (parent->parent_prop.num_of_children == 8) &&
+		   (parent->parent_prop.arbitration == PP_QOS_ARBITRATION_WRR),
+		   "Node %u is not a WRR parent with 8 children\n",
+		   get_phy_from_node(qdev->nodes, parent));
+
+	first_phy = parent->parent_prop.first_child_phy;
+	child = get_node_from_phy(qdev->nodes, first_phy);
+	last_child = child + 7;
+	for (; child <= last_child; ++child) {
+		if (node_internal(child) &&
+				(child->parent_prop.num_of_children < 8)) {
+			return
+				phy_alloc_parent_has_less_than_8_children(
+					qdev,
+					child,
+					QOS_INVALID_PRIORITY);
+		}
+	}
+
+	child = get_node_from_phy(qdev->nodes, first_phy);
+	while (child < last_child && node_internal(child))
+		++child;
+	if (!node_internal(child))
+		return create_internal_scheduler_on_node(qdev, child);
+
+	/*
+	 * If we reach this point all children are full internal schedulers
+	 * We will do a breadth first traversal on the tree i.e. look for a
+	 * place for the child under the internal schedulers children of the
+	 * parent. For this we will need a queue
+	 */
+	for (i = 0; i < 8; ++i) {
+		if (pp_queue_enqueue(qdev->queue, first_phy + i)) {
+			QOS_LOG("Queue is full\n");
+			return QOS_INVALID_PHY;
+		}
+	}
+
+	parent_phy = pp_queue_dequeue(qdev->queue);
+	while (QOS_PHY_VALID(parent_phy)) {
+		phy = phy_alloc_for_new_child_parent_has_8_children(
+				qdev,
+				get_node_from_phy(qdev->nodes, parent_phy));
+		if (QOS_PHY_VALID(phy))
+			return phy;
+	}
+
+	return phy;
+}
+
+unsigned int phy_alloc_by_parent(
+		struct pp_qos_dev *qdev,
+		struct qos_node *parent,
+		unsigned int child_priority)
+{
+	unsigned int phy;
+
+	if (parent->parent_prop.num_of_children < 8) {
+		phy =  phy_alloc_parent_has_less_than_8_children(
+				qdev,
+				parent,
+				child_priority);
+	} else if (parent->parent_prop.arbitration == PP_QOS_ARBITRATION_WSP) {
+		QOS_LOG_ERR("WSP parent %u already has 8 children\n",
+				get_phy_from_node(qdev->nodes, parent));
+		phy = QOS_INVALID_PHY;
+	} else {
+		pp_queue_reset(qdev->queue);
+		phy =  phy_alloc_for_new_child_parent_has_8_children(qdev,
+				parent);
+	}
+	return phy;
+}
+
+int check_sync_with_fw(struct pp_qos_dev *qdev)
+{
+	unsigned int i;
+	unsigned int used;
+	unsigned int res;
+	unsigned int id;
+	int rc;
+	const struct qos_node *node;
+	struct pp_pool *pool;
+	struct pp_qos_node_info info;
+
+	rc = 0;
+	res = 0;
+	pool = pp_pool_init(NUM_OF_NODES, QOS_INVALID_ID);
+	if (pool == NULL) {
+		QOS_LOG_ERR("Can't create pool for firmware sync check\n");
+		return -1;
+	}
+
+	used = 0;
+	node = get_const_node_from_phy(qdev->nodes, 0);
+	for (i = 0; i < NUM_OF_NODES; ++i) {
+		if (node_used(node)) {
+			id = get_id_from_phy(qdev->mapping, i);
+			QOS_ASSERT(QOS_ID_VALID(id),
+					"Invalid id for phy %u\n", i);
+			pp_pool_put(pool, id);
+			++used;
+		}
+		++node;
+	}
+
+	create_num_used_nodes_cmd(qdev, qdev->hwconf.fw_stat, &res);
+	update_cmd_id(&qdev->drvcmds);
+	transmit_cmds(qdev);
+	QOS_ASSERT(
+			res == used,
+			"Driver's DB has %u used nodes, while firmware reports %u\n",
+			used,
+			res);
+
+	id = pp_pool_get(pool);
+	while (QOS_ID_VALID(id)) {
+		if (pp_qos_get_node_info(qdev, id, &info)) {
+			QOS_LOG_ERR("Error info on %u\n", id);
+			rc = -1;
+			break;
+		}
+		id = pp_pool_get(pool);
+	}
+
+	pp_pool_clean(pool);
+	return rc;
+}
+
+/* Return 1 if device is not in assert and was initialized */
+int qos_device_ready(const struct pp_qos_dev *qdev)
+{
+	if (PP_QOS_DEVICE_IS_ASSERT(qdev)) {
+		QOS_LOG_CRIT("!!!!! Qos driver in unstable mode !!!!!\n");
+		return 0;
+	}
+
+	if (!qdev->initialized) {
+		QOS_LOG_ERR("Device was not initialized\n");
+		return 0;
+	}
+
+	return 1;
+}
+
+void update_children_position(
+			struct pp_qos_dev *qdev,
+			struct qos_node *child,
+			struct qos_node *parent,
+			unsigned int position,
+			struct qos_node *node_src)
+{
+	unsigned int old_phy;
+	unsigned int new_phy;
+	unsigned int id;
+	unsigned int dst_port;
+	struct qos_node *node;
+	unsigned int first;
+	unsigned int cnt;
+
+	QOS_ASSERT(parent->parent_prop.arbitration == PP_QOS_ARBITRATION_WSP,
+			"Parent is not wsp\n");
+
+	old_phy = get_phy_from_node(qdev->nodes, child);
+	new_phy = parent->parent_prop.first_child_phy + position;
+
+	if (new_phy == old_phy)
+		return;
+
+	first = parent->parent_prop.first_child_phy;
+	cnt = parent->parent_prop.num_of_children;
+	dst_port = get_port(qdev->nodes, old_phy);
+	nodes_modify_used_status(qdev, old_phy, 1, 0);
+	create_move_cmd(qdev, PP_QOS_TMP_NODE, old_phy, dst_port);
+
+	id = get_id_from_phy(qdev->mapping, old_phy);
+	if (new_phy > old_phy)
+		octet_nodes_shift(qdev, old_phy + 1, new_phy - old_phy, -1);
+	else if (new_phy < old_phy)
+		octet_nodes_shift(qdev, new_phy, old_phy - new_phy, 1);
+
+	create_move_cmd(qdev, new_phy, PP_QOS_TMP_NODE, dst_port);
+
+	map_id_phy(qdev->mapping, id, new_phy);
+	if (node_parent(child))
+		node_update_children(qdev, old_phy, new_phy);
+
+	node = get_node_from_phy(qdev->nodes, new_phy);
+	memcpy(node, node_src, sizeof(struct qos_node));
+	QOS_BITS_CLEAR(node->flags, QOS_NODE_FLAGS_USED);
+	nodes_modify_used_status(qdev, new_phy, 1, 1);
+	parent->parent_prop.first_child_phy = first;
+	parent->parent_prop.num_of_children = cnt;
+	if (node_parent(child))
+		tree_update_predecessors(qdev, new_phy);
+}
diff --git a/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_utils.h b/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_utils.h
new file mode 100644
index 000000000000..853af6afea74
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/ppv4/qos/pp_qos_utils.h
@@ -0,0 +1,684 @@
+/*
+ * GPL LICENSE SUMMARY
+ *
+ *  Copyright(c) 2017 Intel Corporation.
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of version 2 of the GNU General Public License as
+ *  published by the Free Software Foundation.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *  The full GNU General Public License is included in this distribution
+ *  in the file called LICENSE.GPL.
+ *
+ *  Contact Information:
+ *  Intel Corporation
+ *  2200 Mission College Blvd.
+ *  Santa Clara, CA  97052
+ */
+#ifndef PP_QOS_UTILS_H
+#define PP_QOS_UTILS_H
+
+#include "pp_qos_common.h"
+#include "pp_qos_drv.h"
+
+/******************************************************************************/
+/*                              CONF                                          */
+/******************************************************************************/
+#define PPV4_QOS_DRV_VER "1.0.3-fl"
+
+#define NUM_OF_NODES	2048U
+#define NUM_OF_QUEUES	256U
+#define NUM_OF_OCTETS ((NUM_OF_NODES / 8) - 1)
+#define PP_QOS_TMP_NODE 2047U
+#define QOS_MAX_SHARED_BW_GRP QOS_MAX_SHARED_BANDWIDTH_GROUP
+
+#define MAX_QOS_INSTANCES 1U
+
+/* for calculating number of ddr pages for qm from resource size */
+#define PPV4_QOS_DESC_SIZE (16U)
+#define PPV4_QOS_DESC_IN_PAGE (16U)
+
+#define PPV4_QOS_LOGGER_MSG_SIZE (128U)
+#define PPV4_QOS_LOGGER_BUF_SIZE (PPV4_QOS_LOGGER_MSG_SIZE * 4096)
+#define PPV4_QOS_STAT_SIZE	(1024U)
+
+#define PPV4_QOS_IVT_START (0x48000U)
+
+#define FIRMWARE_FILE "ppv4-qos-fw.bin"
+/******************************************************************************/
+/*                              DEFINES                                       */
+/******************************************************************************/
+
+#define QOS_INVALID_OCTET   0xFFFFU
+#define QOS_INVALID_ID      0xFFFFU
+#define QOS_INVALID_RLM     0xFFFFU
+#define QOS_INVALID_PHY     0xFFFFU
+#define QOS_UNKNOWN_PHY     0xFFFEU
+
+#define QOS_OCTET_VALID(octet)	((octet) < NUM_OF_OCTETS)
+#define QOS_ID_VALID(id)	((id) < NUM_OF_NODES)
+#define QOS_PHY_VALID(phy)	((phy) < NUM_OF_NODES)
+#define QOS_PHY_UNKNOWN(phy)	((phy) == QOS_UNKNOWN_PHY)
+#define QOS_BW_GRP_VALID(id)	((id) && ((id) <= QOS_MAX_SHARED_BW_GRP))
+
+#define QOS_MODIFIED_NODE_TYPE			BIT(0)
+#define QOS_MODIFIED_BANDWIDTH_LIMIT		BIT(2)
+#define QOS_MODIFIED_SHARED_GROUP_ID		BIT(3)
+#define QOS_MODIFIED_ARBITRATION		BIT(5)
+#define QOS_MODIFIED_BEST_EFFORT		BIT(6)
+#define QOS_MODIFIED_VIRT_BW_SHARE		BIT(8)
+#define QOS_MODIFIED_PARENT			BIT(9)
+#define QOS_MODIFIED_RING_ADDRESS		BIT(10)
+#define QOS_MODIFIED_RING_SIZE			BIT(11)
+#define QOS_MODIFIED_CREDIT			BIT(12)
+#define QOS_MODIFIED_DISABLE			BIT(13)
+#define QOS_MODIFIED_MAX_BURST			BIT(15)
+#define QOS_MODIFIED_BLOCKED			BIT(16)
+#define QOS_MODIFIED_WRED_ENABLE		BIT(17)
+#define QOS_MODIFIED_WRED_MIN_GREEN		BIT(18)
+#define QOS_MODIFIED_WRED_MAX_GREEN		BIT(19)
+#define QOS_MODIFIED_WRED_SLOPE_GREEN		BIT(20)
+#define QOS_MODIFIED_WRED_MIN_YELLOW		BIT(21)
+#define QOS_MODIFIED_WRED_MAX_YELLOW		BIT(22)
+#define QOS_MODIFIED_WRED_SLOPE_YELLOW		BIT(23)
+#define QOS_MODIFIED_WRED_MAX_ALLOWED		BIT(24)
+#define QOS_MODIFIED_WRED_MIN_GUARANTEED	BIT(25)
+#define QOS_MODIFIED_RLM			BIT(26)
+#define QOS_MODIFIED_WRED_FIXED_DROP_PROB_ENABLE	BIT(27)
+#define QOS_MODIFIED_WRED_FIXED_GREEN_PROB	BIT(28)
+#define QOS_MODIFIED_WRED_FIXED_YELLOW_PROB	BIT(29)
+
+#define MAX_MOVING_NODES 8
+
+/******************************************************************************/
+/*                              TYPES                                         */
+/******************************************************************************/
+
+#define NODE_TYPE(OP)		\
+	OP(TYPE_UNKNOWN)	\
+	OP(TYPE_PORT)		\
+	OP(TYPE_SCHED)		\
+	OP(TYPE_QUEUE)
+
+enum node_type {
+	NODE_TYPE(GEN_ENUM)
+};
+
+struct child_node_properties {
+	uint16_t	parent_phy;
+	uint8_t		virt_bw_share;
+};
+
+struct parent_node_properties {
+	enum pp_qos_arbitration		arbitration;
+	uint16_t			first_child_phy;
+	uint8_t				num_of_children;
+};
+
+struct qos_node {
+	enum node_type			type;
+	uint32_t			bandwidth_limit;
+	uint16_t			shared_bandwidth_group;
+	struct child_node_properties	child_prop;
+	struct parent_node_properties	parent_prop;
+	union _data {
+		struct {
+			void		*ring_address;
+			size_t		ring_size;
+			unsigned int	credit;
+			int		disable;
+		} port;
+		struct {
+		} sched;
+		struct _queue {
+			uint32_t	green_min;
+			uint32_t	green_max;
+			uint32_t	yellow_min;
+			uint32_t	yellow_max;
+			uint32_t	max_allowed;
+			uint32_t	min_guaranteed;
+			uint32_t	fixed_green_prob;
+			uint32_t	fixed_yellow_prob;
+			uint16_t	max_burst;
+			uint16_t	rlm;
+			uint8_t		green_slope;
+			uint8_t		yellow_slope;
+		} queue;
+	} data;
+
+#define QOS_NODE_FLAGS_USED					BIT(0)
+#define QOS_NODE_FLAGS_INTERNAL					BIT(1)
+#define QOS_NODE_FLAGS_PARENT_BEST_EFFORT_ENABLE		BIT(4)
+#define QOS_NODE_FLAGS_PORT_PACKET_CREDIT_ENABLE		BIT(7)
+#define QOS_NODE_FLAGS_PORT_BYTE_CREDIT_ENABLE			BIT(8)
+#define QOS_NODE_FLAGS_QUEUE_WRED_ENABLE			BIT(10)
+#define QOS_NODE_FLAGS_QUEUE_BLOCKED				BIT(11)
+#define QOS_NODE_FLAGS_QUEUE_WRED_FIXED_DROP_PROB_ENABLE	BIT(12)
+	uint16_t	flags;
+};
+
+
+struct shared_bandwidth_group {
+	uint32_t	limit;
+	uint8_t		used;
+	uint8_t		reserved;
+};
+
+struct driver_cmds {
+	struct cmd_queue	*cmdq;
+	struct cmd_queue	*pendq;
+	unsigned int		cmd_id;
+	unsigned int		cmd_fw_id;
+};
+
+/* Communication to/from FW */
+struct fw_com {
+	void		*cmdbuf;
+	size_t		cmdbuf_sz;
+	uint32_t	*mems;
+	size_t		mems_size;
+	void __iomem	*mbx_to_uc;
+	void __iomem	*mbx_from_uc;
+	unsigned int	irqline;
+};
+
+/* HW configuration */
+struct hw_conf {
+	unsigned int	wred_total_avail_resources;
+	unsigned int	wred_prioritize_pop;
+	unsigned int	wred_const_p;
+	unsigned int	wred_max_q_size;
+	unsigned int	qm_ddr_start;
+	unsigned int	qm_num_pages;
+	unsigned int	fw_logger_start;
+	unsigned int	fw_stat;
+};
+
+struct fw_ver {
+	unsigned int major;
+	unsigned int minor;
+	unsigned int build;
+};
+
+struct pp_qos_dev {
+	LOCK				lock;
+	int				initialized;
+	unsigned int			max_port;
+	unsigned int			reserved_ports[QOS_MAX_PORTS];
+	struct shared_bandwidth_group	groups[QOS_MAX_SHARED_BW_GRP + 1];
+	struct pp_pool			*ids;
+	struct pp_pool			*rlms;
+	struct pp_pool			*portsphys;
+	struct pp_nodes			*nodes;
+	struct pp_mapping		*mapping;
+	struct pp_octets		*octets;
+	struct pp_queue			*queue;
+	struct driver_cmds		drvcmds;
+	struct fw_com			fwcom;
+	struct fw_ver			fwver;
+	void				*fwbuf;
+	struct hw_conf			hwconf;
+	void				*pdev;
+	void				*stat;
+#define PP_QOS_FLAGS_ASSERT		0x1U
+	unsigned int			flags;
+};
+
+#define PP_QOS_DEVICE_IS_ASSERT(qdev)\
+	(QOS_BITS_IS_SET((qdev)->flags, PP_QOS_FLAGS_ASSERT))
+
+/* Info from platform statically or device tree */
+struct ppv4_qos_platform_data  {
+	int		id;
+	unsigned int	max_port;
+	unsigned int	wred_prioritize_pop;
+	unsigned int	qm_ddr_start;
+	unsigned int	qm_num_pages;
+	unsigned int	fw_logger_start;
+	unsigned int	fw_stat;
+};
+
+/* Info needed to create descriptor */
+struct qos_dev_init_info {
+	struct ppv4_qos_platform_data pl_data;
+	struct fw_com fwcom;
+};
+
+
+/******************************************************************************/
+/*                              UTILS                                         */
+/******************************************************************************/
+#define QOS_LOG(format, arg...) QOS_LOG_DEBUG(format, ##arg)
+void stop_run(void);
+#define QOS_ASSERT(condition, format, arg...)				\
+do {									\
+	if (!(condition)) {						\
+		QOS_LOG_CRIT("!!! Assertion failed !!! on %s:%d: " format,\
+		__func__, __LINE__, ##arg);				\
+		stop_run();						\
+	}								\
+} while (0)
+
+#define QOS_BITS_SET(flags, bits)	((flags) |= (bits))
+#define QOS_BITS_CLEAR(flags, bits)	((flags) &= ~(bits))
+#define QOS_BITS_TOGGLE(flags, bits)	((flags) ^= (bits))
+#define QOS_BITS_IS_SET(flags, bits)	((flags) & (bits))
+
+
+/*
+ * Phy octet mapping
+ */
+static inline int octet_of_phy(unsigned int phy)
+{
+	QOS_ASSERT(QOS_PHY_VALID(phy), "Invalid phy %u\n", phy);
+	return (phy >> 3);
+}
+
+/*
+ * Offset of phy on octet (0..7)
+ */
+static inline int octet_phy_offset(unsigned int phy)
+{
+	QOS_ASSERT(QOS_PHY_VALID(phy), "Invalid phy %u\n", phy);
+	return (phy & 0x7);
+}
+
+/*
+ * Return true iff both phys are on same octet
+ */
+static inline int same_octet(unsigned int first_phy, unsigned int second_phy)
+{
+	return octet_of_phy(first_phy) == octet_of_phy(second_phy);
+}
+
+/*
+ * Predicates on nodes
+ */
+static inline int node_used(const struct qos_node *node)
+{
+	return QOS_BITS_IS_SET(node->flags, QOS_NODE_FLAGS_USED);
+}
+
+static inline int node_internal(const struct qos_node *node)
+{
+	return node_used(node) && QOS_BITS_IS_SET(node->flags,
+						  QOS_NODE_FLAGS_INTERNAL);
+}
+
+static inline int node_child(const struct qos_node *node)
+{
+	return node_used(node) && (node->type == TYPE_SCHED ||
+				   node->type == TYPE_QUEUE);
+}
+
+static inline int node_parent(const struct qos_node *node)
+{
+	return node_used(node) && (node->type == TYPE_SCHED ||
+				   node->type == TYPE_PORT);
+}
+
+static inline int node_port(const struct qos_node *node)
+{
+	return node_used(node) && (node->type == TYPE_PORT);
+}
+
+static inline int node_sched(const struct qos_node *node)
+{
+	return node_used(node) && (node->type == TYPE_SCHED);
+}
+
+static inline int node_queue(const struct qos_node *node)
+{
+	return node_used(node) && (node->type == TYPE_QUEUE);
+}
+
+/*
+ * Id and phy mapping
+ */
+unsigned int get_id_from_phy(const struct pp_mapping *map, unsigned int phy);
+unsigned int get_phy_from_id(const struct pp_mapping *map, unsigned int id);
+
+/* Map id <==> phy */
+void map_id_phy(struct pp_mapping *map, unsigned int id, unsigned int phy);
+
+/* Map id ==>  QOS_UNKNOWN_PHY */
+void map_id_reserved(struct pp_mapping *map, unsigned int id);
+
+/* Invalidate both the id and the phy that is currently mapped to it */
+void map_invalidate_id(struct pp_mapping *map, unsigned int id);
+
+/*
+ * Phy node mapping
+ */
+struct qos_node *get_node_from_phy(struct pp_nodes *nodes, unsigned int phy);
+const struct qos_node *get_const_node_from_phy(const struct pp_nodes *nodes,
+					       unsigned int phy);
+unsigned int get_phy_from_node(const struct pp_nodes *nodes,
+		      const struct qos_node *node);
+
+/*
+ * Queue of uint16_t data.
+ * Data is dequeued from queue's head and enqueued into its tail
+ */
+/* return head of queue or QOS_INVALID_ID if queue is empty */
+uint16_t pp_queue_dequeue(struct pp_queue *queue);
+
+/* return 0 on success -1 if queue is full */
+int pp_queue_enqueue(struct pp_queue *queue, uint16_t data);
+
+/* empty queue */
+void pp_queue_reset(struct pp_queue *queue);
+
+/*
+ * Pool (stack) of uint16_t data
+ */
+uint16_t pp_pool_get(struct pp_pool *pool);
+int pp_pool_put(struct pp_pool *pool, uint16_t data);
+
+/*
+ * Cyclic buffer
+ */
+/*
+ * Copy size bytes from buffer to command,
+ * return 0 on success, -1 if there are less than size bytes
+ * in buffer.
+ * get remove read bytes from buffer while peek not
+ */
+int cmd_queue_get(struct cmd_queue *q, void *cmd, size_t size);
+int cmd_queue_peek(struct cmd_queue *q, void *_cmd, size_t size);
+int cmd_queue_is_empty(struct cmd_queue *q);
+
+/*
+ * Copy size bytes into cyclic buffer
+ * return 0 on success, -1 if buffer has no for size bytes
+ */
+int cmd_queue_put(struct cmd_queue *q, void *cmd, size_t size);
+
+/*
+ * Octets manipulation
+ */
+
+/**
+ * octet_get_with_at_least_free_entries() - find an octet with free entries.
+ * @octets:
+ * @count: The minimum number of free entries the octet should have
+ * Return: an octet which satisfies the following:
+ *  - It has at least count free entries
+ *  - No other octet that satisfies the previous condition has less free entries
+ *    than this octet
+ *
+ * If no such octet is found QOS_INVALID_OCTET is returned
+ *
+ * Note octet is removed from container, user should call
+ * to octet_set_use_count (or nodes_move/octet_nodes_shift which call it) to
+ * return it
+ */
+unsigned int octet_get_with_at_least_free_entries(struct pp_octets *octets,
+						  unsigned int count);
+
+/**
+ * octet_get_least_free_entries() - The octet that has the least free entries
+ * @octets:
+ * Return:
+ *
+ * Note octet is removed from container, user should call
+ * to octet_set_use_count (or nodes_move/octet_nodes_shift which call it) to
+ * return it
+ */
+unsigned int octet_get_least_free_entries(struct pp_octets *octets);
+
+/*
+ * Initialize qos dev, max port designates phy of highest port
+ */
+struct pp_qos_dev *_qos_init(unsigned int max_port);
+void _qos_clean(struct pp_qos_dev *qdev);
+
+/*
+ * Create qos dev with hardware parameters
+ */
+struct pp_qos_dev *create_qos_dev_desc(struct qos_dev_init_info *initinfo);
+
+/**
+ * octet_get_min_sibling_group() - Find "min parent" - a parent with least
+ * children on octet
+ * @qdev:
+ * @octet:
+ * @spc_parent:      The number of children for this parent is increased by one
+ *                   only for the sake of finding the "min parent"
+ * @num_of_children: Holds the number of children of "min parent"
+ * Return:	     The min parent, or NULL if octet is empty
+ *
+ * Note If several parents hold the same number of children on the octet the
+ * first is returned
+ *
+ */
+struct qos_node *octet_get_min_sibling_group(const struct pp_qos_dev *qdev,
+					     unsigned int octet,
+					     const struct qos_node *spc_parent,
+					     unsigned int *num_of_children);
+
+/**
+ * nodes_modify_used_status() - Set/Clear node's use status
+ * @qdev:
+ * @first_phy: First node's phy
+ * @count:     Number of nodes
+ * @status:    Desired status (0/1)
+ */
+void nodes_modify_used_status(struct pp_qos_dev *qdev, unsigned int first_phy,
+			      unsigned int count, unsigned int status);
+
+
+/*
+ * Remove node
+ */
+/* remove node and free its resources, assume node has no children */
+int node_remove(struct pp_qos_dev *qdev, struct qos_node *node);
+/*
+ * mark the node unused, update parents, id and rlm are not freed.
+ * assume node has no children
+ */
+void node_phy_delete(struct pp_qos_dev *qdev, unsigned int phy);
+
+void release_rlm(struct pp_pool *rlms, unsigned int rlm);
+
+int node_flush(struct pp_qos_dev *qdev, struct qos_node *node);
+
+/**
+ * node_cfg_valid() - check if node configuration is valid
+ * The process of modifying a node involves cloning orig node to
+ * temporary node, making the modification on this temporary node
+ * according to new configuration requested by user
+ * and check the validity of the result, if its valid than copy temporary node
+ * to orig node.
+ *
+ * @qdev:
+ * @node: The temporary node whose configuration should be checked
+ * @orig_node: original node from which node was copied from, NULL if new node
+ * @prev_virt_parenti_phy: phy of prev virtual parent
+ * Return: 1 if configuration is valid, 0 if not
+ */
+int node_cfg_valid(const struct pp_qos_dev *qdev,
+		   const struct qos_node *node,
+		   const struct qos_node *orig_node,
+		   unsigned int prev_virt_parent_phy);
+
+int get_node_prop(const struct pp_qos_dev *qdev,
+		  const struct qos_node *node,
+		  struct pp_qos_common_node_properties *common,
+		  struct pp_qos_parent_node_properties *parent,
+		  struct pp_qos_child_node_properties *child);
+
+int set_node_prop(struct pp_qos_dev *qdev,
+		  struct qos_node *node,
+		  const struct pp_qos_common_node_properties *common,
+		  const struct pp_qos_parent_node_properties *parent,
+		  const struct pp_qos_child_node_properties *child,
+		  uint32_t *modified);
+
+/*
+ * Init node with default values
+ * common, parent and child dictate which
+ * components of node should be initialized
+ */
+void node_init(const struct pp_qos_dev *qdev,
+	       struct qos_node *node,
+	       unsigned int common,
+	       unsigned int parent,
+	       unsigned int child);
+/**
+ * get_node_queues() - Return all queues on a subtree
+ * @qdev:
+ * @phy:       Phy of subtree's node
+ * @queue_ids: Array to store the queues ids - may be NULL
+ * @size:      Size of array - may be 0
+ * @queues_num: The number of queues on the subtree
+ */
+void get_node_queues(struct pp_qos_dev *qdev, unsigned int phy,
+		     uint16_t *queue_ids,
+		     unsigned int size, unsigned int *queues_num);
+
+/**
+ * phy_alloc_by_parent() - Allocate new phy for a node
+ * @qdev:
+ * @parent:          Node's parent
+ * @child_priority:  Relevant only for WSP parent
+ * Return:
+ */
+unsigned int phy_alloc_by_parent(struct pp_qos_dev *qdev,
+				 struct qos_node *parent,
+				 unsigned int child_priority);
+
+/* Return the first non internal ancestor of a node */
+unsigned int get_virtual_parent_phy(const struct pp_nodes *nodes,
+				    const struct qos_node *child);
+
+void node_update_children(struct pp_qos_dev *qdev,
+				 unsigned int phy,
+				 unsigned int new_phy);
+
+void tree_update_predecessors(struct pp_qos_dev *qdev, unsigned int phy);
+/*
+ * Assume that sched is internal scheduler
+ * 1. Update sched's virtual bandwidth share to be the sum of its
+ *    children's share.
+ * 2. If sched's parent is also internal scheduler
+ *    update its virtual bandwidth share also to be the update
+ *    bw share sum of its children. And keep doing that
+ *    up the tree hirarchy so long parent is an internal scheduler
+ * 3. For each
+ */
+void update_internal_bandwidth(const struct pp_qos_dev *qdev,
+			       struct qos_node *sched);
+
+/* Remove all nodes (including root) of a subtree */
+int tree_remove(struct pp_qos_dev *qdev, unsigned int phy);
+
+/* Flush all nodes (including root) of a subtree */
+int tree_flush(struct pp_qos_dev *qdev, unsigned int phy);
+
+/* Modify blocked status of queues on a subtree */
+int tree_modify_blocked_status(struct pp_qos_dev *qdev,
+			       unsigned int phy,
+			       unsigned int status);
+void get_bw_grp_members_under_node(struct pp_qos_dev *qdev, unsigned int id,
+				   unsigned int phy, uint16_t *ids,
+				   unsigned int size,
+				   unsigned int *ids_num);
+
+/* Creates pool of free ports phys */
+struct pp_pool *free_ports_phys_init(unsigned int *reserved,
+				     unsigned int max_port,
+				     const unsigned int *reserved_ports,
+				     unsigned int size);
+
+/* return the port ancestor of a node */
+static inline unsigned int get_port(const struct pp_nodes *nodes,
+				    unsigned int phy)
+{
+	const struct qos_node *node;
+
+	node = get_const_node_from_phy(nodes, phy);
+	while (node_child(node))
+		node = get_const_node_from_phy(nodes,
+					       node->child_prop.parent_phy);
+
+	QOS_ASSERT(node_port(node), "Did not reach port node for phy %u\n",
+		   phy);
+	return get_phy_from_node(nodes, node);
+}
+
+int qos_device_ready(const struct pp_qos_dev *qdev);
+void qos_module_init(void);
+void remove_qos_instance(unsigned int id);
+int _pp_qos_queue_block(struct pp_qos_dev *qdev, unsigned int id);
+int _pp_qos_queue_unblock(struct pp_qos_dev *qdev, unsigned int id);
+int _pp_qos_queue_flush(struct pp_qos_dev *qdev, unsigned int id);
+void wake_uc(void *data);
+void update_cmd_id(struct driver_cmds *drvcmds);
+void transmit_cmds(struct pp_qos_dev *qdev);
+void update_children_position(
+			struct pp_qos_dev *qdev,
+			struct qos_node *child,
+			struct qos_node *parent,
+			unsigned int position,
+			struct qos_node *node_src);
+int allocate_ddr_for_qm(struct pp_qos_dev *qdev);
+int allocate_ddr_for_qm_on_platform(struct pp_qos_dev *qdev);
+
+#ifdef PP_QOS_TEST
+void test_cmd_queue(void);
+void basic_tests(void);
+void advance_tests(void);
+void reposition_test(void);
+void falcon_test(void);
+void simple_test(void);
+void stat_test(void);
+void info_test(void);
+void wsp_test(void);
+void wa_min_test(void);
+void wa_test(void);
+void load_fw_test(void);
+void tests(void);
+void test_init_instance(struct pp_qos_dev *qdev);
+int add_qos_dev(void);
+void remove_qos_dev(void);
+int port_cfg_valid(const struct pp_qos_dev *qdev, const struct qos_node *node,
+		   const struct qos_node *orig_node);
+int queue_cfg_valid(const struct pp_qos_dev *qdev, const struct qos_node *node,
+		    const struct qos_node *orig_node,
+		    unsigned int prev_virt_parent_phy);
+int sched_cfg_valid(const struct pp_qos_dev *qdev, const struct qos_node *node,
+		    const struct qos_node *orig_node,
+		    unsigned int prev_virt_parent_phy);
+struct pp_octets *octets_init(unsigned int last_port_octet);
+void octets_clean(struct pp_octets *octets);
+void debug_verify_octet_usage(struct pp_octets *octets, unsigned int octet,
+			      unsigned int usage);
+void nodes_move(struct pp_qos_dev *qdev, unsigned int dst_phy,
+		unsigned int src_phy, unsigned int count);
+void octet_nodes_shift(struct pp_qos_dev *qdev, unsigned int first_phy,
+		       unsigned int count, int shift);
+unsigned int get_children_bandwidth_share(const struct pp_qos_dev *qdev,
+					  const struct qos_node *parent);
+unsigned int get_virt_children_bandwidth_share(const struct pp_qos_dev *qdev,
+					       const struct qos_node *parent);
+unsigned int get_children_bandwidth_share(const struct pp_qos_dev *qdev,
+					  const struct qos_node *parent);
+void transmit_cmds(struct pp_qos_dev *qdev);
+void update_cmd_id(struct driver_cmds *drvcmds);
+int check_sync_with_fw(struct pp_qos_dev *qdev);
+#define STATIC_UNLESS_TEST
+#else
+#define STATIC_UNLESS_TEST static
+#endif
+
+#endif
