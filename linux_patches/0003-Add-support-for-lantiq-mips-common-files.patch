From 0771988ced65114d326594e31aec7e1466af11d1 Mon Sep 17 00:00:00 2001
From: Hua Ma <hua.ma@linux.intel.com>
Date: Thu, 21 Jun 2018 17:37:40 +0800
Subject: [PATCH] Add support for lantiq mips common files

---
 arch/mips/configs/falconmx_bootcore_defconfig      |   81 +
 arch/mips/include/asm/ltq_itc.h                    |   36 +
 arch/mips/include/asm/ltq_vmb.h                    |  215 +++
 arch/mips/include/asm/mach-lantiq/ioremap.h        |   57 +
 .../include/asm/mach-lantiq/kernel-entry-init.h    |  230 +++
 arch/mips/include/asm/mach-lantiq/lantiq.h         |   46 +-
 arch/mips/include/asm/mach-lantiq/lantiq_atm.h     |  196 +++
 arch/mips/include/asm/mach-lantiq/lantiq_pcie.h    |  113 ++
 arch/mips/include/asm/mach-lantiq/lantiq_ptm.h     |  203 +++
 arch/mips/include/asm/mach-lantiq/pci-ath-fixup.h  |    6 +
 arch/mips/include/asm/mach-lantiq/spaces.h         |  300 ++++
 arch/mips/include/asm/mach-lantiq/war.h            |   23 +
 arch/mips/lantiq/Kconfig                           |  116 +-
 arch/mips/lantiq/Makefile                          |    8 +-
 arch/mips/lantiq/Platform                          |   24 +-
 arch/mips/lantiq/clk.h                             |  187 ++-
 arch/mips/lantiq/lantiq-amon.c                     |   81 +
 arch/mips/lantiq/lantiq-itc.c                      |  242 +++
 arch/mips/lantiq/lantiq-vmb.c                      | 1635 ++++++++++++++++++++
 include/linux/dma/lantiq_dma.h                     |  311 ++++
 include/linux/dma/lantiq_dmax.h                    | 1331 ++++++++++++++++
 include/linux/ltq_system_reset.h                   |  506 ++++++
 22 files changed, 5916 insertions(+), 31 deletions(-)

diff --git a/arch/mips/configs/falconmx_bootcore_defconfig b/arch/mips/configs/falconmx_bootcore_defconfig
new file mode 100644
index 000000000000..53ad8783dc0c
--- /dev/null
+++ b/arch/mips/configs/falconmx_bootcore_defconfig
@@ -0,0 +1,81 @@
+CONFIG_LANTIQ=y
+CONFIG_SOC_FALCONMX_BOOTCORE=y
+CONFIG_DT_FALCONMX_SFU_BOOTCORE=y
+CONFIG_CPU_MIPS32_R2=y
+# CONFIG_COMPACTION is not set
+# CONFIG_SECCOMP is not set
+CONFIG_MIPS_RAW_APPENDED_DTB=y
+CONFIG_MIPS_CMDLINE_FROM_BOOTLOADER=y
+CONFIG_SYSVIPC=y
+# CONFIG_FHANDLE is not set
+CONFIG_HIGH_RES_TIMERS=y
+CONFIG_RCU_EXPERT=y
+CONFIG_BLK_DEV_INITRD=y
+# CONFIG_RD_GZIP is not set
+# CONFIG_RD_BZIP2 is not set
+# CONFIG_RD_LZMA is not set
+# CONFIG_RD_LZO is not set
+# CONFIG_RD_LZ4 is not set
+CONFIG_CC_OPTIMIZE_FOR_SIZE=y
+CONFIG_BPF_SYSCALL=y
+# CONFIG_AIO is not set
+CONFIG_EMBEDDED=y
+CONFIG_SLAB_FREELIST_RANDOM=y
+CONFIG_MODULES=y
+CONFIG_MODULE_UNLOAD=y
+# CONFIG_BLOCK is not set
+# CONFIG_SUSPEND is not set
+CONFIG_NET=y
+CONFIG_UNIX=y
+CONFIG_NETFILTER=y
+# CONFIG_NETFILTER_ADVANCED is not set
+CONFIG_NETLINK_DIAG=y
+CONFIG_UEVENT_HELPER_PATH="/sbin/hotplug"
+# CONFIG_FIRMWARE_IN_KERNEL is not set
+CONFIG_FW_LOADER_USER_HELPER_FALLBACK=y
+CONFIG_NETDEVICES=y
+CONFIG_NET_VENDOR_AURORA=y
+CONFIG_PHYLIB=y
+# CONFIG_WLAN_VENDOR_ADMTEK is not set
+# CONFIG_WLAN_VENDOR_ATH is not set
+# CONFIG_WLAN_VENDOR_ATMEL is not set
+# CONFIG_WLAN_VENDOR_BROADCOM is not set
+# CONFIG_WLAN_VENDOR_CISCO is not set
+# CONFIG_WLAN_VENDOR_INTEL is not set
+# CONFIG_WLAN_VENDOR_INTERSIL is not set
+# CONFIG_WLAN_VENDOR_MARVELL is not set
+# CONFIG_WLAN_VENDOR_MEDIATEK is not set
+# CONFIG_WLAN_VENDOR_RALINK is not set
+# CONFIG_WLAN_VENDOR_REALTEK is not set
+# CONFIG_WLAN_VENDOR_RSI is not set
+# CONFIG_WLAN_VENDOR_ST is not set
+# CONFIG_WLAN_VENDOR_TI is not set
+# CONFIG_WLAN_VENDOR_ZYDAS is not set
+CONFIG_ISDN=y
+# CONFIG_INPUT is not set
+CONFIG_SERIO_LIBPS2=y
+# CONFIG_VT is not set
+# CONFIG_UNIX98_PTYS is not set
+# CONFIG_LEGACY_PTYS is not set
+CONFIG_SERIAL_GRX500_BOOTCORE_CONSOLE=y
+# CONFIG_HW_RANDOM is not set
+# CONFIG_LTQ_VOIP_TIMER is not set
+# CONFIG_LTQ_MPS2 is not set
+CONFIG_GPIO_SYSFS=y
+# CONFIG_HWMON is not set
+# CONFIG_USB_SUPPORT is not set
+# CONFIG_MIPS_PLATFORM_DEVICES is not set
+# CONFIG_IOMMU_SUPPORT is not set
+# CONFIG_DNOTIFY is not set
+CONFIG_OVERLAY_FS=y
+CONFIG_TMPFS=y
+CONFIG_TMPFS_XATTR=y
+CONFIG_PRINTK_TIME=y
+CONFIG_DEBUG_INFO=y
+# CONFIG_ENABLE_MUST_CHECK is not set
+CONFIG_STRIP_ASM_SYMS=y
+CONFIG_DEBUG_FS=y
+CONFIG_MAGIC_SYSRQ=y
+CONFIG_PANIC_ON_OOPS=y
+# CONFIG_FTRACE is not set
+# CONFIG_EARLY_PRINTK is not set
diff --git a/arch/mips/include/asm/ltq_itc.h b/arch/mips/include/asm/ltq_itc.h
new file mode 100644
index 000000000000..bd3c3a824ab5
--- /dev/null
+++ b/arch/mips/include/asm/ltq_itc.h
@@ -0,0 +1,36 @@
+/*
+ *  This program is free software; you can redistribute it and/or modify it
+ *  under the terms of the GNU General Public License version 2 as published
+ *  by the Free Software Foundation.
+ *
+ *  Copyright (C) 2009~2015 Lantiq Deutschland GmbH
+ *  Copyright (C) 2016 Intel Corporation.
+ */
+
+#ifndef _LTQ_ITC_H
+#define _LTQ_ITC_H
+
+
+#define ITC_Block		0x1c300000
+#define ERRCTL_ITC		(1 << 26)
+#define ITC_BypassView		0x00000000
+#define ITC_ControlView		0x00000008
+#define ITC_EmptyFullSyncView	0x00000010
+#define ITC_EmptyFullTryView	0x00000018
+#define ITC_PVSyncView		0x00000020
+#define ITC_PVTryView		0x00000028
+#define ITC_En			0x00000001
+#define ITC_E			0x00000001
+#define ITC_NumEntries		18
+#define ITC_FIFO_Entries	2
+#define ITC_SEM_Entries		16
+#define ITC_AddrMask		0x3f	/* 128K ITC address space */
+#define ITC_EntryGrain		0	/* 128 bytes between Entries (Cells)*/
+
+#define DEBUG_ITC
+
+int32_t itc_init(void);
+void itc_sem_wait(uint8_t semId);
+void itc_sem_post(uint8_t semId);
+uint32_t itc_sem_addr(uint8_t semId);
+#endif
diff --git a/arch/mips/include/asm/ltq_vmb.h b/arch/mips/include/asm/ltq_vmb.h
new file mode 100755
index 000000000000..093e2fb8dfd8
--- /dev/null
+++ b/arch/mips/include/asm/ltq_vmb.h
@@ -0,0 +1,215 @@
+/*
+ *  This program is free software; you can redistribute it and/or modify it
+ *  under the terms of the GNU General Public License version 2 as published
+ *  by the Free Software Foundation.
+ *
+ *  Copyright (C) 2009~2015 Lantiq Deutschland GmbH
+ *  Copyright (C) 2016 Intel Corporation.
+ */
+
+#ifndef _LTQ_VMB_H
+#define _LTQ_VMB_H
+
+#define MAX_CORE 2
+#define MAX_VPE 2
+#define MAX_TC	6	/* Used internally for Database ONLY */
+#define MAX_TCS	4
+#define MAX_CPU (MAX_CORE * MAX_VPE)
+
+#define NLINUX_CPUS (vmb_vpes - NR_CPUS)
+
+#define vmb_msg_size (sizeof(struct VMB_fw_msg_t) + sizeof(struct FW_vmb_msg_t))
+#define CPU_LAUNCH 0xa0000000
+
+/* REVERT BACK TO 10 */
+#define QUEUE_TIMEOUT (100 * HZ)
+
+#define get_cpu_id(core, vpe)   ((core * 2) + vpe)
+#define which_core(cpu)		 (cpu / 2)
+#define vpe_in_core(cpu)		(cpu % 2)
+
+
+#define VMB_CPU_START	0x00000001
+#define VMB_CPU_STOP	0x00000002
+#define VMB_TC_START	0x00000004
+#define VMB_TC_STOP	0x00000008
+#define VMB_TC_PAUSE	0x00000010
+#define VMB_TC_RESUME	0x00000020
+
+#define FW_VMB_ACK	0x00000001
+#define FW_VMB_NACK	0x00000002
+#define FW_RESET	0x00000004
+#define IBL_IN_WAIT	0x00000008
+#define FW_VMB_PRIV_INFO	0x00000010
+
+
+#define IBL_ACTIVE	0x1
+#define IBL_INACTIVE	0x2
+
+#define CORE_ACTIVE	0x1
+#define CORE_INACTIVE	0x2
+
+#define CPU_ACTIVE	0x1
+#define CPU_INACTIVE	0x2
+#define CPU_BOOTUP_DT	0x10
+
+#define TC_ACTIVE	0x1
+#define TC_INACTIVE	0x2
+
+#define MAX_YIELD_INTF	16
+#define	YR_ACTIVE	IBL_ACTIVE
+#define YR_INACTIVE	IBL_INACTIVE
+
+#define MAX_ITC_SEMID	16
+#define	ITC_ACTIVE	IBL_ACTIVE
+#define ITC_INACTIVE	IBL_INACTIVE
+
+/* ERROR codes */
+#define VMB_SUCCESS		1
+#define VMB_ERROR		2
+#define VMB_EBUSY		3
+#define VMB_EAVAIL		4
+#define VMB_ERESET		5
+#define VMB_ETIMEOUT		6
+#define VMB_ENACK		7
+#define VMB_ENOPRM		8
+
+struct CPU_launch_t {
+	uint32_t	start_addr;
+	uint32_t	sp;
+	uint32_t	gp;
+	uint32_t	a0;
+	uint32_t	eva;
+	uint32_t	mt_group;
+	uint32_t	yield_res;
+	uint32_t	priv_info;
+};
+
+struct TC_launch_t {
+	uint32_t	tc_num;
+	uint32_t	mt_group;
+	uint32_t	start_addr;
+	uint32_t	sp;
+	uint32_t	gp;
+	uint32_t	a0;
+	uint32_t	state;
+	uint32_t	priv_info;
+};
+
+struct VMB_fw_msg_t {
+	uint32_t	msg_id;
+	struct CPU_launch_t	cpu_launch;
+	struct TC_launch_t	tc_launch[MAX_TCS];
+	uint32_t	tc_num;
+};
+
+struct FW_vmb_msg_t {
+	uint32_t	status;
+	uint32_t	priv_info;
+};
+
+/*
+ * Structure for Event Handling used
+ * during handshake between VMB and FW/Linux
+ */
+
+struct VMB_evt_t {
+	wait_queue_head_t	vmb_wq;		/* event object */
+	uint8_t		wakeup_vpe;	/* wakeup condition flag */
+};
+
+struct VMB_tc_t {
+	uint8_t		vpe_id; /* Indicates which VPE this TC is attached */
+	uint8_t		tc_status;
+	uint8_t		 tcmt_grp;
+};
+
+struct VMB_yr_t {
+	int8_t cpu_id;
+	int8_t yr_status;
+};
+
+struct VMB_itc_t {
+	int8_t cpu_id;
+	int8_t tc_id;
+	int8_t itc_status;
+};
+
+struct VMB_vpe_t {
+	char name[16];	/* Name of the FW or LinuxOS */
+	int8_t		bl_status;
+	int8_t		cpu_status;
+	int8_t		core_id;
+	int8_t		cpu_id;
+	/*
+	 * Keep track of CPU numbers like
+	 * CORE0/VPE0 - CPU0,CORE0/VPE1 - CPU1,
+	 * CORE1/VPE0 - CPU2, CORE1/VPE1 - CPU3.
+	 * This will save lot of internal processing
+	 */
+	spinlock_t	vpe_lock;
+	void	(*vmb_callback_fn)(uint32_t status);
+	struct VMB_evt_t	v_wq;
+	struct FW_vmb_msg_t		fw_vmb;
+	uint8_t		vpemt_grp;
+};
+
+struct VMB_core_t {
+	uint8_t		active;
+/* May be used for power management or hotplug to check the status of the core*/
+	struct VMB_vpe_t	vpe_t[MAX_VPE];
+	struct VMB_tc_t	tc_t[MAX_TC];
+	/*
+	 * TCs can be unevenly distrubited to
+	 * VPEs so should be under core structure and not in vpe structure
+	 */
+	struct VMB_yr_t	yr_t[MAX_YIELD_INTF];
+};
+
+/* IRQ Numbers and handler definations
+ *	#define	 VMB_CPU_IPI1		20
+ *	#define	 VMB_CPU_IPI2		21
+ *	#define	 VMB_CPU_IPI3		85
+ *IPI numbers generated from FW (MPE/Voice) --> VMB for handling of requests
+ *  w.r.t initial  boot-up handshake OR reset/NMI scenarios
+ *  The fw_vmb_msg_t->status field acts as the differentiator to recognise if
+ *  the IPI is from FW  due FW_VMB_ACK or NACK or FW_RESET (Async reset) IPI
+ *  numbers generated from InterAptiv-BL --> VMB to indicate that BL is ready.
+ * The fw_vmb_msg_t->status field acts as the differentiator to recognise if
+ *  the IPI is from FW (FW_VMB_ACK or NACK) OR from InterAptiv-BL (IBL_IN_WAIT)
+ *  #define FW_VMB_IPI1 (87 + MIPS_GIC_IRQ_BASE) * FW/IBL on CPU1 -> VMB
+ *  #define FW_VMB_IPI2 (88 + MIPS_GIC_IRQ_BASE) * FW/IBL on CPU2 -> VMB
+ *  #define FW_VMB_IPI3 (110 + MIPS_GIC_IRQ_BASE) * FW/IBL on CPU3 -> VMB
+ */
+
+/* Function Definations as per Spec*/
+/* return type should be int[16/8]_t due to negative error return types */
+/* if int8_t used then retrun type cannot cross +- 253 */
+
+typedef void (*vmb_callback_func)(uint32_t status);
+
+int8_t vmb_cpu_alloc(int8_t cpu, char *fw_name);
+int8_t vmb_cpu_start(int8_t cpu, struct CPU_launch_t cpu_launch,
+	struct TC_launch_t tc_launch[], uint8_t num_tcs, uint8_t num_yr);
+int8_t vmb_cpu_free(int8_t cpu);
+int8_t vmb_cpu_stop(int8_t cpu);
+int8_t vmb_cpu_force_stop(int8_t cpu);
+void vmb_register_callback(uint8_t cpu_num, vmb_callback_func func);
+
+int8_t vmb_tc_stop(uint8_t cpu, uint8_t tc_num);
+int8_t vmb_tc_start(uint8_t cpu, struct TC_launch_t tc_launch[],
+					uint8_t num_tcs);
+int8_t vmb_tc_alloc(uint8_t cpu);
+int8_t vmb_tc_free(int8_t cpu, int8_t tc_num);
+int8_t vmb_tc_pause(uint8_t cpu, uint8_t tc_num);
+int8_t vmb_tc_resume(uint8_t cpu, uint8_t tc_num);
+int8_t vmb_get_vpeid(uint8_t cpu, uint8_t tc_num);
+void *VMB_get_msg_addr(int cpu, int direction);
+int32_t vmb_yr_get(uint8_t cpu, uint16_t num_yr);
+void vmb_yr_free(uint8_t cpu, int16_t yr);
+int32_t vmb_itc_sem_get(uint8_t cpu, uint8_t tc_num);
+void vmb_itc_sem_free(int8_t semID);
+int32_t fw_vmb_get_irq(uint8_t cpu);
+int is_linux_OS_vmb(int cpu);
+int vmb_run_tc(uint8_t cpu, struct TC_launch_t *tc_launch);
+#endif
diff --git a/arch/mips/include/asm/mach-lantiq/ioremap.h b/arch/mips/include/asm/mach-lantiq/ioremap.h
new file mode 100755
index 000000000000..7ab669ddeb55
--- /dev/null
+++ b/arch/mips/include/asm/mach-lantiq/ioremap.h
@@ -0,0 +1,57 @@
+/*
+ *  This program is free software; you can redistribute it and/or modify it
+ *  under the terms of the GNU General Public License version 2 as published
+ *  by the Free Software Foundation.
+ *
+ *  Copyright (C) 2014 Lei Chuanhua <Chuanhua.lei@lantiq.com>
+ *  Copyright (C) 2017 Intel Corporation.
+ */
+#ifndef __ASM_MACH_LANTIQ_IOREMAP_H
+#define __ASM_MACH_LANTIQ_IOREMAP_H
+
+#include <linux/types.h>
+
+static inline phys_addr_t fixup_bigphys_addr(phys_addr_t phys_addr,
+					     phys_addr_t size)
+{
+	return phys_addr;
+}
+
+#ifdef CONFIG_SOC_GRX500
+
+/* TOP IO Space definition for SSX7 components /PCIe/ToE/Memcpy
+ * physical 0xa0000000 --> virtual 0xe0000000
+ */
+#define GRX500_TOP_IOREMAP_BASE			0xA0000000
+#define GRX500_TOP_IOREMAP_SIZE			0x20000000
+#define GRX500_TOP_IOREMAP_PHYS_VIRT_OFFSET	0x40000000
+
+static inline void __iomem *plat_ioremap(phys_addr_t offset, unsigned long size,
+					 unsigned long flags)
+{
+	if ((offset >= GRX500_TOP_IOREMAP_BASE) &&
+	    offset < (GRX500_TOP_IOREMAP_BASE + GRX500_TOP_IOREMAP_SIZE))
+		return (void __iomem *)(unsigned long)
+			(offset + GRX500_TOP_IOREMAP_PHYS_VIRT_OFFSET);
+	return NULL;
+}
+
+static inline int plat_iounmap(const volatile void __iomem *addr)
+{
+	return (unsigned long)addr >= (unsigned long)
+		(GRX500_TOP_IOREMAP_BASE + GRX500_TOP_IOREMAP_PHYS_VIRT_OFFSET);
+}
+#else
+static inline void __iomem *plat_ioremap(phys_addr_t offset, unsigned long size,
+					 unsigned long flags)
+{
+	return NULL;
+}
+
+static inline int plat_iounmap(const volatile void __iomem *addr)
+{
+	return 0;
+}
+#endif /* CONFIG_SOC_GRX500 */
+#endif /* __ASM_MACH_LANTIQ_IOREMAP_H */
+
diff --git a/arch/mips/include/asm/mach-lantiq/kernel-entry-init.h b/arch/mips/include/asm/mach-lantiq/kernel-entry-init.h
new file mode 100755
index 000000000000..532a29ececba
--- /dev/null
+++ b/arch/mips/include/asm/mach-lantiq/kernel-entry-init.h
@@ -0,0 +1,230 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Chris Dearman (chris@mips.com)
+ * Leonid Yegoshin (yegoshin@mips.com)
+ * Copyright (C) 2012 Mips Technologies, Inc.
+ */
+#ifndef __ASM_MACH_LANTIQ_KERNEL_ENTRY_INIT_H
+#define __ASM_MACH_LANTIQ_KERNEL_ENTRY_INIT_H
+
+	/*
+	 * Prepare segments for EVA boot:
+	 *
+	 * This is in case the processor boots in legacy configuration
+	 * (SI_EVAReset is de-asserted and CONFIG5.K == 0)
+	 *
+	 * On entry, t1 is loaded with CP0_CONFIG
+	 *
+	 * ========================= Mappings =============================
+	 * Virtual memory           Physical memory           Mapping
+	 * 0x20000000 - 0x9fffffff  0x20000000 - 0x9ffffffff   MUSUK (kuseg)
+	 *                          Flat 2GB physical memory
+	 *
+	 * 0x80000000 - 0x9fffffff  0x80000000 - 0x9ffffffff   MUSUK (kseg0)
+	 * 0xa0000000 - 0xbf000000  0x00000000 - 0x1ffffffff   MUSUK (kseg1)
+	 * 0xc0000000 - 0xdfffffff             -                 MK  (kseg2)
+	 * 0xe0000000 - 0xffffffff  0xa0000000 - 0xbfffffff    UK    (kseg3)
+	 *
+	 *
+	 * Lowmem is expanded to 2GB
+	 * The last 64KB of physical memory are reserved for correct HIGHMEM
+	 * macros arithmetics.
+	 */
+
+#ifdef CONFIG_LTQ_EVA_2GB
+	.macro  platform_eva_init
+
+	.set    push
+	.set    reorder
+	/*
+	 * Get Config.K0 value and use it to program
+	 * the segmentation registers
+	 */
+	mfc0    t1, CP0_CONFIG
+	andi    t1, 0x7 /* CCA */
+	move    t2, t1
+	ins     t2, t1, 16, 3
+	/* SegCtl0 */
+	li      t0, ((MIPS_SEGCFG_UK << MIPS_SEGCFG_AM_SHIFT) |              \
+		(5 << MIPS_SEGCFG_PA_SHIFT) | (2 << MIPS_SEGCFG_C_SHIFT) |   \
+		(1 << MIPS_SEGCFG_EU_SHIFT)) |                               \
+		(((MIPS_SEGCFG_MSK << MIPS_SEGCFG_AM_SHIFT) |                \
+		(0 << MIPS_SEGCFG_PA_SHIFT) |                                \
+		(1 << MIPS_SEGCFG_EU_SHIFT)) << 16)
+	ins     t0, t1, 16, 3
+	mtc0    t0, $5, 2
+
+	/* SegCtl1 */
+	li      t0, ((MIPS_SEGCFG_UK << MIPS_SEGCFG_AM_SHIFT) |             \
+		(1 << MIPS_SEGCFG_PA_SHIFT) | (2 << MIPS_SEGCFG_C_SHIFT) |  \
+		(1 << MIPS_SEGCFG_EU_SHIFT)) |                              \
+		(((MIPS_SEGCFG_UK << MIPS_SEGCFG_AM_SHIFT) |                \
+		(4 << MIPS_SEGCFG_PA_SHIFT) |                               \
+		(1 << MIPS_SEGCFG_EU_SHIFT)) << 16)
+	ins     t0, t1, 16, 3
+	mtc0    t0, $5, 3
+
+	/* SegCtl2 */
+	li      t0, ((MIPS_SEGCFG_MUSUK << MIPS_SEGCFG_AM_SHIFT) |          \
+		(2 << MIPS_SEGCFG_PA_SHIFT) |                               \
+		(1 << MIPS_SEGCFG_EU_SHIFT)) |                              \
+		(((MIPS_SEGCFG_MUSUK << MIPS_SEGCFG_AM_SHIFT) |             \
+		(0 << MIPS_SEGCFG_PA_SHIFT) |                               \
+		(1 << MIPS_SEGCFG_EU_SHIFT)) << 16)
+	or      t0, t2
+	mtc0    t0, $5, 4
+
+	jal     mips_ihb
+	mfc0    t0, $16, 5
+	li      t2, 0x40000000      /* K bit */
+	or      t0, t0, t2
+	mtc0    t0, $16, 5
+	sync
+	jal     mips_ihb
+	.set    pop
+	.endm
+
+#elif defined(CONFIG_LTQ_EVA_1GB)
+	.macro  platform_eva_init
+
+	.set    push
+	.set    reorder
+	/*
+	 * Get Config.K0 value and use it to program
+	 * the segmentation registers
+	 */
+	mfc0    t1, CP0_CONFIG
+	andi    t1, 0x7 /* CCA */
+	move    t2, t1
+	ins     t2, t1, 16, 3
+	/* SegCtl0 */
+	li      t0, ((MIPS_SEGCFG_UK << MIPS_SEGCFG_AM_SHIFT) |              \
+		(5 << MIPS_SEGCFG_PA_SHIFT) | (2 << MIPS_SEGCFG_C_SHIFT) |   \
+		(1 << MIPS_SEGCFG_EU_SHIFT)) |                               \
+		(((MIPS_SEGCFG_MSK << MIPS_SEGCFG_AM_SHIFT) |                \
+		(0 << MIPS_SEGCFG_PA_SHIFT) |                                \
+		(1 << MIPS_SEGCFG_EU_SHIFT)) << 16)
+	ins     t0, t1, 16, 3
+	mtc0    t0, $5, 2
+
+	/* SegCtl1 */
+	li      t0, ((MIPS_SEGCFG_UK << MIPS_SEGCFG_AM_SHIFT) |              \
+		(1 << MIPS_SEGCFG_PA_SHIFT) | (2 << MIPS_SEGCFG_C_SHIFT) |   \
+		(1 << MIPS_SEGCFG_EU_SHIFT)) |                               \
+		(((MIPS_SEGCFG_UK << MIPS_SEGCFG_AM_SHIFT) |                 \
+		(2 << MIPS_SEGCFG_PA_SHIFT) |                                \
+		(1 << MIPS_SEGCFG_EU_SHIFT)) << 16)
+	ins     t0, t1, 16, 3
+	mtc0    t0, $5, 3
+
+	/* SegCtl2 */
+	li      t0, ((MIPS_SEGCFG_MUSUK << MIPS_SEGCFG_AM_SHIFT) |           \
+		(0 << MIPS_SEGCFG_PA_SHIFT) |                                \
+		(1 << MIPS_SEGCFG_EU_SHIFT)) |                               \
+		(((MIPS_SEGCFG_MUSK << MIPS_SEGCFG_AM_SHIFT) |               \
+		(0 << MIPS_SEGCFG_PA_SHIFT) |                                \
+		(1 << MIPS_SEGCFG_EU_SHIFT)) << 16)
+	ins     t0, t1, 0, 3
+	mtc0    t0, $5, 4
+
+	jal     mips_ihb
+	mfc0    t0, $16, 5
+	li      t2, 0x40000000      /* K bit */
+	or      t0, t0, t2
+	mtc0    t0, $16, 5
+	sync
+	jal     mips_ihb
+
+	.set    pop
+	.endm
+#elif defined(CONFIG_LTQ_EVA_LEGACY)
+	.macro  platform_eva_init
+	.set    push
+	.set    reorder
+	/*
+	 * Get Config.K0 value and use it to program
+	 * the segmentation registers
+	 */
+	mfc0    t1, CP0_CONFIG
+	andi    t1, 0x7 /* CCA */
+	move    t2, t1
+	ins     t2, t1, 16, 3
+	li      t0, ((MIPS_SEGCFG_UK << MIPS_SEGCFG_AM_SHIFT) |               \
+		(5 << MIPS_SEGCFG_PA_SHIFT) | (2 << MIPS_SEGCFG_C_SHIFT) |    \
+		(1 << MIPS_SEGCFG_EU_SHIFT)) |                                \
+		(((MIPS_SEGCFG_MSK << MIPS_SEGCFG_AM_SHIFT) |                 \
+		(0 << MIPS_SEGCFG_PA_SHIFT)/* | (5 << MIPS_SEGCFG_C_SHIFT)*/ |\
+		(1 << MIPS_SEGCFG_EU_SHIFT)) << 16)
+	ins     t0, t1, 16, 3
+	mtc0    t0, $5, 2
+	li      t0, ((MIPS_SEGCFG_UK << MIPS_SEGCFG_AM_SHIFT) |                \
+		(1 << MIPS_SEGCFG_PA_SHIFT) | (2 << MIPS_SEGCFG_C_SHIFT) |     \
+		(1 << MIPS_SEGCFG_EU_SHIFT)) |                                 \
+		(((MIPS_SEGCFG_UK << MIPS_SEGCFG_AM_SHIFT) |                   \
+		(1 << MIPS_SEGCFG_PA_SHIFT)/* | (5 << MIPS_SEGCFG_C_SHIFT) */ |\
+		(1 << MIPS_SEGCFG_EU_SHIFT)) << 16)
+	ins     t0, t1, 16, 3
+	mtc0    t0, $5, 3
+	li      t0, ((MIPS_SEGCFG_MUSK << MIPS_SEGCFG_AM_SHIFT) |              \
+		(0 << MIPS_SEGCFG_PA_SHIFT)/* | (5 << MIPS_SEGCFG_C_SHIFT) */ |\
+		(1 << MIPS_SEGCFG_EU_SHIFT)) |                                 \
+		(((MIPS_SEGCFG_MUSK << MIPS_SEGCFG_AM_SHIFT) |                 \
+		(0 << MIPS_SEGCFG_PA_SHIFT)/* | (5 << MIPS_SEGCFG_C_SHIFT) */ |\
+		(1 << MIPS_SEGCFG_EU_SHIFT)) << 16)
+	or      t0, t2
+	mtc0    t0, $5, 4
+	jal     mips_ihb
+
+	mfc0    t0, $16, 5
+	li      t2, 0x40000000      /* K bit */
+	or      t0, t0, t2
+	mtc0    t0, $16, 5
+	sync
+	jal	mips_ihb
+	.set    pop
+	.endm
+#else
+	/* Pure legacy + segment control for PCIE/MPE/Hwmemcpy and ToE */
+	.macro  platform_eva_init
+	.set    push
+	.set    reorder
+	/*
+	 * Get Config.K0 value and use it to program
+	 * the segmentation registers
+	 */
+
+	mfc0    t1, CP0_CONFIG
+	andi    t1, 0x7 /* CCA */
+	move    t2, t1
+	ins     t2, t1, 16, 3
+	li      t0, ((MIPS_SEGCFG_UK << MIPS_SEGCFG_AM_SHIFT) |               \
+		(5 << MIPS_SEGCFG_PA_SHIFT) | (2 << MIPS_SEGCFG_C_SHIFT) |    \
+		(1 << MIPS_SEGCFG_EU_SHIFT)) |                                \
+		(((MIPS_SEGCFG_MK << MIPS_SEGCFG_AM_SHIFT) |                  \
+		(0 << MIPS_SEGCFG_PA_SHIFT)/* | (5 << MIPS_SEGCFG_C_SHIFT)*/ |\
+		(1 << MIPS_SEGCFG_EU_SHIFT)) << 16)
+	ins     t0, t1, 16, 3
+	mtc0    t0, $5, 2
+	jal     mips_ihb
+	.set    pop
+	.endm
+#endif /* Pure Legacy */
+
+	.macro	kernel_entry_setup
+	sync
+	ehb
+	platform_eva_init
+	.endm
+/*
+ * Do SMP slave processor setup necessary before we can safely execute C code.
+ */
+	.macro	smp_slave_setup
+	sync
+	ehb
+	platform_eva_init
+	.endm
+
+#endif /* __ASM_MACH_LANTIQ_KERNEL_ENTRY_INIT_H */
diff --git a/arch/mips/include/asm/mach-lantiq/lantiq.h b/arch/mips/include/asm/mach-lantiq/lantiq.h
old mode 100644
new mode 100755
index 8064d7a4b33d..14975f034015
--- a/arch/mips/include/asm/mach-lantiq/lantiq.h
+++ b/arch/mips/include/asm/mach-lantiq/lantiq.h
@@ -15,41 +15,45 @@
 /* generic reg access functions */
 #define ltq_r32(reg)		__raw_readl(reg)
 #define ltq_w32(val, reg)	__raw_writel(val, reg)
+
 #define ltq_w32_mask(clear, set, reg)	\
 	ltq_w32((ltq_r32(reg) & ~(clear)) | (set), reg)
+
+#define ltq_r16(reg)		__raw_readw(reg)
+#define ltq_w16(val, reg)	__raw_writew(val, reg)
+
 #define ltq_r8(reg)		__raw_readb(reg)
 #define ltq_w8(val, reg)	__raw_writeb(val, reg)
 
-/* register access macros for EBU and CGU */
-#define ltq_ebu_w32(x, y)	ltq_w32((x), ltq_ebu_membase + (y))
-#define ltq_ebu_r32(x)		ltq_r32(ltq_ebu_membase + (x))
-#define ltq_ebu_w32_mask(x, y, z) \
-	ltq_w32_mask(x, y, ltq_ebu_membase + (z))
-extern __iomem void *ltq_ebu_membase;
-
 /* spinlock all ebu i/o */
 extern spinlock_t ebu_lock;
 
 /* some irq helpers */
-extern void ltq_disable_irq(struct irq_data *data);
-extern void ltq_mask_and_ack_irq(struct irq_data *data);
-extern void ltq_enable_irq(struct irq_data *data);
-extern int ltq_eiu_get_irq(int exin);
+void ltq_disable_irq(struct irq_data *data);
+void ltq_mask_and_ack_irq(struct irq_data *data);
+void ltq_enable_irq(struct irq_data *data);
+int ltq_eiu_get_irq(int exin);
 
 /* clock handling */
-extern int clk_activate(struct clk *clk);
-extern void clk_deactivate(struct clk *clk);
-extern struct clk *clk_get_cpu(void);
-extern struct clk *clk_get_fpi(void);
-extern struct clk *clk_get_io(void);
-extern struct clk *clk_get_ppe(void);
+int clk_activate(struct clk *clk);
+void clk_deactivate(struct clk *clk);
+struct clk *clk_get_cpu(void);
+struct clk *clk_get_ddr(void);
+struct clk *clk_get_fpi(void);
+struct clk *clk_get_io(void);
+struct clk *clk_get_ppe(void);
+struct clk *clk_get_xbar(void);
 
 /* find out what bootsource we have */
-extern unsigned char ltq_boot_select(void);
+unsigned char ltq_boot_select(void);
 /* find out what caused the last cpu reset */
-extern int ltq_reset_cause(void);
-/* find out the soc type */
-extern int ltq_soc_type(void);
+int ltq_reset_cause(void);
+void ltq_reset_once(unsigned int module, ulong u);
+void ltq_hw_reset(unsigned int module);
+void ltq_rst_init(void);
+unsigned int ltq_get_cpu_id(void);
+unsigned int ltq_get_soc_type(void);
+unsigned int ltq_get_soc_rev(void);
 
 #define IOPORT_RESOURCE_START	0x10000000
 #define IOPORT_RESOURCE_END	0xffffffff
diff --git a/arch/mips/include/asm/mach-lantiq/lantiq_atm.h b/arch/mips/include/asm/mach-lantiq/lantiq_atm.h
new file mode 100755
index 000000000000..bf045a9754eb
--- /dev/null
+++ b/arch/mips/include/asm/mach-lantiq/lantiq_atm.h
@@ -0,0 +1,196 @@
+/******************************************************************************
+**
+** FILE NAME    : ifx_atm.h
+** PROJECT      : UEIP
+** MODULES      : ATM
+**
+** DATE         : 17 Jun 2009
+** AUTHOR       : Xu Liang
+** DESCRIPTION  : Global ATM driver header file
+** COPYRIGHT    :       Copyright (c) 2006
+**                      Infineon Technologies AG
+**                      Am Campeon 1-12, 85579 Neubiberg, Germany
+**
+**    This program is free software; you can redistribute it and/or modify
+**    it under the terms of the GNU General Public License as published by
+**    the Free Software Foundation; either version 2 of the License, or
+**    (at your option) any later version.
+**
+** HISTORY
+** $Date        $Author         $Comment
+** 07 JUL 2009  Xu Liang        Init Version
+*******************************************************************************/
+
+#ifndef IFX_ATM_H
+#define IFX_ATM_H
+
+
+
+/*!
+  \defgroup IFX_ATM UEIP Project - ATM driver module
+  \brief UEIP Project - ATM driver module, support Danube, Amazon-SE, AR9, VR9.
+ */
+
+/*!
+  \defgroup IFX_ATM_IOCTL IOCTL Commands
+  \ingroup IFX_ATM
+  \brief IOCTL Commands used by user application.
+ */
+
+/*!
+  \defgroup IFX_ATM_STRUCT Structures
+  \ingroup IFX_ATM
+  \brief Structures used by user application.
+ */
+
+/*!
+  \file ifx_atm.h
+  \ingroup IFX_ATM
+  \brief ATM driver header file
+ */
+
+
+
+/*
+ * ####################################
+ *              Definition
+ * ####################################
+ */
+
+/*!
+  \addtogroup IFX_ATM_STRUCT
+ */
+/*@{*/
+
+/*
+ *  ATM MIB
+ */
+
+/*!
+  \struct atm_cell_ifEntry_t
+  \brief Structure used for Cell Level MIB Counters.
+
+  User application use this structure to call IOCTL command "PPE_ATM_MIB_CELL".
+ */
+typedef struct {
+	__u32	ifHCInOctets_h;     /*!< byte counter of ingress cells (upper 32 bits, total 64 bits)   */
+	__u32	ifHCInOctets_l;     /*!< byte counter of ingress cells (lower 32 bits, total 64 bits)   */
+	__u32	ifHCOutOctets_h;    /*!< byte counter of egress cells (upper 32 bits, total 64 bits)    */
+	__u32	ifHCOutOctets_l;    /*!< byte counter of egress cells (lower 32 bits, total 64 bits)    */
+	__u32	ifInErrors;         /*!< counter of error ingress cells     */
+	__u32	ifInUnknownProtos;  /*!< counter of unknown ingress cells   */
+	__u32	ifOutErrors;        /*!< counter of error egress cells      */
+} atm_cell_ifEntry_t;
+
+/*!
+  \struct atm_aal5_ifEntry_t
+  \brief Structure used for AAL5 Frame Level MIB Counters.
+
+  User application use this structure to call IOCTL command "PPE_ATM_MIB_AAL5".
+ */
+typedef struct {
+	__u32	ifHCInOctets_h;     /*!< byte counter of ingress packets (upper 32 bits, total 64 bits) */
+	__u32	ifHCInOctets_l;     /*!< byte counter of ingress packets (lower 32 bits, total 64 bits) */
+	__u32	ifHCOutOctets_h;    /*!< byte counter of egress packets (upper 32 bits, total 64 bits)  */
+	__u32	ifHCOutOctets_l;    /*!< byte counter of egress packets (lower 32 bits, total 64 bits)  */
+	__u32	ifInUcastPkts;      /*!< counter of ingress packets         */
+	__u32	ifOutUcastPkts;     /*!< counter of egress packets          */
+	__u32	ifInErrors;         /*!< counter of error ingress packets   */
+	__u32	ifInDiscards;       /*!< counter of dropped ingress packets */
+	__u32	ifOutErros;         /*!< counter of error egress packets    */
+	__u32	ifOutDiscards;      /*!< counter of dropped egress packets  */
+} atm_aal5_ifEntry_t;
+
+/*!
+  \struct atm_aal5_vcc_t
+  \brief Structure used for per PVC AAL5 Frame Level MIB Counters.
+
+  This structure is a part of structure "atm_aal5_vcc_x_t".
+ */
+typedef struct {
+	__u32	aal5VccCrcErrors;       /*!< counter of ingress packets with CRC error  */
+	__u32	aal5VccSarTimeOuts;     /*!< counter of ingress packets with Re-assemble timeout    */  //no timer support yet
+	__u32	aal5VccOverSizedSDUs;   /*!< counter of oversized ingress packets       */
+} atm_aal5_vcc_t;
+
+/*!
+  \struct atm_aal5_vcc_x_t
+  \brief Structure used for per PVC AAL5 Frame Level MIB Counters.
+
+  User application use this structure to call IOCTL command "PPE_ATM_MIB_VCC".
+ */
+typedef struct {
+	int             vpi;        /*!< VPI of the VCC to get MIB counters */
+	int             vci;        /*!< VCI of the VCC to get MIB counters */
+	atm_aal5_vcc_t  mib_vcc;    /*!< structure to get MIB counters      */
+} atm_aal5_vcc_x_t;
+
+/*@}*/
+
+
+
+/*
+ * ####################################
+ *                IOCTL
+ * ####################################
+ */
+
+/*!
+  \addtogroup IFX_ATM_IOCTL
+ */
+/*@{*/
+
+/*
+ *  ioctl Command
+ */
+/*!
+  \brief ATM IOCTL Magic Number
+ */
+#define PPE_ATM_IOC_MAGIC       'o'
+/*!
+  \brief ATM IOCTL Command - Get Cell Level MIB Counters
+
+   This command is obsolete. User can get cell level MIB from DSL API.
+   This command uses structure "atm_cell_ifEntry_t" as parameter for output of MIB counters.
+ */
+#define PPE_ATM_MIB_CELL        _IOW(PPE_ATM_IOC_MAGIC,  0, atm_cell_ifEntry_t)
+/*!
+  \brief ATM IOCTL Command - Get AAL5 Level MIB Counters
+
+   Get AAL5 packet counters.
+   This command uses structure "atm_aal5_ifEntry_t" as parameter for output of MIB counters.
+ */
+#define PPE_ATM_MIB_AAL5        _IOW(PPE_ATM_IOC_MAGIC,  1, atm_aal5_ifEntry_t)
+/*!
+  \brief ATM IOCTL Command - Get Per PVC MIB Counters
+
+   Get AAL5 packet counters for each PVC.
+   This command uses structure "atm_aal5_vcc_x_t" as parameter for input of VPI/VCI information and output of MIB counters.
+ */
+#define PPE_ATM_MIB_VCC         _IOWR(PPE_ATM_IOC_MAGIC, 2, atm_aal5_vcc_x_t)
+/*!
+  \brief Total Number of ATM IOCTL Commands
+ */
+#define PPE_ATM_IOC_MAXNR       3
+
+/*@}*/
+
+
+
+/*
+ * ####################################
+ *                 API
+ * ####################################
+ */
+
+#ifdef __KERNEL__
+struct port_cell_info {
+    unsigned int    port_num;
+    unsigned int    tx_link_rate[2];
+};
+#endif
+
+
+
+#endif  //  IFX_ATM_H
+
diff --git a/arch/mips/include/asm/mach-lantiq/lantiq_pcie.h b/arch/mips/include/asm/mach-lantiq/lantiq_pcie.h
new file mode 100755
index 000000000000..9b470b16e90f
--- /dev/null
+++ b/arch/mips/include/asm/mach-lantiq/lantiq_pcie.h
@@ -0,0 +1,113 @@
+/*
+ *  This program is free software; you can redistribute it and/or modify it
+ *  under the terms of the GNU General Public License version 2 as published
+ *  by the Free Software Foundation.
+ *
+ *  Copyright (C) 2011~2013 Lei Chuanhua <chuanhua.lei@lantiq.com>
+ */
+
+/** \defgroup IFX_PCIE_EP_VRX318 PCIE EP Functions Reference
+ *  This chapter describes the entire interfaces to the PCIE EP interface.
+ */
+#ifndef LANTIQ_PCIE_H
+#define LANTIQ_PCIE_H
+#include <linux/types.h>
+#include <linux/pci.h>
+#include <linux/device.h>
+
+/* @{ */
+
+/*! \def IFX_PCIE_EP_MAX_PEER
+ *  \brief how many EP partners existed. In most cases, this number should be
+ *  one for bonding application For the future extension, it could be bigger
+ *  value. For example, multiple bonding
+ */
+#define IFX_PCIE_EP_MAX_PEER     1
+
+/** Structure used to specify interrupt source so that EP can assign unique
+ *  interruot to it
+ */
+typedef enum ifx_pcie_ep_int_module {
+	IFX_PCIE_EP_INT_PPE, /*!< PPE2HOST_INT 0/1 */
+	IFX_PCIE_EP_INT_MEI, /*!< DSL MEI_IRQ */
+	IFX_PCIE_EP_INT_DYING_GASP, /*!< DSL Dying_Gasp */
+	IFX_PCIE_EP_INT_EDMA, /*!< PCIe eDMA */
+	IFX_PCIE_EP_INT_FPI_BCU, /*!< FPI BUC */
+	IFX_PCIE_EP_INT_ARC_LED0, /*!< ARC LED0 */
+	IFX_PCIE_EP_INT_ARC_LED1, /*!< ARC LED1 */
+	IFX_PCIE_EP_INT_DMA, /*!< Central DMA */
+	IFX_PCIE_EP_INT_MODULE_MAX,
+} ifx_pcie_ep_int_module_t;
+
+/** Structure used to extract attached EP detailed information
+ *  for PPE/DSL_MEI driver/Bonding
+ */
+typedef struct pcie_ep_dev {
+	struct device *dev;
+	u32 irq;          /*!< MSI interrupt number for this device */
+	/*!< The EP inbound memory base address derived from BAR0, SoC
+	 *   virtual address for PPE/DSL_MEI driver
+	 */
+	u8 __iomem *membase;
+	u32 phy_membase;  /*!< The EP inbound memory base address derived
+			   * from BAR0, physical address for PPE FW
+			   */
+	u32 peer_num;    /*!< Bonding peer number available */
+	/*!< The bonding peer EP inbound memory base address derived from
+	 *   its BAR0, SoC virtual address for PPE/DSL_MEI driver
+	 */
+	u8 __iomem *peer_membase[IFX_PCIE_EP_MAX_PEER];
+	/*!< The bonding peer EP inbound memory base address derived from
+	 *   its BAR0, physical address for PPE FW
+	 */
+	u32 peer_phy_membase[IFX_PCIE_EP_MAX_PEER];
+} ifx_pcie_ep_dev_t;
+
+/**
+ * This function returns the total number of EPs attached. Normally,
+ * the number should be one <standard smartPHY EP> or two <smartPHY
+ * off-chip bonding cases>. Extended case is also considered
+
+ * \param[in/out]  dev_num   Pointer to detected EP numbers in total.
+ * \return         -EIO      Invalid total EP number which means this
+ *			     module is not initialized properly
+ * \return         0         Successfully return the detected EP numbers
+ */
+int ifx_pcie_ep_dev_num_get(int *dev_num);
+
+/**
+ * This function returns detailed EP device information for PPE/DSL/Bonding
+ * partner by its logical index obtained
+ * by \ref ifx_pcie_ep_dev_num_get and its interrupt module number
+ * \ref ifx_pcie_ep_int_module_t
+ *
+ * \param[in]      dev_idx   Logical device index referred to the related
+ *			     device
+ * \param[in]      module    EP interrupt module user<PPE/MEI/eDMA/CDMA>
+ * \param[in/out]  dev       Pointer to returned detail device structure
+ *			     \ref ifx_pcie_ep_dev_t
+ * \return         -EIO      Invalid logical device index or too many modules
+ *			     referred to this module
+ * \return         0         Successfully return required device information
+ *
+ * \remarks This function normally will be called to trace the detailed device
+ *	    information after calling \ref ifx_pcie_ep_dev_num_get
+ */
+int ifx_pcie_ep_dev_info_req(int dev_idx, ifx_pcie_ep_int_module_t module,
+			     ifx_pcie_ep_dev_t *dev);
+
+/**
+ * This function releases the usage of this module by PPE/DSL
+ *
+ * \param[in]  dev_idx   Logical device index referred to the related device
+ * \return     -EIO      Invalid logical device index or release too many
+ *			 times to refer to this module
+ *  \return     0         Successfully release the usage of this module
+ *
+ * \remarks This function should be called once their reference is over.
+ *	    The reference usage must matches \ref ifx_pcie_ep_dev_info_req
+ */
+int ifx_pcie_ep_dev_info_release(int dev_idx);
+
+/* @} */
+#endif /* LANTIQ_PCIE_H */
diff --git a/arch/mips/include/asm/mach-lantiq/lantiq_ptm.h b/arch/mips/include/asm/mach-lantiq/lantiq_ptm.h
new file mode 100755
index 000000000000..698e5c3564a1
--- /dev/null
+++ b/arch/mips/include/asm/mach-lantiq/lantiq_ptm.h
@@ -0,0 +1,203 @@
+/******************************************************************************
+**
+** FILE NAME    : ifx_ptm.h
+** PROJECT      : UEIP
+** MODULES      : PTM
+**
+** DATE         : 17 Jun 2009
+** AUTHOR       : Xu Liang
+** DESCRIPTION  : Global PTM driver header file
+** COPYRIGHT    :       Copyright (c) 2006
+**                      Infineon Technologies AG
+**                      Am Campeon 1-12, 85579 Neubiberg, Germany
+**
+**    This program is free software; you can redistribute it and/or modify
+**    it under the terms of the GNU General Public License as published by
+**    the Free Software Foundation; either version 2 of the License, or
+**    (at your option) any later version.
+**
+** HISTORY
+** $Date        $Author         $Comment
+** 07 JUL 2009  Xu Liang        Init Version
+*******************************************************************************/
+
+#ifndef IFX_PTM_H
+#define IFX_PTM_H
+
+
+
+/*!
+  \defgroup IFX_PTM UEIP Project - PTM driver module
+  \brief UEIP Project - PTM driver module, support Danube, Amazon-SE, AR9, VR9.
+ */
+
+/*!
+  \defgroup IFX_PTM_IOCTL IOCTL Commands
+  \ingroup IFX_PTM
+  \brief IOCTL Commands used by user application.
+ */
+
+/*!
+  \defgroup IFX_PTM_STRUCT Structures
+  \ingroup IFX_PTM
+  \brief Structures used by user application.
+ */
+
+/*!
+  \file ifx_ptm.h
+  \ingroup IFX_PTM
+  \brief PTM driver header file
+ */
+
+
+
+/*
+ * ####################################
+ *              Definition
+ * ####################################
+ */
+
+
+
+/*
+ * ####################################
+ *                IOCTL
+ * ####################################
+ */
+
+/*!
+  \addtogroup IFX_PTM_IOCTL
+ */
+/*@{*/
+
+/*
+ *  ioctl Command
+ */
+/*!
+  \brief PTM IOCTL Command - Get codeword MIB counters.
+
+  This command uses structure "PTM_CW_IF_ENTRY_T" to get codeword level MIB counters.
+ */
+#define IFX_PTM_MIB_CW_GET              SIOCDEVPRIVATE + 1
+/*!
+  \brief PTM IOCTL Command - Get packet MIB counters.
+
+  This command uses structure "PTM_FRAME_MIB_T" to get packet level MIB counters.
+ */
+#define IFX_PTM_MIB_FRAME_GET           SIOCDEVPRIVATE + 2
+/*!
+  \brief PTM IOCTL Command - Get firmware configuration (CRC).
+
+  This command uses structure "IFX_PTM_CFG_T" to get firmware configuration (CRC).
+ */
+#define IFX_PTM_CFG_GET                 SIOCDEVPRIVATE + 3
+/*!
+  \brief PTM IOCTL Command - Set firmware configuration (CRC).
+
+  This command uses structure "IFX_PTM_CFG_T" to set firmware configuration (CRC).
+ */
+#define IFX_PTM_CFG_SET                 SIOCDEVPRIVATE + 4
+/*!
+  \brief PTM IOCTL Command - Program priority value to TX queue mapping.
+
+  This command uses structure "IFX_PTM_PRIO_Q_MAP_T" to program priority value to TX queue mapping.
+ */
+#define IFX_PTM_MAP_PKT_PRIO_TO_Q       SIOCDEVPRIVATE + 14
+
+/*@}*/
+
+
+/*!
+  \addtogroup IFX_PTM_STRUCT
+ */
+/*@{*/
+
+/*
+ *  ioctl Data Type
+ */
+
+/*!
+  \typedef PTM_CW_IF_ENTRY_T
+  \brief Wrapping of structure "ptm_cw_ifEntry_t".
+ */
+/*!
+  \struct ptm_cw_ifEntry_t
+  \brief Structure used for CodeWord level MIB counters.
+ */
+typedef struct ptm_cw_ifEntry_t {
+    uint32_t    ifRxNoIdleCodewords;    /*!< output, number of ingress user codeword */
+    uint32_t    ifRxIdleCodewords;      /*!< output, number of ingress idle codeword */
+    uint32_t    ifRxCodingViolation;    /*!< output, number of error ingress codeword */
+    uint32_t    ifTxNoIdleCodewords;    /*!< output, number of egress user codeword */
+    uint32_t    ifTxIdleCodewords;      /*!< output, number of egress idle codeword */
+} PTM_CW_IF_ENTRY_T;
+
+/*!
+  \typedef PTM_FRAME_MIB_T
+  \brief Wrapping of structure "ptm_frame_mib_t".
+ */
+/*!
+  \struct ptm_frame_mib_t
+  \brief Structure used for packet level MIB counters.
+ */
+typedef struct ptm_frame_mib_t {
+    uint32_t    RxCorrect;      /*!< output, number of ingress packet */
+    uint32_t    TC_CrcError;    /*!< output, number of egress packet with CRC error */
+    uint32_t    RxDropped;      /*!< output, number of dropped ingress packet */
+    uint32_t    TxSend;         /*!< output, number of egress packet */
+} PTM_FRAME_MIB_T;
+
+/*!
+  \typedef IFX_PTM_CFG_T
+  \brief Wrapping of structure "ptm_cfg_t".
+ */
+/*!
+  \struct ptm_cfg_t
+  \brief Structure used for ETH/TC CRC configuration.
+ */
+typedef struct ptm_cfg_t {
+    uint32_t    RxEthCrcPresent;    /*!< input/output, ingress packet has ETH CRC */
+    uint32_t    RxEthCrcCheck;      /*!< input/output, check ETH CRC of ingress packet */
+    uint32_t    RxTcCrcCheck;       /*!< input/output, check TC CRC of ingress codeword */
+    uint32_t    RxTcCrcLen;         /*!< input/output, length of TC CRC of ingress codeword */
+    uint32_t    TxEthCrcGen;        /*!< input/output, generate ETH CRC for egress packet */
+    uint32_t    TxTcCrcGen;         /*!< input/output, generate TC CRC for egress codeword */
+    uint32_t    TxTcCrcLen;         /*!< input/output, length of TC CRC of egress codeword */
+} IFX_PTM_CFG_T;
+
+/*!
+  \typedef IFX_PTM_PRIO_Q_MAP_T
+  \brief Wrapping of structure "ppe_prio_q_map".
+ */
+/*!
+  \struct ppe_prio_q_map
+  \brief Structure used for Priority Value to TX Queue mapping.
+ */
+typedef struct ppe_prio_q_map {
+    int             pkt_prio;
+    int             qid;
+    int             vpi;    //  ignored in eth interface
+    int             vci;    //  ignored in eth interface
+} IFX_PTM_PRIO_Q_MAP_T;
+
+/*@}*/
+
+
+
+/*
+ * ####################################
+ *                 API
+ * ####################################
+ */
+
+#ifdef __KERNEL__
+struct port_cell_info {
+    unsigned int    port_num;
+    unsigned int    tx_link_rate[2];
+};
+#endif
+
+
+
+#endif  //  IFX_PTM_H
+
diff --git a/arch/mips/include/asm/mach-lantiq/pci-ath-fixup.h b/arch/mips/include/asm/mach-lantiq/pci-ath-fixup.h
new file mode 100755
index 000000000000..095d2619ce2c
--- /dev/null
+++ b/arch/mips/include/asm/mach-lantiq/pci-ath-fixup.h
@@ -0,0 +1,6 @@
+#ifndef _PCI_ATH_FIXUP
+#define _PCI_ATH_FIXUP
+
+void ltq_pci_ath_fixup(unsigned slot, u16 *cal_data) __init;
+
+#endif /* _PCI_ATH_FIXUP */
diff --git a/arch/mips/include/asm/mach-lantiq/spaces.h b/arch/mips/include/asm/mach-lantiq/spaces.h
new file mode 100755
index 000000000000..8498d30de1cd
--- /dev/null
+++ b/arch/mips/include/asm/mach-lantiq/spaces.h
@@ -0,0 +1,300 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Author: Leonid Yegoshin (yegoshin@mips.com)
+ * Copyright (C) 2012 MIPS Technologies, Inc.
+ * Copyright (C) 2014 Lei Chuanhua <Chuanhua.lei@lantiq.com>
+ * Copyright (C) 2017 Intel Corporation.
+ */
+
+#ifndef _ASM_LANTIQ_SPACES_H
+#define _ASM_LANTIQ_SPACES_H
+
+#include <linux/sizes.h>
+
+#ifdef CONFIG_SOC_GRX500
+#ifdef CONFIG_EVA
+
+#ifdef CONFIG_LTQ_EVA_2GB
+
+#define MIPS_RESERVED_MEM_SIZE	0
+
+#define PHYS_START		_AC(0x20000000, UL)
+#define VIRT_START		_AC(0x20000000, UL)
+
+#define PAGE_OFFSET		(VIRT_START + MIPS_RESERVED_MEM_SIZE)
+#define PHYS_OFFSET		(PHYS_START + MIPS_RESERVED_MEM_SIZE)
+
+/* No Highmem Support */
+#define HIGHMEM_START		_AC(0xffff0000, UL)
+
+#define UNCAC_BASE		(_AC(0xa0000000, UL) + MIPS_RESERVED_MEM_SIZE)
+#define CAC_BASE		(_AC(0x20000000, UL) + MIPS_RESERVED_MEM_SIZE)
+#define IO_BASE			UNCAC_BASE /* Must be the same */
+
+#define KSEG
+#define KUSEG			0x00000000
+#define KSEG0			0x20000000
+#define KSEG1			0xa0000000
+#define KSEG2			0xc0000000
+#define KSEG3			0xe0000000
+
+#define CKUSEG			0x00000000
+#define CKSEG0			0x20000000
+#define CKSEG1			0xa0000000
+#define CKSEG2			_AC(0xc0000000, UL)
+#define CKSEG3			0xe0000000
+
+/* Range from 0x2000.0000 ~ 0x9FFF.FFFF for KSEG0 */
+#define KSEGX(a)		(((_ACAST32_(a)) < 0xA0000000) ?	\
+					KSEG0 : ((_ACAST32_(a)) & 0xE0000000))
+
+#define INDEX_BASE		CKSEG0
+#define MAP_BASE		CKSEG2
+#define VMALLOC_END		(MAP_BASE + _AC(0x20000000, UL) - 2 * PAGE_SIZE)
+
+#define IO_SIZE			_AC(0x10000000 - MIPS_RESERVED_MEM_SIZE, UL)
+#define IO_SHIFT		_AC(0x10000000, UL)
+#ifdef CONFIG_DMA_COHERENT
+#define CPHYSADDR(a)		((_ACAST32_(a))  - PAGE_OFFSET +	\
+				 PHYS_OFFSET + 0xA0000000)
+#else
+#define CPHYSADDR(a)		((_ACAST32_(a)) - PAGE_OFFSET + PHYS_OFFSET)
+#endif
+#define RPHYSADDR(a)		((_ACAST32_(a)) & 0x1fffffff)
+
+#define LEGACY_KSEC0(a)		(RPHYSADDR(a) | 0x80000000)
+
+/* DRAM one */
+#define CKSEG0ADDR(a)		((_ACAST32_(a)))
+
+/* IO space one */
+#define CKSEG1ADDR(a)		(RPHYSADDR(a) | KSEG1)
+
+/*
+ * Map an address to a certain kernel segment
+ */
+/* DRAM one */
+#define KSEG0ADDR(a)		(CKSEG0ADDR(a))
+
+/* IO space one */
+#define KSEG1ADDR(a)		(RPHYSADDR(a) | KSEG1)
+#define __pa_symbol(x)		__pa(x)
+
+#elif defined(CONFIG_LTQ_EVA_1GB)
+#define MIPS_RESERVED_MEM_SIZE	0
+
+#define PHYS_START		_AC(0x20000000, UL)
+#define VIRT_START		_AC(0x60000000, UL)
+
+#define PAGE_OFFSET		(VIRT_START + MIPS_RESERVED_MEM_SIZE)
+#define PHYS_OFFSET		(PHYS_START + MIPS_RESERVED_MEM_SIZE)
+
+/* No Highmem Support */
+#define HIGHMEM_START		_AC(0xffff0000, UL)
+
+#define UNCAC_BASE		(_AC(0xa0000000, UL) + MIPS_RESERVED_MEM_SIZE)
+#define CAC_BASE		(_AC(0x60000000, UL) + MIPS_RESERVED_MEM_SIZE)
+#define IO_BASE			UNCAC_BASE /* Must be the same */
+
+#define KSEG
+#define KUSEG			0x00000000
+#define KSEG0			0x60000000
+#define KSEG1			0xa0000000
+#define KSEG2			0xc0000000
+#define KSEG3			0xe0000000
+
+#define CKUSEG			0x00000000
+#define CKSEG0			0x60000000
+#define CKSEG1			0xa0000000
+#define CKSEG2			_AC(0xc0000000, UL)
+#define CKSEG3			0xe0000000
+
+/* Range from 0x6000.0000 ~ 0x9FFF.FFFF for KSEG0 */
+#define KSEGX(a)		(((_ACAST32_(a)) < 0xA0000000) ?	\
+					KSEG0 : ((_ACAST32_(a)) & 0xE0000000))
+
+#define INDEX_BASE		CKSEG0
+#define MAP_BASE		CKSEG2
+#define VMALLOC_END		(MAP_BASE + _AC(0x20000000, UL) - 2 * PAGE_SIZE)
+
+#define IO_SIZE			_AC(0x10000000 - MIPS_RESERVED_MEM_SIZE, UL)
+#define IO_SHIFT		_AC(0x10000000, UL)
+#ifdef CONFIG_DMA_COHERENT
+#define CPHYSADDR(a)		((_ACAST32_(a))  - PAGE_OFFSET +	\
+				 PHYS_OFFSET + 0xA0000000) /* 1GB limitation */
+#else
+#define CPHYSADDR(a)		((_ACAST32_(a)) - PAGE_OFFSET + PHYS_OFFSET)
+#endif
+#define RPHYSADDR(a)		((_ACAST32_(a)) & 0x1fffffff)
+
+#define LEGACY_KSEC0(a)		(RPHYSADDR(a) | 0x80000000)
+
+/* DRAM one */
+#define CKSEG0ADDR(a)		((_ACAST32_(a)))
+
+/* IO space one */
+#define CKSEG1ADDR(a)		(RPHYSADDR(a) | KSEG1)
+
+/*
+ * Map an address to a certain kernel segment
+ */
+/* DRAM one */
+#define KSEG0ADDR(a)		(CKSEG0ADDR(a))
+
+/* IO space one */
+#define KSEG1ADDR(a)		(RPHYSADDR(a) | KSEG1)
+#define __pa_symbol(x)		__pa(x)
+
+#elif defined(CONFIG_LTQ_EVA_LEGACY)
+
+#define MIPS_RESERVED_MEM_SIZE	0
+
+#define PHYS_START		_AC(0x20000000, UL)
+#define VIRT_START		_AC(0x80000000, UL)
+
+#define PAGE_OFFSET		(VIRT_START + MIPS_RESERVED_MEM_SIZE)
+#define PHYS_OFFSET		(PHYS_START + MIPS_RESERVED_MEM_SIZE)
+
+/* No Highmem Support */
+#define HIGHMEM_START		_AC(0xffff0000, UL)
+
+#define UNCAC_BASE		(_AC(0xa0000000, UL) + MIPS_RESERVED_MEM_SIZE)
+#define CAC_BASE		(_AC(0x80000000, UL) + MIPS_RESERVED_MEM_SIZE)
+#define IO_BASE			UNCAC_BASE
+
+#define KSEG
+#define KUSEG			0x00000000
+#define KSEG0			0x80000000
+#define KSEG1			0xa0000000
+#define KSEG2			0xc0000000
+#define KSEG3			0xe0000000
+
+#define CKUSEG			0x00000000
+#define CKSEG0			0x80000000
+#define CKSEG1			0xa0000000
+#define CKSEG2			_AC(0xc0000000, UL)
+#define CKSEG3			0xe0000000
+
+#define KSEGX(a)		((_ACAST32_(a)) & 0xe0000000)
+
+#define INDEX_BASE		CKSEG0
+#define MAP_BASE		CKSEG2
+#define VMALLOC_END		(MAP_BASE + _AC(0x20000000, UL) - 2 * PAGE_SIZE)
+
+#define IO_SIZE			_AC(0x10000000, UL)
+#define IO_SHIFT		_AC(0x10000000, UL)
+#define CPHYSADDR(a)		((_ACAST32_(a)) - PAGE_OFFSET + PHYS_OFFSET)
+#define RPHYSADDR(a)		((_ACAST32_(a)) & 0x1fffffff)
+#define LEGACY_KSEC0(a)		(RPHYSADDR(a) | KSEG0)
+
+/* DRAM one */
+#define CKSEG0ADDR(a)		((_ACAST32_(a)))
+
+/* IO space one */
+#define CKSEG1ADDR(a)		(RPHYSADDR(a) | KSEG1)
+
+/*
+ * Map an address to a certain kernel segment
+ */
+/* DRAM one */
+#define KSEG0ADDR(a)		(CKSEG0ADDR(a))
+
+/* IO space one */
+#define KSEG1ADDR(a)		(RPHYSADDR(a) | KSEG1)
+#define __pa_symbol(x)		__pa(x)
+#else
+#error "Wrong EVA mode chosen"
+#endif
+
+#else /* Legacy  */
+
+#define MIPS_RESERVED_MEM_SIZE	0
+
+#define PHYS_START		_AC(0x20000000, UL)
+#define VIRT_START		_AC(0x80000000, UL)
+
+#define PAGE_OFFSET		(VIRT_START + MIPS_RESERVED_MEM_SIZE)
+#define PHYS_OFFSET		(PHYS_START + MIPS_RESERVED_MEM_SIZE)
+
+/* No Highmem Support */
+#define HIGHMEM_START		_AC(0xffff0000, UL)
+
+#define UNCAC_BASE		(_AC(0xa0000000, UL) + MIPS_RESERVED_MEM_SIZE)
+#define CAC_BASE		(_AC(0x80000000, UL) + MIPS_RESERVED_MEM_SIZE)
+
+#define IO_BASE			UNCAC_BASE
+
+#define KSEG
+#define KUSEG			0x00000000
+#define KSEG0			0x80000000
+#define KSEG1			0xa0000000
+#define KSEG2			0xc0000000
+#define KSEG3			0xe0000000
+
+#define CKUSEG			0x00000000
+#define CKSEG0			0x80000000
+#define CKSEG1			0xa0000000
+#define CKSEG2			_AC(0xc0000000, UL)
+#define CKSEG3			0xe0000000
+
+#define KSEGX(a)		((_ACAST32_(a)) & 0xe0000000)
+
+#define INDEX_BASE		CKSEG0
+#define MAP_BASE		CKSEG2
+#define VMALLOC_END		(MAP_BASE + _AC(0x20000000, UL) - 2 * PAGE_SIZE)
+
+#define IO_SIZE			_AC(0x10000000, UL)
+#define IO_SHIFT		_AC(0x10000000, UL)
+#define CPHYSADDR(a)		((_ACAST32_(a)) - PAGE_OFFSET + PHYS_OFFSET)
+#define RPHYSADDR(a)		((_ACAST32_(a)) & 0x1fffffff)
+#define LEGACY_KSEC0(a)		(RPHYSADDR(a) | KSEG0)
+
+/* DRAM one */
+#define CKSEG0ADDR(a)		((_ACAST32_(a)))
+
+/* IO space one */
+#define CKSEG1ADDR(a)		(RPHYSADDR(a) | KSEG1)
+
+/*
+ * Map an address to a certain kernel segment
+ */
+/* DRAM one */
+#define KSEG0ADDR(a)		(CKSEG0ADDR(a))
+
+/* IO space one */
+#define KSEG1ADDR(a)		(RPHYSADDR(a) | KSEG1)
+#define __pa_symbol(x)		__pa(x)
+#endif /* CONFIG_EVA */
+#endif /* CONFIG_SOC_GRX500 */
+
+#ifdef CONFIG_SOC_TYPE_GRX500_TEP
+
+/* skip first 128 MB DDR */
+#if defined(CONFIG_SOC_FALCONMX_BOOTCORE) && defined(CONFIG_USE_EMULATOR)
+#define MIPS_RESERVED_MEM_SIZE	_AC(0x06000000, UL)
+#else
+#define MIPS_RESERVED_MEM_SIZE	_AC(0x08000000, UL)
+#endif
+
+#define PHYS_START		_AC(0x00000000, UL)
+#define VIRT_START		_AC(0x80000000, UL)
+
+#define PAGE_OFFSET		(VIRT_START + MIPS_RESERVED_MEM_SIZE)
+#define PHYS_OFFSET		(PHYS_START + MIPS_RESERVED_MEM_SIZE)
+
+/* No Highmem Support */
+#define HIGHMEM_START		_AC(0xffff0000, UL)
+
+#define UNCAC_BASE		(_AC(0xa0000000, UL) + MIPS_RESERVED_MEM_SIZE)
+#define CAC_BASE		(_AC(0x80000000, UL) + MIPS_RESERVED_MEM_SIZE)
+
+#define IO_BASE			UNCAC_BASE
+
+#endif /* CONFIG_SOC_TYPE_GRX500_TEP */
+
+#include <asm/mach-generic/spaces.h>
+
+#endif /* __ASM_MALTA_SPACES_H */
diff --git a/arch/mips/include/asm/mach-lantiq/war.h b/arch/mips/include/asm/mach-lantiq/war.h
new file mode 100755
index 000000000000..358ca979c1bd
--- /dev/null
+++ b/arch/mips/include/asm/mach-lantiq/war.h
@@ -0,0 +1,23 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ */
+#ifndef __ASM_MIPS_MACH_LANTIQ_WAR_H
+#define __ASM_MIPS_MACH_LANTIQ_WAR_H
+
+#define R4600_V1_INDEX_ICACHEOP_WAR	0
+#define R4600_V1_HIT_CACHEOP_WAR	0
+#define R4600_V2_HIT_CACHEOP_WAR	0
+#define R5432_CP0_INTERRUPT_WAR		0
+#define BCM1250_M3_WAR			0
+#define SIBYTE_1956_WAR			0
+#define MIPS4K_ICACHE_REFILL_WAR	0
+#define MIPS_CACHE_SYNC_WAR		0
+#define TX49XX_ICACHE_INDEX_INV_WAR	0
+#define ICACHE_REFILLS_WORKAROUND_WAR	0
+#define R10000_LLSC_WAR			0
+#define MIPS34K_MISSED_ITLB_WAR		0
+
+#endif
diff --git a/arch/mips/lantiq/Kconfig b/arch/mips/lantiq/Kconfig
old mode 100644
new mode 100755
index 177769dbb0e8..aa24ce56c4fb
--- a/arch/mips/lantiq/Kconfig
+++ b/arch/mips/lantiq/Kconfig
@@ -5,6 +5,14 @@ config SOC_TYPE_XWAY
 	select PINCTRL_XWAY
 	default n
 
+config SOC_TYPE_GRX500_TEP
+	bool 
+	select CEVT_R4K
+	select CSRC_R4K
+	select CPU_MIPSR2_IRQ_VI
+	select CPU_MIPSR2_IRQ_EI
+	default n
+
 choice
 	prompt "SoC Type"
 	default SOC_XWAY
@@ -16,13 +24,79 @@ config SOC_AMAZON_SE
 config SOC_XWAY
 	bool "XWAY"
 	select SOC_TYPE_XWAY
+	select CEVT_R4K
+	select CSRC_R4K
 	select HW_HAS_PCI
+	select ARCH_SUPPORTS_MSI
+	select ARCH_HAS_RESET_CONTROLLER
+	select RESET_CONTROLLER
 
 config SOC_FALCON
 	bool "FALCON"
+	select CEVT_R4K
+	select CSRC_R4K
 	select PINCTRL_FALCON
 
+config SOC_GRX500
+	bool "GRX500"
+	select GENERIC_ISA_DMA
+	select MIPS_GIC
+	select INTEL_GPTC
+	select MIPS_CPU_SCACHE
+	select SYS_HAS_CPU_MIPS32_R3_5
+	select CPU_MIPSR2_IRQ_VI
+	select CPU_MIPSR2_IRQ_EI
+	select SYS_SUPPORTS_MIPS_CPS
+	select SYS_SUPPORTS_MIPS_CMP
+	select SYS_SUPPORTS_ZBOOT
+	select SERIAL_EARLYCON
+	select HW_HAS_PCI
+	select ARCH_SUPPORTS_MSI
+	select ARCH_HAS_RESET_CONTROLLER
+	select RESET_CONTROLLER
+	select COMMON_CLK
+	select CPU_SUPPORTS_CPUFREQ
+	select MIPS_EXTERNAL_TIMER
+	select PCI_DRIVERS_GENERIC
+config SOC_GRX500_BOOTCORE
+	bool "GRX500_BOOTCORE"
+	select SOC_TYPE_GRX500_TEP
+
+config SOC_FALCONMX_BOOTCORE
+	bool "FALCONMX_BOOTCORE"
+	select SOC_TYPE_GRX500_TEP
+
+endchoice
+
+config XBAR_LE
+	bool "Crossbar Word Level Little Endian Support in Big Endian CPU"
+	depends on SOC_GRX500 && CPU_BIG_ENDIAN
+	help
+	  Falcon Mountain has complicated endianness for different datawidth.
+	  To avoid too many confusion, XBAR word level little endian is added
+	  So that one clean solution can be provided.
+	  It will be only used if Falcon Mountain is chosen.
+
+menu "Emulator Support"
+config USE_EMULATOR
+	bool "Support for FPGA emulation platform"
+	default no
+	help
+	Use FPGA emulator as platform
+
+choice
+	prompt "Emulator"
+	depends on USE_EMULATOR
+	default USE_PALLADIUM
+
+config USE_HAPS
+	bool "HAPS"
+
+config USE_PALLADIUM
+	bool "Palladium"
+
 endchoice
+endmenu
 
 choice
 	prompt "Built-in device tree"
@@ -41,11 +115,47 @@ config DT_EASY50712
 	bool "Easy50712"
 	depends on SOC_XWAY
 	select BUILTIN_DTB
+
+config DT_ANYWAN
+	bool "GRX500 Anywan Board"
+	select BUILTIN_DTB
+	depends on SOC_GRX500
+
+config DT_EASY350550_BOOTCORE
+	bool "Easy350550 Bootcore"
+	select BUILTIN_DTB
+	depends on SOC_GRX500_BOOTCORE
+
+config DT_FALCONMXHAPS_BOOTCORE
+	bool "FALCONMX HAPS Bootcore"
+	select BUILTIN_DTB
+	depends on SOC_FALCONMX_BOOTCORE && USE_EMULATOR
+
+config DT_FALCONMX_SFU_BOOTCORE
+	bool "FALCONMX SFU Bootcore"
+	select BUILTIN_DTB
+	depends on SOC_FALCONMX_BOOTCORE
+
+
+
 endchoice
 
-config PCI_LANTIQ
-	bool "PCI Support"
-	depends on SOC_XWAY && PCI
+choice
+	prompt "TOS_SIZE"
+	depends on SOC_TYPE_GRX500_TEP
+	default TOS_SIZE_32M
+config TOS_SIZE_16M
+	bool "16M tos size"
+
+config TOS_SIZE_32M
+	bool "32M tos size"
+
+config TOS_SIZE_64M
+	bool "64M tos size"
+
+config TOS_SIZE_128M
+	bool "128M tos size"
+endchoice
 
 config XRX200_PHY_FW
 	bool "XRX200 PHY firmware loader"
diff --git a/arch/mips/lantiq/Makefile b/arch/mips/lantiq/Makefile
old mode 100644
new mode 100755
index 2718652e7466..cd1e5ba96e2e
--- a/arch/mips/lantiq/Makefile
+++ b/arch/mips/lantiq/Makefile
@@ -4,9 +4,13 @@
 # under the terms of the GNU General Public License version 2 as published
 # by the Free Software Foundation.
 
-obj-y := irq.o clk.o prom.o
-
+obj-$(CONFIG_SOC_XWAY) += prom.o irq.o clk.o
 obj-$(CONFIG_EARLY_PRINTK) += early_printk.o
 
 obj-$(CONFIG_SOC_TYPE_XWAY) += xway/
 obj-$(CONFIG_SOC_FALCON) += falcon/
+obj-$(CONFIG_SOC_GRX500) += grx500/
+obj-$(CONFIG_SOC_TYPE_GRX500_TEP) += grx500_bootcore/
+obj-y$(CONFIG_LTQ_VMB)+=lantiq-amon.o
+obj-$(CONFIG_LTQ_VMB)+=lantiq-vmb.o
+obj-$(CONFIG_LTQ_ITC)+=lantiq-itc.o
diff --git a/arch/mips/lantiq/Platform b/arch/mips/lantiq/Platform
old mode 100644
new mode 100755
index b3ec49838fd7..f4c09e967d5c
--- a/arch/mips/lantiq/Platform
+++ b/arch/mips/lantiq/Platform
@@ -4,6 +4,28 @@
 
 platform-$(CONFIG_LANTIQ)	+= lantiq/
 cflags-$(CONFIG_LANTIQ)		+= -I$(srctree)/arch/mips/include/asm/mach-lantiq
-load-$(CONFIG_LANTIQ)		= 0xffffffff80002000
+ifdef CONFIG_EVA
+	ifdef CONFIG_LTQ_EVA_2GB
+	load-$(CONFIG_LANTIQ)		= 0xffffffff20020000
+	endif
+	ifdef CONFIG_LTQ_EVA_1GB
+	load-$(CONFIG_LANTIQ)		= 0xffffffff60020000
+	endif
+	ifdef CONFIG_LTQ_EVA_LEGACY
+	load-$(CONFIG_LANTIQ)   = 0xffffffff80020000
+	endif
+else
+	ifdef CONFIG_SOC_TYPE_GRX500_TEP
+		ifdef CONFIG_BOOTCORE_LOAD_ADDR
+			load-$(CONFIG_LANTIQ)           = $(CONFIG_BOOTCORE_LOAD_ADDR)
+		else
+			load-$(CONFIG_LANTIQ)           = 0xffffffff88000000
+		endif
+	else
+		load-$(CONFIG_LANTIQ)   = 0xffffffff80020000
+	endif
+endif
 cflags-$(CONFIG_SOC_TYPE_XWAY)	+= -I$(srctree)/arch/mips/include/asm/mach-lantiq/xway
 cflags-$(CONFIG_SOC_FALCON)	+= -I$(srctree)/arch/mips/include/asm/mach-lantiq/falcon
+cflags-$(CONFIG_SOC_GRX500)     += -I$(srctree)/arch/mips/include/asm/mach-lantiq/grx500
+cflags-$(CONFIG_SOC_TYPE_GRX500_TEP)           += -I$(srctree)/arch/mips/include/asm/mach-lantiq/grx500
diff --git a/arch/mips/lantiq/clk.h b/arch/mips/lantiq/clk.h
index e806e048ffc2..e3bd6c81cefd 100644
--- a/arch/mips/lantiq/clk.h
+++ b/arch/mips/lantiq/clk.h
@@ -3,7 +3,8 @@
  *  under the terms of the GNU General Public License version 2 as published
  *  by the Free Software Foundation.
  *
- * Copyright (C) 2010 John Crispin <john@phrozen.org>
+ * Copyright (C) 2010 John Crispin <blogic@openwrt.org>
+ * Copyright (C) 2013 Lei Chuanhua <Chuanhua.lei@lantiq.com>
  */
 
 #ifndef _LTQ_CLK_H__
@@ -12,7 +13,9 @@
 #include <linux/clkdev.h>
 
 /* clock speeds */
+#define CLOCK_16M	16000000
 #define CLOCK_33M	33333333
+#define CLOCK_50M	50000000
 #define CLOCK_60M	60000000
 #define CLOCK_62_5M	62500000
 #define CLOCK_83M	83333333
@@ -43,6 +46,10 @@
 #define CLOCK_600M	600000000
 #define CLOCK_666M	666666666
 #define CLOCK_720M	720000000
+#define CLOCK_800M	800000000
+#define CLOCK_1000M	1000000000
+#define CLOCK_2000M	2000000000UL
+#define CLOCK_2400M	2400000000UL
 
 /* clock out speeds */
 #define CLOCK_32_768K	32768
@@ -57,6 +64,159 @@
 #define CLOCK_50M	50000000
 #define CLOCK_60M	60000000
 
+/* clock control register for legacy */
+#define CGU_IFCCR	0x0018
+#define CGU_IFCCR_VR9	0x0024
+/* system clock register for legacy */
+#define CGU_SYS		0x0010
+/* pci control register */
+#define CGU_PCICR	0x0034
+#define CGU_PCICR_VR9	0x0038
+/* ephy configuration register */
+#define CGU_EPHY	0x10
+
+/* Legacy PMU register for ar9, ase, danube */
+/* power control register */
+#define PMU_PWDCR	0x1C
+/* power status register */
+#define PMU_PWDSR	0x20
+/* power control register */
+#define PMU_PWDCR1	0x24
+/* power status register */
+#define PMU_PWDSR1	0x28
+/* power control register */
+#define PWDCR(x) ((x) ? (PMU_PWDCR1) : (PMU_PWDCR))
+/* power status register */
+#define PWDSR(x) ((x) ? (PMU_PWDSR1) : (PMU_PWDSR))
+
+
+/* PMU register for ar10 and grx390 */
+
+/* First register set */
+#define PMU_CLK_SR	0x20 /* status */
+#define PMU_CLK_CR_A	0x24 /* Enable */
+#define PMU_CLK_CR_B	0x28 /* Disable */
+/* Second register set */
+#define PMU_CLK_SR1	0x30 /* status */
+#define PMU_CLK_CR1_A	0x34 /* Enable */
+#define PMU_CLK_CR1_B	0x38 /* Disable */
+/* Third register set */
+#define PMU_ANA_SR	0x40 /* status */
+#define PMU_ANA_CR_A	0x44 /* Enable */
+#define PMU_ANA_CR_B	0x48 /* Disable */
+
+#define PWDCR_EN_XRX(x)		(pmu_clk_cr_a[(x)])
+#define PWDCR_DIS_XRX(x)	(pmu_clk_cr_b[(x)])
+#define PWDSR_XRX(x)		(pmu_clk_sr[(x)])
+
+/* clock gates that we can en/disable */
+#define PMU_USB0_P	BIT(0)
+#define PMU_ASE_SDIO	BIT(2) /* ASE special */
+#define PMU_PCI		BIT(4)
+#define PMU_DMA		BIT(5)
+#define PMU_USB0	BIT(6)
+#define PMU_ASC0	BIT(7)
+#define PMU_EPHY	BIT(7)	/* ase */
+#define PMU_USIF	BIT(7) /* from vr9 until grx390 */
+#define PMU_SPI		BIT(8)
+#define PMU_DFE		BIT(9)
+#define PMU_EBU		BIT(10)
+#define PMU_STP		BIT(11)
+#define PMU_GPT		BIT(12)
+#define PMU_AHBS	BIT(13) /* vr9 */
+#define PMU_FPI		BIT(14)
+#define PMU_AHBM	BIT(15)
+#define PMU_SDIO	BIT(16) /* danube, ar9, vr9 */
+#define PMU_ASC1	BIT(17)
+#define PMU_PPE_QSB	BIT(18)
+#define PMU_PPE_SLL01	BIT(19)
+#define PMU_DEU		BIT(20)
+#define PMU_PPE_TC	BIT(21)
+#define PMU_PPE_EMA	BIT(22)
+#define PMU_PPE_DPLUM	BIT(23)
+#define PMU_PPE_DP	BIT(23)
+#define PMU_PPE_DPLUS	BIT(24)
+#define PMU_USB1_P	BIT(26)
+#define PMU_USB1	BIT(27)
+#define PMU_SWITCH	BIT(28)
+#define PMU_PPE_TOP	BIT(29)
+#define PMU_GPHY	BIT(30)
+#define PMU_PCIE_CLK	BIT(31)
+
+#define PMU1_PCIE_PHY	BIT(0)	/* vr9-specific,moved in ar10/grx390 */
+#define PMU1_PCIE_CTL	BIT(1)
+#define PMU1_PCIE_PDI	BIT(4)
+#define PMU1_PCIE_MSI	BIT(5)
+#define PMU1_CKE	BIT(6)
+#define PMU1_PCIE1_CTL	BIT(17)
+#define PMU1_PCIE1_PDI	BIT(20)
+#define PMU1_PCIE1_MSI	BIT(21)
+#define PMU1_PCIE2_CTL	BIT(25)
+#define PMU1_PCIE2_PDI	BIT(26)
+#define PMU1_PCIE2_MSI	BIT(27)
+
+#define PMU_ANALOG_USB0_P	BIT(0)
+#define PMU_ANALOG_USB1_P	BIT(1)
+#define PMU_ANALOG_PCIE0_P	BIT(8)
+#define PMU_ANALOG_PCIE1_P	BIT(9)
+#define PMU_ANALOG_PCIE2_P	BIT(10)
+#define PMU_ANALOG_DSL_AFE	BIT(16)
+#define PMU_ANALOG_DCDC_2V5	BIT(17)
+#define PMU_ANALOG_DCDC_1VX	BIT(18)
+#define PMU_ANALOG_DCDC_1V0	BIT(19)
+
+#define pmu_w32(x, y)	ltq_w32((x), pmu_membase + (y))
+#define pmu_r32(x)	ltq_r32(pmu_membase + (x))
+
+#define XBAR_ALWAYS_LAST	0x430
+#define XBAR_FPI_BURST_EN	BIT(1)
+#define XBAR_AHB_BURST_EN	BIT(2)
+
+#define xbar_w32(x, y)	ltq_w32((x), ltq_xbar_membase + (y))
+#define xbar_r32(x)	ltq_r32(ltq_xbar_membase + (x))
+
+enum {
+	STATIC_CPU_CLK = 1,
+	STATIC_FPI_CLK,
+	STATIC_IO_CLK,
+	STATIC_PPE_CLK,
+	STATIC_NO_PARENT = 0xff,
+};
+
+struct vol_sg {/*voltage speed-grade*/
+	unsigned int slow[4];
+	unsigned int typ[4];
+	unsigned int fast[4];
+};
+
+struct clk_rates {
+	unsigned int cpu_freq;
+	unsigned int ddr_freq;
+	unsigned int cpu_clkm_sel;
+	unsigned int pll_clk1;
+	unsigned int pll_clk2;
+	unsigned int pll_clk4;
+	struct vol_sg core_vol350;
+	struct vol_sg core_vol550;
+};
+
+enum spd_class_t {
+	FAST0 = 0,
+	FAST1,
+	FAST2,
+	FAST3,
+	MEDIUM0,
+	MEDIUM1,
+	MEDIUM2,
+	MEDIUM3,
+	SLOW0,
+	SLOW1,
+	SLOW2,
+	SLOW3,
+	SLOW_DEFAULT,
+	UNDEF
+};
+
 struct clk {
 	struct clk_lookup cl;
 	unsigned long rate;
@@ -89,8 +249,27 @@ extern unsigned long ltq_ar10_cpu_hz(void);
 extern unsigned long ltq_ar10_fpi_hz(void);
 extern unsigned long ltq_ar10_pp32_hz(void);
 
-extern unsigned long ltq_grx390_cpu_hz(void);
-extern unsigned long ltq_grx390_fpi_hz(void);
-extern unsigned long ltq_grx390_pp32_hz(void);
+extern unsigned long ltq_grx500_cpu_hz(void);
+extern unsigned long ltq_grx500_fpi_hz(void);
+extern unsigned long ltq_grx500_pp32_hz(void);
+
+extern unsigned long ltq_grx500_cpu_hz(void);
+extern int ltq_grx500_set_cpu_hz(unsigned long cpu_freq);
+extern unsigned long ltq_grx500_cpu_vol(unsigned long *rate);
+
+extern unsigned long ltq_grx500_fpi_hz(void);
+extern int ltq_grx500_set_fpi_hz(unsigned long fpi_freq);
+
+extern unsigned long ltq_grx500_cbm_hz(void);
+extern int ltq_grx500_set_cbm_hz(unsigned long cbm_freq);
+
+extern unsigned long ltq_grx500_ngi_hz(void);
+extern int ltq_grx500_set_ngi_hz(unsigned long ngi_freq);
+
+extern unsigned long ltq_grx500_ddr_hz(void);
 
+extern unsigned long *ltq_get_avail_scaling_rates(int sel);
+extern unsigned long ltq_grx500_cpu_vol(unsigned long *rate);
+extern int ltq_grx500_get_speed_grade(void);
+extern void ltq_grx500_set_speed_grade(int spg);
 #endif
diff --git a/arch/mips/lantiq/lantiq-amon.c b/arch/mips/lantiq/lantiq-amon.c
new file mode 100644
index 000000000000..b212bde61bba
--- /dev/null
+++ b/arch/mips/lantiq/lantiq-amon.c
@@ -0,0 +1,81 @@
+/*
+ * Copyright (C) 2007  MIPS Technologies, Inc.
+ *	All rights reserved.
+
+ *  This program is free software; you can distribute it and/or modify it
+ *  under the terms of the GNU General Public License (Version 2) as
+ *  published by the Free Software Foundation.
+ *
+ *  This program is distributed in the hope it will be useful, but WITHOUT
+ *  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ *  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ *  for more details.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with this program; if not, write to the Free Software Foundation, Inc.,
+ *  59 Temple Place - Suite 330, Boston MA 02111-1307, USA.
+ *
+ * Arbitrary Monitor interface
+ */
+
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/smp.h>
+
+#include <asm/addrspace.h>
+#include <asm/mips-boards/launch.h>
+#include <asm/mipsmtregs.h>
+
+int amon_cpu_avail(int cpu)
+{
+	struct cpulaunch *launch = (struct cpulaunch *)CKSEG0ADDR(CPULAUNCH);
+
+	if (cpu < 0 || cpu >= NCPULAUNCH) {
+		pr_debug("avail: cpu%d is out of range\n", cpu);
+		return 0;
+	}
+
+	launch += cpu;
+	if (!(launch->flags & LAUNCH_FREADY)) {
+		pr_debug("avail: cpu%d is not ready\n", cpu);
+		return 0;
+	}
+	if (launch->flags & (LAUNCH_FGO|LAUNCH_FGONE)) {
+		pr_debug("avail: too late.. cpu%d is already gone\n", cpu);
+		return 0;
+	}
+
+	return 1;
+}
+
+void amon_cpu_start(int cpu,
+		    unsigned long pc, unsigned long sp,
+		    unsigned long gp, unsigned long a0)
+{
+	struct cpulaunch *launch =
+		(struct cpulaunch  *)CKSEG0ADDR(CPULAUNCH);
+
+	if (!amon_cpu_avail(cpu))
+		return;
+	if (cpu == smp_processor_id()) {
+		pr_debug("launch: I am cpu%d!\n", cpu);
+		return;
+	}
+	launch += cpu;
+
+	pr_debug("launch: starting cpu%d\n", cpu);
+
+	launch->pc = pc;
+	launch->gp = gp;
+	launch->sp = sp;
+	launch->a0 = a0;
+
+	smp_wmb();		/* Target must see parameters before go */
+	launch->flags |= LAUNCH_FGO;
+	smp_wmb();		/* Target must see go before we poll  */
+
+	while ((launch->flags & LAUNCH_FGONE) == 0)
+		;
+	smp_rmb();	/* Target will be updating flags soon */
+	pr_debug("launch: cpu%d gone!\n", cpu);
+}
diff --git a/arch/mips/lantiq/lantiq-itc.c b/arch/mips/lantiq/lantiq-itc.c
new file mode 100644
index 000000000000..2c85aa9d8549
--- /dev/null
+++ b/arch/mips/lantiq/lantiq-itc.c
@@ -0,0 +1,242 @@
+/*
+ *  This program is free software; you can redistribute it and/or modify it
+ *  under the terms of the GNU General Public License version 2 as published
+ *  by the Free Software Foundation.
+ *
+ *  Copyright (C) 2009~2015 Lantiq Deutschland GmbH
+ *  Copyright (C) 2016 Intel Corporation.
+ */
+
+#include <linux/init.h>
+#include <linux/string.h>
+#include <linux/kernel.h>
+#include <linux/cpu.h>
+#include <linux/sched.h>
+#include <asm/mipsregs.h>
+#include <asm/ltq_itc.h>
+
+
+uint32_t		*ITC_base;
+uint32_t		*ITC_Sem_base;
+uint32_t		*ITC_FIFO_base;
+uint32_t	itcn;
+
+int32_t itc_init(void)
+{
+	uint32_t	errctlreg, Config_ITC;
+	uint32_t	ITCAddressMap0, ITCAddressMap1;
+	uint32_t	*ITC_BlockNC;
+	uint32_t	*ITC_Cell_Sem;
+	uint32_t	*ITC_Cell_FIFO;
+	uint32_t	ITC_Cell_Sem_off;
+	uint32_t	i;
+
+/* Configure ITC Tags using Cache opts .*/
+/* Set ITC bit in ErrCtl register to enable Cache mode for ITC Tags */
+
+	errctlreg = read_c0_ecc();
+	Config_ITC = errctlreg | ERRCTL_ITC;
+	write_c0_ecc(Config_ITC);
+	mips_ihb();
+
+		/* Read reset-value ITC_Address_Map0 */
+	__asm__ volatile
+		("\
+			cache 5, 0($0);  \
+			ehb; \
+			"
+		);
+	pr_info("\nInitial ITC_Address_Map0 %08x\n", read_c0_dtaglo());
+
+	if (read_c0_dtaglo() & ITC_En) {
+		pr_info("ITC Memory is already initialised for Core %d at address %x !!!",
+			(smp_processor_id()/2), (read_c0_dtaglo() & 0xffff0000));
+
+		/* return ErrCtl to it previous state */
+			write_c0_ecc(errctlreg);
+		mips_ihb();
+
+		return 0;
+	}
+
+#ifdef DEBUG_ITC
+		/* Read reset-value ITC_Address_Map1 */
+	__asm__ volatile
+		("\
+			cache 5, 8($0);  \
+			ehb; \
+			"
+		);
+	pr_info("\ndef ITC_Address_Map1 %08x", read_c0_dtaglo());
+#endif
+
+/*
+ *configure Number of entries Address mask bits and
+ * Entry Grain in ITC tag index 8
+ */
+	ITCAddressMap1 = ((ITC_AddrMask << 10) | ITC_EntryGrain);
+
+	write_c0_dtaglo(ITCAddressMap1);
+
+	__asm__ volatile
+		("\
+			cache 9, 8($0);  \
+			ehb; \
+			"
+		);
+
+#ifdef DEBUG_ITC
+		/* Read new-value ITC_Address_Map1 */
+	__asm__ volatile
+		("\
+			cache 5, 8($0);  \
+			ehb; \
+			"
+		);
+	pr_info("\nnew ITC_Address_Map1 %08x", read_c0_dtaglo());
+#endif
+
+/*
+ *configure Base address and ITC_En (enable bit) in
+ *  ITC tag index 0 and Use physical address
+ */
+	ITC_BlockNC = (unsigned int *)((unsigned int)ITC_Block & 0x7fffffff);
+
+#ifdef DEBUG_ITC
+		/* Read reset-value ITC_Address_Map0 */
+	__asm__ volatile
+		("\
+			cache 5, 0($0);  \
+			ehb; \
+			"
+		);
+	pr_info("\ndef ITC_Address_Map0 %08x", read_c0_dtaglo());
+#endif
+
+	ITCAddressMap0 = ((unsigned int)ITC_BlockNC | ITC_En);
+	write_c0_dtaglo(ITCAddressMap0);
+
+	__asm__ volatile
+		("\
+			cache 9, 0($0); \
+			ehb; \
+			"
+		);
+
+#ifdef DEBUG_ITC
+		/* Read new-value ITC_Address_Map0 */
+	__asm__ volatile
+		("\
+			cache 5, 0($0); \
+			ehb; \
+			"
+		);
+	pr_info("\nnew ITC_Address_Map0 %08x", read_c0_dtaglo());
+#endif
+
+/* return ErrCtl to it previous state */
+	write_c0_ecc(errctlreg);
+	mips_ihb();
+
+/* Enable ITC Entry :  Use unmapped address */
+	ITC_BlockNC = (unsigned int *)((unsigned int)ITC_Block);
+
+/* Change to unmapped memory */
+	ITC_BlockNC = (unsigned int *)CKSEG1ADDR(ITC_BlockNC);
+
+	ITC_base = (unsigned int *)((unsigned int)ITC_BlockNC);
+	ITC_FIFO_base = (unsigned int *)((unsigned int)ITC_BlockNC);
+
+	ITC_Cell_Sem_off = 0;
+
+/*Use Control View to access Entry Tag*/
+
+	for (i = 0; i < ITC_FIFO_Entries; i++) {
+		ITC_Cell_FIFO = (unsigned int *)(((unsigned int)ITC_BlockNC + ITC_Cell_Sem_off) | ITC_BypassView);
+		*ITC_Cell_FIFO = 0;
+
+		ITC_Cell_FIFO = (unsigned int *)(((unsigned int)ITC_BlockNC + ITC_Cell_Sem_off) | ITC_ControlView);
+		*ITC_Cell_FIFO = ITC_E;
+
+		ITC_Cell_Sem_off = ITC_Cell_Sem_off + (128 * (0x1 << ITC_EntryGrain));
+	}
+
+	ITC_Sem_base = (unsigned int *)((unsigned int)ITC_BlockNC + ITC_Cell_Sem_off) ;
+
+/*
+ * For each SEM entries clear the content of Cell using ITC_BypassView
+ * otherwise default value seen in cell is 5.
+ * Set the ITC_ControlView to set ITC_E.
+ * Use ITC_PVSyncView to init the Sem Cell to 1 to unblock the first access
+ */
+
+	for (i = 0; i < ITC_SEM_Entries; i++) {
+		ITC_Cell_Sem = (unsigned int *)(((unsigned int)ITC_BlockNC + ITC_Cell_Sem_off) | ITC_BypassView);
+		*ITC_Cell_Sem = 0;
+
+		ITC_Cell_Sem = (unsigned int *)(((unsigned int)ITC_BlockNC + ITC_Cell_Sem_off) | ITC_ControlView);
+		*ITC_Cell_Sem = ITC_E;
+
+		ITC_Cell_Sem = (unsigned int *)(((unsigned int)ITC_BlockNC + ITC_Cell_Sem_off) | ITC_PVSyncView);
+		*ITC_Cell_Sem = 1;
+
+		ITC_Cell_Sem_off = ITC_Cell_Sem_off + (128 * (0x1 << ITC_EntryGrain));
+	}
+
+	pr_info("\n ITC Init done on Core %d on CPU %d !!!\n",
+		(smp_processor_id() / 2), smp_processor_id());
+
+	return 0;
+}
+early_initcall(itc_init);
+
+
+/*
+ * Read the ITC cell using P/V Sync View,
+ *If the Cell Contains 0, this will block
+ */
+
+void itc_sem_wait(uint8_t semId)
+{
+	uint32_t *ITC_Cell;
+	uint32_t Sem_off = 0;
+
+	Sem_off = semId * (128 * (0x1 << ITC_EntryGrain));
+
+	ITC_Cell = (uint32_t *)(((uint32_t)ITC_Sem_base + Sem_off) | ITC_PVSyncView);
+
+	itcn = *ITC_Cell;
+}
+EXPORT_SYMBOL(itc_sem_wait);
+
+/*
+ *Write to the ITC cell, to increment its value,
+ * This will unblock the lock.
+ */
+
+void itc_sem_post(uint8_t semId)
+{
+	uint32_t *ITC_Cell;
+	uint32_t Sem_off = 0;
+
+	Sem_off = semId * (128 * (0x1 << ITC_EntryGrain));
+
+	ITC_Cell = (uint32_t *)(((uint32_t)ITC_Sem_base + Sem_off) | ITC_PVSyncView);
+
+	*ITC_Cell = 1;
+}
+EXPORT_SYMBOL(itc_sem_post);
+
+uint32_t itc_sem_addr(uint8_t semId)
+{ /*copy from itc_sem_wait */
+	uint32_t Sem_off = 0;
+
+	Sem_off = semId * (128 * (0x1 << ITC_EntryGrain));
+
+	return ((uint32_t)ITC_Sem_base + Sem_off) | ITC_PVSyncView;
+}
+EXPORT_SYMBOL(itc_sem_addr);
+
+
+
+
diff --git a/arch/mips/lantiq/lantiq-vmb.c b/arch/mips/lantiq/lantiq-vmb.c
new file mode 100644
index 000000000000..63d92969245f
--- /dev/null
+++ b/arch/mips/lantiq/lantiq-vmb.c
@@ -0,0 +1,1635 @@
+/*
+ *  This program is free software; you can redistribute it and/or modify it
+ *  under the terms of the GNU General Public License version 2 as published
+ *  by the Free Software Foundation.
+ *
+ *  Copyright (C) 2009~2015 Lantiq Deutschland GmbH
+ *  Copyright (C) 2016 Intel Corporation.
+ */
+#include <linux/init.h>
+#include <linux/string.h>
+#include <linux/kernel.h>
+#include <linux/cpu.h>
+#include <linux/interrupt.h>
+#include <linux/module.h>
+#include <asm/cacheflush.h>
+#include <asm/smp-ops.h>
+#include <asm/traps.h>
+#include <asm/fw/fw.h>
+#include <linux/of.h>
+#include <linux/of_platform.h>
+#include <linux/of_irq.h>
+#include <linux/of_fdt.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+#include <linux/sched.h>
+#include <linux/bootmem.h>
+#include <asm/ltq_vmb.h>
+#include <linux/irqchip/mips-gic.h>
+
+#undef LINUX_SHM_DDR
+#undef LTQ_VMB_DEBUG
+
+/* Number of cores*/
+struct VMB_core_t core_t[MAX_CORE];
+static DEFINE_SPINLOCK(vmb_lock);
+static int g_fw_vmb_irq[MAX_CPU];
+static int g_fw_vmb_hwirq[MAX_CPU];
+static int g_cpu_vmb_irq[MAX_CPU];
+/* ITC SemIDs, Commom to all cores */
+struct VMB_itc_t itc_t[MAX_ITC_SEMID];
+static struct proc_dir_entry *vmb_proc, *vmb_proc_dir;
+
+void vmb_itc_sem_free_pcpu(int8_t cpu)
+{
+	int i;
+
+	for (i = 0; i < MAX_ITC_SEMID; i++) {
+		if (itc_t[i].cpu_id == cpu)
+			vmb_itc_sem_free(i);
+	}
+}
+
+void vmb_itc_sem_free(int8_t semID)
+{
+	itc_t[semID].itc_status &= ~ITC_ACTIVE;
+	itc_t[semID].itc_status |= ITC_INACTIVE;
+	itc_t[semID].cpu_id = -1;
+	itc_t[semID].tc_id = -1;
+}
+EXPORT_SYMBOL(vmb_itc_sem_free);
+
+/* API to get the memory DDR for strutures VMB_fw_msg_t/struct FW_vmb_msg_t */
+void *VMB_get_msg_addr(int cpu, int direction)
+{
+	void *msg_t =  (void *)(CPU_LAUNCH);
+
+	/* VMB --> FW : VMB_fw_msg_t structure */
+	if (direction == 0)
+		msg_t = msg_t + (vmb_msg_size * cpu) +
+			sizeof(struct FW_vmb_msg_t);
+	else
+		msg_t = msg_t + (vmb_msg_size * cpu);
+
+	return msg_t;
+}
+EXPORT_SYMBOL(VMB_get_msg_addr);
+
+static irqreturn_t fw_vmb_ipi1_hdlr(int irq, void *ptr)
+{
+	int cpu, cid = 0, vid = 0;
+	struct VMB_vpe_t *vpet_t;
+	struct FW_vmb_msg_t *fw_t;
+
+	pr_info("[%s]:[%d] CPU = %d irq = %d\n", __func__, __LINE__,
+		smp_processor_id(), (irq - MIPS_GIC_IRQ_BASE));
+
+	/* This handler is from FW_VMB_IPI1 so called from CPU1 */
+	cpu = 1;
+	cid = which_core(cpu);
+	vid = vpe_in_core(cpu);
+
+	pr_info("[%s]:[%d] cid = %d, vid = %d\n", __func__, __LINE__, cid, vid);
+	vpet_t = &core_t[cid].vpe_t[vid];
+	fw_t = (struct FW_vmb_msg_t *)VMB_get_msg_addr(cpu, 1);
+
+	/*
+	 * May lead to deadlock in case of FW_RESET as spinlock for
+	 * that VPE is taken and then wait_event_called thus
+	 *  commenting the locks if this doesnt work then need
+	 * to use IBL_IN_ handler to wakeup this MPE handler
+	 */
+
+	memcpy(&vpet_t->fw_vmb, fw_t, sizeof(struct FW_vmb_msg_t));
+	/* W.r.t FW copy the DDR section to Internal DB */
+	smp_rmb();
+
+	if (vpet_t->fw_vmb.status == FW_RESET) {
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+
+		pr_info("[%s]:[%d], cpu = %d\n", __func__, __LINE__, cpu);
+		if (vpet_t->vmb_callback_fn && vpet_t->vmb_callback_fn != NULL)
+			vpet_t->vmb_callback_fn(vpet_t->fw_vmb.status);
+
+/* Be cautious:TEST Throughly, VPE will go for a reset in deactivated STATE*/
+		vpet_t->bl_status = IBL_INACTIVE;
+
+		goto res1_ext;
+
+	} else if (vpet_t->fw_vmb.status == IBL_IN_WAIT) {
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+
+		vpet_t->bl_status = IBL_ACTIVE;
+		pr_info("[%s]:[%d], cpu = %d\n", __func__, __LINE__, cpu);
+	}
+
+	/* Clear DDR section */
+	if (vpet_t->fw_vmb.status == (uint32_t)FW_VMB_ACK)
+		pr_info("[%s]:[%d]\n", __func__, __LINE__);
+
+	pr_info("[%s]:[%d]\n", __func__, __LINE__);
+	pr_info("  vmb_wq = %p\n",  &vpet_t->v_wq.vmb_wq);
+
+	/* Set the wakeup_vpe and wakeup the waitqueue */
+	vpet_t->v_wq.wakeup_vpe = 1;
+	wake_up_interruptible(&vpet_t->v_wq.vmb_wq);
+
+res1_ext:
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t fw_vmb_ipi2_hdlr(int irq, void *ptr)
+{
+	int cpu, cid = 0, vid = 0;
+	struct VMB_vpe_t *vpet_t;
+	struct FW_vmb_msg_t *fw_t;
+
+	/* This handler is from FW_VMB_IPI2 so called from CPU2 */
+	cpu = 2;
+	cid = which_core(cpu);
+	vid = vpe_in_core(cpu);
+
+	vpet_t = &core_t[cid].vpe_t[vid];
+	fw_t = (struct FW_vmb_msg_t *)VMB_get_msg_addr(cpu, 1);
+
+	/*
+	 * May lead to deadlock in case of FW_RESET as spinlock
+	 * for that VPE is taken and then wait_event_called thus
+	 * commenting the locks if this doesnt work then need to use
+	 * IBL_IN_ handler to wakeup this MPE handler
+	 */
+
+	memcpy(&vpet_t->fw_vmb, fw_t, sizeof(struct FW_vmb_msg_t));
+	/* W.r.t FW copy the DDR section to Internal DB */
+	smp_rmb();
+
+	if (vpet_t->fw_vmb.status == FW_RESET) {
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+
+		pr_info("[%s]:[%d], cpu = %d\n", __func__, __LINE__, cpu);
+		if (vpet_t->vmb_callback_fn && vpet_t->vmb_callback_fn != NULL)
+			vpet_t->vmb_callback_fn(vpet_t->fw_vmb.status);
+
+/* Be cautious:TEST Throughly, VPE will go a reset and in deactivated STATE*/
+		vpet_t->bl_status = IBL_INACTIVE;
+
+		goto res2_ext;
+
+	} else if (vpet_t->fw_vmb.status == IBL_IN_WAIT) {
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+		vpet_t->bl_status = IBL_ACTIVE;
+		pr_info("[%s]:[%d], cpu = %d\n", __func__, __LINE__, cpu);
+	}
+
+	/* Set the wakeup_vpe and wakeup the waitqueue */
+	vpet_t->v_wq.wakeup_vpe = 1;
+	wake_up_interruptible(&vpet_t->v_wq.vmb_wq);
+
+res2_ext:
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t fw_vmb_ipi3_hdlr(int irq, void *ptr)
+{
+	int cpu, cid = 0, vid = 0;
+	struct VMB_vpe_t *vpet_t;
+	struct FW_vmb_msg_t *fw_t;
+
+	pr_info("[%s]:[%d] CPU = %d irq = %d\n", __func__,
+		__LINE__, smp_processor_id(), (irq - MIPS_GIC_IRQ_BASE));
+
+	/* This handler is from FW_VMB_IPI3 so called from CPU3 */
+	cpu = 3;
+	cid = which_core(cpu);
+	vid = vpe_in_core(cpu);
+
+	pr_info("[%s]:[%d] cid = %d, vid = %d\n",
+		__func__, __LINE__, cid, vid);
+	vpet_t = &core_t[cid].vpe_t[vid];
+	fw_t = (struct FW_vmb_msg_t *)VMB_get_msg_addr(cpu, 1);
+
+	/*
+	 * May lead to deadlock in case of FW_RESET as spinlock
+	 * for that VPE is taken and then wait_event_called thus
+	 * commenting the locks if this doesnt work then need to use
+	 * BL_IN_ handler to wakeup this MPE handler
+	 */
+
+	memcpy(&vpet_t->fw_vmb, fw_t, sizeof(struct FW_vmb_msg_t));
+	/* W.r.t FW copy the DDR section to Internal DB */
+	smp_rmb();
+
+	if (vpet_t->fw_vmb.status == FW_RESET) {
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+
+		pr_info("[%s]:[%d], cpu = %d\n", __func__, __LINE__, cpu);
+		if (vpet_t->vmb_callback_fn && vpet_t->vmb_callback_fn != NULL)
+			vpet_t->vmb_callback_fn(vpet_t->fw_vmb.status);
+
+/* Be cautious:TEST Throughly, VPE will go for a reset and deactivated STATE*/
+		vpet_t->bl_status = IBL_INACTIVE;
+
+		goto res3_ext;
+
+	} else if (vpet_t->fw_vmb.status == IBL_IN_WAIT) {
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+
+		vpet_t->bl_status = IBL_ACTIVE;
+		pr_info("[%s]:[%d], cpu = %d\n", __func__, __LINE__, cpu);
+	}
+
+	if (vpet_t->fw_vmb.status == (uint32_t)FW_VMB_ACK)
+		pr_info("[%s]:[%d]\n", __func__, __LINE__);
+
+	pr_info("[%s]:[%d]\n", __func__, __LINE__);
+	pr_info("  vmb_wq = %p\n",  &vpet_t->v_wq.vmb_wq);
+
+	/* Set the wakeup_vpe and wakeup the waitqueue */
+	vpet_t->v_wq.wakeup_vpe = 1;
+	wake_up_interruptible(&vpet_t->v_wq.vmb_wq);
+
+res3_ext:
+	return IRQ_HANDLED;
+}
+
+/*
+ * Update a dummy vmb_msg_t->status in DB as this is for Linux
+ *  SMP which doesn't update DDR in Linux --> VMB direction
+ */
+static int linux_cpu_ipi_update(unsigned long cpu)
+{
+	unsigned long flags;
+	int c_id = which_core(cpu);
+	int v_id = vpe_in_core(cpu);
+	struct VMB_vpe_t *vpet_t = &core_t[c_id].vpe_t[v_id];
+
+#ifdef LINUX_SHM_DDR
+	struct FW_vmb_msg_t *fw_t = (struct FW_vmb_msg_t *)VMB_get_msg_addr(cpu, 1);
+
+	spin_lock_irqsave(&vpet_t->vpe_lock, flags);
+
+	memcpy(&vpet_t->fw_vmb, fw_t, sizeof(struct FW_vmb_msg_t));
+	/* W.r.t FW copy the DDR section to Internal DB */
+	smp_rmb();
+
+	/* Clear DDR section */
+	memset(fw_t, 0, sizeof(struct FW_vmb_msg_t));
+
+	/* Set the wakeup_vpe and wakeup the waitqueue */
+	vpet_t->v_wq.wakeup_vpe = 1;
+	wake_up_interruptible(&vpet_t->v_wq.vmb_wq);
+
+	/* unlock */
+	spin_unlock_irqrestore(&vpet_t->vpe_lock, flags);
+#else
+	spin_lock_irqsave(&vpet_t->vpe_lock, flags);
+
+	vpet_t->fw_vmb.status = (uint32_t)FW_VMB_ACK;
+	vpet_t->fw_vmb.priv_info = 0;
+
+	/* Set the wakeup_vpe and wakeup the waitqueue */
+	vpet_t->v_wq.wakeup_vpe = 1;
+	wake_up_interruptible(&vpet_t->v_wq.vmb_wq);
+
+	/* unlock */
+	spin_unlock_irqrestore(&vpet_t->vpe_lock, flags);
+#endif
+	return 0;
+}
+
+static int __init vmb_cpu_active(unsigned int hcpu)
+{
+	pr_info("[%s]:[%d], cpu = %ld\n", __func__, __LINE__, (long)hcpu);
+	if (hcpu != 0)
+		linux_cpu_ipi_update((unsigned long)hcpu);
+
+	return 0;
+}
+
+/* Lock --> Access and Copy DDR --> check for status and update Internal DB */
+static void vmb_check_IBL_fw_msg(void)
+{
+	int i, j;
+	struct VMB_core_t *ct = core_t;
+	struct FW_vmb_msg_t *fw_msg_t;
+
+	for (i = 0; i < MAX_CORE; i++) {
+		struct VMB_vpe_t *vpet = ct[i].vpe_t;
+		int cpu;
+
+		for (j = 0; j < MAX_VPE; j++) {
+			if ((i == 0) && (j == 0))
+				continue;
+
+			spin_lock(&vpet[j].vpe_lock);
+			cpu = get_cpu_id(i, j);
+			pr_info("[%s]:[%d] vpet[j].bl_status = %d, cpu = %d\n",
+				__func__, __LINE__, vpet[j].bl_status, cpu);
+			fw_msg_t = (struct FW_vmb_msg_t *)VMB_get_msg_addr(cpu, 1);
+			memcpy(&vpet[j].fw_vmb, fw_msg_t,
+			       sizeof(struct FW_vmb_msg_t));
+			if (vpet[j].fw_vmb.status == IBL_IN_WAIT)
+				vpet[j].bl_status = IBL_ACTIVE;
+			else
+				vpet[j].bl_status = IBL_INACTIVE;
+			spin_unlock(&vpet[j].vpe_lock);
+		}
+	}
+}
+
+static int update_DB_from_DT(struct VMB_vpe_t *vt)
+{
+	struct device_node *np;
+	char str1[16], *name;
+	int ret;
+	struct resource irqres;
+
+	memset(str1, '\0', sizeof(str1));
+	sprintf(str1, "%s%d", "/cpus/cpu@", vt->cpu_id);
+
+	np = of_find_node_by_path(str1);
+	if (!np)
+		return -ENODEV;
+
+	ret = of_property_read_string_index(np,
+				"default-OS", 0, (const char **)&name);
+	if (ret < 0 && ret != -EINVAL) {
+		pr_info("ERROR : Property could be read from DT\n");
+		return ret;
+	}
+
+	strncpy(vt->name, name, sizeof(vt->name));
+	vt->cpu_status |= CPU_BOOTUP_DT;
+
+	pr_info("[%s]:[%d], cpuid = %d name = %s\n",
+		__func__, __LINE__, vt->cpu_id, vt->name);
+
+	/* for CPU 0, no need to get IRQ for VMB */
+	if (vt->cpu_id == 0)
+		return 0;
+
+	ret = of_irq_to_resource_table(np, &irqres, 1);
+	if (ret != 1) {
+		pr_info("failed to get irq for vmb since ret = %d\n", ret);
+		return -ENODEV;
+	}
+	/* get the interrupt numbers from DT */
+	g_fw_vmb_irq[vt->cpu_id] = irqres.start;
+
+	ret = of_property_read_u32_index(np, "interrupts", 1,
+					 &g_fw_vmb_hwirq[vt->cpu_id]);
+
+	if (ret < 0) {
+		pr_info("failed to get hwirq for vmb since ret = %d\n", ret);
+		return -ENODEV;
+	}
+
+	ret = of_property_read_u32(np, "vmb-fw-ipi",
+				   &g_cpu_vmb_irq[vt->cpu_id]);
+	if (ret < 0 && ret != -EINVAL) {
+		pr_info("ERROR : Property could be read from DT\n");
+		return ret;
+	}
+
+	return 0;
+}
+
+static void initialise_vmb_DB(void)
+{
+	struct VMB_core_t *coret = core_t;
+	int i, j;
+
+	coret[0].active |= CORE_ACTIVE;
+
+	for (i = 0; i < MAX_CORE; i++) {
+		struct VMB_tc_t *tct = coret[i].tc_t;
+		struct VMB_vpe_t *vpet = coret[i].vpe_t;
+		struct VMB_yr_t *yrit = coret[i].yr_t;
+
+		for (j = 0; j < MAX_VPE; j++) {
+			vpet[j].vpemt_grp = 0;
+			vpet[j].core_id = (((i * 2) + j)/2);
+			spin_lock_init(&vpet[j].vpe_lock);
+			init_waitqueue_head(&vpet[j].v_wq.vmb_wq);
+			vpet[j].v_wq.wakeup_vpe = 0;
+			vpet[j].cpu_id = ((i * 2) + j);
+			if ((i == 0) && (j == 0)) {
+				/* Core0/VPE0 always active*/
+				vpet[j].bl_status = IBL_ACTIVE;
+				vpet[j].cpu_status = CPU_ACTIVE;
+			} else {
+				vpet[j].bl_status = IBL_INACTIVE;
+				vpet[j].cpu_status = CPU_INACTIVE;
+			}
+			tct[j].vpe_id = j;
+/* Get the information from DT for FW names also update the bootflag for cpu */
+			update_DB_from_DT(&vpet[j]);
+
+			if (is_linux_OS_vmb(vpet[j].cpu_id))
+				set_cpu_present(vpet[j].cpu_id, true);
+		}
+
+		for (j = 0; j < MAX_TC; j++) {
+			if (j >= 2) {
+				tct[j].vpe_id = MAX_VPE;
+				tct[j].tc_status = TC_INACTIVE;
+			} else {
+				tct[j].vpe_id = j;
+				tct[j].tc_status = TC_ACTIVE;
+			}
+			tct[j].tcmt_grp = 0;
+		}
+
+		for (j = 0; j < MAX_YIELD_INTF; j++) {
+			yrit[j].yr_status = YR_INACTIVE;
+			yrit[j].cpu_id = -1;
+		}
+
+		for (j = 0; j < MAX_ITC_SEMID; j++) {
+			itc_t[j].itc_status = ITC_INACTIVE;
+			itc_t[j].cpu_id = -1;
+			itc_t[j].tc_id = -1;
+		}
+	}
+}
+
+static void initialise_vmb_IRQhdlr(void)
+{
+	int err = 0;
+
+	if (g_fw_vmb_irq[1]) {
+		err = request_irq(g_fw_vmb_irq[1], fw_vmb_ipi1_hdlr,
+				  0, "fw_vmb_ipi1", NULL);
+		if (err)
+			pr_info("request_irq for IRQ = %d failed err = %d\n",
+				g_fw_vmb_irq[1], err);
+	} else {
+		pr_err("no IRQ was specified for CPU1 to VMB\n");
+	}
+
+	if (g_fw_vmb_irq[2]) {
+		err = request_irq(g_fw_vmb_irq[2], fw_vmb_ipi2_hdlr,
+				  0, "fw_vmb_ipi2", NULL);
+		if (err)
+			pr_info("request_irq for IRQ = %d failed err = %d\n",
+				g_fw_vmb_irq[2], err);
+	} else {
+		pr_err("no IRQ was specified for CPU2 to VMB\n");
+	}
+
+	if (g_fw_vmb_irq[3]) {
+		err = request_irq(g_fw_vmb_irq[3], fw_vmb_ipi3_hdlr,
+				  0, "fw_vmb_ipi3", NULL);
+		if (err)
+			pr_info("request_irq for IRQ = %d failed err = %d\n",
+				g_fw_vmb_irq[3], err);
+	} else {
+		pr_err("no IRQ was specified for CPU3 to VMB\n");
+	}
+}
+
+/* Dump the tree */
+static int dump_vmb_tree(struct seq_file *s, void *v)
+{
+	struct VMB_core_t *coret = core_t;
+	int i, j, k, cpu_id;
+
+	for (i = 0; i < MAX_CORE; i++) {
+		struct VMB_tc_t *tct = coret[i].tc_t;
+		struct VMB_vpe_t *vpet = coret[i].vpe_t;
+		struct VMB_yr_t *yr = coret[i].yr_t;
+
+		for (j = 0; j < MAX_VPE; j++) {
+			spin_lock(&vpet[j].vpe_lock);
+			cpu_id = get_cpu_id(i, j);
+			seq_puts(s, "CORE ID ");
+			seq_printf(s, "%d\n", i);
+			seq_puts(s, "\t - VPE ID ");
+			seq_printf(s, "%d\n", j);
+			seq_puts(s, "\t \t - OS Name = ");
+			seq_printf(s, "%s\n", vpet[j].name);
+			seq_puts(s, "\t \t - InterAptiv-BL status = ");
+			seq_printf(s, "%s\n", (vpet[j].bl_status == IBL_ACTIVE ?
+					       "ACTIVE" : "INACTIVE"));
+			seq_puts(s, "\t \t - CPU status = ");
+			seq_printf(s, "%s\n",
+				   ((vpet[j].cpu_status & CPU_ACTIVE) == CPU_ACTIVE ?
+				   "ACTIVE" : "INACTIVE"));
+			seq_puts(s, "\t \t - CPU ID = ");
+			seq_printf(s, "%d\n", vpet[j].cpu_id);
+			seq_puts(s, "\t \t - Allocated TC(s) =  ");
+			for (k = 0; k < MAX_TC; k++) {
+				if (tct[k].tc_status == TC_ACTIVE &&
+				    cpu_id == get_cpu_id(i, tct[k].vpe_id)) {
+					seq_puts(s, "TC");
+					seq_printf(s, "%d", k);
+					seq_puts(s, ", ");
+				}
+			}
+			seq_puts(s, "\t \t\n");
+			seq_puts(s, "\t \t - Allocated Yield Resource(s)=  ");
+			for (k = 0; k < MAX_YIELD_INTF; k++) {
+				if (yr[k].yr_status == YR_ACTIVE &&
+				    cpu_id == yr[k].cpu_id) {
+					seq_printf(s, "%d", k);
+					seq_puts(s, ", ");
+				}
+			}
+			seq_puts(s, "\t \t\n");
+			seq_puts(s, "\t \t - Allocated ITC Resource(s) =  ");
+			for (k = 0; k < MAX_ITC_SEMID; k++) {
+				if (itc_t[k].itc_status == ITC_ACTIVE &&
+				    cpu_id == itc_t[k].cpu_id) {
+					seq_printf(s, "%d", k);
+					seq_puts(s, ", ");
+				}
+			}
+			seq_puts(s, "\t \t\n");
+			seq_puts(s, "\t \t - FW_VMB_MSG Structure\n");
+			seq_puts(s, "\t \t \t - status = ");
+			seq_printf(s, "0X%x\n", vpet[j].fw_vmb.status);
+			seq_puts(s, "\t \t \t - priv_info = ");
+			seq_printf(s, "0X%x\n", vpet[j].fw_vmb.priv_info);
+
+			spin_unlock(&vpet[j].vpe_lock);
+		}
+	}
+
+	return 0;
+}
+
+static int vmb_proc_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, dump_vmb_tree, NULL);
+}
+
+static const struct file_operations vmb_proc_fops = {
+	.open	      = vmb_proc_open,
+	.read	      = seq_read,
+	.llseek	  = seq_lseek,
+	.release		= single_release,
+};
+
+/* Initialise VMB structures and IRQ */
+static int __init vmb_init(void)
+{
+	memset(core_t, 0, sizeof(struct VMB_core_t));
+
+	pr_info("MAXCORE = %d, MAXCPU = %d, MAXTCS = %d MAX_VPE = %d\n",
+		MAX_CORE, MAX_CPU, MAX_TC, MAX_VPE);
+
+	/*
+	 * CPU notifier for Linux SMP bootup indication notify (CPU_STARTING)
+	 * from start_secondary
+	 */
+	cpuhp_setup_state(CPUHP_AP_INTEL_VMB_ONLINE,
+			  "VMB_ON_LINE", vmb_cpu_active, NULL);
+
+	/*
+	 * Initialise a New core before vmbDB init as it will get time for
+	 * dumping IBL_INWAIT in DDR which can be picked by vmb_check_IBL()
+	 */
+
+	/* Basic initialisation of the DB */
+	initialise_vmb_DB();
+
+	/* register IRQ handlers for VMB_FW_IRQx */
+	initialise_vmb_IRQhdlr();
+
+	/* Initialise the /proc/vm/debug to dump the Internal DB */
+	vmb_proc_dir = proc_mkdir("vmb", NULL);
+	vmb_proc = proc_create("status", 0644, vmb_proc_dir, &vmb_proc_fops);
+
+	/*
+	 * API to update th IBL status if the IBL_IN_WAIT IPI
+	 * is missed during initialisation. Mainly a fail-safe Linux
+	 * running VPE updates as there is no IPI mentioned
+	 * to handle IBL_INWAIT for Linux
+	 */
+	vmb_check_IBL_fw_msg();
+
+	spin_lock_init(&vmb_lock);
+
+	return 0;
+}
+
+early_initcall(vmb_init);
+
+/*
+ * This function is needed to trigger GIC IPI Interrupt.
+ * Needed as IPI IRQ number are not consecutive .
+ *Currently we use 20,21 and 85 for CPU1,2 adnd 3 respectviely
+ */
+
+static void gic_trigger(int8_t cpu)
+{
+	switch (cpu) {
+	case 1:
+		pr_info("[%s]:[%d] VMB_CPU_IPI1 - %d CPU = %d\n",
+			__func__, __LINE__, g_cpu_vmb_irq[1], cpu);
+		gic_send_ipi_simple(g_cpu_vmb_irq[1], 0);
+		break;
+	case 2:
+		pr_info("[%s]:[%d] VMB_CPU_IPI2 - %d CPU = %d\n",
+			__func__, __LINE__, g_cpu_vmb_irq[2], cpu);
+		gic_send_ipi_simple(g_cpu_vmb_irq[2], 0);
+		break;
+	case 3:
+		pr_info("[%s]:[%d] VMB_CPU_IPI3 - %d CPU = %d\n",
+			__func__, __LINE__, g_cpu_vmb_irq[3], cpu);
+		gic_send_ipi_simple(g_cpu_vmb_irq[3], 0);
+		break;
+	default:
+		break;
+	}
+}
+
+/* Allocate Free CPU from the pool */
+int8_t vmb_cpu_alloc(int8_t cpu, char *fw_name)
+{
+	int ret = -VMB_ERROR;
+	struct VMB_core_t *coret = core_t;
+	int i, j;
+/*
+ *CHECK : May be for alloc we need to have a global lock to
+ * avoid a scenario where 2 FWs call at the same time race
+ * condiand getting same cpu number
+ */
+
+	if (cpu != MAX_CPU) {
+		int c_id = which_core(cpu);
+		int v_id = vpe_in_core(cpu);
+		struct VMB_vpe_t *vpet = &core_t[c_id].vpe_t[v_id];
+
+		if ((vpet->cpu_status & CPU_BOOTUP_DT) == CPU_BOOTUP_DT) {
+			int cmp_res = strncmp(vpet->name, fw_name,
+					      sizeof(vpet->name));
+			if (cmp_res != 0) {
+				pr_info("Per DT Bootup %s not run on CPU %d\n",
+					fw_name, cpu);
+				pr_info("Please retry with MAX_CPUS !!!\n");
+				ret = -VMB_EAVAIL;
+				goto fin_alloc;
+			}
+		}
+
+		if ((vpet->bl_status == IBL_ACTIVE) &&
+		    ((vpet->cpu_status & CPU_INACTIVE) == CPU_INACTIVE)) {
+			ret = cpu;
+			vpet->cpu_status &= ~CPU_INACTIVE;
+			vpet->cpu_status |= CPU_ACTIVE;
+			pr_info("[%s]:[%d] CPU vpet.cpu_status = %x\n",
+				__func__, __LINE__, vpet->cpu_status);
+#ifdef CONFIG_LTQ_DYN_CPU_ALLOC
+			if ((vpet->cpu_status & CPU_BOOTUP_DT) == CPU_BOOTUP_DT)
+				vpet->cpu_status &= ~CPU_BOOTUP_DT;
+			else
+				strncpy(vpet->name, fw_name,
+					sizeof(vpet->name));
+#endif
+		} else {
+			ret = -VMB_EBUSY;
+		}
+		return ret;
+	}
+
+	for (i = 0; i < MAX_CORE; i++) {
+		struct VMB_vpe_t *vpet = coret[i].vpe_t;
+
+		for (j = 0; j < MAX_VPE; j++) {
+			if ((vpet[j].cpu_status & CPU_BOOTUP_DT) == CPU_BOOTUP_DT) {
+				if (strncmp(vpet[j].name, fw_name, sizeof(vpet[j].name)) != 0) {
+					ret = -VMB_EAVAIL;
+					continue;
+				}
+			}
+
+			if ((vpet[j].bl_status == IBL_ACTIVE) &&
+			    ((vpet[j].cpu_status & CPU_INACTIVE) == CPU_INACTIVE)) {
+				ret = get_cpu_id(i, j);
+				vpet[j].cpu_status &= ~CPU_INACTIVE;
+				vpet[j].cpu_status |= CPU_ACTIVE;
+#ifdef CONFIG_LTQ_DYN_CPU_ALLOC
+				if ((vpet[j].cpu_status & CPU_BOOTUP_DT) == CPU_BOOTUP_DT)
+					vpet[j].cpu_status &= ~CPU_BOOTUP_DT;
+				else
+					strncpy(vpet[j].name, fw_name,
+						sizeof(vpet[j].name));
+#endif
+				goto fin_alloc;
+			} else {
+				ret = -VMB_EBUSY;
+			}
+		}
+	}
+
+fin_alloc:
+	return ret;
+}
+EXPORT_SYMBOL(vmb_cpu_alloc);
+
+void vmb_register_callback(uint8_t cpu, vmb_callback_func func)
+{
+	int c_id = which_core(cpu);
+	int v_id = vpe_in_core(cpu);
+	struct VMB_vpe_t *vpet = &core_t[c_id].vpe_t[v_id];
+
+	vpet->vmb_callback_fn = func;
+}
+EXPORT_SYMBOL(vmb_register_callback);
+
+/* Mark the VPE as free, Free all the allocated Yield resources to CPU/VPE */
+int8_t vmb_cpu_free(int8_t cpu)
+{
+	int c_id = which_core(cpu);
+	int v_id = vpe_in_core(cpu);
+	struct VMB_vpe_t *vpet = &core_t[c_id].vpe_t[v_id];
+
+	if (vpet->bl_status != IBL_ACTIVE)
+		pr_info("WARNING : IBL is not active for CPU = %d !!!\n", cpu);
+
+	vpet->cpu_status &= ~CPU_ACTIVE;
+	vpet->cpu_status |= CPU_INACTIVE;
+
+	/* Free Yield resorces allocated to this CPU */
+	vmb_yr_free(cpu, -1);
+
+	/* Free ITC SemIDs allocated to this CPU */
+	vmb_itc_sem_free_pcpu(cpu);
+
+	return VMB_SUCCESS;
+}
+EXPORT_SYMBOL(vmb_cpu_free);
+
+int8_t vmb_cpu_force_stop(int8_t cpu)
+{
+	int c_id, v_id;
+	struct VMB_vpe_t *vpet;
+	int ret, ret1;
+
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+	vpet->bl_status &= ~IBL_ACTIVE;
+	vpet->bl_status |= IBL_INACTIVE;
+
+	/* Generate a NMI for that VPE*/
+
+	/* Wait for IBL_IN_WAIT or timeout */
+	ret1 = wait_event_interruptible_timeout(vpet->v_wq.vmb_wq,
+					(vpet->v_wq.wakeup_vpe == 1),
+					QUEUE_TIMEOUT);
+	if (ret1 <= 0) {
+		pr_info("[%s]:[%d] wait_event timeout occured for CPU = %d\n",
+			__func__, __LINE__, cpu);
+		pr_info("Consider reseting the CPU using vmb_cpu_force_stop.\n");
+		vpet->v_wq.wakeup_vpe = 0;
+
+		ret = -VMB_ETIMEOUT;
+		goto fin_fstop;
+	}
+
+	pr_info("[%s]:[%d] OUTSIDE wakeup_vpe = %d vpet->fw_vmb.status = %d\n",
+		__func__, __LINE__, vpet->v_wq.wakeup_vpe, vpet->fw_vmb.status);
+
+	vpet->v_wq.wakeup_vpe = 0;
+
+	if (vpet->fw_vmb.status == (uint32_t)IBL_IN_WAIT) {
+		ret = VMB_SUCCESS;
+		pr_err("[%s]:[%d] ret=%d\n", __func__, __LINE__, ret);
+	} else {
+		ret = -VMB_ENACK;
+		pr_err("[%s]:[%d] -ENACK recieved from FW. Resetting\n",
+		       __func__, __LINE__);
+	}
+
+fin_fstop:
+	return ret;
+}
+
+int8_t vmb_cpu_stop(int8_t cpu)
+{
+	int c_id, v_id;
+	struct VMB_vpe_t *vpet;
+	int ret, ret1;
+	struct VMB_fw_msg_t *vmb_fw_msg_t;
+
+/*GLOBAL LOCK needed as "cpu" can for a simutaneous vmb_cpu_start from 2 FWs */
+
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+/*GLOBAL UNLOCK needed as "cpu" can for a simutaneous vmb_cpu_start from 2 FWs*/
+
+/*Per-VPE Lock and update the vmb_fw_msg_t struct and update DDR */
+
+	spin_lock(&vpet->vpe_lock);
+
+	if ((vpet->bl_status == IBL_ACTIVE) &&
+	    ((vpet->cpu_status & CPU_ACTIVE) == CPU_ACTIVE)) {
+		vmb_fw_msg_t = (struct VMB_fw_msg_t *)VMB_get_msg_addr(cpu, 0);
+		memset(vmb_fw_msg_t, 0, sizeof(struct VMB_fw_msg_t));
+		vmb_fw_msg_t->msg_id = VMB_CPU_STOP;
+	} else {
+		ret = -VMB_ERROR;
+		spin_unlock(&vpet->vpe_lock);
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+		goto fin_stop;
+	}
+
+	spin_unlock(&vpet->vpe_lock);
+
+	/* Generate an IPI */
+	gic_trigger(cpu);
+	mips_ihb();
+
+	/* Wait for timeout */
+	ret1 = wait_event_interruptible_timeout(vpet->v_wq.vmb_wq,
+					(vpet->v_wq.wakeup_vpe == 1),
+					QUEUE_TIMEOUT);
+	if (ret1 <= 0) {
+		pr_info("[%s]:[%d] wait_event timeout occured for CPU = %d\n",
+			__func__, __LINE__, cpu);
+		pr_info("Consider reseting the CPU using vmb_cpu_force_stop.\n");
+		vpet->v_wq.wakeup_vpe = 0;
+		memset(vmb_fw_msg_t, 0, sizeof(struct VMB_fw_msg_t));
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+		vmb_cpu_force_stop(cpu);
+		ret = -VMB_ETIMEOUT;
+		goto fin_stop;
+	}
+
+	pr_info("[%s]:[%d] OUTSIDE wakeup_vpe = %d vpet->fw_vmb.status = %d\n",
+		__func__, __LINE__, vpet->v_wq.wakeup_vpe, vpet->fw_vmb.status);
+	memset(vmb_fw_msg_t, 0, sizeof(struct VMB_fw_msg_t));
+	vpet->v_wq.wakeup_vpe = 0;
+
+	if ((vpet->fw_vmb.status == (uint32_t)FW_VMB_ACK) ||
+	    (vpet->fw_vmb.status == (uint32_t)IBL_IN_WAIT)) {
+		ret = VMB_SUCCESS;
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+		pr_err("[%s]:[%d]  ret = %d\n",
+		       __func__, __LINE__, ret);
+	} else {
+		ret = -VMB_ENACK;
+		pr_err("[%s]:[%d] -ENACK recieved from FW. Resetting CPU\n",
+		       __func__, __LINE__);
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+		vmb_cpu_force_stop(cpu);
+	}
+
+fin_stop:
+	return ret;
+}
+EXPORT_SYMBOL(vmb_cpu_stop);
+
+int8_t vmb_cpu_start(int8_t cpu, struct CPU_launch_t cpu_launch,
+		struct TC_launch_t tc_launch[], uint8_t num_tcs, uint8_t num_yr)
+{
+	int c_id, v_id;
+	struct VMB_vpe_t *vpet;
+	int ret, ret1, i;
+	struct VMB_fw_msg_t *vmb_fw_msg_t;
+	int32_t yieldres_t = 0;
+
+/*
+ * CHECK :  GLOBAL LOCK needed as "cpu" can for
+ * a simutaneous vmb_cpu_start from 2 FWs
+ */
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+/*
+ * CHECK : GLOBAL UNLOCK needed as "cpu" can for
+ * a simutaneous vmb_cpu_start from 2 FWs
+ */
+
+/* After getting the cpu then we may use Per-VPE locks */
+
+	if ((void *)&cpu_launch == NULL)
+		return -VMB_ENOPRM;
+
+	if (num_tcs > MAX_TCS) {
+		pr_info("[%s]:[%d] num_tcs %d greater MAX_TCS (%d) reseting\n",
+			__func__, __LINE__, num_tcs, MAX_TCS);
+		num_tcs = MAX_TCS;
+	}
+
+/* Per-VPE Lock and update the vmb_fw_msg_t struct and update DDR */
+	spin_lock(&vpet->vpe_lock);
+
+/* Request num_yr should be a non-zero value */
+	if (num_yr > 0) {
+		yieldres_t = vmb_yr_get(cpu, num_yr);
+		if (yieldres_t == -VMB_EBUSY) {
+			spin_unlock(&vpet->vpe_lock);
+			pr_err("[%s]:[%d] failed as %d Yield not free.\n",
+			       __func__, __LINE__, num_yr);
+			ret = -VMB_EBUSY;
+			goto fin;
+		}
+	}
+
+	vmb_fw_msg_t = (struct VMB_fw_msg_t *)VMB_get_msg_addr(cpu, 0);
+
+	memset(vmb_fw_msg_t, 0, sizeof(struct VMB_fw_msg_t));
+
+	pr_info("[%s]:[%d] vmb_fw_msg_t = %p cpu = %d !\n",
+		__func__, __LINE__, vmb_fw_msg_t, cpu);
+
+	vmb_fw_msg_t->msg_id = VMB_CPU_START;
+	memcpy(&vmb_fw_msg_t->cpu_launch, &cpu_launch,
+	       sizeof(vmb_fw_msg_t->cpu_launch));
+	if ((void *)tc_launch != NULL) {
+		for (i = 0; i < (num_tcs); i++) {
+			memcpy(&vmb_fw_msg_t->tc_launch[i], &tc_launch[i],
+			       sizeof(vmb_fw_msg_t->tc_launch[i]));
+			vmb_fw_msg_t->tc_launch[i].mt_group = vpet->vpemt_grp;
+		}
+	}
+
+	pr_info("[%s]:[%d]  start_address = %u cpu = %d !!!!\n",
+		__func__, __LINE__,
+		(unsigned int)(vmb_fw_msg_t->cpu_launch.start_addr), cpu);
+
+	/* update the allocated yield resource bitmap */
+	vmb_fw_msg_t->cpu_launch.yield_res = (uint32_t)yieldres_t;
+
+	/* update tc_num to indicate number of TCs only in cpu and tc start */
+	vmb_fw_msg_t->tc_num = num_tcs;
+
+	/* Set the vpe mt_grp */
+	vmb_fw_msg_t->cpu_launch.mt_group = vpet->vpemt_grp;
+	spin_unlock(&vpet->vpe_lock);
+	gic_trigger(cpu);
+	mips_ihb();
+
+	pr_info("[%s]:[%d] WAITING FOR RESPONSE vpet->v_wq.wakeup_vpe = %d !\n",
+		__func__, __LINE__, vpet->v_wq.wakeup_vpe);
+
+	/* Wait for timeout */
+	ret1 = wait_event_interruptible_timeout(vpet->v_wq.vmb_wq,
+					(vpet->v_wq.wakeup_vpe == 1),
+					QUEUE_TIMEOUT);
+	if (ret1 <= 0) {
+		pr_info("[%s]:[%d] wait_event timeout for CPU = %d FW = %s\n",
+			__func__, __LINE__, cpu, vpet->name);
+		pr_info("CPU not responding so going for a force cpu stop.\n");
+		memset(vmb_fw_msg_t, 0, sizeof(struct VMB_fw_msg_t));
+		vpet->v_wq.wakeup_vpe = 0;
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+		vmb_cpu_force_stop(cpu);
+		ret = -VMB_ETIMEOUT;
+		goto fin;
+	}
+	pr_info("[%s]:[%d] OUTSIDE wakeup_vpe = %d vpet->fw_vmb.status = %d\n",
+		__func__, __LINE__, vpet->v_wq.wakeup_vpe, vpet->fw_vmb.status);
+	memset(vmb_fw_msg_t, 0, sizeof(struct VMB_fw_msg_t));
+	vpet->v_wq.wakeup_vpe = 0;
+
+	if (vpet->fw_vmb.status == (uint32_t)FW_VMB_ACK) {
+		ret = VMB_SUCCESS;
+		pr_err("[%s]:[%d] ret = %d\n", __func__, __LINE__, ret);
+	} else {
+		ret = -VMB_ENACK;
+		pr_err("[%s]:[%d] -ENACK recieved from FW ..\n",
+		       __func__, __LINE__);
+		pr_err("Consider reseting the CPU\n");
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+		vmb_cpu_force_stop(cpu);
+	}
+
+fin:
+	return ret;
+}
+EXPORT_SYMBOL(vmb_cpu_start);
+
+/********************************** VMB TC APIs **************************/
+
+int8_t vmb_tc_alloc(uint8_t cpu)
+{
+	int ret = -VMB_ERROR;
+	int i, c_id, v_id;
+	struct VMB_core_t *coret = core_t;
+	struct VMB_vpe_t *vpet;
+
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+	if ((vpet->bl_status == IBL_ACTIVE) &&
+	    ((vpet->cpu_status & CPU_ACTIVE) == CPU_ACTIVE)) {
+		struct VMB_tc_t *tct = coret[c_id].tc_t;
+
+		for (i = 2; i < MAX_TC; i++) {
+			if ((tct[i].tc_status & TC_INACTIVE) == TC_INACTIVE) {
+				tct[i].tc_status &= ~TC_INACTIVE;
+				tct[i].tc_status |= TC_ACTIVE;
+				ret = i;
+				tct[i].vpe_id = v_id;
+				pr_info("[%s]:[%d] CPU tct tc_status = %x\n",
+					__func__, __LINE__, tct[i].tc_status);
+				goto fin_talloc;
+			} else {
+				ret = -VMB_EBUSY;
+			}
+		}
+	}
+fin_talloc:
+		return ret;
+}
+EXPORT_SYMBOL(vmb_tc_alloc);
+
+int8_t vmb_tc_free(int8_t cpu, int8_t tc_num)
+{
+	int ret = -VMB_ERROR;
+	int i, c_id, v_id;
+	struct VMB_core_t *coret = core_t;
+	struct VMB_vpe_t *vpet;
+
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+	if ((vpet->bl_status == IBL_ACTIVE) &&
+	    ((vpet->cpu_status & CPU_ACTIVE) == CPU_ACTIVE))
+		pr_info("VPE %d is not active !still freeing the TC %d\n",
+			cpu, tc_num);
+
+	if (tc_num == -1) {
+		struct VMB_tc_t *tct = coret[c_id].tc_t;
+
+		for (i = 2; i < MAX_TC; i++) {
+			ret = get_cpu_id(c_id, tct[i].vpe_id);
+			if (ret == cpu) {
+				tct[i].tc_status &= ~TC_ACTIVE;
+				tct[i].tc_status |= TC_INACTIVE;
+			}
+		}
+		ret = VMB_SUCCESS;
+	} else {
+		struct VMB_tc_t *tct = &coret[c_id].tc_t[tc_num];
+
+		ret = get_cpu_id(c_id, tct->vpe_id);
+
+		if (ret == cpu) {
+			tct->tc_status &= ~TC_ACTIVE;
+			tct->tc_status |= TC_INACTIVE;
+			ret = VMB_SUCCESS;
+		} else {
+			pr_info(" TC %d is not attached to CPU %d.\n",
+				tc_num, cpu);
+			ret = -VMB_EAVAIL;
+		}
+	}
+	return ret;
+}
+EXPORT_SYMBOL(vmb_tc_free);
+
+int8_t vmb_get_vpeid(uint8_t cpu, uint8_t tc_num)
+{
+	int ret = -VMB_ERROR;
+	int c_id;
+	struct VMB_core_t *coret = core_t;
+	struct VMB_tc_t *tct;
+
+	c_id = which_core(cpu);
+	tct = &coret[c_id].tc_t[tc_num];
+	ret = get_cpu_id(c_id, tct->vpe_id);
+
+	return ret;
+}
+EXPORT_SYMBOL(vmb_get_vpeid);
+
+int8_t vmb_tc_start(uint8_t cpu, struct TC_launch_t tc_launch[],
+		    uint8_t num_tcs)
+{
+	int c_id, v_id;
+	struct VMB_vpe_t *vpet;
+	int ret, ret1, i;
+	struct VMB_fw_msg_t *vmb_fw_msg_t;
+
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+/* After getting the cpu then we may use Per-VPE locks */
+
+	if ((void *)tc_launch == NULL)
+		return -VMB_ENOPRM;
+
+	if (num_tcs > MAX_TCS) {
+		pr_info("num_tcs %d greater than MAX_TCS %d reseting num_tcs\n",
+			num_tcs, MAX_TCS);
+		num_tcs = MAX_TCS;
+	}
+
+/* Per-VPE Lock and update the vmb_fw_msg_t struct and update DDR */
+
+	spin_lock(&vpet->vpe_lock);
+
+	vmb_fw_msg_t = (struct VMB_fw_msg_t *)VMB_get_msg_addr(cpu, 0);
+	memset(vmb_fw_msg_t, 0, sizeof(struct VMB_fw_msg_t));
+
+	vmb_fw_msg_t->msg_id = VMB_TC_START;
+
+	for (i = 0; i < (num_tcs); i++)
+		memcpy(&vmb_fw_msg_t->tc_launch[i], &tc_launch[i],
+		       sizeof(vmb_fw_msg_t->tc_launch[i]));
+
+	/* update tc_num to indicate number of TCs only in cpu and tc start */
+	vmb_fw_msg_t->tc_num = num_tcs;
+
+	spin_unlock(&vpet->vpe_lock);
+
+	/* Generate an IPI */
+	gic_trigger(cpu);
+	mips_ihb();
+
+	pr_info("[%s]:[%d] OUTSIDE wakeup_vpe = %d vpet->fw_vmb.status = %d\n",
+		__func__, __LINE__, vpet->v_wq.wakeup_vpe, vpet->fw_vmb.status);
+
+	/* Wait for timeout */
+	ret1 = wait_event_interruptible_timeout(vpet->v_wq.vmb_wq,
+					(vpet->v_wq.wakeup_vpe == 1),
+					QUEUE_TIMEOUT);
+	if (ret1 <= 0) {
+		pr_info("[%s]:[%d] wait_event timeout occured for CPU = %d\n",
+			__func__, __LINE__, cpu);
+		pr_info("Consider reseting the CPU using vmb_cpu_force_stop\n");
+		vpet->v_wq.wakeup_vpe = 0;
+		memset(vmb_fw_msg_t, 0, sizeof(struct VMB_fw_msg_t));
+		vmb_tc_free(cpu, -1);
+		ret = -VMB_ETIMEOUT;
+		goto fin_tcstart;
+	}
+
+	pr_info("[%s]:[%d] OUTSIDE wakeup_vpe = %d vpet->fw_vmb.status = %d\n",
+		__func__, __LINE__, vpet->v_wq.wakeup_vpe, vpet->fw_vmb.status);
+
+	memset(vmb_fw_msg_t, 0, sizeof(struct VMB_fw_msg_t));
+	vpet->v_wq.wakeup_vpe = 0;
+
+	if (vpet->fw_vmb.status == (uint32_t)FW_VMB_ACK) {
+		ret = VMB_SUCCESS;
+		pr_err("[%s]:[%d] ret = %d\n", __func__, __LINE__, ret);
+	} else {
+		ret = -VMB_ENACK;
+		pr_err("[%s]:[%d] -ENACK recieved from FW. Resetting\n",
+		       __func__, __LINE__);
+		vmb_tc_free(cpu, -1);
+	}
+
+fin_tcstart:
+	return ret;
+}
+EXPORT_SYMBOL(vmb_tc_start);
+
+
+int8_t vmb_tc_stop(uint8_t cpu, uint8_t tc_num)
+{
+	int c_id, v_id;
+	struct VMB_vpe_t *vpet;
+	int ret, ret1;
+	struct VMB_fw_msg_t *vmb_fw_msg_t;
+
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+/* Per-VPE Lock and update the vmb_fw_msg_t struct and update DDR */
+
+	spin_lock(&vpet->vpe_lock);
+
+	vmb_fw_msg_t = (struct VMB_fw_msg_t *)VMB_get_msg_addr(cpu, 0);
+	memset(vmb_fw_msg_t, 0, sizeof(struct VMB_fw_msg_t));
+
+	vmb_fw_msg_t->msg_id = VMB_TC_STOP;
+	vmb_fw_msg_t->tc_num = tc_num;
+
+	spin_unlock(&vpet->vpe_lock);
+
+	/* Generate an IPI */
+	gic_trigger(cpu);
+	mips_ihb();
+	/* Wait for timeout */
+	ret1 = wait_event_interruptible_timeout(vpet->v_wq.vmb_wq,
+						(vpet->v_wq.wakeup_vpe == 1),
+						QUEUE_TIMEOUT);
+	if (ret1 <= 0) {
+		pr_info("[%s]:[%d] wait_event timeout CPU = %d FW = %s\n",
+			__func__, __LINE__, cpu, vpet->name);
+		vpet->v_wq.wakeup_vpe = 0;
+		memset(vmb_fw_msg_t, 0, sizeof(struct VMB_fw_msg_t));
+		ret = -VMB_ETIMEOUT;
+		goto fin_tcstop;
+	}
+
+	pr_info("[%s]:[%d] OUTSIDE wakeup_vpe = %d vpet->fw_vmb.status = %d\n",
+		__func__, __LINE__, vpet->v_wq.wakeup_vpe, vpet->fw_vmb.status);
+	memset(vmb_fw_msg_t, 0, sizeof(struct VMB_fw_msg_t));
+	vpet->v_wq.wakeup_vpe = 0;
+
+	if (vpet->fw_vmb.status == (uint32_t)FW_VMB_ACK) {
+		ret = VMB_SUCCESS;
+		pr_err("[%s]:[%d]  ret = %d\n", __func__, __LINE__, ret);
+	} else {
+		ret = -VMB_ENACK;
+		pr_err("[%s]:[%d] -ENACK recieved from FW. Resetting\n",
+		       __func__, __LINE__);
+	}
+
+fin_tcstop:
+	vmb_tc_free(cpu, tc_num);
+	return ret;
+}
+EXPORT_SYMBOL(vmb_tc_stop);
+
+int8_t vmb_tc_pause(uint8_t cpu, uint8_t tc_num)
+{
+	int c_id, v_id;
+	struct VMB_vpe_t *vpet;
+	int ret, ret1;
+	struct VMB_fw_msg_t *vmb_fw_msg_t;
+
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+/* Per-VPE Lock and update the vmb_fw_msg_t struct and update DDR */
+
+	spin_lock(&vpet->vpe_lock);
+
+	vmb_fw_msg_t = (struct VMB_fw_msg_t *)VMB_get_msg_addr(cpu, 0);
+	memset(vmb_fw_msg_t, 0, sizeof(struct VMB_fw_msg_t));
+	vmb_fw_msg_t->msg_id = VMB_TC_PAUSE;
+	vmb_fw_msg_t->tc_num = tc_num;
+
+	spin_unlock(&vpet->vpe_lock);
+
+	/* Generate an IPI */
+	gic_trigger(cpu);
+	mips_ihb();
+	pr_info("[%s]:[%d] WAITING FOR RESPONSE vpet->v_wq.wakeup_vpe = %d\n",
+		__func__, __LINE__, vpet->v_wq.wakeup_vpe);
+
+	/* Wait for timeout */
+	ret1 = wait_event_interruptible_timeout(vpet->v_wq.vmb_wq,
+					(vpet->v_wq.wakeup_vpe == 1),
+					QUEUE_TIMEOUT);
+	if (ret1 <= 0) {
+		pr_info("[%s]:[%d] wait_event timeout CPU = %d FW = %s\n",
+			__func__, __LINE__, cpu, vpet->name);
+		pr_info("CPU could not be Paused !\n");
+		vpet->v_wq.wakeup_vpe = 0;
+		memset(vmb_fw_msg_t, 0, sizeof(struct VMB_fw_msg_t));
+		ret = -VMB_ETIMEOUT;
+		goto fin_tcpause;
+	}
+
+	pr_info("[%s]:[%d] OUTSIDE wakeup_vpe = %d vpet->fw_vmb.status = %d\n",
+		__func__, __LINE__, vpet->v_wq.wakeup_vpe, vpet->fw_vmb.status);
+
+	memset(vmb_fw_msg_t, 0, sizeof(struct VMB_fw_msg_t));
+	vpet->v_wq.wakeup_vpe = 0;
+
+	if (vpet->fw_vmb.status == (uint32_t)FW_VMB_ACK) {
+		ret = VMB_SUCCESS;
+		pr_err("[%s]:[%d]  ret = %d\n", __func__, __LINE__, ret);
+	} else {
+		ret = -VMB_ENACK;
+		pr_err("[%s]:[%d] -ENACK recieved from FW . Resetting\n",
+		       __func__, __LINE__);
+	}
+
+fin_tcpause:
+	return ret;
+}
+EXPORT_SYMBOL(vmb_tc_pause);
+
+int8_t vmb_tc_resume(uint8_t cpu, uint8_t tc_num)
+{
+	int c_id, v_id;
+	struct VMB_vpe_t *vpet;
+	int ret, ret1;
+	struct VMB_fw_msg_t *vmb_fw_msg_t;
+
+/*
+ * CHECK :  GLOBAL LOCK needed as "cpu" can
+ * for a simutaneous vmb_cpu_start from 2 FWs
+ */
+
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+/*
+ *CHECK : GLOBAL UNLOCK needed as "cpu" can for a simutaneous
+ *vmb_cpu_start from 2 FWs
+ */
+/*
+ *Per-VPE Lock and update the vmb_fw_msg_t struct and update DDR
+ */
+
+	spin_lock(&vpet->vpe_lock);
+
+	vmb_fw_msg_t = (struct VMB_fw_msg_t *)VMB_get_msg_addr(cpu, 0);
+	memset(vmb_fw_msg_t, 0, sizeof(struct VMB_fw_msg_t));
+	vmb_fw_msg_t->msg_id = VMB_TC_RESUME;
+	vmb_fw_msg_t->tc_num = tc_num;
+
+	spin_unlock(&vpet->vpe_lock);
+	/* Generate an IPI */
+	gic_trigger(cpu);
+	mips_ihb();
+	pr_info("[%s]:[%d] WAITING FOR RESPONSE vpet->v_wq.wakeup_vpe = %d !\n",
+		__func__, __LINE__, vpet->v_wq.wakeup_vpe);
+
+	/* Wait for timeout */
+	ret1 = wait_event_interruptible_timeout(vpet->v_wq.vmb_wq,
+					(vpet->v_wq.wakeup_vpe == 1),
+					QUEUE_TIMEOUT);
+	if (ret1 <= 0) {
+		pr_err("[%s]:[%d] wait_event timeout occured for CPU = %d",
+		       __func__, __LINE__, cpu);
+		pr_err("starting FW = %sCPU could not be Resumed !!!....\n",
+		       vpet->name);
+		vpet->v_wq.wakeup_vpe = 0;
+		memset(vmb_fw_msg_t, 0, sizeof(struct VMB_fw_msg_t));
+		ret = -VMB_ETIMEOUT;
+		goto fin_tcres;
+	}
+
+	pr_info("[%s]:[%d] OUTSIDE WAIT wakeup_vpe = %d fw_vmb.status = %d",
+		__func__, __LINE__, vpet->v_wq.wakeup_vpe, vpet->fw_vmb.status);
+	pr_info("ACK = %d cpu = %d ret1 = %d vpet = %p, smp id() = %d",
+		FW_VMB_ACK, cpu, ret1, vpet, smp_processor_id());
+	pr_info("vpet->fw_vmb = %p !!!!\n", &vpet->fw_vmb);
+
+	memset(vmb_fw_msg_t, 0, sizeof(struct VMB_fw_msg_t));
+	vpet->v_wq.wakeup_vpe = 0;
+
+	if (vpet->fw_vmb.status == (uint32_t)FW_VMB_ACK) {
+		ret = VMB_SUCCESS;
+		pr_err("[%s]:[%d] ret = %d\n", __func__, __LINE__, ret);
+	} else {
+		ret = -VMB_ENACK;
+		pr_err("CPU could not be Resumed !!!....\n");
+		pr_err("[%s]:[%d] -ENACK recieved from FW ..\n",
+		       __func__, __LINE__);
+	}
+
+fin_tcres:
+	return ret;
+}
+
+int amon_cpu_avail(int cpu)
+{
+	return 1;
+}
+EXPORT_SYMBOL(vmb_tc_resume);
+
+/*
+ *Need this function to check if a CPU needs to run Linux or not.
+ *Linux traverses the NR_CPU sequentially to bring the CPU up but will have
+ *issues when we need to run Linux on Core0/VPE0 (CPU 0) and Core1/VPE0 (CPU 2).
+ *We cannot set NR_CPU as 2 as again it means CPU0 and CPU1 and is referenced
+ *as mentioned in whole kernel . So we cannot set bit 2 as CPU3 instead set
+ *NR_CPUS=4 and selectively start the Linux based on DT
+ */
+
+int is_linux_OS_vmb(int cpu)
+{
+	int c_id, v_id;
+	struct VMB_vpe_t *vpet;
+	int ret = 0;
+
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+	if (!strncmp(vpet->name, "LINUX", sizeof(vpet->name)))
+		ret = 1;
+
+	return ret;
+}
+
+int32_t vmb_yr_get(uint8_t cpu, uint16_t num_yr)
+{
+	int32_t ret = -VMB_ERROR;
+	int i, c_id, n_yr;
+	struct VMB_core_t *coret = core_t;
+	struct VMB_yr_t *yri;
+	int32_t yield_bits = 0x0;
+
+	c_id = which_core(cpu);
+	yri = coret[c_id].yr_t;
+	n_yr = 0;
+
+	/*
+	 * Make sure requested # of Yield resources are available
+	 * before proceeding to allocate. if not return -EBUSY.
+	 */
+	for (i = 0; i < MAX_YIELD_INTF; i++)
+		if ((yri[i].yr_status & YR_INACTIVE) == YR_INACTIVE)
+			n_yr++;
+
+	if (n_yr < num_yr) {
+		ret = -VMB_EBUSY;
+		goto yralloc_failed;
+	}
+
+	n_yr = 0;
+	for (i = 0; i < MAX_YIELD_INTF; i++) {
+		if ((yri[i].yr_status & YR_INACTIVE) == YR_INACTIVE) {
+			if (n_yr == num_yr)
+				break;
+
+			yri[i].yr_status &= ~YR_INACTIVE;
+			yri[i].yr_status |= YR_ACTIVE;
+			yield_bits |= 0x1 << i;
+			yri[i].cpu_id = cpu; /* Add the CPU  */
+			n_yr++;
+			ret = yield_bits;
+		}
+	}
+
+yralloc_failed:
+	return ret;
+}
+EXPORT_SYMBOL(vmb_yr_get);
+
+static inline void vmb_yr_free_error(int i, uint8_t cpu)
+{
+	pr_err("ield ID %d mismatch CPU%d\n", i, cpu);
+}
+
+void vmb_yr_free(uint8_t cpu, int16_t yr)
+{
+	int i, c_id;
+	struct VMB_core_t *coret = core_t;
+	struct VMB_yr_t *yri;
+	int16_t n_yr;
+
+	c_id = which_core(cpu);
+	yri = coret[c_id].yr_t;
+	i = 0;
+
+	if (yr == -1) {
+		for (i = 0; i < MAX_YIELD_INTF; i++) {
+			if (yri[i].cpu_id == cpu) {
+				yri[i].yr_status &= ~YR_ACTIVE;
+				yri[i].yr_status |= YR_INACTIVE;
+				yri[i].cpu_id = -1;
+			}
+		}
+	} else {
+		do {
+			n_yr = (yr >> i) & 0x1;
+			if (n_yr) {
+				if (yri[i].cpu_id == cpu) {
+					yri[i].yr_status &= ~YR_ACTIVE;
+					yri[i].yr_status |= YR_INACTIVE;
+					yri[i].cpu_id = -1;
+				} else {
+					vmb_yr_free_error(i, cpu);
+				}
+			}
+			i++;
+		} while (i < MAX_YIELD_INTF);
+	}
+}
+EXPORT_SYMBOL(vmb_yr_free);
+
+int32_t vmb_itc_sem_get(uint8_t cpu, uint8_t tc_num)
+{
+	int32_t ret = -VMB_EBUSY;
+	int i;
+
+	for (i = 0; i < MAX_ITC_SEMID; i++) {
+		if ((itc_t[i].itc_status & ITC_INACTIVE) == ITC_INACTIVE) {
+			ret = i;
+			itc_t[i].itc_status &= ~ITC_INACTIVE;
+			itc_t[i].itc_status |= ITC_ACTIVE;
+			itc_t[i].cpu_id = cpu; /* Add the CPU number*/
+			itc_t[i].tc_id = tc_num;
+			break;
+		}
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL(vmb_itc_sem_get);
+
+int32_t fw_vmb_get_irq(uint8_t cpu)
+{
+	int32_t ret = -VMB_EAVAIL;
+
+	if (cpu < MAX_CPU)
+		ret = g_fw_vmb_hwirq[cpu];
+
+	pr_info("[%s]:[%d] CPU = %d  IRQ = %d !!!\n",
+		__func__, __LINE__, cpu, ret);
+
+	return ret;
+}
+EXPORT_SYMBOL(fw_vmb_get_irq);
+
+int vmb_run_tc(uint8_t vpe_num, struct TC_launch_t *tc_launch)
+{
+	u32 tc_num;
+
+	if (!tc_launch)
+		return -VMB_ENOPRM;
+
+	tc_num = tc_launch->tc_num;
+
+	/* VPC=1 EVP=0.*/
+	write_c0_mvpcontrol((read_c0_mvpcontrol() & ~MVPCONTROL_EVP) |
+			     MVPCONTROL_VPC);
+	ehb();
+	pr_debug("getmvpctrl = %x\n", (unsigned int)read_c0_mvpcontrol());
+
+	/* set the target to tc_num */
+	settc(tc_num);
+	ehb();
+	pr_debug("getvpectrl for target TC = %x\n",
+		 (unsigned int)read_c0_vpecontrol());
+
+	/* set DA bit so yields will work. */
+	write_tc_c0_tcstatus(read_tc_c0_tcstatus() | TCSTATUS_DA);
+	ehb();
+	pr_debug("gettcstatus = %x\n", (unsigned int)read_tc_c0_tcstatus());
+
+	/* Make sure tc is halted. (This done in h/w as part of reset.) */
+	write_tc_c0_tchalt(TCHALT_H); /* H=1. */
+	ehb();
+	pr_debug("tc is halted = %x\n", (unsigned int)read_tc_c0_tchalt());
+
+	/* Make sure tc is bound to vpe.*/
+	write_tc_c0_tcbind((vpe_num << TCBIND_CURVPE_SHIFT) |
+			    (tc_num << TCBIND_CURTC_SHIFT));
+	ehb();
+	pr_debug("gettcbind = %x\n", (unsigned int)read_tc_c0_tcbind());
+
+	/* Set the stack pointer so we can call a c function */
+	write_tc_gpr_sp(tc_launch->sp);
+	ehb();
+	pr_debug("stack top = %x\n", tc_launch->sp);
+
+	/* Set the global pointer */
+	write_tc_gpr_gp(tc_launch->gp);
+	ehb();
+
+	if (tc_launch->priv_info) {
+		/* Pass the Arg0 */
+		mttgpr(4, tc_launch->priv_info);
+		ehb();
+	}
+	/* Point tc to code */
+	write_tc_c0_tcrestart(tc_launch->start_addr);
+	ehb();
+	pr_debug("start address of TC = %x\n",
+		 (unsigned int)read_tc_c0_tcrestart());
+
+	write_tc_c0_tcstatus(read_tc_c0_tcstatus() |
+			     (TCSTATUS_A | TCSTATUS_DA));
+	ehb();
+	pr_debug("current TC status = %x\n",
+		 (unsigned int)read_tc_c0_tcstatus());
+
+	/* Unhalt tc. */
+	write_tc_c0_tchalt(0); /* H=0 (allow vpe_num/tc_num to execute).*/
+	ehb();
+	pr_debug("tc is unhalted = %x\n", (unsigned int)read_tc_c0_tchalt());
+
+	/* Enable threading. TE=1.*/
+	write_vpe_c0_vpecontrol(read_vpe_c0_vpecontrol() | VPECONTROL_TE);
+	ehb();
+	pr_debug("enable threading = %x\n",
+		 (unsigned int)read_vpe_c0_vpecontrol());
+
+	/* Turn off the vpe configuration flag and enable (other) vpe. */
+	write_c0_mvpcontrol((read_c0_mvpcontrol() & ~MVPCONTROL_VPC) |
+			     MVPCONTROL_EVP); /* VPC=0 EVP=1. */
+	ehb();
+	pr_debug("current mvp control = %x\n",
+		 (unsigned int)read_c0_mvpcontrol());
+
+	return 0;
+}
+EXPORT_SYMBOL(vmb_run_tc);
diff --git a/include/linux/dma/lantiq_dma.h b/include/linux/dma/lantiq_dma.h
new file mode 100755
index 000000000000..b21bdeb5ae2c
--- /dev/null
+++ b/include/linux/dma/lantiq_dma.h
@@ -0,0 +1,311 @@
+/*
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ *
+ * Copyright (C) 2009~1012 Reddy <Reddy.Mallikarjun@lantiq.com>
+ * Copyright (C) 2013 Lei Chuanhua <chuanhua.lei@lantiq.com>
+ * Copyright (C) 2016 Intel Corporation.
+ */
+#ifndef LANTIQ_DMA0_H
+#define LANTIQ_DMA0_H
+/*!
+ * \defgroup LTQ_DMA_CORE UEIP Project - Central DMA core driver
+ * \brief UEIP Project - Central DMA core Module, supports LTQ CPE
+ * \ platforms(Danube/ASE/ARx/VRx/GRX).
+ */
+
+/*!
+ * \defgroup LTQ_DMA_DRV_API External APIs
+ * \ingroup LTQ_DMA_CORE
+ * \brief External APIs definitions for other modules.
+ */
+
+/*!
+ * \defgroup LTQ_DMA_DRV_STRUCTURE Driver Structures
+ * \ingroup LTQ_DMA_CORE
+ * \brief Definitions/Structures of LTQ dma core module.
+ */
+
+/*!
+ * \file lantiq_dma.h
+ * \ingroup LTQ_DMA_CORE
+ * \brief Header file for LTQ Central DMA core driver
+ */
+
+#define MAX_DMA_DEVICE_NUM              7
+
+#define MAX_DMA_CHANNEL_NUM            16
+
+#define DMA_DEV_NAME_LEN                8
+
+/*
+ * Config the Num of descriptors from Kernel configurations
+ * or else if will take default number of  descriptors per channel
+ */
+
+#define MAX_DMA_DESC_NUM               8
+
+
+/*!
+ * \addtogroup LTQ_DMA_DRV_STRUCTURE
+ */
+/*@{*/
+
+/*!
+ * \enum  dma_psuedeo_interrupts_t
+ * \brief DMA pseudo interrupts.
+ *  These interrupts are generated by dma core driver to sync with client
+ *  drivers to handle the data between the clinet and core driver.
+*/
+typedef enum {
+	RCV_INT = 1,		/*!< Receive psuedo interrupt */
+	TX_BUF_FULL_INT = 2,	/*!< Tx channel descriptors full interrupt */
+	TRANSMIT_CPT_INT = 4, /*!< Tx channel descriptors available interrupt */
+} dma_psuedeo_interrupts_t;
+
+/*!
+ * \enum ifx_dma_channel_onoff_t
+ * \brief dma channel is on/ off.
+*/
+typedef enum {
+	IFX_DMA_CH_OFF = 0,	/*!< DMA channel is OFF */
+	IFX_DMA_CH_ON = 1,	/*!< DMA channel is ON */
+} ifx_dma_channel_onoff_t;
+
+/*!
+ * \enum ifx_dma_class_t
+ * \brief dma channel class value.
+*/
+typedef enum {
+	IFX_DMA_CLASS_0 = 0,
+	IFX_DMA_CLASS_1,
+	IFX_DMA_CLASS_2,
+	IFX_DMA_CLASS_3,
+	IFX_DMA_CLASS_4,
+	IFX_DMA_CLASS_5,
+	IFX_DMA_CLASS_6,
+	IFX_DMA_CLASS_7,
+} ifx_dma_class_t;
+
+/*!
+ * \enum ifx_dma_endian_t
+ * \brief DMA endiannes type.
+ */
+typedef enum {
+	IFX_DMA_ENDIAN_TYPE0 = 0,	/*!< No byte Swapping */
+	IFX_DMA_ENDIAN_TYPE1,	/*!< Byte Swap(B0B1B2B3 => B1B0B3B2) */
+	IFX_DMA_ENDIAN_TYPE2,	/*!< Word Swap (B0B1B2B3 => B2B3B0B1) */
+	IFX_DMA_ENDIAN_TYPE3,	/*!< DWord Swap (B0B1B2B3 => B3B2B1B0) */
+} ifx_dma_endian_t;
+
+enum {
+	/** 2 DWORDS */
+	IFX_DMA_BURSTL_2 = 1,
+	/** 4 DWORDS */
+	IFX_DMA_BURSTL_4 = 2,
+	/** 8 DWORDS */
+	IFX_DMA_BURSTL_8 = 3,
+};
+
+/*!
+ * \enum ifx_dma_burst_len_t
+ * \brief DMA Burst length.
+*/
+typedef enum {
+	DMA_BURSTL_2DW = 2,	/*!< 2 DWORD DMA burst length */
+	DMA_BURSTL_4DW = 4,	/*!< 4 DWORD DMA burst length */
+	DMA_BURSTL_8DW = 8,	/*!< 8 DWORD DMA burst length
+				(not supported by all peripherals) */
+} ifx_dma_burst_len_t;
+
+/*!
+ * \typedef _dma_arbitration_info
+ * \brief Parameter Structure to used to configure DMA arbitration
+ * based on packet or burst also Descriptor read back enabled/disabled
+ * (Supported only VR9)
+ * Used by reference dma_device_info
+*/
+typedef struct dma_arbitration_info {
+	__u32 packet_arbitration; /*!< enabled/disabled packet arbitration */
+
+	__u32 multiple_burst_arbitration; /*!< Enabled/Disabled Multi burst */
+	 /*!<
+	  * Counter of the Multi burst arbitration(Num of bursts that served
+	  * before the arbitration of another peri port)
+	  */
+	unsigned int multiple_burst_counter;
+	__u32 desc_read_back;	/*!< enabled/disabled Descriptor read back */
+} _dma_arbitration_info;
+
+/*!
+ * \typedef _dma_channel_info
+ * \brief The parameter structure is used to configure the DMA channel info
+ * when the peripheral driver need to register with DMA core device driver.
+*/
+typedef struct dma_channel_info {
+	int rel_chan_no;	/*!< Relative channel number */
+	int dir;		/*!< Direction of channel */
+	int irq;		/*!< DMA channel IRQ number */
+	unsigned int desc_base;	/*!< Channel descriptor base address */
+	unsigned int desc_phys;
+	int desc_len;		/*!< Num of descriptors per channel */
+	int curr_desc;		/*!< Current Descriptor number */
+	int prev_desc;		/*!< Previous Descriptor number */
+	int byte_offset;	/*!< Byte offset */
+	int desc_handle;	/*!< Descriptor handled flag
+				( to handle Rx Descriptor by client driver) */
+	int weight;		/*!< WFQ present weight value for DMA channel */
+	int default_weight;	/*!< WFQ default weight value to handle in
+				driver for DMA channel */
+	int tx_channel_weight;	/*!< Config the Tx DMA channel weight value */
+	ifx_dma_class_t class_value;	/*!< Config the DMA class value */
+	int packet_size;	/*!< Size of the packet length */
+	int channel_packet_drop_enable;	/*!< Config channel based packet drop */
+	/*!< Channel based packet drop counter */
+	int channel_packet_drop_counter;
+	int peri_to_peri;	/*!< Config Peripheral to Peripheral */
+	int global_buffer_len;	/*!< Config global buffer length, valid only
+				when enabled peri_to_peri) */
+	int loopback_enable; /*!< Config Loop back between the DMA channels */
+	int loopback_channel_number;/*!< Config the loopback Channel number */
+	int req_irq_to_free;	/*!< Release the DMA channel IRQ requested */
+	int dur;		/*!< Flag for Descriptor underrun interrupt */
+	spinlock_t irq_lock;	/*!< spin lock */
+	ifx_dma_channel_onoff_t control; /*!< Channel on/off flag */
+	void *opt[MAX_DMA_DESC_NUM];	/*!< Optional info */
+	void *dma_dev;		/*!< Pointing to the devices */
+	void (*open) (struct dma_channel_info *pCh);	/*!< DMA channel ON */
+	void (*close) (struct dma_channel_info *pCh);	/*!< DMA channel OFF */
+	void (*reset) (struct dma_channel_info *pCh); /*!< Reset DMA channel */
+	/*!< Enable channel interrupt */
+	void (*enable_irq) (struct dma_channel_info *pCh);
+	/*!< Disable channel interrupt */
+	void (*disable_irq) (struct dma_channel_info *pCh);
+	void *pdev;
+} _dma_channel_info;
+
+/*!
+ * \typedef _dma_device_info
+ * \brief The parameter structure is used to configure the DMA Peripheral ports
+ * info when the peripheral driver need to register with DMA core device driver.
+*/
+typedef struct dma_device_info {
+	char device_name[DMA_DEV_NAME_LEN];	/*!< Peripheral Device name */
+	int port_reserved;	/*!< Reserve the device by client driver */
+	int port_num;		/*!< Port number */
+	ifx_dma_burst_len_t tx_burst_len; /*!< Configure the Tx burst length */
+	ifx_dma_burst_len_t rx_burst_len; /*!< Conigure the Rx burst length */
+	int port_tx_weight;	/*!< Configure the Port based weight value */
+	int port_packet_drop_enable;	/*!< Packet drop Enabled/Disabled */
+	int port_packet_drop_counter;	/*!< Packet drop counter */
+	int mem_port_control;	/*!< Configure the mem port control */
+	ifx_dma_endian_t tx_endianness_mode; /*!< Configure TX Endiannes */
+	ifx_dma_endian_t rx_endianness_mode;/*!< Configure RX Endiannes */
+	int current_tx_chan;	/*!< Current Tx channel of the device */
+	int current_rx_chan;	/*!< Current Rx channel of the device */
+	int num_tx_chan;	/*!< Config the num of Tx channels for device */
+	int num_rx_chan;	/*!< Config the num of Rx channels for device */
+	int max_rx_chan_num;	/*!< Max number of Rx channels supported */
+	int max_tx_chan_num;	/*!< Max number of Tx channels supported */
+	spinlock_t irq_lock;	/*!< spin lock */
+	_dma_arbitration_info arbitration_info;	/*!< arbitration config */
+	_dma_channel_info * tx_chan[MAX_DMA_CHANNEL_NUM]; /*!< Max TX channel */
+	_dma_channel_info * rx_chan[MAX_DMA_CHANNEL_NUM]; /*!< Max RX channel */
+	u8 *(*buffer_alloc) (int len, int *offset, void **opt);
+	int (*buffer_free) (u8 *dataptr, void *opt); /*!< Buffer free */
+	/*!< DMA pseudo interrupt handler */
+	int (*intr_handler) (struct dma_device_info *info, int status);
+	/*!< activate the polling  (Used when NAPI enables) */
+	void (*activate_poll) (struct dma_device_info *dma_dev);
+	/*!< Deactivate the polling (used when NAPI enabled) */
+	void (*inactivate_poll) (struct dma_device_info *dma_dev);
+	void *priv;	/*!< Pointer to the device private structure */
+} _dma_device_info;
+
+/* @} */
+/* Reserve the dma device port
+ *  This function should call before the dma_device_register
+ */
+extern _dma_device_info *dma_device_reserve(char *dev_name);
+
+/*
+ * Unreseve the dma device port
+ * This function will called after the dma_device_unregister
+ */
+extern int dma_device_release(_dma_device_info *dev);
+
+/*
+ * Register with DMA device driver.
+ * This function should call after dma_device_reserve function.
+ *  This function register with dma device driver to handle dma functionality.
+ *  Should provide the required configuration info during the register with
+ *  dma device.
+ *  if not provide config info, then take default values.
+ */
+extern int dma_device_register(_dma_device_info *info);
+
+/*
+ * Unregister with DMA core driver
+ *  This function unregister with dma core driver. Once it unregister there is
+ *  no DMA handling with client driver.
+ */
+extern int dma_device_unregister(_dma_device_info *info);
+
+/*
+ * Read data packet from DMA Rx channel.
+ *  This function gets the data from the current rx descriptor of the DMA
+ *  channel and send to the client driver.
+ *  This functions is called when the client driver gets a pseudo DMA interrupt
+ *  (RCV_INT). Handle with care when call this function as well as
+ * dma_device_desc_setup function.
+ */
+extern int dma_device_read(struct dma_device_info *info, u8 **dataptr,
+			void **opt);
+
+/*
+ * Write data Packet to DMA Tx channel.
+ *  This function gets the data packet from the client driver and send over on
+ *  DMA channel.*/
+extern int dma_device_write(struct dma_device_info *info, u8 *dataptr,
+			int len, void *opt);
+
+/*
+ * Setup the DMA channel descriptor.
+ *  This function setup the descriptor of the DMA channel used by client driver.
+ *  The client driver will take care the buffer allocation and do proper
+ *  checking of buffer for DMA burst alignment.Handle with care when call this
+ *  function as well as dma_device_read function
+ */
+extern int dma_device_desc_setup(_dma_device_info *dma_dev, char *buf,
+				size_t len);
+
+/*
+ * Clear the interrupt status flag
+ *  This function used to exit from DMA tasklet(tasklet don't need to run again
+ *  and again ) This is also used to avoid multiple psuedo interrupt (RCV_INT)
+ *  per packet.
+ */
+extern int dma_device_clear_int(_dma_device_info *dma_dev, int dir);
+
+/*
+ *Clear the descriptor status word from the client driver once receive
+ *  a pseudo interrupt(RCV_INT) from the DMA module to avoid duplicate
+ *  interrupts from tasklet.
+*/
+extern int dma_device_clear_desc_status_word(_dma_device_info *dma_dev,
+					int dir);
+
+
+/*
+ * setup the dma channel class value
+ *  This function setup the class of service value for DMA channel.
+ */
+extern void dma_device_setup_class_val(_dma_channel_info *pCh, int cls);
+
+/*
+ * poll DMA ownership bit to ensure that rx transactions are complete
+ *  to prevent descriptor errors
+ */
+extern void poll_dma_ownership_bit(_dma_device_info *dma_dev);
+#endif /* LANTIQ_DMA0_H */
diff --git a/include/linux/dma/lantiq_dmax.h b/include/linux/dma/lantiq_dmax.h
new file mode 100755
index 000000000000..48302ec5ee24
--- /dev/null
+++ b/include/linux/dma/lantiq_dmax.h
@@ -0,0 +1,1331 @@
+/*
+ *  This program is free software; you can redistribute it and/or modify it
+ *  under the terms of the GNU General Public License version 2 as published
+ *  by the Free Software Foundation.
+ *
+ *  Copyright (C) 2014 ~ 2015 Lei Chuanhua <chuanhua.lei@lantiq.com>
+ *  Copyright (C) 2016 ~ 2017 Intel Corporation.
+ */
+#ifndef LANTIQ_DMAX_H
+#define LANTIQ_DMAX_H
+#include <linux/types.h>
+
+/*!
+ * \defgroup LTQ_DMA_CORE Intel High Performance DMA Core Driver
+ * \brief High Performance DMA Core Module, supports LTQ major SoC platform
+ */
+
+/*!
+ * \defgroup LTQ_DMA_DRV_API DMA Common APIs
+ * \ingroup LTQ_DMA_CORE
+ * \brief DMA Common APIs definitions for other modules
+ *
+ * \note A note on dma api call sequence
+
+ *  All drivers needing DMA channels, should allocate and release them
+ *  through the public routines \ref ltq_request_dma and \ref ltq_free_dma.\n
+ *  <b>A generic initialization sequence</b>\n
+ *  1. \ref ltq_request_dma (mandatory)\n
+ *  2. \ref ltq_dma_chan_pktsize_cfg (optional)\n
+ *  3. Alloc / free callback setup by calling\n
+ *     \ref ltq_dma_chan_buf_alloc_callback_cfg\n
+ *     \ref ltq_dma_chan_buf_free_callback_cfg (optional)\n
+ *  4. tx or rx descriptor or data buffer setup by calling\n
+ *    \ref ltq_dma_chan_desc_alloc\n
+ *    \ref ltq_dma_chan_data_buf_alloc\n
+ *	for CBM \n
+ *   \ref ltq_dma_chan_desc_cfg\n
+ *  5. P2P configuration\n
+ *  6. Channel on\n
+ *
+ *  <b>A generic teardown sequence</b>\n
+ *  1. channel off/reset\n
+ *  2. free descriptor and data buffer\n
+ *  3. free dma channel\n
+ */
+/*!
+ * \file lantiq_dmax.h
+ * \ingroup LTQ_DMA_CORE
+ * \brief Header file for intel high performance DMA core driver
+ */
+
+/*
+ * DMA controller, port and channel encoding: 32 bits total as the following
+ * layout:
+ * controller | port  | channe number
+ * 31.......24|23...16|15...........0
+ */
+#define _DMA_CHANBITS	16
+#define _DMA_PORTBITS	8
+#define _DMA_CTRLBITS	8
+
+#define _DMA_CHANMASK	((1 << _DMA_CHANBITS) - 1)
+#define _DMA_PORTMASK	((1 << _DMA_PORTBITS) - 1)
+#define _DMA_CTRLMASK	((1 << _DMA_CTRLBITS) - 1)
+
+#define _DMA_CHANSHIFT	0
+#define _DMA_PORTSHIFT	(_DMA_CHANSHIFT + _DMA_CHANBITS)
+#define _DMA_CTRLSHIFT	(_DMA_PORTSHIFT + _DMA_PORTBITS)
+
+#define _DMA_C(controller, port, channel) \
+	(((controller)  << _DMA_CTRLSHIFT) | \
+	 ((port)  << _DMA_PORTSHIFT) | \
+	 ((channel)  << _DMA_CHANSHIFT))
+
+#define _DMA_CONTROLLER(nr)	(((nr) >> _DMA_CTRLSHIFT) & _DMA_CTRLMASK)
+#define _DMA_PORT(nr)		(((nr) >> _DMA_PORTSHIFT) & _DMA_PORTMASK)
+#define _DMA_CHANNEL(nr)	(((nr) >> _DMA_CHANSHIFT) & _DMA_CHANMASK)
+
+#define MAX_DMA_CHAN_PER_PORT	64
+#define MAX_DMA_PORT_PER_CTRL	4
+
+enum dma_controller {
+	DMA0 = 0,
+	DMA1TX,
+	DMA1RX,
+	DMA2TX,
+	DMA2RX,
+	DMA3,
+	DMA4,
+	DMAMAX,
+};
+
+enum dma_ctrl_port {
+	DMA0_SPI0 = 0,
+	DMA0_SPI1 = 1,
+	DMA0_HSNAND = 2,
+	DMA0_MEMCOPY = 3,
+	DMA1TX_PORT = 0,
+	DMA1RX_PORT = 0,
+	DMA2TX_PORT = 0,
+	DMA2RX_PORT = 0,
+	DMA3_PORT = 0,
+	DMA4_PORT = 0,
+};
+
+enum dma_endian {
+	DMA_ENDIAN_TYPE0 = 0,
+	DMA_ENDIAN_TYPE1,
+	DMA_ENDIAN_TYPE2,
+	DMA_ENDIAN_TYPE3,
+	DMA_ENDIAN_MAX,
+};
+
+enum dma_burst {
+	DMA_BURSTL_2DW = 1,
+	DMA_BURSTL_4DW = 2,
+	DMA_BURSTL_8DW = 3,
+	DMA_BURSTL_16DW = 16,
+	DMA_BURSTL_32DW = 32,
+};
+
+enum dma_pkt_drop {
+	DMA_PKT_DROP_DISABLE = 0,
+	DMA_PKT_DROP_ENABLE,
+};
+
+enum dma_channel {
+	DMA_CHANNEL_0 = 0,
+	DMA_CHANNEL_1,
+	DMA_CHANNEL_2,
+	DMA_CHANNEL_3,
+	DMA_CHANNEL_4,
+	DMA_CHANNEL_5,
+	DMA_CHANNEL_6,
+	DMA_CHANNEL_7,
+	DMA_CHANNEL_8,
+	DMA_CHANNEL_9,
+	DMA_CHANNEL_10,
+	DMA_CHANNEL_11,
+	DMA_CHANNEL_12,
+	DMA_CHANNEL_13,
+	DMA_CHANNEL_14,
+	DMA_CHANNEL_15,
+	DMA_CHANNEL_16,
+	DMA_CHANNEL_17,
+	DMA_CHANNEL_18,
+	DMA_CHANNEL_19,
+	DMA_CHANNEL_20,
+	DMA_CHANNEL_21,
+	DMA_CHANNEL_22,
+	DMA_CHANNEL_23,
+	DMA_CHANNEL_24,
+	DMA_CHANNEL_25,
+	DMA_CHANNEL_26,
+	DMA_CHANNEL_27,
+	DMA_CHANNEL_28,
+	DMA_CHANNEL_29,
+	DMA_CHANNEL_30,
+	DMA_CHANNEL_31,
+	DMA_CHANNEL_32,
+	DMA_CHANNEL_33,
+	DMA_CHANNEL_34,
+	DMA_CHANNEL_35,
+	DMA_CHANNEL_36,
+	DMA_CHANNEL_37,
+	DMA_CHANNEL_38,
+	DMA_CHANNEL_39,
+	DMA_CHANNEL_40,
+	DMA_CHANNEL_41,
+	DMA_CHANNEL_42,
+	DMA_CHANNEL_43,
+	DMA_CHANNEL_44,
+	DMA_CHANNEL_45,
+	DMA_CHANNEL_46,
+	DMA_CHANNEL_47,
+	DMA_CHANNEL_48,
+	DMA_CHANNEL_49,
+	DMA_CHANNEL_50,
+	DMA_CHANNEL_51,
+	DMA_CHANNEL_52,
+	DMA_CHANNEL_53,
+	DMA_CHANNEL_54,
+	DMA_CHANNEL_55,
+	DMA_CHANNEL_56,
+	DMA_CHANNEL_57,
+	DMA_CHANNEL_58,
+	DMA_CHANNEL_59,
+	DMA_CHANNEL_60,
+	DMA_CHANNEL_61,
+	DMA_CHANNEL_62,
+	DMA_CHANNEL_63,
+};
+
+/* API channel list will be used by DMA clients */
+
+/* DMA0 */
+#define DMA0_SPI0_RX		_DMA_C(DMA0, DMA0_SPI0, DMA_CHANNEL_0)
+#define DMA0_SPI0_TX		_DMA_C(DMA0, DMA0_SPI0, DMA_CHANNEL_1)
+#define DMA0_SPI1_RX		_DMA_C(DMA0, DMA0_SPI1, DMA_CHANNEL_2)
+#define DMA0_SPI1_TX		_DMA_C(DMA0, DMA0_SPI1, DMA_CHANNEL_3)
+#define DMA0_HSNAND_RX		_DMA_C(DMA0, DMA0_HSNAND, DMA_CHANNEL_4)
+#define DMA0_HSNAND_TX		_DMA_C(DMA0, DMA0_HSNAND, DMA_CHANNEL_5)
+#define DMA0_MEMCPY_CLASS0_RX	_DMA_C(DMA0, DMA0_MEMCOPY, DMA_CHANNEL_12)
+#define DMA0_MEMCPY_CLASS0_TX	_DMA_C(DMA0, DMA0_MEMCOPY, DMA_CHANNEL_13)
+#define DMA0_MEMCPY_CLASS1_RX	_DMA_C(DMA0, DMA0_MEMCOPY, DMA_CHANNEL_14)
+#define DMA0_MEMCPY_CLASS1_TX	_DMA_C(DMA0, DMA0_MEMCOPY, DMA_CHANNEL_15)
+
+/* DMA1TX */
+#define DMA1TX_LAN_SWITCH_CLASS0	\
+	_DMA_C(DMA1TX, DMA1TX_PORT, DMA_CHANNEL_0)
+
+#define DMA1TX_LAN_SWITCH_CLASS1	\
+	_DMA_C(DMA1TX, DMA1TX_PORT, DMA_CHANNEL_1)
+#define DMA1TX_LAN_SWITCH_CLASS2	\
+	_DMA_C(DMA1TX, DMA1TX_PORT, DMA_CHANNEL_2)
+#define DMA1TX_LAN_SWITCH_CLASS3	\
+	_DMA_C(DMA1TX, DMA1TX_PORT, DMA_CHANNEL_3)
+
+#define DMA1TX_CHAN4_RESERV	_DMA_C(DMA1TX, DMA1TX_PORT, DMA_CHANNEL_4)
+#define DMA1TX_LOOP_FCS_REGEN	_DMA_C(DMA1TX, DMA1TX_PORT, DMA_CHANNEL_5)
+#define DMA1TX_CHAN6_RESERV	_DMA_C(DMA1TX, DMA1TX_PORT, DMA_CHANNEL_6)
+
+#define DMA1TX_EXT_WLAN_PCIE_CLASS7	\
+	_DMA_C(DMA1TX, DMA1TX_PORT, DMA_CHANNEL_7)
+#define DMA1TX_INTERNAL_WLAN_CLASS8	\
+	_DMA_C(DMA1TX, DMA1TX_PORT, DMA_CHANNEL_8)
+
+#define DMA1TX_USB_LAN_CLASS9	_DMA_C(DMA1TX, DMA1TX_PORT, DMA_CHANNEL_9)
+#define DMA1TX_USB_LAN_CLASS10	_DMA_C(DMA1TX, DMA1TX_PORT, DMA_CHANNEL_10)
+#define DMA1TX_CHAN11_RESERV	_DMA_C(DMA1TX, DMA1TX_PORT, DMA_CHANNEL_11)
+#define DMA1TX_USB_WAN_CLASS12	_DMA_C(DMA1TX, DMA1TX_PORT, DMA_CHANNEL_12)
+
+#define DMA1TX_DSL_WAN_CBMP18_CLASS13	\
+	_DMA_C(DMA1TX, DMA1TX_PORT, DMA_CHANNEL_13)
+#define DMA1TX_DMA1RX_CH14_CLASS14	\
+	_DMA_C(DMA1TX, DMA1TX_PORT, DMA_CHANNEL_14)
+#define DMA1TX_GSWIP_R_WAN_CBMP19_CLASS15	\
+	_DMA_C(DMA1TX, DMA1TX_PORT, DMA_CHANNEL_15)
+
+/* DMA1RX */
+#define DMA1RX_TMU_CLASS0	_DMA_C(DMA1RX, DMA1RX_PORT, DMA_CHANNEL_0)
+#define DMA1RX_TMU_CLASS1	_DMA_C(DMA1RX, DMA1RX_PORT, DMA_CHANNEL_1)
+#define DMA1RX_CBM_P10_CLASS9	_DMA_C(DMA1RX, DMA1RX_PORT, DMA_CHANNEL_9)
+#define DMA1RX_CBM_P11_CLASS10	_DMA_C(DMA1RX, DMA1RX_PORT, DMA_CHANNEL_10)
+#define DMA1RX_CBM_P12_CLASS11	_DMA_C(DMA1RX, DMA1RX_PORT, DMA_CHANNEL_11)
+#define DMA1RX_CBM_P13_CLASS12	_DMA_C(DMA1RX, DMA1RX_PORT, DMA_CHANNEL_12)
+#define DMA1RX_CBM_P14_CLASS13	_DMA_C(DMA1RX, DMA1RX_PORT, DMA_CHANNEL_13)
+#define DMA1RX_DMA1TX_CH14_CLASS14	\
+	_DMA_C(DMA1RX, DMA1RX_PORT, DMA_CHANNEL_14)
+#define DMA1RX_CBM_P8_CLASS15	_DMA_C(DMA1RX, DMA1RX_PORT, DMA_CHANNEL_15)
+#define DMA1RX_CBM_P8_CLASS16_JUMBO	\
+	_DMA_C(DMA1RX, DMA1RX_PORT, DMA_CHANNEL_16)
+#define DMA1RX_CBM_P10_CLASS25_JUMBO	\
+	_DMA_C(DMA1RX, DMA1RX_PORT, DMA_CHANNEL_25)
+#define DMA1RX_CBM_P11_CLASS26_JUMBO	\
+	_DMA_C(DMA1RX, DMA1RX_PORT, DMA_CHANNEL_26)
+#define DMA1RX_CBM_P12_CLASS27_JUMBO	\
+	_DMA_C(DMA1RX, DMA1RX_PORT, DMA_CHANNEL_27)
+#define DMA1RX_CBM_P13_CLASS28_JUMBO	\
+	_DMA_C(DMA1RX, DMA1RX_PORT, DMA_CHANNEL_28)
+#define DMA1RX_CBM_P14_CLASS29_JUMBO	\
+	_DMA_C(DMA1RX, DMA1RX_PORT, DMA_CHANNEL_29)
+/* DMA2TX */
+#define DMA2TX_CBM_P6_CLASS0	_DMA_C(DMA2TX, DMA2TX_PORT, DMA_CHANNEL_0)
+#define DMA2TX_CBM_P6_CLASS1	_DMA_C(DMA2TX, DMA2TX_PORT, DMA_CHANNEL_1)
+#define DMA2TX_CBM_P7_CLASS2	_DMA_C(DMA2TX, DMA2TX_PORT, DMA_CHANNEL_2)
+#define DMA2TX_CBM_P8_CLASS3	_DMA_C(DMA2TX, DMA2TX_PORT, DMA_CHANNEL_3)
+#define DMA2TX_CBM_P9_CLASS4	_DMA_C(DMA2TX, DMA2TX_PORT, DMA_CHANNEL_4)
+#define DMA2TX_CBM_P10_CLASS5	_DMA_C(DMA2TX, DMA2TX_PORT, DMA_CHANNEL_5)
+#define DMA2TX_CBM_P11_CLASS6	_DMA_C(DMA2TX, DMA2TX_PORT, DMA_CHANNEL_6)
+#define DMA2TX_CBM_P12_CLASS9	_DMA_C(DMA2TX, DMA2TX_PORT, DMA_CHANNEL_9)
+#define DMA2TX_CBM_P13_CLASS10	_DMA_C(DMA2TX, DMA2TX_PORT, DMA_CHANNEL_10)
+#define DMA2TX_CBM_P14_CLASS11	_DMA_C(DMA2TX, DMA2TX_PORT, DMA_CHANNEL_11)
+#define DMA2TX_CBM_P15_CLASS12	_DMA_C(DMA2TX, DMA2TX_PORT, DMA_CHANNEL_12)
+#define DMA2TX_CBM_P16_CLASS13	_DMA_C(DMA2TX, DMA2TX_PORT, DMA_CHANNEL_13)
+#define DMA2TX_CBM_P17_CLASS14	_DMA_C(DMA2TX, DMA2TX_PORT, DMA_CHANNEL_14)
+/* DMA2RX */
+#define DMA2RX_GSWIP_R_CLASS0	_DMA_C(DMA2RX, DMA2RX_PORT, DMA_CHANNEL_0)
+#define DMA2RX_GSWIP_R_CLASS1	_DMA_C(DMA2RX, DMA2RX_PORT, DMA_CHANNEL_1)
+#define DMA2RX_GSWIP_R_CLASS2	_DMA_C(DMA2RX, DMA2RX_PORT, DMA_CHANNEL_2)
+#define DMA2RX_GSWIP_R_CLASS3	_DMA_C(DMA2RX, DMA2RX_PORT, DMA_CHANNEL_3)
+
+#define DMA2RX_GSWIP_R_CLASS4	_DMA_C(DMA2RX, DMA2RX_PORT, DMA_CHANNEL_4)
+#define DMA2RX_GSWIP_R_CLASS5	_DMA_C(DMA2RX, DMA2RX_PORT, DMA_CHANNEL_5)
+#define DMA2RX_GSWIP_R_CLASS6	_DMA_C(DMA2RX, DMA2RX_PORT, DMA_CHANNEL_6)
+
+#define DMA2RX_CBMP5_CLASS14	_DMA_C(DMA2RX, DMA2RX_PORT, DMA_CHANNEL_14)
+#define DMA2RX_CBMP6_CLASS15	_DMA_C(DMA2RX, DMA2RX_PORT, DMA_CHANNEL_15)
+#define DMA2RX_CBMP5_CLASS30_JUMBO	\
+	_DMA_C(DMA2RX, DMA2RX_PORT, DMA_CHANNEL_30)
+#define DMA2RX_CBMP6_CLASS31_JUMBO	\
+	_DMA_C(DMA2RX, DMA2RX_PORT, DMA_CHANNEL_31)
+/* DMA3 */
+#define DMA3_TOE_MEMCOPY_CLASS0_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_0)
+#define DMA3_TOE_MEMCOPY_CLASS0_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_1)
+#define DMA3_TOE_MEMCOPY_CLASS1_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_2)
+#define DMA3_TOE_MEMCOPY_CLASS1_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_3)
+#define DMA3_TOE_MEMCOPY_CLASS2_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_4)
+#define DMA3_TOE_MEMCOPY_CLASS2_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_5)
+#define DMA3_TOE_MEMCOPY_CLASS3_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_6)
+#define DMA3_TOE_MEMCOPY_CLASS3_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_7)
+#define DMA3_TOE_MEMCOPY_CLASS4_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_8)
+#define DMA3_TOE_MEMCOPY_CLASS4_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_9)
+#define DMA3_TOE_MEMCOPY_CLASS5_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_10)
+#define DMA3_TOE_MEMCOPY_CLASS5_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_11)
+#define DMA3_MCOPY_MEMCOPY_CLASS6_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_12)
+#define DMA3_MCOPY_MEMCOPY_CLASS6_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_13)
+#define DMA3_MCOPY_MEMCOPY_CLASS7_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_14)
+#define DMA3_MCOPY_MEMCOPY_CLASS7_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_15)
+#define DMA3_MCOPY_MEMCOPY_CLASS8_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_16)
+#define DMA3_MCOPY_MEMCOPY_CLASS8_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_17)
+#define DMA3_MCOPY_MEMCOPY_CLASS9_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_18)
+#define DMA3_MCOPY_MEMCOPY_CLASS9_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_19)
+#define DMA3_MCOPY_MEMCOPY_CLASS10_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_20)
+#define DMA3_MCOPY_MEMCOPY_CLASS10_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_21)
+#define DMA3_MCOPY_MEMCOPY_CLASS11_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_22)
+#define DMA3_MCOPY_MEMCOPY_CLASS11_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_23)
+#define DMA3_MCOPY_MEMCOPY_CLASS12_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_24)
+#define DMA3_MCOPY_MEMCOPY_CLASS12_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_25)
+#define DMA3_MCOPY_MEMCOPY_CLASS13_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_26)
+#define DMA3_MCOPY_MEMCOPY_CLASS13_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_27)
+#define DMA3_MCOPY_MEMCOPY_CLASS14_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_28)
+#define DMA3_MCOPY_MEMCOPY_CLASS14_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_29)
+#define DMA3_TOE_MEMCOPY_CLASS15_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_30)
+#define DMA3_TOE_MEMCOPY_CLASS15_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_31)
+#define DMA3_TOE_MEMCOPY_CLASS16_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_32)
+#define DMA3_TOE_MEMCOPY_CLASS16_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_33)
+#define DMA3_TOE_MEMCOPY_CLASS17_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_34)
+#define DMA3_TOE_MEMCOPY_CLASS17_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_35)
+#define DMA3_TOE_MEMCOPY_CLASS18_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_36)
+#define DMA3_TOE_MEMCOPY_CLASS18_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_37)
+#define DMA3_TOE_MEMCOPY_CLASS19_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_38)
+#define DMA3_TOE_MEMCOPY_CLASS19_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_39)
+#define DMA3_TOE_MEMCOPY_CLASS20_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_40)
+#define DMA3_TOE_MEMCOPY_CLASS20_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_41)
+#define DMA3_TOE_MEMCOPY_CLASS21_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_42)
+#define DMA3_TOE_MEMCOPY_CLASS21_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_43)
+#define DMA3_TOE_MEMCOPY_CLASS22_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_44)
+#define DMA3_TOE_MEMCOPY_CLASS22_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_45)
+#define DMA3_TOE_MEMCOPY_CLASS23_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_46)
+#define DMA3_TOE_MEMCOPY_CLASS23_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_47)
+#define DMA3_TOE_MEMCOPY_CLASS24_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_48)
+#define DMA3_TOE_MEMCOPY_CLASS24_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_49)
+#define DMA3_TOE_MEMCOPY_CLASS25_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_50)
+#define DMA3_TOE_MEMCOPY_CLASS25_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_51)
+#define DMA3_TOE_MEMCOPY_CLASS26_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_52)
+#define DMA3_TOE_MEMCOPY_CLASS26_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_53)
+#define DMA3_TOE_MEMCOPY_CLASS27_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_54)
+#define DMA3_TOE_MEMCOPY_CLASS27_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_55)
+#define DMA3_TOE_MEMCOPY_CLASS28_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_56)
+#define DMA3_TOE_MEMCOPY_CLASS28_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_57)
+#define DMA3_TOE_MEMCOPY_CLASS29_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_58)
+#define DMA3_TOE_MEMCOPY_CLASS29_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_59)
+#define DMA3_TOE_MEMCOPY_CLASS30_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_60)
+#define DMA3_TOE_MEMCOPY_CLASS30_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_61)
+#define DMA3_TOE_MEMCOPY_CLASS31_RX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_62)
+#define DMA3_TOE_MEMCOPY_CLASS31_TX	_DMA_C(DMA3, DMA3_PORT, DMA_CHANNEL_63)
+/* DMA4 */
+#define DMA4_MPE_MEMCOPY_CLASS0_RX	_DMA_C(DMA4, DMA4_PORT, DMA_CHANNEL_0)
+#define DMA4_MPE_MEMCOPY_CLASS0_TX	_DMA_C(DMA4, DMA4_PORT, DMA_CHANNEL_1)
+#define DMA4_MPE_MEMCOPY_CLASS1_RX	_DMA_C(DMA4, DMA4_PORT, DMA_CHANNEL_2)
+#define DMA4_MPE_MEMCOPY_CLASS1_TX	_DMA_C(DMA4, DMA4_PORT, DMA_CHANNEL_3)
+#define DMA4_MPE_MEMCOPY_CLASS2_RX	_DMA_C(DMA4, DMA4_PORT, DMA_CHANNEL_4)
+#define DMA4_MPE_MEMCOPY_CLASS2_TX	_DMA_C(DMA4, DMA4_PORT, DMA_CHANNEL_5)
+#define DMA4_MPE_MEMCOPY_CLASS3_RX	_DMA_C(DMA4, DMA4_PORT, DMA_CHANNEL_6)
+#define DMA4_MPE_MEMCOPY_CLASS3_TX	_DMA_C(DMA4, DMA4_PORT, DMA_CHANNEL_7)
+#define DMA4_MPE_MEMCOPY_CLASS4_RX	_DMA_C(DMA4, DMA4_PORT, DMA_CHANNEL_8)
+#define DMA4_MPE_MEMCOPY_CLASS4_TX	_DMA_C(DMA4, DMA4_PORT, DMA_CHANNEL_9)
+#define DMA4_MPE_MEMCOPY_CLASS5_RX	_DMA_C(DMA4, DMA4_PORT, DMA_CHANNEL_10)
+#define DMA4_MPE_MEMCOPY_CLASS5_TX	_DMA_C(DMA4, DMA4_PORT, DMA_CHANNEL_11)
+#define DMA4_MPE_MEMCOPY_CLASS6_RX	_DMA_C(DMA4, DMA4_PORT, DMA_CHANNEL_12)
+#define DMA4_MPE_MEMCOPY_CLASS6_TX	_DMA_C(DMA4, DMA4_PORT, DMA_CHANNEL_13)
+#define DMA4_MPE_MEMCOPY_CLASS7_RX	_DMA_C(DMA4, DMA4_PORT, DMA_CHANNEL_14)
+#define DMA4_MPE_MEMCOPY_CLASS7_TX	_DMA_C(DMA4, DMA4_PORT, DMA_CHANNEL_15)
+
+enum dma_pseudo_irq {
+	RCV_INT = 1,
+	TX_BUF_FULL_INT = 2,
+	TRANSMIT_CPT_INT = 4,
+};
+
+#ifdef CONFIG_CPU_BIG_ENDIAN
+
+#ifdef CONFIG_XBAR_LE
+/* Four DWs descriptor format */
+struct dma_rx_desc {
+	union {
+		struct {
+			u32 resv:3;
+			u32 tunnel_id:4;
+			u32 flow_id:8;
+			u32 eth_type:2;
+			u32 dest_id:15;
+		} __packed field;
+		u32 all;
+	} __packed dw0;
+	union {
+		struct {
+			u32 session_id:12;
+			u32 tcp_err:1;
+			u32 nat:1;
+			u32 dec:1;
+			u32 enc:1;
+			u32 mpe2:1;
+			u32 mpe1:1;
+			u32 color:2;
+			u32 ep:4;
+			u32 resv:4;
+			u32 cla:4;
+		} __packed field;
+		u32 all;
+	} __packed dw1;
+	dma_addr_t data_pointer;/* dw2 */
+	union {
+		struct {
+			u32 own:1;
+			u32 c:1;
+			u32 sop:1;
+			u32 eop:1;
+			u32 dic:1;
+			u32 pdu:1;
+			u32 byte_offset:3;
+			u32 qid:4;
+			u32 mpoa_pt:1;
+			u32 mpoa_mode:2;
+			u32 data_len:16;
+		} __packed field;
+		u32 all;
+	} __packed status; /*dw3 */
+
+} __packed __aligned(16);
+
+struct dma_tx_desc {
+	union {
+		struct {
+			u32 resv:3;
+			u32 tunnel_id:4;
+			u32 flow_id:8;
+			u32 eth_type:2;
+			u32 dest_id:15;
+		} __packed field;
+		u32 all;
+	} __packed dw0;
+	union {
+		struct {
+			u32 session_id:12;
+			u32 tcp_err:1;
+			u32 nat:1;
+			u32 dec:1;
+			u32 enc:1;
+			u32 mpe2:1;
+			u32 mpe1:1;
+			u32 color:2;
+			u32 ep:4;
+			u32 resv:4;
+			u32 cla:4;
+		} __packed field;
+		u32 all;
+	} __packed dw1;
+	dma_addr_t data_pointer;/* dw2 */
+	union {
+		struct {
+			u32 own:1;
+			u32 c:1;
+			u32 sop:1;
+			u32 eop:1;
+			u32 dic:1;
+			u32 pdu:1;
+			u32 byte_offset:3;
+			u32 qid:4;
+			u32 mpoa_pt:1;
+			u32 mpoa_mode:2;
+			u32 data_len:16;
+		} __packed field;
+		u32 all;
+	} __packed status; /* dw3 */
+} __packed __aligned(16);
+
+#else /* Normal big endian */
+/* Four DWs descriptor format */
+struct dma_rx_desc {
+	union {
+		struct {
+			u32 session_id:12;
+			u32 tcp_err:1;
+			u32 nat:1;
+			u32 dec:1;
+			u32 enc:1;
+			u32 mpe2:1;
+			u32 mpe1:1;
+			u32 color:2;
+			u32 ep:4;
+			u32 resv:4;
+			u32 cla:4;
+		} __packed field;
+		u32 all;
+	} __packed dw1;
+	union {
+		struct {
+			u32 resv:3;
+			u32 tunnel_id:4;
+			u32 flow_id:8;
+			u32 eth_type:2;
+			u32 dest_id:15;
+		} __packed field;
+		u32 all;
+	} __packed dw0;
+	union {
+		struct {
+			u32 own:1;
+			u32 c:1;
+			u32 sop:1;
+			u32 eop:1;
+			u32 dic:1;
+			u32 pdu:1;
+			u32 byte_offset:3;
+			u32 qid:4;
+			u32 mpoa_pt:1;
+			u32 mpoa_mode:2;
+			u32 data_len:16;
+		} __packed field;
+		u32 all;
+	} __packed status; /*dw3 */
+	dma_addr_t data_pointer;/* dw2 */
+} __packed __aligned(16);
+
+struct dma_tx_desc {
+	union {
+		struct {
+			u32 session_id:12;
+			u32 tcp_err:1;
+			u32 nat:1;
+			u32 dec:1;
+			u32 enc:1;
+			u32 mpe2:1;
+			u32 mpe1:1;
+			u32 color:2;
+			u32 ep:4;
+			u32 resv:4;
+			u32 cla:4;
+		} __packed field;
+		u32 all;
+	} __packed dw1;
+	union {
+		struct {
+			u32 resv:3;
+			u32 tunnel_id:4;
+			u32 flow_id:8;
+			u32 eth_type:2;
+			u32 dest_id:15;
+		} __packed field;
+		u32 all;
+	} __packed dw0;
+	union {
+		struct {
+			u32 own:1;
+			u32 c:1;
+			u32 sop:1;
+			u32 eop:1;
+			u32 dic:1;
+			u32 pdu:1;
+			u32 byte_offset:3;
+			u32 qid:4;
+			u32 mpoa_pt:1;
+			u32 mpoa_mode:2;
+			u32 data_len:16;
+		} __packed field;
+		u32 all;
+	} __packed status; /* dw3 */
+	dma_addr_t data_pointer;/* dw2 */
+} __packed __aligned(16);
+
+#endif /* CONFIG_XBAR_LE */
+
+/* 2 DWs format descriptor */
+struct dma_rx_desc_2dw {
+	union {
+		struct {
+			u32 own:1;
+			u32 c:1;
+			u32 sop:1;
+			u32 eop:1;
+			u32 reserve_25_27:3;
+			u32 byte_offset:2;
+			u32 rx_sideband:4;
+			u32 reserve16_18:3;
+			u32 data_len:16;
+		} __packed field;
+		u32 all;
+	} __packed status;
+	dma_addr_t data_pointer; /* Descriptor data pointer */
+} __packed __aligned(8);
+
+struct dma_tx_desc_2dw {
+	union {
+		struct {
+			u32 own:1;
+			u32 c:1;
+			u32 sop:1;
+			u32 eop:1;
+			u32 byte_offset:5;
+			u32 reserved:7;
+			u32 data_len:16;
+		} __packed field;
+		u32 all;
+	} __packed status;
+	dma_addr_t data_pointer; /* Descriptor data pointer */
+} __packed __aligned(8);
+#else
+/* Four DWs descriptor format */
+struct dma_rx_desc {
+	union {
+		struct {
+			u32 dest_id:15;
+			u32 eth_type:2;
+			u32 flow_id:8;
+			u32 tunnel_id:4;
+			u32 resv:3;
+		} __packed field;
+		u32 all;
+	} __packed dw0;
+	union {
+		struct {
+			u32 cla:4;
+			u32 resv:4;
+			u32 ep:4;
+			u32 color:2;
+			u32 mpe1:1;
+			u32 mpe2:1;
+			u32 enc:1;
+			u32 dec:1;
+			u32 nat:1;
+			u32 tcp_err:1;
+			u32 session_id:12;
+		} __packed field;
+		u32 all;
+	} __packed dw1;
+	dma_addr_t data_pointer; /* dw2 */
+	union {
+		struct {
+			u32 data_len:16;
+			u32 mpoa_mode:2;
+			u32 mpoa_pt:1;
+			u32 qid:4;
+			u32 byte_offset:3;
+			u32 pdu:1;
+			u32 dic:1;
+			u32 eop:1;
+			u32 sop:1;
+			u32 c:1;
+			u32 own:1;
+		} __packed field;
+		u32 all;
+	} __packed status; /*dw3 */
+} __packed __aligned(16);
+
+struct dma_tx_desc {
+	union {
+		struct {
+			u32 dest_id:15;
+			u32 eth_type:2;
+			u32 flow_id:8;
+			u32 tunnel_id:4;
+			u32 resv:3;
+		} __packed field;
+		u32 all;
+	} __packed dw0;
+	union {
+		struct {
+			u32 cla:4;
+			u32 resv:4;
+			u32 ep:4;
+			u32 color:2;
+			u32 mpe1:1;
+			u32 mpe2:1;
+			u32 enc:1;
+			u32 dec:1;
+			u32 nat:1;
+			u32 tcp_err:1;
+			u32 session_id:12;
+		} __packed field;
+		u32 all;
+	} __packed dw1;
+	dma_addr_t data_pointer; /* dw2 */
+	union {
+		struct {
+			u32 data_len:16;
+			u32 mpoa_mode:2;
+			u32 mpoa_pt:1;
+			u32 qid:4;
+			u32 byte_offset:3;
+			u32 pdu:1;
+			u32 dic:1;
+			u32 eop:1;
+			u32 sop:1;
+			u32 c:1;
+			u32 own:1;
+		} __packed field;
+		u32 all;
+	} __packed status; /* dw3 */
+} __packed __aligned(16);
+
+/* 2 DWs format descriptor */
+struct dma_rx_desc_2dw {
+	union {
+		struct {
+			u32 data_len:16;
+			u32 reserve16_18:3;
+			u32 rx_sideband:4;
+			u32 byte_offset:2;
+			u32 reserve_25_27:3;
+			u32 eop:1;
+			u32 sop:1;
+			u32 c:1;
+			u32 own:1;
+		} __packed field;
+		u32 all;
+	} __packed status;
+	dma_addr_t data_pointer; /* Descriptor data pointer */
+} __packed __aligned(8);
+
+struct dma_tx_desc_2dw {
+	union {
+		struct {
+			u32 data_len:16;
+			u32 reserved:7;
+			u32 byte_offset:5;
+			u32 eop:1;
+			u32 sop:1;
+			u32 c:1;
+			u32 own:1;
+		} __packed field;
+		u32 all;
+	} __packed status;
+	dma_addr_t data_pointer; /* Descriptor data pointer */
+} __packed __aligned(8);
+#endif /* CONFIG_CPU_BIG_ENDIAN */
+
+/*!
+ * \addtogroup LTQ_DMA_DRV_API
+ */
+/*@{*/
+
+/*!
+ * \fn char * (*buffer_alloc_t)(int len, int *byte_offset, void **opt)
+ * \brief This callback function is for buffer allocation
+ * \param[in] len   buffer length
+ * \param[out] byte_offset   byte offset after alignment
+ * \param[out] opt   optional return data pointer
+ *
+ * \return   NULL if failed to alloca memory
+ * \return   allocated buffer pointer
+ */
+typedef char * (*buffer_alloc_t)(int len, int *byte_offset, void **opt);
+
+/*!
+ * \fn int (*buffer_free_t)(char *dataptr, void *opt)
+ * \brief This callback function is for buffer free
+ * \param[in] dataptr   to be freed data pointer
+ * \param[out] opt   optional return data pointer
+ *
+ * \return   0 free memory successfully
+ * \return   < 0 failed to free memory
+ */
+typedef int (*buffer_free_t)(char *dataptr, void *opt);
+
+/*!
+ * \fn int (*intr_handler_t)(u32 lnr, void *priv, int flags)
+ * \brief This callback function is for interrupt callback
+ * \param[in] lnr   logical channel number
+ * \param[in] priv   interrupt specific private data
+ * \param[in] flags   indicates TX/RX, buffer full interrupt
+ *
+ * \return   0 interrupt handling properly
+ * \return   < 0, failed to handle interrupt
+ */
+typedef int (*intr_handler_t)(u32 lnr, void *priv, int flags);
+
+/*!
+ * \fn int ltq_request_dma(u32 chan, const char *device_id)
+ * \brief Request and reserve a DMA channel
+ * \param[in] chan   DMA logical channel number
+ * \param[in] device_id   reserving device ID string, used to identify
+ *			  who is the owner
+ * \return   0 on success
+ * \return   < 0 on failure
+ *
+ * This function is called before any other DMA API. The
+ * requested DMA channel can be freed by calling \ref ltq_free_dma
+ */
+int ltq_request_dma(u32 chan, const char *device_id);
+
+/*!
+ * \fn int ltq_free_dma(u32 chan)
+ * \brief Free a reserved DMA channel
+ * \param[in] chan   DMA logical channel number
+ * \return   0 on success
+ * \return   < 0 on failure
+ *
+ * This function is called after all DMA resource released
+ */
+int ltq_free_dma(u32 chan);
+
+/*!
+ * \fn int ltq_dma_chan_on(u32 chan)
+ * \brief Switch on a DMA channel
+ * \param[in] chan   DMA logical channel number
+ * \return   0 on success
+ * \return   kernel bug reported on failure
+ *
+ * This function turns on the specified channel so that it
+ * can start DMA transaction. Before calling this function,
+ * assume that descriptor, data buffer has been prepared.
+ * This channel can be turned off by calling \ref ltq_dma_chan_off
+ */
+int ltq_dma_chan_on(u32 chan);
+
+/*!
+ * \fn int ltq_dma_chan_off(u32 chan)
+ * \brief Switch off a DMA channel
+ * \param[in] chan   DMA logical channel number
+ * \return   0 on success
+ * \return   kernel bug reported on failure
+ *
+ * This function turns off the specified channel.In case
+ * it is planned to interrupt an ongoing transfer for
+ * a while and the channel off mechanism should be used.
+ * The transfer can be continued by switching the
+ * channel on again at any time
+ */
+int ltq_dma_chan_off(u32 chan);
+
+/*!
+ * \fn int ltq_dma_chan_open(u32 chan)
+ * \brief Open a DMA channel
+ * \param[in] chan   DMA logical channel number
+ * \return   0 on success
+ * \return   kernel bug reported on failure
+ *
+ * This function turns on the specified channel like
+ * \ref ltq_dma_chan_on, it also enable interrupt for RX
+ * channel
+ */
+int ltq_dma_chan_open(u32 chan);
+
+/*!
+ * \fn int ltq_dma_chan_close(u32 chan)
+ * \brief Close a DMA channel
+ * \param[in] chan   DMA logical channel number
+ * \return   0 on success
+ * \return   kernel bug reported on failure
+ *
+ * This function turns off the specified channel like
+ * \ref ltq_dma_chan_off, it also disable channel interrupt
+ */
+int ltq_dma_chan_close(u32 chan);
+
+/*!
+ * \fn int ltq_dma_chan_irq_enable(u32 chan)
+ * \brief Enable channel interrupt
+ * \param[in] chan   DMA logical channel number
+ * \return 0 on success
+ * \return kernel bug reported on failure
+ *
+ * This function enable channel interrupt. It normally
+ * follows \ref ltq_dma_chan_on. \ref ltq_dma_chan_open is a
+ * combination of \ref ltq_dma_chan_on and this function
+ */
+int ltq_dma_chan_irq_enable(u32 chan);
+
+/*!
+ * \fn int ltq_dma_chan_irq_disable(u32 chan)
+ * \brief Disable channel interrupt
+ * \param[in] chan   DMA logical channel number
+ * \return   0 on success
+ * \return   kernel bug reported on failure
+ *
+ * This function disable channel interrupt. It normally
+ * follows \ref ltq_dma_chan_off. \ref ltq_dma_chan_close is a
+ * combination of \ref ltq_dma_chan_off and this function
+ */
+int ltq_dma_chan_irq_disable(u32 chan);
+
+/*!
+ * \fn int ltq_dma_chan_reset(u32 chan)
+ * \brief Reset a DMA channel
+ * \param[in] chan   DMA logical channel number
+ * \return   0 on success
+ * \return   kernel bug reported on failure
+
+ * This function resets the dma channel.In case a
+ * channel need to be stopped immediately, only
+ * the channel reset can be used.
+ * A channel reset will reset the internal descriptor
+ * list pointer inside the Descriptor Manager is
+ * to the base address of the descriptor list
+ * programmed in DMA_CDBA
+ * it is recommended to use channel off at first to
+ * make sure that the current transfer will be finished
+ * smoothly. After the related last descriptor is finally
+ * updated ) the channel can be reset to get a clean
+ * starting point to restart this channel at any time.
+ */
+int ltq_dma_chan_reset(u32 chan);
+
+/*!
+ * \fn int ltq_dma_chan_pktsize_cfg(u32 chan, size_t pktsize)
+ * \brief Set up channel packet size
+ * \param[in] chan   DMA logical channel number
+ * \param[in] pktsize   packet size
+ * \return   0 on success
+ * \return   kernel bug reported on failure
+ *
+ * This function configures packet size for the specified
+ * dma channel
+ */
+int ltq_dma_chan_pktsize_cfg(u32 chan, size_t pktsize);
+
+/*!
+ * \fn int ltq_dma_chan_desc_alloc(u32 chan, u32 desc_num)
+ * \brief Alloc dma descriptors
+ * \param[in] chan   DMA logical channel number
+ * \param[in] desc_num   descriptor number
+ * \return   0 on success
+ * \return   kernel bug reported on failure
+ *
+ * This function allocate dma descriptor for the specified
+ * dma channel. It will gurannte the descriptor is coherent
+ * The allocated descriptors can be freed by calling
+ * \ref ltq_dma_chan_desc_free
+ */
+int ltq_dma_chan_desc_alloc(u32 chan, u32 desc_num);
+
+/*!
+ * \fn int ltq_dma_chan_desc_free(u32 chan)
+ * \brief Free dma descriptors
+ * \param[in] chan   DMA logical channel number
+ * \return   0 on success
+ * \return   kernel bug reported on failure
+ *
+ * This function free dma descriptor for the specified
+ * dma channel.
+ */
+int ltq_dma_chan_desc_free(u32 chan);
+
+/*!
+ * \fn int ltq_dma_chan_data_buf_alloc(u32 chan)
+ * \brief Alloc dma data buffer
+ * \param[in] chan   DMA logical channel number
+ * \return   0 on success
+ * \return   kernel bug reported on failure
+ *
+ * This function allocate dma data buffer for the specified
+ * dma channel. By default, it will call kmalloc to allocate
+ * data buffer. However, callers can register their own
+ * data buffer allocation callback function by calling
+ * ltq_dma_chan_buf_alloc_callback_cfg first.
+ * The allocated data buffer can be freed by calling
+ * \ref ltq_dma_chan_desc_free
+ */
+int ltq_dma_chan_data_buf_alloc(u32 chan);
+
+/*!
+ * \fn int ltq_dma_chan_data_buf_free(u32 chan)
+ * \brief Free dma data buffer
+ * \param[in] chan   DMA logical channel number
+ * \return   0 on success
+ * \return   kernel bug reported on failure
+ *
+ * This function free dma data buffer for the specified
+ * dma channel. By default, it will call kfree to free
+ * data buffer. However, callers can register their own
+ * data buffer free callback function by calling
+ * \ref ltq_dma_chan_buf_free_callback_cfg. If callers register
+ * their own data buffer allocation callback function
+ * \ref ltq_dma_chan_buf_alloc_callback_cfg, they also should
+ * register the corresponding data buffer free callback
+ * function \ref ltq_dma_chan_buf_free_callback_cfg
+ */
+int ltq_dma_chan_data_buf_free(u32 chan);
+
+/*!
+ * \fn int ltq_dma_chan_buf_alloc_callback_cfg(u32 chan, buffer_alloc_t alloc)
+ * \brief Register data buffer allocate callback
+ * \param[in] chan   DMA logical channel number
+ * \param[in] alloc   Data buffer allocation callback
+ * \return   0 on success
+ * \return   kernel bug reported on failure
+ *
+ * This function register dma data buffer allocation callback function
+ * It is called before \ref ltq_dma_chan_desc_alloc
+ */
+int ltq_dma_chan_buf_alloc_callback_cfg(u32 chan, buffer_alloc_t alloc);
+
+/*!
+ * \fn int ltq_dma_chan_buf_free_callback_cfg(u32 chan, buffer_free_t free)
+ * \brief Register data buffer free callback
+ * \param[in] chan DMA logical channel number
+ * \param[in] free Data buffer free callback
+ * \return   0 on success
+ * \return   kernel bug reported on failure
+ *
+ * This function register dma data buffer free callback function
+ * It is called before \ref ltq_dma_chan_data_buf_free
+ */
+int ltq_dma_chan_buf_free_callback_cfg(u32 chan, buffer_free_t free);
+
+/*!
+ * \fn ltq_dma_chan_irq_callback_cfg(u32 chan, irq_handler_t handler,
+ * void *data)
+ * \brief Register channel interrupt callback
+ * \param[in] chan   DMA logical channel number
+ * \param[in] handler   interrupt handler
+ * \param[in] data   interrupt handler specific data
+ * \return   0 on success
+ * \return   kernel bug reported on failure
+ *
+ * This function register channel interrupt callback function. It provides
+ * a chance to caller to change default channel interrupt handler.
+ * It maybe is useful for some real-time application which wants to handler
+ * DMA related stuff in hardware interrupt instead of soft interrupt context
+ * Some more work and verification is needed for this API, extra data
+ * structure has to be exported
+ */
+int ltq_dma_chan_irq_callback_cfg(u32 chan, irq_handler_t handler,
+				  void *data);
+
+/*!
+ * \fn int ltq_dma_chan_pseudo_irq_handler_callback_cfg(u32 chan,
+ * intr_handler_t handler, void *priv)
+ * \brief Register pseudo interrupt callback
+ * \param[in] chan   DMA logical channel number
+ * \param[in] handler   pseudo interrupt handler
+ * \param[in] priv   interrupt handler specific data
+ * \return   0 on success
+ * \return   kernel bug reported on failure
+ *
+ * This function register channel pseudo interrupt callback function. Its major
+ * purpose to provide tasklet level soft interrupt callback to be compatible
+ * with legacy DMA/DMA0
+ */
+int ltq_dma_chan_pseudo_irq_handler_callback_cfg(u32 chan,
+						 intr_handler_t handler,
+						 void *priv);
+
+/*!
+ * \fn dma_addr_t ltq_dma_chan_get_desc_phys_base(u32 chan)
+ * \brief Get descriptor base physical address
+ * \param[in] chan   DMA logical channel number
+ * \return channel descriptor base physical address
+ *
+ * This function returns the descriptor base physical address to
+ * the caller.
+ * In normal case, callers should not care about it. It provides a chance
+ * to caller to manipulate the DMA related stuff by itself
+ */
+dma_addr_t ltq_dma_chan_get_desc_phys_base(u32 chan);
+
+/*!
+ * \fn int ltq_dma_chan_desc_cfg(u32 chan, dma_addr_t desc_base, int desc_num)
+ * \brief Configure low level channel descriptors
+ * \param[in] chan   DMA logical channel number
+ * \param[in] desc_base   descriptor base physical address
+ * \param[in] desc_num   number of descriptors
+ * \return   0 on success
+ * \return   kernel bug reported on failure
+ *
+ * This function configure the low level channel descriptors. It will be
+ * used by CBM whose descriptor is not DDR, actually some registers.
+ */
+int ltq_dma_chan_desc_cfg(u32 chan, dma_addr_t desc_base, int desc_num);
+
+/*!
+ * \fn int ltq_dma_p2p_cfg(u32 rx_chan, u32 tx_chan)
+ * \brief Peripheral to Peripheral configuration
+ * \param[in] rx_chan   P2P RX logical channel number
+ * \param[in] tx_chan   P2P TX logical channel number
+ * \return   0 on success
+ * \return   kernel bug reported on failure
+ *
+ * This function configure DMA P2P functions.
+ * 1) RX and TX channel has the same descriptor list so that they can
+ * link together.\n
+ * 2) TX channel has to enable P2P\n
+ * 3) Global buffer length has to be configured on TX channel DMA instance. \n
+ * 4) P2P can be extended from intraDMA in leagcy SoC to interDMA.\n
+ * Assume RX and TX has been configured using DMA descriptor and data
+ * buffer functions such as \ref ltq_dma_chan_desc_alloc and
+ * \ref ltq_dma_chan_data_buf_alloc
+ */
+int ltq_dma_p2p_cfg(u32 rx_chan, u32 tx_chan);
+
+/*!
+ * \fn int ltq_dma_chan_sync_desc_setup(u32 chan, char *buf, size_t len)
+ * \brief Peripheral to Peripheral configuration
+ * \param[in] chan   DMA logical channel number
+ * \param[in] buf   data buffer provided by caller, must be unmapped address
+ * \param[in] len   length of data buffer
+ * \return   0 on success
+ * \return   kernel bug reported on failure
+ *
+ * This function configures application drived synchronous DMA operation
+ * such as SSC/HSNAND. In these applications, callers will provide data
+ * buffer for transmitting or receiving. Some textbook called this DMA
+ * type as synchronous DMA to differentiate from external event driving
+ * DMA like NIC and etc.
+ */
+int ltq_dma_chan_sync_desc_setup(u32 chan, char *buf, size_t len);
+
+/*!
+ * \fn int ltq_dma_chan_pkt_drop_cfg(u32 chan, int enable)
+ * \brief DMA packet drop configuration
+ * \param[in] chan   DMA logical channel number
+ * \param[in] enable   disable or enable channel packet drop
+ * \return   0 on success
+ * \return   kernel bug reported on failure
+ *
+ * This function configures dma channel packet drop (enable or disable)
+ * dma also has port packet drop function. However, both port or channel
+ * packet drop will be reflected on the same counter. In P2P case, packet
+ * drop should not be enabled
+ */
+int ltq_dma_chan_pkt_drop_cfg(u32 chan, int enable);
+
+/*!
+ * \fn int ltq_dma_chan_txwgt_cfg(u32 chan, int txwgt)
+ * \brief TX channel weight configuration
+ * \param[in] chan   DMA logical channel number
+ * \param[in] txwgt   TX weight
+ * \return   0 on success
+ * \return   kernel bug reported on failure
+ *
+ * This function configures dma channel weight to change the transmission
+ * priority. the higher the bigger
+ */
+int ltq_dma_chan_txwgt_cfg(u32 chan, int txwgt);
+
+/*!
+ * \fn int ltq_dma_chan_fast_path_cfg(u32 tx_chan, u32 rx_chan, int enable)
+ * \brief DMA fast path configuration
+ * \param[in] tx_chan   fast path dma tx logical channel
+ * \param[in] rx_chan   Fast path dma rx logical channel
+ * \param[in] enable: disable or enable fast path
+ * \return   0 on success
+ * \return   kernel bug reported on failure
+ *
+ * This function configures dma fast path function. There is no DDR
+ * or descriptor involved at all. Only RX channel needs to be configure.
+ * However, TX channel number has to be known before. It only applies to
+ * the same DMA instance. There is no user case for GRX350/5xx
+ */
+int ltq_dma_chan_fast_path_cfg(u32 tx_chan, u32 rx_chan, int enable);
+
+/*!
+ * \fn int ltq_dma_chan_read(u32 chan, char **dataptr, void **opt)
+ * \brief Get data packet from DMA
+ * \param[in] chan   DMA logical channel number
+ * \param[out] dataptr   pointer to received data
+ * \param[out] opt   generic pointer
+ * \return >0   valid packet data length
+ * \return <0   on failure
+ *
+ * This function is called when the client driver gets a pseudo
+ * DMA interrupt(RCV_INT). It is backward compatible with legacy DMA/DMA0
+ */
+int ltq_dma_chan_read(u32 chan, char **dataptr, void **opt);
+
+/*!
+ * \fn int ltq_dma_chan_write(u32 chan, char *dataptr, int len, int sop,
+ * int eop, void *opt)
+ * \brief Write data packet through DMA
+ * \param[in] chan   DMA logical channel number
+ * \param[in] dataptr   pointer to transmit data
+ * \param[in] len   length of transmit data
+ * \param[in] sop   start of packet
+ * \param[in] eop   end of packet
+ * \param[in] opt   generic pointer
+ * \return >0   valid packet data length
+ * \return <0   on failure
+ *
+ * This function gets the data packet from the client driver and
+ * sends over on DMA channel. It also supports simple scatter/gather operation
+ * by sop and eop. More advanced sg to be implemented
+ */
+int ltq_dma_chan_write(u32 chan, char *dataptr, int len, int sop, int eop,
+		       void *opt);
+
+/*!
+ * \fn int ltq_dma_chan_polling_cfg(u32 chan, u32 nonarb_cnt, u32 arb_cnt)
+ * \brief DMA per channel polling configuration
+ * \param[in] chan   DMA logical channel number
+ * \param[in] nonarb_cnt   non arbitration polling counter
+ * \param[in] arb_cnt   arbitration polling counter
+ * \return 0    on success
+ * \return -EINVAL   invalid input parameters
+ * \return -EPERM    hardware not supported this feature
+ *
+ * This function configures per channel polling counter for arbitration and
+ * non-arbitration channel. Please note, non-arbitration counter has lower
+ * priority, therefore, non-arbitration counter should be less than arbitration
+ * counter. Without per channel configuration, it will fall back to DMA global
+ * polling counter
+ */
+int ltq_dma_chan_polling_cfg(u32 chan, u32 nonarb_cnt, u32 arb_cnt);
+
+/*!
+ * \fn int ltq_dma_chan_hdrm_cfg(u32 chan, u32 hdr_len)
+ * \brief DMA per channel header only data transfer configuration
+ * \param[in] chan   DMA logical channel number
+ * \param[in] hdr_len   header only data transfer mode header length
+ * \return 0    on success
+ * \return -EINVAL   invalid input parameters
+ * \return -EPERM    hardware not supported this feature
+ *
+ * This function configures DMA per channel header only data transfer header
+ * length. The valid header length should be less than 255. other value will
+ * disable header mode data transfer
+ */
+int ltq_dma_chan_hdrm_cfg(u32 chan, u32 hdr_len);
+
+/*!
+ * \fn int ltq_dma_chan_byte_offset_cfg(u32 chan, u32 boff_len)
+ * \brief DMA per channel byte offset length configuration
+ * \param[in] chan   DMA logical channel number
+ * \param[in] boff_len   byte offset length length
+ * \return 0    on success
+ * \return -EINVAL   invalid input parameters
+ * \return -EPERM    hardware not supported this feature
+ *
+ * This function configures DMA per channel byte offset length. The valid byte
+ * offset length should be less than 255. other value will disalbe per channel
+ * byte offset function
+ */
+int ltq_dma_chan_byte_offset_cfg(u32 chan, u32 boff_len);
+
+/*!
+ * \fn int ltq_dma_chan_int_coalesc_cfg(u32 chan, u32 coal_len, u32 timeout)
+ * \brief DMA per channel interrupt coalescing configuration
+ * \param[in] chan   DMA logical channel number
+ * \param[in] coal_len   interrupt coalescing number
+ * \param[in] timeout    Interrupt timeout interval
+ * \return 0    on success
+ * \return -EINVAL   invalid input parameters
+ * \return -EPERM    hardware not supported this feature
+ *
+ * This function configures DMA per channel interrupt coalescing number.
+ * The valid interrupt coalescing length should be less than 255. other value
+ * will disalbe per channel interrupt coalescing function. Timeout mechanism
+ * also supported. For example, if the last several interrupts are less than
+ * interrupt coalescing length, once timeout is configured, timeout interrupt
+ * will be generated. It is the software responsbility to retrieve the pending
+ * interrupts.
+ */
+int ltq_dma_chan_int_coalesc_cfg(u32 chan, u32 coal_len, u32 timeout);
+
+/*!
+ * \fn int ltq_dma_chan_sw_poll_cfg(u32 chan, int enable)
+ * \brief DMA per channel software polling configuration
+ * \param[in] chan   DMA logical channel number
+ * \param[in] enable   enable or disable software polling
+ * \return 0    on success
+ * \return -EINVAL   invalid input parameters
+ * \return -EPERM    hardware not supported this feature
+ *
+ * This function configures DMA per channel software polling feature. Once
+ * DMA descriptors prepared by software, then enable it. After descriptors
+ * are used out by DMA, HW will clear it. Software has to re enable software
+ * polling for next transaction. It will reduce hardware polling chances.
+ */
+int ltq_dma_chan_sw_poll_cfg(u32 chan, int enable);
+
+/*!
+ * \fn int ltq_dma_chan_data_endian_cfg(u32 chan, u32 endian_type)
+ * \brief DMA per channel data payload endianness configuration
+ * \param[in] chan   DMA logical channel number
+ * \param[in] endian_type   endianness types
+ * \return 0    on success
+ * \return -EINVAL   invalid input parameters
+ * \return -EPERM    hardware not supported this feature
+ *
+ * This function configures DMA per channel data payload endianness feature
+ * according to different endianness requirement. By default, without this
+ * configuration, it will fall back to port based endianness configuration
+ */
+int ltq_dma_chan_data_endian_cfg(u32 chan, u32 endian_type);
+
+/*!
+ * \fn int ltq_dma_chan_data_endian_disable(u32 chan)
+ * \brief DMA per channel data payload endianness disable configuration
+ * \param[in] chan   DMA logical channel number
+ * \return 0    on success
+ * \return -EINVAL   invalid input parameters
+ * \return -EPERM    hardware not supported this feature
+ *
+ * This function configures DMA per channel data payload endianness, then it
+ * wll fall back to port based endianness configuation. It usually will be used
+ * by shutdown routines.
+ */
+int ltq_dma_chan_data_endian_disable(u32 chan);
+
+/*!
+ * \fn int ltq_dma_chan_desc_endian_cfg(u32 chan, u32 endian_type)
+ * \brief DMA per channel descriptor endianness configuration
+ * \param[in] chan   DMA logical channel number
+ * \param[in] endian_type   endianness types
+ * \return 0    on success
+ * \return -EINVAL   invalid input parameters
+ * \return -EPERM    hardware not supported this feature
+ *
+ * This function configures DMA per channel descriptor endianness feature
+ * according to different endianness requirement.By default, without this
+ * configuration, it will fall back to port based endianness configuration
+ */
+int ltq_dma_chan_desc_endian_cfg(u32 chan, u32 endian_type);
+
+/*!
+ * \fn int ltq_dma_chan_desc_endian_disable(u32 chan)
+ * \brief DMA per channel descriptor endianness disable configuration
+ * \param[in] chan   DMA logical channel number
+ * \return 0    on success
+ * \return -EINVAL   invalid input parameters
+ * \return -EPERM    hardware not supported this feature
+ *
+ * This function configures DMA per channel descriptor endianness, then it
+ * wll fall back to port based endianness configuation. It usually will be used
+ * by shutdown routines.
+ */
+int ltq_dma_chan_desc_endian_disable(u32 chan);
+/* @ } */
+#endif /* LANTIQ_DMAX_H */
+
diff --git a/include/linux/ltq_system_reset.h b/include/linux/ltq_system_reset.h
new file mode 100644
index 000000000000..8484fddf8370
--- /dev/null
+++ b/include/linux/ltq_system_reset.h
@@ -0,0 +1,506 @@
+/*
+ *
+ * FILE NAME	: ltq_system_reset.h
+ * PROJECT		: grx500
+ * MODULES		: SYSRST (System Reset Driver)
+ *
+ * DESCRIPTION	: Global System Reset Driver header file
+ * COPYRIGHT	:		Copyright (c) 2014
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * HISTORY
+ */
+
+#ifndef SYSTEM_RESET_H
+#define SYSTEM_RESET_H
+
+#define SYSRST_AFFTECTED_DOMAIN_MAX 4
+#define SYSRST_RCU_AFFTECTED_DOMAIN_MAX 8
+#define SYSRST_NGI_AFFTECTED_DOMAIN_MAX 8
+
+#define SYSRST_DOMAIN_INVALID_ID 0xff
+
+enum {
+	/* TOE */
+	LTQ_NGI_DOMAIN_TA_TOE,
+	/* MPE */
+	LTQ_NGI_DOMAIN_TA_MPE,
+	/* MemMax */
+	LTQ_NGI_DOMAIN_TA_DDR,
+	/* CBM */
+	LTQ_NGI_DOMAIN_TA_CMB1,
+	LTQ_NGI_DOMAIN_TA_CMB2,
+	/* DMA4 */
+	LTQ_NGI_DOMAIN_TA_DMA4,
+	LTQ_NGI_DOMAIN_IA_DMA4,
+	/* DMA3 */
+	LTQ_NGI_DOMAIN_TA_DMA3,
+	LTQ_NGI_DOMAIN_IA_DMA3,
+	/* EIP-123 */
+	LTQ_NGI_DOMAIN_TA_EIP_123,
+	/* EIP-97 */
+	LTQ_NGI_DOMAIN_TA_EIP_97,
+	/* EX05 */
+	LTQ_NGI_DOMAIN_TA_EX05,
+	/* OCP2OTP */
+	LTQ_NGI_DOMAIN_TA_OCP2OTP,
+	/* I2C */
+	LTQ_NGI_DOMAIN_TA_I2C,
+	/* UART0 */
+	LTQ_NGI_DOMAIN_TA_UART0,
+	/* UART0 */
+	LTQ_NGI_DOMAIN_TA_UART1,
+	/* GPTC0 */
+	LTQ_NGI_DOMAIN_TA_GPTC0,
+	LTQ_NGI_DOMAIN_TA_GPTC1,
+	LTQ_NGI_DOMAIN_TA_GPTC2,
+	/* SPI0 */
+	LTQ_NGI_DOMAIN_TA_SPI0,
+	/* SPI1 */
+	LTQ_NGI_DOMAIN_TA_SPI1,
+	/* DMA0 */
+	LTQ_NGI_DOMAIN_TA_DMA0,
+	LTQ_NGI_DOMAIN_IA_DMA0,
+	/* EBU */
+	LTQ_NGI_DOMAIN_TA_EBU,
+	/* HSNAND */
+	LTQ_NGI_DOMAIN_TA_HSNAND,
+	/* DMA2 TX */
+	LTQ_NGI_DOMAIN_TA_DMA2_TX,
+	LTQ_NGI_DOMAIN_IA_DMA2_TX,
+	/* DMA2 RX */
+	LTQ_NGI_DOMAIN_TA_DMA2_RX,
+	LTQ_NGI_DOMAIN_IA_DMA2_RX,
+	/* GSWIP L */
+	LTQ_NGI_DOMAIN_TA_GSWIP_L,
+	/* DMA1 TX */
+	LTQ_NGI_DOMAIN_TA_DMA1_TX,
+	LTQ_NGI_DOMAIN_IA_DMA1_TX,
+	/* DMA1 RX */
+	LTQ_NGI_DOMAIN_TA_DMA1_RX,
+	LTQ_NGI_DOMAIN_IA_DMA1_RX,
+	/* GSWIP R */
+	LTQ_NGI_DOMAIN_TA_GSWIP_R,
+	/* PCIE1 */
+	LTQ_NGI_DOMAIN_TA_PCIE1,
+	LTQ_NGI_DOMAIN_TA_PCIE1_A,
+	LTQ_NGI_DOMAIN_TA_PCIE1_C,
+	LTQ_NGI_DOMAIN_IA_PCIE1_W,
+	LTQ_NGI_DOMAIN_IA_PCIE1_R,
+	/* PCIE2 */
+	LTQ_NGI_DOMAIN_TA_PCIE2,
+	LTQ_NGI_DOMAIN_TA_PCIE2_A,
+	LTQ_NGI_DOMAIN_TA_PCIE2_C,
+	LTQ_NGI_DOMAIN_IA_PCIE2_W,
+	LTQ_NGI_DOMAIN_IA_PCIE2_R,
+	/* PCIE3 */
+	LTQ_NGI_DOMAIN_TA_PCIE3,
+	LTQ_NGI_DOMAIN_TA_PCIE3_A,
+	LTQ_NGI_DOMAIN_TA_PCIE3_C,
+	LTQ_NGI_DOMAIN_IA_PCIE3_W,
+	LTQ_NGI_DOMAIN_IA_PCIE3_R,
+	LTQ_NGI_MAX
+};
+
+#define LTQ_NGI_DECLARE_NAME(var)\
+	char *(var)[] = {\
+	"TA_TOE",\
+	"TA_MPE",\
+	"TA_DDR",\
+	"TA_CMB1",\
+	"TA_CBM2",\
+	"TA_DMA4",\
+	"IA_DMA4",\
+	"TA_DMA3",\
+	"IA_DMA3",\
+	"EIP-123",\
+	"EIP-97",\
+	"EX05",\
+	"OCP2OTP",\
+	"TA_I2C",\
+	"TA_UART0",\
+	"TA_UART1",\
+	"TA_GPTC0",\
+	"TA_GPTC1",\
+	"TA_GPTC2",\
+	"TA_SPI0",\
+	"TA_SPI1",\
+	"TA_DMA0",\
+	"IA_DMA0",\
+	"TA_EBU",\
+	"TA_HSNAND",\
+	"TA_DMA2_TX",\
+	"IA_DMA2_TX",\
+	"TA_DMA2_RX",\
+	"IA_DMA2_RX",\
+	"TA_GSWIP_L",\
+	"TA_DMA1_TX",\
+	"IA_DMA1_TX",\
+	"TA_DMA1_RX",\
+	"IA_DMA1_RX",\
+	"TA_GSWIP_R",\
+	"TA_PCIE1",\
+	"TA_PCIE1_A",\
+	"TA_PCIE1_C",\
+	"IA_PCIE1_W",\
+	"IA_PCIE1_R",\
+	"TA_PCIE2",\
+	"TA_PCIE2_A",\
+	"TA_PCIE2_C",\
+	"IA_PCIE2_W",\
+	"IA_PCIE2_R",\
+	"TA_PCIE3",\
+	"TA_PCIE3_A",\
+	"TA_PCIE3_C",\
+	"IA_PCIE3_W",\
+	"IA_PCIE3_R",\
+}
+
+enum {
+	SYSRST_DOMAIN_ONBOARD_PERIPHERAL = 0,
+	SYSRST_DOMAIN_TEP,	/* Boot core, MIPS4kec */
+	SYSRST_DOMAIN_CPS_SUBSYSTEM, /* CPS Sub System */
+	SYSRST_DOMAIN_TOE,/* TCP Offload Engine */
+	SYSRST_DOMAIN_TMU,	/* Traffic Management Unit */
+	SYSRST_DOMAIN_MPE,	/* Multi-core Processing Engine */
+	SYSRST_DOMAIN_PCIE_PHY3,/* PCIe PHY3 */
+	SYSRST_DOMAIN_PCIE_PHY1,/* PCIe PHY1 */
+	SYSRST_DOMAIN_PCIE_PHY2,/* PCIe PHY2 */
+	SYSRST_DOMAIN_XBAR6,	/* Crossbar 6 */
+	SYSRST_DOMAIN_GSWIP_L,	/* GSWIP_L */
+	SYSRST_DOMAIN_WLAN,		/* WLAN */
+	SYSRST_DOMAIN_PHY5,		/* PHY5 reset when set 1 */
+	SYSRST_DOMAIN_PHY4,		/* PHY4 reset when set 1 */
+	SYSRST_DOMAIN_PHY3,		/* PHY3 reset when set 1 */
+	SYSRST_DOMAIN_PHY2,		/* PHY2 reset when set 1 */
+	SYSRST_DOMAIN_GLB_SW,	/* Global software reset */
+	SYSRST_DOMAIN_PHY6F,	/* GPHY6 reset when set 1 */
+	SYSRST_DOMAIN_VCODEC,	/* Voice Codec */
+	SYSRST_DOMAIN_OCP2SRAM,	/* OCP2SRAM Bridge */
+	SYSRST_DOMAIN_XBAR7,	/* Crossbar 7 */
+	SYSRST_DOMAIN_SL40,		/* SL40 */
+	SYSRST_DOMAIN_USB_PHY0,	/* USB_PHY0 */
+	SYSRST_DOMAIN_USB_PHY1,	/* USB_PHY1 */
+	SYSRST_DOMAIN_SR_PHYF,	/* Soft GPHY6 */
+	SYSRST_DOMAIN_SR_PHY2,	/* Soft PHY2 */
+	SYSRST_DOMAIN_SR_PHY3,	/* Soft PHY3 */
+	SYSRST_DOMAIN_TEMPS,	/* Temperature sensor reset */
+	SYSRST_DOMAIN_SR_PHY5,	/* Soft PHY5 */
+	SYSRST_DOMAIN_SR_PHY4,	/* Soft PHY4 */
+	SYSRST_DOMAIN_XBAR3,	/* Crossbar 3 */
+	SYSRST_DOMAIN_XBAR2,	/* Crossbar 2 */
+	SYSRST_DOMAIN_XBAR1,	/* Crossbar 1 */
+	SYSRST_DOMAIN_XBAR0,	/* Crossbar 0 */
+	SYSRST_DOMAIN_GSWIP_R,	/* GSWIP_R */
+	SYSRST_DOMAIN_E123_ABORT_REQ,/* EIP123 reset/abort request */
+	SYSRST_DOMAIN_E123_ABORT_ACK,/* EIP123 to reset/abort */
+	SYSRST_DOMAIN_GPHY_CDB,	/* GPHY CDB */
+	SYSRST_DOMAIN_DDR_CTL,	/* DDR Controller */
+	SYSRST_DOMAIN_DDR_PUB,	/* DDR PUB */
+	SYSRST_DOMAIN_DDR_PHY,	/* DDR PHY */
+	SYSRST_DOMAIN_MEMMAX,	/* Memory max */
+	SYSRST_DOMAIN_CBM,	/* Central Buffer Management */
+	SYSRST_DOMAIN_DMA4,
+	SYSRST_DOMAIN_DMA3,
+	SYSRST_DOMAIN_EIP123,
+	SYSRST_DOMAIN_EIP97,
+	SYSRST_DOMAIN_EX05,
+	SYSRST_DOMAIN_OCP2OTP,
+	SYSRST_DOMAIN_I2C,
+	SYSRST_DOMAIN_UART0,
+	SYSRST_DOMAIN_UART1,
+	SYSRST_DOMAIN_GPTC0,
+	SYSRST_DOMAIN_GPTC1,
+	SYSRST_DOMAIN_GPTC2,
+	SYSRST_DOMAIN_SPI0,
+	SYSRST_DOMAIN_SPI1,
+	SYSRST_DOMAIN_DMA0,
+	SYSRST_DOMAIN_EBU,
+	SYSRST_DOMAIN_HSNAND,
+	SYSRST_DOMAIN_DMA2TX,
+	SYSRST_DOMAIN_DMA2RX,
+	SYSRST_DOMAIN_DMA1TX,
+	SYSRST_DOMAIN_DMA1RX,
+	SYSRST_DOMAIN_PCIE0,
+	SYSRST_DOMAIN_PCIE1,
+	SYSRST_DOMAIN_PCIE2,
+	SYSRST_DOMAIN_CPU_CLUSTER_RST,
+	SYSRST_DOMAIN_CPU_CLUSTER_RLS,
+	/* extension for falconmx */
+	SYSRST_DOMAIN_GPHY,
+	SYSRST_DOMAIN_PONIP,
+	SYSRST_DOMAIN_HYST0,
+	SYSRST_DOMAIN_PCIE_CTRL1,
+	SYSRST_DOMAIN_PCIE_CTRL0,
+	SYSRST_DOMAIN_GSWIP,
+	SYSRST_DOMAIN_CPHYSS,
+	SYSRST_DOMAIN_WANSS,
+	SYSRST_DOMAIN_XPCS2,
+	SYSRST_DOMAIN_XPCS1,
+	SYSRST_DOMAIN_XPCS0,
+	SYSRST_DOMAIN_PHY1,
+	SYSRST_DOMAIN_PHY0,
+	SYSRST_DOMAIN_QSPI,
+	SYSRST_DOMAIN_EPMSI,
+	SYSRST_DOMAIN_DDR_APB,
+	SYSRST_DOMAIN_GPHY_PWR_DOWN,
+	SYSRST_DOMAIN_ACASL,
+	SYSRST_DOMAIN_HOSTIF,
+	SYSRST_DOMAIN_ACADMA,
+	SYSRST_DOMAIN_SPIDBG,
+	SYSRST_DOMAIN_SR_GPHY,
+	SYSRST_DOMAIN_SSX4,
+	SYSRST_DOMAIN_OTP,
+
+	/* add more component in the future */
+	SYSRST_DOMAIN_MAX,
+};
+
+#define SYSRST_DECLARE_DOMAIN_NAME(var)\
+	char *(var)[] = {\
+	"HRST",\
+	"TEP",\
+	"CPS_SYSTEM",\
+	"TOE",\
+	"TMU",\
+	"MPE",\
+	"PCIE_PHY3",\
+	"PCIE_PHY1",\
+	"PCIE_PHY2",\
+	"XBAR6",\
+	"GSWIP_L",\
+	"WLAN",\
+	"PHY5",\
+	"PHY4",\
+	"PHY3",\
+	"PHY2",\
+	"GLB_SW",\
+	"PHY6F",\
+	"VCODEC",\
+	"OCP2SRAM",\
+	"XBAR7",\
+	"SL40",\
+	"USB_PHY0",\
+	"USB_PHY1",\
+	"SR_PHYF",\
+	"SR_PHY2",\
+	"SR_PHY3",\
+	"TEMPS",\
+	"SR_PHY5",\
+	"SR_PHY4",\
+	"XBAR3",\
+	"XBAR2",\
+	"XBAR1",\
+	"XBAR0",\
+	"GSWIP_R",\
+	"E123_ABORT_REQ",\
+	"E123_ABORT_ACK",\
+	"GPHY_CDB",\
+	"DDR_CTL",\
+	"DDR_PUB",\
+	"DDR_PHY",\
+	"MemMax",\
+	"CBM",\
+	"DMA4",\
+	"DMA3",\
+	"EIP_123",\
+	"EIP_97",\
+	"EX05",\
+	"OCP2OTP",\
+	"I2C",\
+	"UART0",\
+	"UART1",\
+	"GPTC0",\
+	"GPTC1",\
+	"GPTC2",\
+	"SPI0",\
+	"SPI1",\
+	"DMA0",\
+	"EBU",\
+	"HSNAND",\
+	"DMA2TX",\
+	"DMA2RX",\
+	"DMA1TX",\
+	"DMA1RX",\
+	"PCIE0",\
+	"PCIE1",\
+	"PCIE2",\
+	"CPU_CLUSTER_RST",\
+	"CPU_CLUSTER_RLS",\
+	"GPHY",\
+	"PONIP",\
+	"HYST0",\
+	"PCIE_CTRL1",\
+	"PCIE_CTRL0",\
+	"GSWIP",\
+	"CPHYSS",\
+	"WANSS",\
+	"XPCS2",\
+	"XPCS1",\
+	"XPCS0",\
+	"PHY1",\
+	"PHY0",\
+	"QSPI",\
+	"EPMSI",\
+	"DDR_APB",\
+	"GPHY_PWR_DOWN",\
+	"ACASL",\
+	"HOSTIF",\
+	"ACADMA",\
+	"SPIDBG",\
+	"SR_GPHY",\
+	"SSX4",\
+	"OTP",\
+}
+
+enum sysrst_rst_event_t {
+	SYSRST_EVENT_PRE_RESET = 0,
+	SYSRST_EVENT_POST_RESET = 1,
+};
+
+enum {
+	SYSRST_MODULE_USB,
+	SYSRST_MODULE_ETH,
+	SYSRST_MODULE_ATM,
+	SYSRST_MODULE_PTM,
+	SYSRST_MODULE_PPA,
+	SYSRST_MODULE_DMA,
+	SYSRST_MODULE_SDIO,
+	SYSRST_MODULE_MEI,
+	SYSRST_MODULE_TAPI,
+	SYSRST_MODULE_PCI,
+	SYSRST_MODULE_NAND,
+	/* add more component in the future */
+	SYSRST_MODULE_MAX,
+};
+
+#define SYSRST_DECLARE_MODULE_NAME(var)\
+	char *(var)[] = {\
+	"USB",\
+	"ETH",\
+	"ATM",\
+	"PTM",\
+	"PPA",\
+	"DMA",\
+	"SDIO",\
+	"MEI",\
+	"TAPI",\
+	"PCI",\
+	"NAND",\
+	}
+
+typedef int32_t (*sysrst_rst_handler_t)(
+	unsigned int reset_domain_id,
+	unsigned int module_id,
+	enum sysrst_rst_event_t event,
+	unsigned int arg
+	);
+typedef int32_t (*sysrst_rst_async_handler_t)(
+	int32_t rst_status,
+	uint32_t arg
+	);
+/*
+ * ####################################
+ *				Data Type
+ * ####################################
+ */
+struct sysrst_handler_t {
+	struct sysrst_handler_t *next;
+	sysrst_rst_handler_t	   fn;
+	enum sysrst_rst_event_t		   event;
+	unsigned int			   arg;
+	unsigned int			   module_id;
+};
+
+struct sysrst_domain_t {
+/* domains affected by reset to this domain (inclusive) */
+unsigned char affected_domains[SYSRST_AFFTECTED_DOMAIN_MAX];
+unsigned char affected_ngi_domains[SYSRST_NGI_AFFTECTED_DOMAIN_MAX];
+unsigned char affected_rcu_domains[SYSRST_RCU_AFFTECTED_DOMAIN_MAX];
+struct sysrst_handler_t *handlers;
+};
+
+/*
+ * ####################################
+ *				  IOCTL
+ * ####################################
+ */
+struct sysrst_ioctl_version {
+	unsigned int	major;	/*!< output, major number of driver */
+	unsigned int	mid;	/*!< output, mid number of driver */
+	unsigned int	minor;	/*!< output, minor number of driver */
+};
+
+/*!
+  \struct sysrst_ioctl_query_rst_domain
+  \brief Structure used to get reset status of given domain module ID.
+ */
+struct sysrst_ioctl_query_rst_domain {
+	unsigned int	domain_id;	/*!< input, domain ID */
+	unsigned int	module_id;	/*!< input, module ID */
+	unsigned int	flags;		/*!< input, flags */
+	int	f_reset;	/*!< output, reset status */
+};
+
+#define SYSRST_IOC_MAGIC			   0xe0
+/*!
+  \def SYSRST_IOC_VERSION
+  \brief RCU IOCTL Command - Get driver version number.
+
+   This command uses struct "sysrst_ioctl_version"
+   as parameter to RCU driver version number.
+ */
+#define SYSRST_IOC_VERSION _IOR(\
+	SYSRST_IOC_MAGIC,\
+	0,\
+	struct sysrst_ioctl_version)
+/*
+  \def SYSRST_IOC_QUERY_RST_DOMAIN
+  \brief RCU IOCTL Command - Get reset status of given hardware module.
+
+   This command uses struct "sysrst_ioctl_query_rst_domain"
+   as parameter to get reset status of given hardware module.
+ */
+#define SYSRST_IOC_QUERY_RST_DOMAIN _IOWR(\
+	SYSRST_IOC_MAGIC,\
+	1,\
+	struct sysrst_ioctl_query_rst_domain)
+
+/*!
+  \def SYSRST_IOC_DO_RST_DOMAIN
+  \brief RCU IOCTL Command - Get reset status of given domain module.
+
+   This command uses struct "sysrst_ioctl_query_rst_domain"
+   as parameter to get reset status of given hardware module.
+ */
+#define SYSRST_IOC_DO_RST_DOMAIN _IOWR(\
+	SYSRST_IOC_MAGIC,\
+	2,\
+	struct sysrst_ioctl_query_rst_domain)
+/*
+ * ####################################
+ *				   Exported API
+ * ####################################
+ */
+
+#ifdef __KERNEL__
+/*
+ *	Reset Operations
+ */
+int sysrst_register(unsigned int reset_domain_id, unsigned int module_id,
+		    sysrst_rst_handler_t callbackfn, unsigned long arg);
+int sysrst_free(unsigned int reset_domain_id, unsigned int module_id);
+int sysrst_rst(unsigned int reset_domain_id, unsigned int module_id,
+	       unsigned int flags);
+void sysrst_clrstatus(unsigned int reset_domain_id);
+int sysrst_rststatus(unsigned int reset_domain_id);
+int sysrst_rst_async(unsigned int reset_domain_id, unsigned int module_id,
+		     sysrst_rst_async_handler_t rst_done, unsigned int arg,
+		     unsigned int flags);
+#endif
+
+#endif	/* SYSTEM_RESET_H */
