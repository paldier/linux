From 3e58f45939ae7ba76dc57a779552f94ed4452d6f Mon Sep 17 00:00:00 2001
From: Hua Ma <hua.ma@linux.intel.com>
Date: Thu, 21 Jun 2018 17:37:47 +0800
Subject: [PATCH] Add support for lantiq cqm common

---
 drivers/net/ethernet/lantiq/cqm/Kconfig       |  34 ++
 drivers/net/ethernet/lantiq/cqm/Makefile      |  11 +
 drivers/net/ethernet/lantiq/cqm/cbm_wrapper.c | 599 ++++++++++++++++++++++++++
 drivers/net/ethernet/lantiq/cqm/cqm_common.c  | 208 +++++++++
 drivers/net/ethernet/lantiq/cqm/cqm_common.h  | 247 +++++++++++
 drivers/net/ethernet/lantiq/cqm/cqm_dev.c     | 186 ++++++++
 drivers/net/ethernet/lantiq/cqm/cqm_dev.h     |  41 ++
 7 files changed, 1326 insertions(+)

diff --git a/drivers/net/ethernet/lantiq/cqm/Kconfig b/drivers/net/ethernet/lantiq/cqm/Kconfig
new file mode 100644
index 000000000000..468c8705bc92
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/cqm/Kconfig
@@ -0,0 +1,34 @@
+#
+# Central QOS Manager(CQM) configuration
+#
+config LTQ_CBM
+	bool "Central Buffer Manager Driver"
+	depends on SOC_GRX500
+	default n
+
+	---help---
+	Turn on this option to enable CBM/CQM driver which is a special hardware
+	present in the XRX500/FALCONMX series of SoCs to manage the network buffers
+	in HW.
+
+choice
+	prompt "SoC platform selection"
+	depends on LTQ_CBM
+
+config FALCONMX_CQM
+	bool "FALCONMX"
+	help
+	  CQM driver for FALCONMX platform.
+if FALCONMX_CQM
+source "drivers/net/ethernet/lantiq/cqm/falconmx/Kconfig"
+endif
+
+config GRX500_CBM
+	bool "GRX500"
+
+	help
+	  CBM driver for GRX500 platform.
+if GRX500_CBM
+source "drivers/net/ethernet/lantiq/cqm/grx500/Kconfig"
+endif
+endchoice
diff --git a/drivers/net/ethernet/lantiq/cqm/Makefile b/drivers/net/ethernet/lantiq/cqm/Makefile
new file mode 100644
index 000000000000..e0d5d3c3a8fc
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/cqm/Makefile
@@ -0,0 +1,11 @@
+#
+# Makefile for CBM driver.
+#
+
+obj-$(CONFIG_LTQ_CBM) +=  cqm_dev.o cqm_common.o cbm_wrapper.o
+ifneq ($(CONFIG_GRX500_CBM),)
+obj-$(CONFIG_LTQ_CBM) += grx500/
+endif
+ifneq ($(CONFIG_FALCONMX_CQM),)
+obj-$(CONFIG_LTQ_CBM) += falconmx/
+endif
diff --git a/drivers/net/ethernet/lantiq/cqm/cbm_wrapper.c b/drivers/net/ethernet/lantiq/cqm/cbm_wrapper.c
new file mode 100644
index 000000000000..0573b364b9b2
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/cqm/cbm_wrapper.c
@@ -0,0 +1,599 @@
+#include "cqm_common.h"
+static const struct cbm_ops *g_cbm_ops;
+void cbm_setup_DMA_p2p(void)
+{
+	if (g_cbm_ops->cbm_setup_DMA_p2p)
+		g_cbm_ops->cbm_setup_DMA_p2p();
+}
+EXPORT_SYMBOL(cbm_setup_DMA_p2p);
+
+int cbm_turn_on_DMA_p2p(void)
+{
+	if (g_cbm_ops->cbm_turn_on_DMA_p2p)
+		return g_cbm_ops->cbm_turn_on_DMA_p2p();
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_turn_on_DMA_p2p);
+
+s32 cbm_queue_map_get(int cbm_inst, s32 queue_id, s32 *num_entries,
+		      cbm_queue_map_entry_t **entries, u32 flags)
+{
+	if (g_cbm_ops->cbm_queue_map_get)
+		return g_cbm_ops->cbm_queue_map_get(
+		cbm_inst, queue_id, num_entries, entries, flags);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_queue_map_get);
+
+s32 cbm_queue_map_set(int cbm_inst, s32 queue_id,
+		      cbm_queue_map_entry_t *entry,
+		      u32 flags)
+{
+	if (g_cbm_ops->cbm_queue_map_set)
+		return g_cbm_ops->cbm_queue_map_set(
+		cbm_inst, queue_id, entry, flags);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_queue_map_set);
+
+s32 cqm_qid2ep_map_set(int qid, int port)
+{
+	if (g_cbm_ops->cqm_qid2ep_map_set)
+		return g_cbm_ops->cqm_qid2ep_map_set(qid, port);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cqm_qid2ep_map_set);
+
+s32 cqm_qid2ep_map_get(int qid, int *port)
+{
+	if (g_cbm_ops->cqm_qid2ep_map_get)
+		return g_cbm_ops->cqm_qid2ep_map_get(qid, port);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cqm_qid2ep_map_get);
+
+s32
+cqm_mode_table_set(int cbm_inst, cbm_queue_map_entry_t *entry, u32 flags)
+{
+	if (g_cbm_ops->cqm_mode_table_set)
+		return g_cbm_ops->cqm_mode_table_set(cbm_inst, entry, flags);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cqm_mode_table_set);
+
+s32
+cqm_mode_table_get(int cbm_inst, int *mode,
+		   cbm_queue_map_entry_t *entry,
+		   u32 flags)
+{
+	if (g_cbm_ops->cqm_mode_table_get)
+		return g_cbm_ops->cqm_mode_table_get(
+		cbm_inst, mode, entry, flags);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cqm_mode_table_get);
+
+struct sk_buff *cbm_build_skb(void *data, unsigned int frag_size,
+			      gfp_t priority)
+{
+	if (g_cbm_ops->cbm_build_skb)
+		return g_cbm_ops->cbm_build_skb(data, frag_size, priority);
+	else
+		return NULL;
+}
+EXPORT_SYMBOL(cbm_build_skb);
+
+int cbm_setup_desc(struct cbm_desc *desc, u32 data_ptr, u32 data_len,
+		   struct sk_buff *skb)
+{
+	if (g_cbm_ops->cbm_setup_desc)
+		return g_cbm_ops->cbm_setup_desc(
+		desc, data_ptr, data_len, skb->DW0, skb->DW1);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_setup_desc);
+
+int cbm_cpu_enqueue_hw(u32 pid, struct cbm_desc *desc, void *data_pointer,
+		       int flags)
+{
+	if (g_cbm_ops->cbm_cpu_enqueue_hw)
+		return g_cbm_ops->cbm_cpu_enqueue_hw(
+		pid, desc, data_pointer, flags);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_cpu_enqueue_hw);
+
+void *cbm_buffer_alloc(u32 pid, u32 flag, u32 size)
+{
+	if (g_cbm_ops->cbm_buffer_alloc)
+		return g_cbm_ops->cbm_buffer_alloc(pid, flag, size);
+	else
+		return NULL;
+}
+EXPORT_SYMBOL(cbm_buffer_alloc);
+
+void *cqm_buffer_alloc_by_policy(u32 pid, u32 flag, u32 policy)
+{
+	if (g_cbm_ops->cqm_buffer_alloc_by_policy)
+		return g_cbm_ops->cqm_buffer_alloc_by_policy(pid, flag, policy);
+	else
+		return NULL;
+}
+EXPORT_SYMBOL(cqm_buffer_alloc_by_policy);
+
+struct sk_buff *cbm_copy_skb(const struct sk_buff *skb, gfp_t gfp_mask)
+{
+	if (g_cbm_ops->cbm_copy_skb)
+		return g_cbm_ops->cbm_copy_skb(skb, gfp_mask);
+	else
+		return NULL;
+}
+EXPORT_SYMBOL(cbm_copy_skb);
+
+struct sk_buff *cbm_alloc_skb(unsigned int size, gfp_t priority)
+{
+	if (g_cbm_ops->cbm_alloc_skb)
+		return g_cbm_ops->cbm_alloc_skb(size, priority);
+	else
+		return NULL;
+}
+EXPORT_SYMBOL(cbm_alloc_skb);
+
+int cbm_buffer_free(u32 pid, void *v_buf, u32 flag)
+{
+	if (g_cbm_ops->cbm_buffer_free)
+		return g_cbm_ops->cbm_buffer_free(pid, v_buf, flag);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_buffer_free);
+
+inline int check_ptr_validation(u32 buf)
+{
+	if (g_cbm_ops->check_ptr_validation)
+		return g_cbm_ops->check_ptr_validation(buf);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(check_ptr_validation);
+
+s32 cbm_cpu_pkt_tx(struct sk_buff *skb, struct cbm_tx_data *data, u32 flags)
+{
+	if (g_cbm_ops->cbm_cpu_pkt_tx)
+		return g_cbm_ops->cbm_cpu_pkt_tx(skb, data, flags);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_cpu_pkt_tx);
+
+s32 cbm_port_quick_reset(s32 cbm_port_id, u32 flags)
+{
+	if (g_cbm_ops->cbm_port_quick_reset)
+		return g_cbm_ops->cbm_port_quick_reset(cbm_port_id, flags);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_port_quick_reset);
+
+s32 cbm_dp_port_alloc(struct module *owner, struct net_device *dev,
+		      u32 dev_port, s32 dp_port, struct cbm_dp_alloc_data *data,
+		      u32 flags)
+{
+	if (g_cbm_ops->cbm_dp_port_alloc)
+		return g_cbm_ops->cbm_dp_port_alloc(
+		owner, dev, dev_port, dp_port, data, flags);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_dp_port_alloc);
+
+int cbm_get_wlan_umt_pid(u32 ep_id, u32 *cbm_pid)
+{
+	if (g_cbm_ops->cbm_get_wlan_umt_pid)
+		return g_cbm_ops->cbm_get_wlan_umt_pid(ep_id, cbm_pid);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_get_wlan_umt_pid);
+
+s32 cbm_dp_enable(struct module *owner, u32 dp_port,
+		  struct cbm_dp_en_data *data, u32 flags, u32 alloc_flags)
+{
+	if (g_cbm_ops->cbm_dp_enable)
+		return g_cbm_ops->cbm_dp_enable(owner, dp_port,
+						data, flags, alloc_flags);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_dp_enable);
+
+s32 cqm_qos_queue_flush(int cqm_inst, int cqm_drop_port, int qid)
+{
+	if (g_cbm_ops->cqm_qos_queue_flush)
+		return g_cbm_ops->cqm_qos_queue_flush(cqm_inst, cqm_drop_port,
+					qid);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cqm_qos_queue_flush);
+
+s32 cbm_queue_flush(s32 cbm_port_id, s32 queue_id, u32 timeout, u32 flags)
+{
+	if (g_cbm_ops->cbm_queue_flush)
+		return g_cbm_ops->cbm_queue_flush(cbm_port_id, queue_id,
+						  timeout, flags);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_queue_flush);
+
+s32 cbm_dp_q_enable(
+	int cbm_inst,
+	s32 dp_port_id,
+	s32 qnum,
+	s32 tmu_port_id,
+	s32 remap_to_qid,
+	u32 timeout,
+	s32 qidt_valid,
+	u32 flags
+	)
+{
+	if (g_cbm_ops->cbm_dp_q_enable)
+		return g_cbm_ops->cbm_dp_q_enable(
+		cbm_inst, dp_port_id, qnum, tmu_port_id,
+		remap_to_qid, timeout,
+		qidt_valid, flags);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_dp_q_enable);
+
+s32 cbm_enqueue_port_resources_get(cbm_eq_port_res_t *res, u32 flags)
+{
+	if (g_cbm_ops->cbm_enqueue_port_resources_get)
+		return g_cbm_ops->cbm_enqueue_port_resources_get(res, flags);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_enqueue_port_resources_get);
+
+s32 cbm_dequeue_port_resources_get(u32 dp_port, cbm_dq_port_res_t *res,
+				   u32 flags)
+{
+	if (g_cbm_ops->cbm_dequeue_port_resources_get)
+		return g_cbm_ops->cbm_dequeue_port_resources_get(dp_port, res,
+								 flags);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_dequeue_port_resources_get);
+
+s32 cbm_dp_port_resources_get(u32 *dp_port, u32 *num_tmu_ports,
+			      cbm_tmu_res_t **res_pp, u32 flags)
+{
+	if (g_cbm_ops->cbm_dp_port_resources_get)
+		return g_cbm_ops->cbm_dp_port_resources_get(
+		dp_port,
+		num_tmu_ports, res_pp, flags);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_dp_port_resources_get);
+
+s32 cbm_reserved_dp_resources_get(u32 *tmu_port, u32 *tmu_sched, u32 *tmu_q)
+{
+	if (g_cbm_ops->cbm_reserved_dp_resources_get)
+		return g_cbm_ops->cbm_reserved_dp_resources_get(
+		tmu_port, tmu_sched, tmu_q);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_reserved_dp_resources_get);
+
+s32 cbm_get_egress_port_info(u32 cbm_port, u32 *tx_ch, u32 *flags)
+{
+	if (g_cbm_ops->cbm_get_egress_port_info)
+		return g_cbm_ops->cbm_get_egress_port_info(cbm_port, tx_ch,
+							   flags);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_get_egress_port_info);
+
+s32 cbm_enqueue_port_overhead_set(s32 port_id, int8_t ovh)
+{
+	if (g_cbm_ops->cbm_enqueue_port_overhead_set)
+		return g_cbm_ops->cbm_enqueue_port_overhead_set(port_id, ovh);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_enqueue_port_overhead_set);
+
+s32 cbm_enqueue_port_overhead_get(s32 port_id, int8_t *ovh)
+{
+	if (g_cbm_ops->cbm_enqueue_port_overhead_get)
+		return g_cbm_ops->cbm_enqueue_port_overhead_get(port_id, ovh);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_enqueue_port_overhead_get);
+
+s32 cbm_enqueue_port_thresh_get(s32 cbm_port_id,
+				cbm_port_thresh_t *thresh, u32 flags)
+{
+	if (g_cbm_ops->cbm_enqueue_port_thresh_get)
+		return g_cbm_ops->cbm_enqueue_port_thresh_get(cbm_port_id,
+							      thresh, flags);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_enqueue_port_thresh_get);
+
+s32 cbm_enqueue_port_thresh_set(s32 cbm_port_id,
+				cbm_port_thresh_t *thresh, u32 flags)
+{
+	if (g_cbm_ops->cbm_enqueue_port_thresh_set)
+		return g_cbm_ops->cbm_enqueue_port_thresh_set(cbm_port_id,
+							      thresh, flags);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_enqueue_port_thresh_set);
+
+s32 cbm_dequeue_cpu_port_stats_get(s32 cbm_port_id, u32 *deq_ctr, u32 flags)
+{
+	if (g_cbm_ops->cbm_dequeue_cpu_port_stats_get)
+		return g_cbm_ops->cbm_dequeue_cpu_port_stats_get(
+		cbm_port_id, deq_ctr, flags);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_dequeue_cpu_port_stats_get);
+
+s32 cbm_enqueue_cpu_port_stats_get(s32 cbm_port_id, u32 *occupancy_ctr,
+				   u32 *enq_ctr, u32 flags)
+{
+	if (g_cbm_ops->cbm_enqueue_cpu_port_stats_get)
+		return g_cbm_ops->cbm_enqueue_cpu_port_stats_get(
+		cbm_port_id, occupancy_ctr, enq_ctr, flags);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_enqueue_cpu_port_stats_get);
+
+s32 cbm_dequeue_dma_port_stats_get(s32 cbm_port_id, u32 *deq_ctr, u32 flags)
+{
+	if (g_cbm_ops->cbm_dequeue_dma_port_stats_get)
+		return g_cbm_ops->cbm_dequeue_dma_port_stats_get(
+		cbm_port_id, deq_ctr, flags);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_dequeue_dma_port_stats_get);
+
+s32 cbm_enqueue_dma_port_stats_get(s32 cbm_port_id, u32 *occupancy_ctr,
+				   u32 *enq_ctr, u32 flags)
+{
+	if (g_cbm_ops->cbm_enqueue_dma_port_stats_get)
+		return g_cbm_ops->cbm_enqueue_dma_port_stats_get(
+		cbm_port_id,
+		occupancy_ctr,
+		enq_ctr,
+		flags);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_enqueue_dma_port_stats_get);
+
+s32 cbm_dp_port_dealloc(struct module *owner, u32 dev_port, s32 cbm_port_id,
+			struct cbm_dp_alloc_data *data, u32 flags)
+{
+	if (g_cbm_ops->cbm_dp_port_dealloc)
+		return g_cbm_ops->cbm_dp_port_dealloc(owner, dev_port,
+						      cbm_port_id, data, flags);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_dp_port_dealloc);
+
+void set_lookup_qid_via_index(u32 index, u32 qid)
+{
+	if (g_cbm_ops->set_lookup_qid_via_index)
+		g_cbm_ops->set_lookup_qid_via_index(index, qid);
+}
+EXPORT_SYMBOL(set_lookup_qid_via_index);
+
+uint8_t get_lookup_qid_via_index(u32 index)
+{
+	if (g_cbm_ops->get_lookup_qid_via_index)
+		return g_cbm_ops->get_lookup_qid_via_index(index);
+	else
+		return 0;
+}
+EXPORT_SYMBOL(get_lookup_qid_via_index);
+
+int cbm_q_thres_get(u32 *length)
+{
+	if (g_cbm_ops->cbm_q_thres_get)
+		return g_cbm_ops->cbm_q_thres_get(length);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_q_thres_get);
+
+int cbm_q_thres_set(u32 length)
+{
+	if (g_cbm_ops->cbm_q_thres_set)
+		return g_cbm_ops->cbm_q_thres_set(length);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_q_thres_set);
+
+s32 cbm_enqueue_mgr_ctrl_get(cbm_eqm_ctrl_t *ctrl, u32 flags)
+{
+	if (g_cbm_ops->cbm_enqueue_mgr_ctrl_get)
+		return g_cbm_ops->cbm_enqueue_mgr_ctrl_get(ctrl, flags);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_enqueue_mgr_ctrl_get);
+
+s32 cbm_enqueue_mgr_ctrl_set(cbm_eqm_ctrl_t *ctrl, u32 flags)
+{
+	if (g_cbm_ops->cbm_enqueue_mgr_ctrl_set)
+		return g_cbm_ops->cbm_enqueue_mgr_ctrl_set(ctrl, flags);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_enqueue_mgr_ctrl_set);
+
+s32 cbm_dequeue_mgr_ctrl_get(cbm_dqm_ctrl_t *ctrl, u32 flags)
+{
+	if (g_cbm_ops->cbm_dequeue_mgr_ctrl_get)
+		return g_cbm_ops->cbm_dequeue_mgr_ctrl_get(ctrl, flags);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_dequeue_mgr_ctrl_get);
+
+s32 cbm_dequeue_mgr_ctrl_set(cbm_dqm_ctrl_t *ctrl, u32 flags)
+{
+	if (g_cbm_ops->cbm_dequeue_mgr_ctrl_set)
+		return g_cbm_ops->cbm_dequeue_mgr_ctrl_set(ctrl, flags);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_dequeue_mgr_ctrl_set);
+
+s32 cbm_igp_delay_set(s32 cbm_port_id, s32 delay)
+{
+	if (g_cbm_ops->cbm_igp_delay_set)
+		return g_cbm_ops->cbm_igp_delay_set(cbm_port_id, delay);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_igp_delay_set);
+
+s32
+cbm_igp_delay_get(s32 cbm_port_id, s32 *delay)
+{
+	if (g_cbm_ops->cbm_igp_delay_get)
+		return g_cbm_ops->cbm_igp_delay_get(cbm_port_id, delay);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_igp_delay_get);
+
+s32
+cbm_queue_delay_enable_set(s32 enable, s32 queue)
+{
+	if (g_cbm_ops->cbm_queue_delay_enable_set)
+		return g_cbm_ops->cbm_queue_delay_enable_set(enable, queue);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_queue_delay_enable_set);
+
+int cbm_counter_mode_set(int enq, int mode)
+{
+	if (g_cbm_ops->cbm_counter_mode_set)
+		return g_cbm_ops->cbm_counter_mode_set(enq, mode);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_counter_mode_set);
+
+int cbm_counter_mode_get(int enq, int *mode)
+{
+	if (g_cbm_ops->cbm_counter_mode_get)
+		return g_cbm_ops->cbm_counter_mode_get(enq, mode);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_counter_mode_get);
+
+u8 get_lookup_qid_via_bits(u32 ep, u32 classid, u32 mpe1, u32 mpe2, u32 enc,
+			   u32 dec, u8 flow_id, u32 dic)
+{
+	if (g_cbm_ops->get_lookup_qid_via_bits)
+		return g_cbm_ops->get_lookup_qid_via_bits(
+		ep, classid, mpe1, mpe2, enc, dec, flow_id, dic);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(get_lookup_qid_via_bits);
+
+s32 cbm_cpu_port_get(struct cbm_cpu_port_data *data, u32 flags)
+{
+	if (g_cbm_ops->cbm_cpu_port_get)
+		return g_cbm_ops->cbm_cpu_port_get(
+		data, flags);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(cbm_cpu_port_get);
+
+s32 pib_program_overshoot(u32 overshoot_bytes)
+{
+	if (g_cbm_ops->pib_program_overshoot)
+		return g_cbm_ops->pib_program_overshoot(
+		overshoot_bytes);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(pib_program_overshoot);
+
+s32 pib_status_get(struct pib_stat *ctrl)
+{
+	if (g_cbm_ops->pib_status_get)
+		return g_cbm_ops->pib_status_get(
+		ctrl);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(pib_status_get);
+
+s32 pib_ovflw_cmd_get(u32 *cmd)
+{
+	if (g_cbm_ops->pib_ovflw_cmd_get)
+		return g_cbm_ops->pib_ovflw_cmd_get(
+		cmd);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(pib_ovflw_cmd_get);
+
+s32 pib_illegal_cmd_get(u32 *cmd)
+{
+	if (g_cbm_ops->pib_illegal_cmd_get)
+		return g_cbm_ops->pib_illegal_cmd_get(
+		cmd);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(pib_illegal_cmd_get);
+
+s32 pon_deq_cntr_get(int port, u32 *count)
+{
+	if (g_cbm_ops->pon_deq_cntr_get)
+		return g_cbm_ops->pon_deq_cntr_get(
+		port, count);
+	else
+		return CBM_FAILURE;
+}
+EXPORT_SYMBOL(pon_deq_cntr_get);
+
+void register_cbm(const struct cbm_ops *cbm_cb)
+{
+	g_cbm_ops = cbm_cb;
+}
diff --git a/drivers/net/ethernet/lantiq/cqm/cqm_common.c b/drivers/net/ethernet/lantiq/cqm/cqm_common.c
new file mode 100644
index 000000000000..737655c719ae
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/cqm/cqm_common.c
@@ -0,0 +1,208 @@
+#include "cqm_common.h"
+
+#ifdef CONFIG_SOC_GRX500
+#include "grx500/reg/cbm_ls.h"
+#else
+#include "falconmx/reg/cbm_ls.h"
+#endif
+
+static void __iomem *cqm_ls_addr_base;
+
+#define LS_BASE cqm_ls_addr_base
+#define LS_TH_SET \
+((DEFAULT_LS_QTHH << LS_CTRL_PORT0_QUEUE_THRSHLD_POS) \
+& (LS_CTRL_PORT0_QUEUE_THRSHLD_MASK))
+#define LS_OVFL_SET \
+((DEFAULT_LS_OVFL_CNT << LS_CTRL_PORT0_CNT_THRSHLD_POS) \
+& (LS_CTRL_PORT0_CNT_THRSHLD_MASK))
+
+void buf_addr_adjust(unsigned int buf_base_addr,
+		     unsigned int buf_size,
+		     unsigned int *adjusted_buf_base,
+		     unsigned int *adjusted_buf_size,
+		     unsigned int align)
+{
+	unsigned int base;
+	unsigned int size;
+
+	pr_info("0x%x 0x%x 0x%x\n", buf_base_addr, buf_size, align);
+	base = ALIGN(buf_base_addr, align);
+	size = buf_base_addr + buf_size - base;
+
+	*adjusted_buf_base = base;
+	*adjusted_buf_size = size;
+	pr_info("0x%x 0x%x\n", base, size);
+}
+
+int cbm_linearise_buf(struct sk_buff *skb, struct cbm_tx_data *data,
+		      int buf_size, u32 new_buf)
+{
+	u32 tmp_buf;
+	void *frag_addr;
+	const skb_frag_t *frag;
+	int i, len = 0, copy_len = 0;
+
+	SKB_FRAG_ASSERT(skb);
+
+	if (new_buf) {
+		tmp_buf = (u32)new_buf;
+		if (skb_is_nonlinear(skb))
+			copy_len = skb_headlen(skb);
+		else
+			copy_len = skb->len;
+		if (data && data->pmac) {
+			memcpy((u8 *)tmp_buf, data->pmac, data->pmac_len);
+			tmp_buf += data->pmac_len;
+		}
+
+		memcpy((u8 *)tmp_buf, skb->data, copy_len);
+		tmp_buf += copy_len;
+		/* If the packet has fragments, copy that also */
+		for (i = 0; i < (skb_shinfo(skb)->nr_frags); i++) {
+			frag = &skb_shinfo(skb)->frags[i];
+			len = skb_frag_size(frag);
+			frag_addr = skb_frag_address(frag);
+			if (len < (buf_size - copy_len)) {
+				memcpy((u8 *)tmp_buf, (u8 *)frag_addr, len);
+				tmp_buf += len;
+				copy_len += len;
+			} else {
+				pr_err("%s:22:copied = %d\n",
+				       __func__, copy_len);
+				pr_err("remaining = %d and skb->head is %x\n",
+				       len, (u32)(skb->head));
+				return CBM_FAILURE;
+			}
+		}
+	} else {
+		pr_err("%s:33:Cannot alloc CBM Buffer !\n", __func__);
+		return CBM_FAILURE;
+	}
+	return CBM_SUCCESS;
+}
+
+/*Load Spreader Initialization*/
+
+static void init_cbm_ls_port(int idx, void __iomem *cqm_ls_addr_base)
+{
+	int lsport = CBM_LS_PORT(idx, ctrl);
+#ifdef CONFIG_CBM_LS_ENABLE
+	/*if ((1 << idx) & g_cpu_port_alloc) {*/
+	if (cpu_online(idx)) {
+	/*Enable spreading only for port 0*/
+#else
+	if (!idx) {
+#endif
+		cbm_w32((LS_BASE + lsport), 0xF
+			| LS_TH_SET
+			| LS_OVFL_SET);
+	} else {
+		cbm_w32((LS_BASE + lsport), 0xD
+			| LS_TH_SET
+			| LS_OVFL_SET);
+	}
+}
+
+void cbm_add_ls_port(int idx, int flag, void __iomem *cqm_ls_addr_base)
+{
+	int lsport = CBM_LS_PORT(idx, ctrl);
+
+	if (flag) {
+		cbm_w32((LS_BASE + lsport), 0xF
+			| LS_TH_SET
+			| LS_OVFL_SET);
+	} else {
+		cbm_w32((LS_BASE + lsport), 0xD
+			| LS_TH_SET
+			| LS_OVFL_SET);
+	}
+}
+
+void cbm_ls_spread_alg_set(u32 alg, void __iomem *cqm_ls_addr_base)
+{
+	cbm_assert(alg < SPREAD_MAX, "illegal cbm load spread alg");
+	set_val((LS_BASE + LS_SPR_CTRL), alg, LS_SPR_CTRL_SPR_SEL_MASK,
+		LS_SPR_CTRL_SPR_SEL_POS);
+}
+
+void cbm_ls_port_weight_set(u32 port_id, u32 weight,
+			    void __iomem *cqm_ls_addr_base)
+{
+	u32 pos, mask;
+
+	cbm_assert(port_id < LS_PORT_NUM, "illegal cbm ls port id");
+
+	pos  = LS_SPR_CTRL_WP0_POS + 2 * port_id;
+	mask = (LS_SPR_CTRL_WP0_MASK >> LS_SPR_CTRL_WP0_POS) << pos;
+	set_val((LS_BASE + LS_SPR_CTRL), weight, mask, pos);
+}
+
+void cbm_ls_flowid_map_set(u32 col, u32 val,
+			   void __iomem *cqm_ls_addr_base)
+{
+	cbm_w32((LS_BASE + LS_FLOWID_MAP_COL0 + (col * 4)), val);
+}
+
+u32 cbm_ls_flowid_map_get(u32 col, void __iomem *ls_base)
+{
+	return cbm_r32(LS_BASE + LS_FLOWID_MAP_COL0 + (col * 4));
+}
+
+u32 cbm_ls_port_weight_get(u32 port_id,
+			   void __iomem *cqm_ls_addr_base)
+{
+	u32 pos, mask;
+
+	cbm_assert(port_id < LS_PORT_NUM, "illegal cbm ls port id");
+
+	pos  = LS_SPR_CTRL_WP0_POS + 2 * port_id;
+	mask = (LS_SPR_CTRL_WP0_MASK >> LS_SPR_CTRL_WP0_POS) << pos;
+	return get_val(cbm_r32(LS_BASE + LS_SPR_CTRL), mask, pos);
+}
+
+void ls_intr_ctrl(u32 val, void __iomem *cqm_ls_addr_base)
+{
+	cbm_w32(LS_BASE + IRNEN_LS, val);
+}
+
+void init_cbm_ls(void __iomem *cqm_ls_addr_base)
+{
+	int i;
+
+	for (i = 0; i < LS_PORT_NUM; i++)
+		init_cbm_ls_port(i, LS_BASE);
+
+	cbm_ls_spread_alg_set(SPREAD_WRR, LS_BASE);
+	for (i = 0; i < LS_PORT_NUM; i++)
+		cbm_ls_port_weight_set(i, DEFAULT_LS_PORT_WEIGHT, LS_BASE);
+
+	cbm_w32((LS_BASE + IRNEN_LS), 0xFF0000);
+	cbm_w32((LS_BASE + LS_GLBL_CTRL), (0x01 << LS_GLBL_CTRL_EN_POS));
+
+	pr_info("Load spreader init successfully\n");
+}
+
+void cbm_dw_memset(u32 *base, int val, u32 size)
+{
+	int i;
+
+	for (i = 0; i < size; i++)
+		base[i] = val;
+}
+
+int cqm_dma_get_controller(char *ctrl)
+{
+	if (!strcmp(ctrl, "DMA1TX"))
+		return DMA1TX;
+	else if (!strcmp(ctrl, "DMA1RX"))
+		return DMA1RX;
+	else if (!strcmp(ctrl, "DMA2TX"))
+		return DMA2TX;
+	else if (!strcmp(ctrl, "DMA2RX"))
+		return DMA2RX;
+	else if (!strcmp(ctrl, "DMA3"))
+		return DMA3;
+	else
+		return DMAMAX;
+}
+
diff --git a/drivers/net/ethernet/lantiq/cqm/cqm_common.h b/drivers/net/ethernet/lantiq/cqm/cqm_common.h
new file mode 100644
index 000000000000..46752ab28d93
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/cqm/cqm_common.h
@@ -0,0 +1,247 @@
+#ifndef __CQM_COMMON_H
+#define __CQM_COMMON_H
+#include <linux/interrupt.h>
+#include <linux/dma-mapping.h>
+#include <linux/dma/lantiq_dmax.h>
+#include <linux/netdevice.h>
+#include <net/lantiq_cbm_api.h>
+
+#define CBM_PMAC_DYNAMIC 1000
+#define CBM_PORT_INVALID 2000
+#define CBM_PMAC_NOT_APPL 3000
+#define CBM_PORT_NOT_APPL 255
+#define LS_PORT_NUM             4
+#define CBM_MAX_INT_LINES       8
+#define CBM_DMA_DESC_OWN        1/*belong to DMA*/
+#define CBM_DMA_DATA_OFFSET     122
+#define DEFAULT_WAIT_CYCLES     20
+#define DEFAULT_LS_QTHH         7
+#define DEFAULT_LS_OVFL_CNT     0x2000
+#define DEFAULT_LS_PORT_WEIGHT  2
+#define CBM_SUCCESS             0
+#define CBM_FAILURE             -1
+#define CBM_EQM_DELAY_ENQ 0x10
+#define CBM_PDU_TYPE 26
+
+#define CBM_LS_PORT(idx, reg) \
+(LS_DESC_DW0_PORT0 + ((idx) * 0x100) + offsetof(struct cbm_ls_reg, reg))
+
+#define cbm_r32(m_reg)		readl(m_reg)
+#define cbm_w32(m_reg, val)	writel(val, m_reg)
+#define cbm_assert(cond, fmt, arg...) \
+do { \
+	if (!(cond)) \
+		pr_err("%d:%s:" fmt "\n", __LINE__, __func__, ##arg); \
+} while (0)
+#define cbm_err(fmt, arg...) \
+pr_err("%d:%s:"fmt "\n", __LINE__, __func__, ##arg)
+
+#ifdef CBM_DEBUG
+#define cbm_debug(fmt, arg...) \
+pr_info(fmt, ##arg)
+#else
+#define cbm_debug(fmt, arg...)
+#endif
+
+#ifdef CBM_DEBUG_LVL_1
+#define cbm_debug_1(fmt, arg...) \
+pr_info(fmt, ##arg)
+#else
+#define cbm_debug_1(fmt, arg...)
+#endif
+
+#define get_val(val, mask, offset) (((val) & (mask)) >> (offset))
+
+enum {
+	SPREAD_WRR = 0,
+	SPREAD_FLOWID = 1,
+	SPREAD_MAX,
+};
+
+struct cbm_ls_reg {
+	struct cbm_desc desc;
+	u32 ctrl;
+	u32 status;
+	u32 resv0[2];
+	struct cbm_desc qdesc[7];
+};
+
+struct qidt_flag_done {
+	u8 cls_done;
+	u8 ep_done;
+	u8 fl_done;
+	u8 fh_done;
+	u8 dec_done;
+	u8 enc_done;
+	u8 mpe1_done;
+	u8 mpe2_done;
+	u8 sub_if_id_done;
+	u8 sub_if_dc_done;
+};
+
+struct cbm_q_info {
+u16 refcnt; /* No of Queue Map table entries pointing to this q */
+u16 qmap_idx_start; /* First index of Queue Map table pointing to q */
+u16 qmap_idx_end; /* last index of Queue Map table pointing to q */
+};
+
+struct cbm_qidt_shadow {
+u32 qidt_shadow;
+u32 qidt_drop_flag;
+};
+
+struct cbm_ops {
+	s32 (*cbm_queue_delay_enable_set)(s32 enable, s32 queue);
+	s32 (*cbm_igp_delay_set)(s32 cbm_port_id, s32 delay);
+	s32 (*cbm_igp_delay_get)(s32 cbm_port_id, s32 *delay);
+	struct sk_buff *(*cbm_build_skb)(void *data, unsigned int frag_size,
+					 gfp_t priority);
+	s32 (*cbm_queue_map_get)(int cbm_inst, s32 queue_id, s32 *num_entries,
+				 cbm_queue_map_entry_t **entries,
+				 u32 flags);
+	s32 (*cbm_queue_map_set)(int cbm_inst, s32 queue_id,
+				 cbm_queue_map_entry_t *entry, u32 flags);
+	s32 (*cqm_qid2ep_map_get)(int qid, int *port);
+	s32 (*cqm_qid2ep_map_set)(int qid, int port);
+	s32 (*cqm_mode_table_get)(int cbm_inst, int *mode,
+				  cbm_queue_map_entry_t *entry, u32 flags);
+	s32 (*cqm_mode_table_set)(int cbm_inst,
+				  cbm_queue_map_entry_t *entry,
+				  u32 flags);
+	int (*cbm_setup_desc)(struct cbm_desc *desc, u32 data_ptr, u32 data_len,
+			      u32 DW0, u32 DW1);
+	int (*cbm_cpu_enqueue_hw)(u32 pid, struct cbm_desc *desc,
+				  void *data_pointer,  int flags);
+	void *(*cbm_buffer_alloc)(u32 pid, u32 flag, u32 size);
+	void *(*cqm_buffer_alloc_by_policy)(u32 pid, u32 flag, u32 policy);
+	struct sk_buff *(*cbm_copy_skb)(const struct sk_buff *skb,
+					gfp_t gfp_mask);
+	struct sk_buff *(*cbm_alloc_skb)(unsigned int size, gfp_t priority);
+	int (*cbm_buffer_free)(u32 pid, void *v_buf, u32 flag);
+	int (*check_ptr_validation)(u32 buf);
+	s32 (*cbm_cpu_pkt_tx)(struct sk_buff *skb, struct cbm_tx_data *data,
+			      u32 flags);
+	s32 (*cbm_port_quick_reset)(s32 cbm_port_id, u32 flags);
+	u32 (*cbm_get_dptr_scpu_egp_count)(u32 cbm_port_id, u32 flags);
+	s32 (*cbm_dp_port_alloc)(struct module *owner, struct net_device *dev,
+				 u32 dev_port, s32 dp_port,
+				 struct cbm_dp_alloc_data *data, u32 flags);
+	int (*cbm_get_wlan_umt_pid)(u32 ep_id, u32 *cbm_pid);
+	s32 (*cbm_dp_enable)(struct module *owner, u32 dp_port,
+			     struct cbm_dp_en_data *data, u32 flags,
+			     u32 alloc_flags);
+	s32 (*cqm_qos_queue_flush)(s32 cqm_inst, s32 cqm_drop_port, int qid);
+	s32 (*cbm_queue_flush)(s32 cbm_port_id, s32 queue_id, u32 timeout,
+			       u32 flags);
+	s32 (*cbm_dp_q_enable)(int cbm_inst, s32 dp_port_id, s32 qnum,
+			       s32 tmu_port_id, s32 remap_to_qid, u32 timeout,
+			       s32 qidt_valid, u32 flags);
+	s32 (*cbm_enqueue_port_resources_get)(cbm_eq_port_res_t *res,
+					      u32 flags);
+	s32 (*cbm_dequeue_port_resources_get)(u32 dp_port,
+					      cbm_dq_port_res_t *res,
+					      u32 flags);
+	s32 (*cbm_dp_port_resources_get)(u32 *dp_port, u32 *num_tmu_ports,
+					 cbm_tmu_res_t **res_pp,
+					 u32 flags);
+	s32 (*cbm_reserved_dp_resources_get)(u32 *tmu_port, u32 *tmu_sched,
+					     u32 *tmu_q);
+	s32 (*cbm_get_egress_port_info)(u32 cbm_port, u32 *tx_ch, u32 *flags);
+	s32 (*cbm_enqueue_port_overhead_set)(s32 port_id, int8_t ovh);
+	s32 (*cbm_enqueue_port_overhead_get)(s32 port_id, int8_t *ovh);
+	s32 (*cbm_enqueue_port_thresh_get)(s32 cbm_port_id,
+					   cbm_port_thresh_t *thresh,
+					   u32 flags);
+	s32 (*cbm_enqueue_port_thresh_set)(s32 cbm_port_id,
+					   cbm_port_thresh_t *thresh,
+					   u32 flags);
+	s32 (*cbm_dequeue_cpu_port_stats_get)(s32 cbm_port_id, u32 *deq_ctr,
+					      u32 flags);
+	s32 (*cbm_enqueue_cpu_port_stats_get)(s32 cbm_port_id,
+					      u32 *occupancy_ctr, u32 *enq_ctr,
+					      u32 flags);
+	s32 (*cbm_dequeue_dma_port_stats_get)(s32 cbm_port_id, u32 *deq_ctr,
+					      u32 flags);
+	s32 (*cbm_enqueue_dma_port_stats_get)(s32 cbm_port_id,
+					      u32 *occupancy_ctr,
+					      u32 *enq_ctr,
+					      u32 flags);
+	void (*set_lookup_qid_via_index)(u32 index, u32 qid);
+	uint8_t (*get_lookup_qid_via_index)(u32 index);
+	u8 (*get_lookup_qid_via_bits)(
+	u32 ep,
+	u32 classid,
+	u32 mpe1,
+	u32 mpe2,
+	u32 enc,
+	u32 dec,
+	u8 flow_id,
+	u32 dic);
+	int (*cbm_q_thres_get)(u32 *length);
+	int (*cbm_q_thres_set)(u32 length);
+	s32 (*cbm_dp_port_dealloc)(struct module *owner, u32 dev_port,
+				   s32 cbm_port_id,
+				   struct cbm_dp_alloc_data *data, u32 flags);
+	s32 (*cbm_enqueue_mgr_ctrl_get)(cbm_eqm_ctrl_t *ctrl, u32 flags);
+	s32 (*cbm_enqueue_mgr_ctrl_set)(cbm_eqm_ctrl_t *ctrl, u32 flags);
+	s32 (*cbm_dequeue_mgr_ctrl_get)(cbm_dqm_ctrl_t *ctrl, u32 flags);
+	s32 (*cbm_dequeue_mgr_ctrl_set)(cbm_dqm_ctrl_t *ctrl, u32 flags);
+	int (*cbm_counter_mode_set)(int enq, int mode);
+	int (*cbm_counter_mode_get)(int enq, int *mode);
+	s32 (*cbm_cpu_port_get)(struct cbm_cpu_port_data *data, u32 flags);
+	s32 (*pib_program_overshoot)(u32 overshoot_bytes);
+	s32 (*pib_status_get)(struct pib_stat *ctrl);
+	s32 (*pib_ovflw_cmd_get)(u32 *cmd);
+	s32 (*pib_illegal_cmd_get)(u32 *cmd);
+	s32 (*pon_deq_cntr_get)(int port, u32 *count);
+	void (*cbm_setup_DMA_p2p)(void);
+	int (*cbm_turn_on_DMA_p2p)(void);
+};
+
+static inline void set_val(void __iomem *reg, u32 val, u32 mask, u32 offset)
+{
+	u32 temp_val = cbm_r32(reg);
+
+	temp_val &= ~(mask);
+	temp_val |= (((val) << (offset)) & (mask));
+	cbm_w32(reg, temp_val);
+}
+
+static inline int cqm_desc_data_len(u32 dw)
+{
+	return dw & 0x0000FFFF;
+}
+
+static inline int cqm_desc_data_off(u32 dw)
+{
+	return (dw & 0x3800000) >> 23;
+}
+
+static inline int cqm_desc_data_pool(u32 dw)
+{
+	return (dw & 0x70000) >> 16;
+}
+
+static inline int cqm_desc_data_policy(u32 dw)
+{
+	return (dw & 0x700000) >> 20;
+}
+
+static inline int get_is_bit_set(u32 flags)
+{
+	return ffs(flags) - 1;
+}
+
+void buf_addr_adjust(unsigned int buf_base_addr, unsigned int buf_size,
+		     unsigned int *adjusted_buf_base,
+		     unsigned int *adjusted_buf_size,
+		     unsigned int align);
+int cbm_linearise_buf(struct sk_buff *skb, struct cbm_tx_data *data,
+		      int buf_size, u32 new_buf);
+void init_cbm_ls(void __iomem *cqm_ls_addr_base);
+int cqm_dma_get_controller(char *ctrl);
+void ls_intr_ctrl(u32 val, void __iomem *cqm_ls_addr_base);
+void cbm_dw_memset(u32 *base, int val, u32 size);
+void register_cbm(const struct cbm_ops *cbm_cb);
+#endif
+
diff --git a/drivers/net/ethernet/lantiq/cqm/cqm_dev.c b/drivers/net/ethernet/lantiq/cqm/cqm_dev.c
new file mode 100644
index 000000000000..a95d8e72a5ec
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/cqm/cqm_dev.c
@@ -0,0 +1,186 @@
+#include "cqm_dev.h"
+
+#define CQM_NUM_DEV_SUPP 2
+#define MAX_NUM_BASE_ADDR 16
+#define MAX_NUM_INTR 8
+static struct dt_node_inst dev_node_name[CQM_NUM_DEV_SUPP] = {
+	{FALCON_DEV_NAME, "cqm", 0},
+	{GRX500_DEV_NAME, "cbm", 1}
+};
+
+static struct device_node *parse_dts(int j, void **pdata, struct resource **res,
+				     int *num_res);
+
+static int add_cqm_dev(int i);
+static int cqm_platdev_parse_dts(void);
+struct device_node *parse_dts(int j, void **pdata, struct resource **res,
+			      int *num_res)
+{
+	struct device_node *node = NULL;
+	struct device_node *ret_node = NULL;
+	int idx = 0;
+	struct cqm_data *cqm_pdata = NULL;
+	unsigned int intr[MAX_NUM_INTR];
+	struct resource resource[MAX_NUM_BASE_ADDR];
+
+	pr_info("[%s] .. [%d]\n", __func__, __LINE__);
+
+	node = of_find_node_by_name(NULL, dev_node_name[j].node_name);
+	if (!node) {
+		pr_err("Unable to get node %s for %s\n",
+		       dev_node_name[j].node_name,
+		       dev_node_name[j].dev_name);
+		return NULL;
+	}
+	*pdata = kzalloc(sizeof(*cqm_pdata), GFP_KERNEL);
+	if (!*pdata) {
+		pr_err("%s: Failed to allocate pdata.\n", __func__);
+		goto err_free_pdata;
+	}
+	cqm_pdata = (struct cqm_data *)(*pdata);
+	for (idx = 0; idx < MAX_NUM_BASE_ADDR; idx++) {
+		if (of_address_to_resource(node, idx, &resource[idx]))
+			break;
+	}
+	*res = kmalloc_array(idx, sizeof(struct resource),
+					 GFP_KERNEL);
+	if (!*res)
+		pr_info("error allocating memory\n");
+	memcpy(*res, resource, (sizeof(struct resource) * idx));
+	cqm_pdata->num_resources = idx;
+	*num_res = idx;
+	pr_info("num_res %d\n", *num_res);
+
+	for (idx = 0; idx < MAX_NUM_INTR; idx++) {
+		intr[idx] = irq_of_parse_and_map(node, idx);
+		pr_info("intr %d\n", intr[idx]);
+		if (!intr[idx])
+			break;
+	}
+	cqm_pdata->num_intrs = idx;
+	cqm_pdata->intrs = kmalloc_array(idx, sizeof(unsigned int),
+						   GFP_KERNEL);
+	memcpy(cqm_pdata->intrs, intr, (sizeof(unsigned int) * idx));
+	cqm_pdata->rcu_reset = of_reset_control_get(node, "cqm");
+	if (IS_ERR(cqm_pdata->rcu_reset)) {
+		pr_err("No rcu reset for %s\n", dev_node_name[j].node_name);
+		/*return PTR_ERR(cqm_pdata->rcu_reset)*/;
+	}
+
+	cqm_pdata->cqm_clk[0] = (void *)of_clk_get_by_name(node, "freq");
+	if (IS_ERR(cqm_pdata->cqm_clk[0]))
+		pr_err("Error getting freq clk\n");
+	cqm_pdata->cqm_clk[1] = (void *)of_clk_get_by_name(node, "cbm");
+	if (IS_ERR(cqm_pdata->cqm_clk[1]))
+		pr_err("Error getting cqm clk\n");
+	cqm_pdata->syscfg = syscon_regmap_lookup_by_phandle(node,
+							    "lantiq,wanmux");
+	if (IS_ERR(cqm_pdata->syscfg)) {
+		pr_err("No syscon phandle specified for wan mux\n");
+		cqm_pdata->syscfg = NULL;
+	}
+	cqm_pdata->force_xpcs = of_property_read_bool(node, "intel,force-xpcs");
+	ret_node = node;
+	return ret_node;
+
+err_free_pdata:
+	kfree(pdata);
+	return NULL;
+}
+
+int add_cqm_dev(int i)
+{
+	struct platform_device *pdev = NULL;
+	void *pdata = NULL;
+	struct device_node *node = NULL;
+	struct resource *res;
+	int ret = CBM_SUCCESS, num_res;
+
+	node = parse_dts(i, &pdata, &res, &num_res);
+	if (!node) {
+		pr_err("%s(#%d): parse_dts fail for %s\n",
+		 __func__, __LINE__, dev_node_name[i].dev_name);
+		return CBM_FAILURE;
+	}
+	pr_info("parse dts done\n");
+#if 1
+
+	pdev = platform_device_alloc(dev_node_name[i].dev_name, 1);
+	if (!pdev) {
+		pr_err("%s(#%d): platform_device_alloc fail for %s\n",
+		       __func__, __LINE__, dev_node_name[i].node_name);
+		return -ENOMEM;
+	}
+
+	/* Attach node into platform device of_node */
+	pdev->dev.of_node = node;
+	/* Set the  private data */
+	if (pdata)
+		platform_set_drvdata(pdev, pdata);
+	/* Add resources to platform device */
+	if ((num_res > 0) && res) {
+		pr_info("adding resources\n");
+		ret = platform_device_add_resources(pdev, res, num_res);
+		if (ret) {
+			pr_info("%s: Failed to add resources for %s.\n",
+				__func__, dev_node_name[i].node_name);
+			goto err_free_pdata;
+		}
+	}
+
+	pr_info("resources added\n");
+	/* Add platform device */
+	ret = platform_device_add(pdev);
+	if (ret) {
+		pr_info("%s: Failed to add platform device for %s.\n",
+			__func__, dev_node_name[i].node_name);
+		goto err_free_pdata;
+	}
+#endif
+
+	pr_info(" Successfully Registered Platform device %s.\n", pdev->name);
+	return ret;
+
+err_free_pdata:
+	kfree(pdata);
+	kfree(res);
+	return ret;
+}
+
+int nodefromdevice(const char *dev)
+{
+	int i;
+
+	for (i = 0; i < CQM_NUM_DEV_SUPP; i++) {
+		if (strcmp(dev_node_name[i].dev_name, dev) == 0)
+			return i;
+	}
+	return -1;
+}
+
+static int cqm_platdev_parse_dts(void)
+{
+	int i, dev_add = 0;
+
+	for (i = 0; i < CQM_NUM_DEV_SUPP; i++) {
+		pr_info("dev %s\n", dev_node_name[i].dev_name);
+		if (!add_cqm_dev(i)) {
+			dev_add++;
+			pr_info("device added\n");
+		}
+	}
+	if (!dev_add)
+		pr_err("Not Even 1 CBM device registered\n");
+	return 0;
+}
+
+static __init int cqm_platdev_init(void)
+{
+	pr_info("%s is called\n", __func__);
+	cqm_platdev_parse_dts();
+	return 0;
+}
+
+arch_initcall(cqm_platdev_init);
+
+MODULE_LICENSE("GPL");
diff --git a/drivers/net/ethernet/lantiq/cqm/cqm_dev.h b/drivers/net/ethernet/lantiq/cqm/cqm_dev.h
new file mode 100644
index 000000000000..647797331ce5
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/cqm/cqm_dev.h
@@ -0,0 +1,41 @@
+#ifndef _CQM_DEV_H
+#define _CQM_DEV_H
+#include <linux/interrupt.h>
+#include <linux/slab.h>
+#include <linux/export.h>
+#include <linux/err.h>
+#include <linux/types.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/clk.h>
+
+#include <linux/of.h>
+#include <linux/of_platform.h>
+#include <linux/of_address.h>
+#include <linux/of_irq.h>
+#include <linux/reset.h>
+#include <lantiq.h>
+#include <linux/platform_device.h>
+#include <linux/mfd/syscon.h>
+#include <lantiq_soc.h>
+#include "cqm_common.h"
+
+#define FALCON_DEV_NAME "falcon-cqm"
+#define GRX500_DEV_NAME "grx500-cbm"
+
+struct dt_node_inst {
+	char *dev_name;
+	char *node_name;
+	int instance_id;
+};
+
+struct cqm_data {
+	int num_resources;
+	int num_intrs;
+	unsigned int *intrs;
+	struct clk *cqm_clk[2];
+	struct reset_control *rcu_reset;
+	struct regmap *syscfg;
+	bool force_xpcs;
+};
+#endif
