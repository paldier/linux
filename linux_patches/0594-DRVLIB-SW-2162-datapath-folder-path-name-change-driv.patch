From 530c27f4c45711bd16d3f186b6deff32cdb858b4 Mon Sep 17 00:00:00 2001
From: Rekha Eswaran <rekha.eswaran@intel.com>
Date: Mon, 13 May 2019 13:41:59 +0800
Subject: [PATCH] DRVLIB_SW-2162 - datapath folder path name change
 drivers/net/ethernet/lantiq/datapath to drivers/net/ethernet/datapath/dpm

---
 .../lantiq/datapath => datapath/dpm}/Kconfig       |    5 +-
 .../lantiq/datapath => datapath/dpm}/Makefile      |    0
 .../lantiq/datapath => datapath/dpm}/datapath.h    |    0
 .../datapath => datapath/dpm}/datapath_api.c       |    0
 .../datapath => datapath/dpm}/datapath_instance.c  |    0
 .../datapath => datapath/dpm}/datapath_instance.h  |    0
 .../datapath => datapath/dpm}/datapath_ioctl.c     |    0
 .../datapath => datapath/dpm}/datapath_ioctl.h     |    0
 .../dpm}/datapath_logical_dev.c                    |    0
 .../datapath => datapath/dpm}/datapath_misc.c      |    0
 .../datapath => datapath/dpm}/datapath_notifier.c  |    0
 .../dpm}/datapath_platform_dev.c                   |    0
 .../datapath => datapath/dpm}/datapath_proc.c      |    0
 .../datapath => datapath/dpm}/datapath_proc_api.c  |    0
 .../datapath => datapath/dpm}/datapath_proc_qos.c  |    0
 .../datapath => datapath/dpm}/datapath_qos.c       |    0
 .../datapath => datapath/dpm}/datapath_soc.c       |    0
 .../datapath => datapath/dpm}/datapath_swdev.c     |    0
 .../datapath => datapath/dpm}/datapath_swdev.h     |    0
 .../datapath => datapath/dpm}/datapath_swdev_api.h |    0
 .../datapath => datapath/dpm}/gswip30/Kconfig      |    0
 .../datapath => datapath/dpm}/gswip30/Makefile     |    0
 .../dpm}/gswip30/datapath_coc.c                    |    0
 .../dpm}/gswip30/datapath_gswip.c                  |    0
 .../dpm}/gswip30/datapath_lookup_proc.c            |    0
 .../dpm}/gswip30/datapath_mib.c                    |    0
 .../dpm}/gswip30/datapath_mib.h                    |    0
 .../dpm}/gswip30/datapath_misc.c                   |    0
 .../dpm}/gswip30/datapath_misc.h                   |    0
 .../dpm}/gswip30/datapath_proc.c                   |    0
 .../dpm}/gswip30/datapath_proc.h                   |    0
 .../dpm}/gswip30/datapath_rx.c                     |    0
 .../dpm}/gswip30/datapath_tx.c                     |    0
 .../datapath => datapath/dpm}/gswip31/Kconfig      |    0
 .../datapath => datapath/dpm}/gswip31/Makefile     |    0
 .../dpm}/gswip31/datapath_coc.c                    |    0
 .../dpm}/gswip31/datapath_ext_vlan.c               |    0
 .../dpm}/gswip31/datapath_gswip.c                  |    0
 .../dpm}/gswip31/datapath_gswip_simulate.c         |    0
 .../dpm}/gswip31/datapath_gswip_simulate.h         |    0
 .../dpm}/gswip31/datapath_lookup_proc.c            |    0
 .../dpm}/gswip31/datapath_mib.c                    |    0
 .../dpm}/gswip31/datapath_mib.h                    |    0
 .../dpm}/gswip31/datapath_misc.c                   |    2 +-
 .../dpm}/gswip31/datapath_misc.h                   |    0
 .../dpm}/gswip31/datapath_ppv4.c                   |    0
 .../dpm}/gswip31/datapath_ppv4.h                   |    0
 .../dpm}/gswip31/datapath_ppv4_api.c               |    0
 .../dpm}/gswip31/datapath_proc.c                   |    0
 .../dpm}/gswip31/datapath_proc.h                   |    0
 .../dpm}/gswip31/datapath_rx.c                     |    0
 .../dpm}/gswip31/datapath_switchdev.c              |    0
 .../dpm}/gswip31/datapath_switchdev.h              |    0
 .../dpm}/gswip31/datapath_tc_asym_vlan.c           |    0
 .../dpm}/gswip31/datapath_tx.c                     |    0
 drivers/net/datapath/dpm/gswip32/Kconfig           |   29 +
 drivers/net/datapath/dpm/gswip32/Makefile          |   18 +
 drivers/net/datapath/dpm/gswip32/datapath_coc.c    |  241 +
 .../net/datapath/dpm/gswip32/datapath_ext_vlan.c   |  623 +++
 drivers/net/datapath/dpm/gswip32/datapath_gswip.c  | 1720 ++++++++
 .../datapath/dpm/gswip32/datapath_gswip_simulate.c |  983 +++++
 .../datapath/dpm/gswip32/datapath_gswip_simulate.h |   39 +
 .../datapath/dpm/gswip32/datapath_lookup_proc.c    |  738 ++++
 drivers/net/datapath/dpm/gswip32/datapath_mib.c    |  384 ++
 drivers/net/datapath/dpm/gswip32/datapath_mib.h    |   19 +
 drivers/net/datapath/dpm/gswip32/datapath_misc.c   | 2240 ++++++++++
 drivers/net/datapath/dpm/gswip32/datapath_misc.h   |  269 ++
 drivers/net/datapath/dpm/gswip32/datapath_ppv4.c   |  682 +++
 drivers/net/datapath/dpm/gswip32/datapath_ppv4.h   |  216 +
 .../net/datapath/dpm/gswip32/datapath_ppv4_api.c   | 4598 ++++++++++++++++++++
 .../datapath/dpm/gswip32/datapath_ppv4_session.c   |  522 +++
 .../datapath/dpm/gswip32/datapath_ppv4_session.h   |   73 +
 drivers/net/datapath/dpm/gswip32/datapath_proc.c   |  397 ++
 drivers/net/datapath/dpm/gswip32/datapath_proc.h   |    9 +
 drivers/net/datapath/dpm/gswip32/datapath_rx.c     |  300 ++
 .../net/datapath/dpm/gswip32/datapath_switchdev.c  |  358 ++
 .../net/datapath/dpm/gswip32/datapath_switchdev.h  |   20 +
 .../datapath/dpm/gswip32/datapath_tc_asym_vlan.c   |  880 ++++
 drivers/net/datapath/dpm/gswip32/datapath_tx.c     |  484 +++
 drivers/net/ethernet/lantiq/Kconfig                |    2 +-
 drivers/net/ethernet/lantiq/Makefile               |    2 +-
 81 files changed, 15848 insertions(+), 5 deletions(-)

diff --git a/drivers/net/ethernet/lantiq/datapath/Kconfig b/drivers/net/datapath/dpm/Kconfig
similarity index 96%
rename from drivers/net/ethernet/lantiq/datapath/Kconfig
rename to drivers/net/datapath/dpm/Kconfig
index ceb6e5c57168..1f555104b496 100644
--- a/drivers/net/ethernet/lantiq/datapath/Kconfig
+++ b/drivers/net/datapath/dpm/Kconfig
@@ -153,7 +153,8 @@ config LTQ_DATAPATH_CPUFREQ
 	bool
 	default INTEL_DATAPATH_CPUFREQ
 
-source "drivers/net/ethernet/lantiq/datapath/gswip31/Kconfig"
-source "drivers/net/ethernet/lantiq/datapath/gswip30/Kconfig"
+source "drivers/net/datapath/dpm/gswip32/Kconfig"
+source "drivers/net/datapath/dpm/gswip31/Kconfig"
+source "drivers/net/datapath/dpm/gswip30/Kconfig"
 endif
 
diff --git a/drivers/net/ethernet/lantiq/datapath/Makefile b/drivers/net/datapath/dpm/Makefile
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/Makefile
rename to drivers/net/datapath/dpm/Makefile
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath.h b/drivers/net/datapath/dpm/datapath.h
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/datapath.h
rename to drivers/net/datapath/dpm/datapath.h
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_api.c b/drivers/net/datapath/dpm/datapath_api.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/datapath_api.c
rename to drivers/net/datapath/dpm/datapath_api.c
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_instance.c b/drivers/net/datapath/dpm/datapath_instance.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/datapath_instance.c
rename to drivers/net/datapath/dpm/datapath_instance.c
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_instance.h b/drivers/net/datapath/dpm/datapath_instance.h
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/datapath_instance.h
rename to drivers/net/datapath/dpm/datapath_instance.h
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_ioctl.c b/drivers/net/datapath/dpm/datapath_ioctl.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/datapath_ioctl.c
rename to drivers/net/datapath/dpm/datapath_ioctl.c
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_ioctl.h b/drivers/net/datapath/dpm/datapath_ioctl.h
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/datapath_ioctl.h
rename to drivers/net/datapath/dpm/datapath_ioctl.h
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_logical_dev.c b/drivers/net/datapath/dpm/datapath_logical_dev.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/datapath_logical_dev.c
rename to drivers/net/datapath/dpm/datapath_logical_dev.c
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_misc.c b/drivers/net/datapath/dpm/datapath_misc.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/datapath_misc.c
rename to drivers/net/datapath/dpm/datapath_misc.c
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_notifier.c b/drivers/net/datapath/dpm/datapath_notifier.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/datapath_notifier.c
rename to drivers/net/datapath/dpm/datapath_notifier.c
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_platform_dev.c b/drivers/net/datapath/dpm/datapath_platform_dev.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/datapath_platform_dev.c
rename to drivers/net/datapath/dpm/datapath_platform_dev.c
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_proc.c b/drivers/net/datapath/dpm/datapath_proc.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/datapath_proc.c
rename to drivers/net/datapath/dpm/datapath_proc.c
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_proc_api.c b/drivers/net/datapath/dpm/datapath_proc_api.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/datapath_proc_api.c
rename to drivers/net/datapath/dpm/datapath_proc_api.c
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_proc_qos.c b/drivers/net/datapath/dpm/datapath_proc_qos.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/datapath_proc_qos.c
rename to drivers/net/datapath/dpm/datapath_proc_qos.c
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_qos.c b/drivers/net/datapath/dpm/datapath_qos.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/datapath_qos.c
rename to drivers/net/datapath/dpm/datapath_qos.c
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_soc.c b/drivers/net/datapath/dpm/datapath_soc.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/datapath_soc.c
rename to drivers/net/datapath/dpm/datapath_soc.c
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_swdev.c b/drivers/net/datapath/dpm/datapath_swdev.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/datapath_swdev.c
rename to drivers/net/datapath/dpm/datapath_swdev.c
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_swdev.h b/drivers/net/datapath/dpm/datapath_swdev.h
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/datapath_swdev.h
rename to drivers/net/datapath/dpm/datapath_swdev.h
diff --git a/drivers/net/ethernet/lantiq/datapath/datapath_swdev_api.h b/drivers/net/datapath/dpm/datapath_swdev_api.h
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/datapath_swdev_api.h
rename to drivers/net/datapath/dpm/datapath_swdev_api.h
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip30/Kconfig b/drivers/net/datapath/dpm/gswip30/Kconfig
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip30/Kconfig
rename to drivers/net/datapath/dpm/gswip30/Kconfig
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip30/Makefile b/drivers/net/datapath/dpm/gswip30/Makefile
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip30/Makefile
rename to drivers/net/datapath/dpm/gswip30/Makefile
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip30/datapath_coc.c b/drivers/net/datapath/dpm/gswip30/datapath_coc.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip30/datapath_coc.c
rename to drivers/net/datapath/dpm/gswip30/datapath_coc.c
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip30/datapath_gswip.c b/drivers/net/datapath/dpm/gswip30/datapath_gswip.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip30/datapath_gswip.c
rename to drivers/net/datapath/dpm/gswip30/datapath_gswip.c
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip30/datapath_lookup_proc.c b/drivers/net/datapath/dpm/gswip30/datapath_lookup_proc.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip30/datapath_lookup_proc.c
rename to drivers/net/datapath/dpm/gswip30/datapath_lookup_proc.c
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip30/datapath_mib.c b/drivers/net/datapath/dpm/gswip30/datapath_mib.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip30/datapath_mib.c
rename to drivers/net/datapath/dpm/gswip30/datapath_mib.c
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip30/datapath_mib.h b/drivers/net/datapath/dpm/gswip30/datapath_mib.h
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip30/datapath_mib.h
rename to drivers/net/datapath/dpm/gswip30/datapath_mib.h
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip30/datapath_misc.c b/drivers/net/datapath/dpm/gswip30/datapath_misc.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip30/datapath_misc.c
rename to drivers/net/datapath/dpm/gswip30/datapath_misc.c
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip30/datapath_misc.h b/drivers/net/datapath/dpm/gswip30/datapath_misc.h
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip30/datapath_misc.h
rename to drivers/net/datapath/dpm/gswip30/datapath_misc.h
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip30/datapath_proc.c b/drivers/net/datapath/dpm/gswip30/datapath_proc.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip30/datapath_proc.c
rename to drivers/net/datapath/dpm/gswip30/datapath_proc.c
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip30/datapath_proc.h b/drivers/net/datapath/dpm/gswip30/datapath_proc.h
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip30/datapath_proc.h
rename to drivers/net/datapath/dpm/gswip30/datapath_proc.h
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip30/datapath_rx.c b/drivers/net/datapath/dpm/gswip30/datapath_rx.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip30/datapath_rx.c
rename to drivers/net/datapath/dpm/gswip30/datapath_rx.c
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip30/datapath_tx.c b/drivers/net/datapath/dpm/gswip30/datapath_tx.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip30/datapath_tx.c
rename to drivers/net/datapath/dpm/gswip30/datapath_tx.c
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/Kconfig b/drivers/net/datapath/dpm/gswip31/Kconfig
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip31/Kconfig
rename to drivers/net/datapath/dpm/gswip31/Kconfig
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/Makefile b/drivers/net/datapath/dpm/gswip31/Makefile
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip31/Makefile
rename to drivers/net/datapath/dpm/gswip31/Makefile
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_coc.c b/drivers/net/datapath/dpm/gswip31/datapath_coc.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip31/datapath_coc.c
rename to drivers/net/datapath/dpm/gswip31/datapath_coc.c
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_ext_vlan.c b/drivers/net/datapath/dpm/gswip31/datapath_ext_vlan.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip31/datapath_ext_vlan.c
rename to drivers/net/datapath/dpm/gswip31/datapath_ext_vlan.c
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_gswip.c b/drivers/net/datapath/dpm/gswip31/datapath_gswip.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip31/datapath_gswip.c
rename to drivers/net/datapath/dpm/gswip31/datapath_gswip.c
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_gswip_simulate.c b/drivers/net/datapath/dpm/gswip31/datapath_gswip_simulate.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip31/datapath_gswip_simulate.c
rename to drivers/net/datapath/dpm/gswip31/datapath_gswip_simulate.c
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_gswip_simulate.h b/drivers/net/datapath/dpm/gswip31/datapath_gswip_simulate.h
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip31/datapath_gswip_simulate.h
rename to drivers/net/datapath/dpm/gswip31/datapath_gswip_simulate.h
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_lookup_proc.c b/drivers/net/datapath/dpm/gswip31/datapath_lookup_proc.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip31/datapath_lookup_proc.c
rename to drivers/net/datapath/dpm/gswip31/datapath_lookup_proc.c
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_mib.c b/drivers/net/datapath/dpm/gswip31/datapath_mib.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip31/datapath_mib.c
rename to drivers/net/datapath/dpm/gswip31/datapath_mib.c
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_mib.h b/drivers/net/datapath/dpm/gswip31/datapath_mib.h
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip31/datapath_mib.h
rename to drivers/net/datapath/dpm/gswip31/datapath_mib.h
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_misc.c b/drivers/net/datapath/dpm/gswip31/datapath_misc.c
similarity index 99%
rename from drivers/net/ethernet/lantiq/datapath/gswip31/datapath_misc.c
rename to drivers/net/datapath/dpm/gswip31/datapath_misc.c
index 6ed9d0f6c568..0558db171c6f 100644
--- a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_misc.c
+++ b/drivers/net/datapath/dpm/gswip31/datapath_misc.c
@@ -465,7 +465,7 @@ int alloc_q_to_port(struct ppv4_q_sch_port *info, u32 flag)
 	if (priv->deq_port_stat[info->cqe_deq].flag == PP_NODE_FREE) {
 		port.cqm_deq_port = info->cqe_deq;
 		port.tx_pkt_credit = info->tx_pkt_credit;
-		port.tx_ring_addr = info->tx_ring_addr;
+		port.tx_ring_addr = info->tx_ring_addr_push;
 		port.tx_ring_size = info->tx_ring_size;
 		port.inst = inst;
 		port.dp_port = info->dp_port;
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_misc.h b/drivers/net/datapath/dpm/gswip31/datapath_misc.h
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip31/datapath_misc.h
rename to drivers/net/datapath/dpm/gswip31/datapath_misc.h
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_ppv4.c b/drivers/net/datapath/dpm/gswip31/datapath_ppv4.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip31/datapath_ppv4.c
rename to drivers/net/datapath/dpm/gswip31/datapath_ppv4.c
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_ppv4.h b/drivers/net/datapath/dpm/gswip31/datapath_ppv4.h
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip31/datapath_ppv4.h
rename to drivers/net/datapath/dpm/gswip31/datapath_ppv4.h
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_ppv4_api.c b/drivers/net/datapath/dpm/gswip31/datapath_ppv4_api.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip31/datapath_ppv4_api.c
rename to drivers/net/datapath/dpm/gswip31/datapath_ppv4_api.c
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_proc.c b/drivers/net/datapath/dpm/gswip31/datapath_proc.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip31/datapath_proc.c
rename to drivers/net/datapath/dpm/gswip31/datapath_proc.c
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_proc.h b/drivers/net/datapath/dpm/gswip31/datapath_proc.h
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip31/datapath_proc.h
rename to drivers/net/datapath/dpm/gswip31/datapath_proc.h
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_rx.c b/drivers/net/datapath/dpm/gswip31/datapath_rx.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip31/datapath_rx.c
rename to drivers/net/datapath/dpm/gswip31/datapath_rx.c
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_switchdev.c b/drivers/net/datapath/dpm/gswip31/datapath_switchdev.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip31/datapath_switchdev.c
rename to drivers/net/datapath/dpm/gswip31/datapath_switchdev.c
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_switchdev.h b/drivers/net/datapath/dpm/gswip31/datapath_switchdev.h
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip31/datapath_switchdev.h
rename to drivers/net/datapath/dpm/gswip31/datapath_switchdev.h
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_tc_asym_vlan.c b/drivers/net/datapath/dpm/gswip31/datapath_tc_asym_vlan.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip31/datapath_tc_asym_vlan.c
rename to drivers/net/datapath/dpm/gswip31/datapath_tc_asym_vlan.c
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_tx.c b/drivers/net/datapath/dpm/gswip31/datapath_tx.c
similarity index 100%
rename from drivers/net/ethernet/lantiq/datapath/gswip31/datapath_tx.c
rename to drivers/net/datapath/dpm/gswip31/datapath_tx.c
diff --git a/drivers/net/datapath/dpm/gswip32/Kconfig b/drivers/net/datapath/dpm/gswip32/Kconfig
new file mode 100644
index 000000000000..e89c6b36fea7
--- /dev/null
+++ b/drivers/net/datapath/dpm/gswip32/Kconfig
@@ -0,0 +1,29 @@
+menuconfig INTEL_DATAPATH_HAL_GSWIP32
+	bool "Datapath HAL_GSWIP32"
+	default y
+	depends on INTEL_DATAPATH && (X86_INTEL_LGM || INTEL_DATAPATH_SIMULATE_GSWIP32)
+	---help---
+	  Datapath Lib is to provide common rx/tx wrapper Lib without taking
+	  care of much HW knowledge and also provide common interface for legacy
+	  devices and different HW like to CBM or LRO.
+	  Take note: All devices need to register to datapath Lib first
+
+if INTEL_DATAPATH_HAL_GSWIP32
+config INTEL_DATAPATH_HAL_GSWIP32_MIB
+	bool "Datapath aggregated mib support"
+	default n
+	---help---
+	  It is to aggregate GSWIP-O, PP and driver's MIB counter
+
+config INTEL_DATAPATH_HAL_GSWIP32_CPUFREQ
+	bool "Datapath DFS(COC) support"
+	depends on INTEL_DATAPATH && LTQ_CPUFREQ && LTQ_ETHSW_API
+	default y
+	---help---
+	  It is to support DFS(COC) in Datapath
+
+config INTEL_DATAPATH_QOS_HAL
+	bool "datapath QOS hal"
+	default y
+	depends on PPV4 || LTQ_PPV4
+endif
diff --git a/drivers/net/datapath/dpm/gswip32/Makefile b/drivers/net/datapath/dpm/gswip32/Makefile
new file mode 100644
index 000000000000..2ad2121d836c
--- /dev/null
+++ b/drivers/net/datapath/dpm/gswip32/Makefile
@@ -0,0 +1,18 @@
+obj-$(CONFIG_INTEL_DATAPATH) += datapath_misc.o datapath_gswip.o datapath_proc.o
+obj-$(CONFIG_INTEL_DATAPATH) += datapath_rx.o datapath_tx.o datapath_lookup_proc.o 
+
+ifneq ($(CONFIG_INTEL_DATAPATH_QOS_HAL),)
+obj-$(CONFIG_INTEL_DATAPATH) += datapath_ppv4_api.o datapath_ppv4.o datapath_ppv4_session.o
+endif
+
+ifneq ($(CONFIG_INTEL_DATAPATH_HAL_GSWIP32_MIB),)
+obj-$(CONFIG_INTEL_DATAPATH) += datapath_mib.o
+endif
+
+ifneq ($(CONFIG_INTEL_DATAPATH_CPUFREQ),)
+obj-$(CONFIG_INTEL_DATAPATH) += datapath_coc.o
+endif
+
+ifneq ($(CONFIG_INTEL_DATAPATH_SWITCHDEV),)
+obj-$(CONFIG_INTEL_DATAPATH) += datapath_switchdev.o datapath_ext_vlan.o datapath_tc_asym_vlan.o
+endif
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_coc.c b/drivers/net/datapath/dpm/gswip32/datapath_coc.c
new file mode 100644
index 000000000000..d0bbc3d1bc4d
--- /dev/null
+++ b/drivers/net/datapath/dpm/gswip32/datapath_coc.c
@@ -0,0 +1,241 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/module.h>
+#include <linux/types.h>	/* size_t */
+#include <linux/timer.h>
+#include <net/datapath_api.h>
+#include <net/datapath_proc_api.h>
+#include "../datapath.h"
+
+#define DP_MODULE  LTQ_CPUFREQ_MODULE_DP
+#define DP_ID 0	 /* this ID should represent the Datapath interface No. */
+static struct timer_list dp_coc_timer;
+static u32 polling_period;	/*seconds */
+static int rmon_timer_en;
+static spinlock_t dp_coc_lock;
+
+/* threshold data for D0:D3 */
+struct ltq_cpufreq_threshold rmon_threshold = { 0, 30, 20, 10 }; /*packets */
+
+/* driver is busy and needs highest performance */
+static int dp_coc_init_stat;	/*DP COC is Initialized or not */
+static int dp_coc_ena;		/*DP COC is enabled or not */
+enum ltq_cpufreq_state dp_coc_ps_curr = LTQ_CPUFREQ_PS_UNDEF;/*current state*/
+/*new statue wanted to switch to */
+enum ltq_cpufreq_state dp_coc_ps_new = LTQ_CPUFREQ_PS_UNDEF;
+
+static GSW_RMON_Port_cnt_t rmon_last[PMAC_MAX_NUM];
+static u64 last_rmon_rx;
+
+/*meter */
+#define PCE_OVERHD 20
+static u32 meter_id;
+/*3 ~ 4 packet size */
+static u32 meter_ncbs = 0x8000 + (1514 + PCE_OVERHD) * 3 + 200;
+/*1 ~ 2 packet size */
+static u32 meter_nebs = 0x8000 + (1514 + PCE_OVERHD) * 1 + 200;
+/*k bits */
+static u32 meter_nrate[4] = { 0/*D0 */, 700/*D1*/, 600/*D2*/, 500/*D3*/};
+
+static int dp_coc_cpufreq_notifier(struct notifier_block *nb,
+				   unsigned long val, void *data);
+static int dp_coc_stateget(enum ltq_cpufreq_state *state);
+static int dp_coc_fss_ena(int ena);
+static int apply_meter_rate(u32 rate, enum ltq_cpufreq_state new_state);
+static int enable_meter_interrupt(void);
+static int clear_meter_interrupt(void);
+
+int dp_set_meter_rate(enum ltq_cpufreq_state stat, unsigned int rate)
+{
+	/*set the rate for upscaling to D0 from specified stat */
+	if (stat == LTQ_CPUFREQ_PS_D1)
+		meter_nrate[1] = rate;
+	else if (stat == LTQ_CPUFREQ_PS_D2)
+		meter_nrate[2] = rate;
+	else if (stat == LTQ_CPUFREQ_PS_D3)
+		meter_nrate[3] = rate;
+	else
+		return -1;
+	if (dp_coc_ps_curr == stat)
+		apply_meter_rate(-1, stat);
+	return 0;
+}
+EXPORT_SYMBOL(dp_set_meter_rate);
+
+static struct notifier_block dp_coc_cpufreq_notifier_block = {
+	.notifier_call = dp_coc_cpufreq_notifier
+};
+
+static inline void coc_lock(void)
+{
+}
+
+static inline void coc_unlock(void)
+{
+	spin_unlock_bh(&dp_coc_lock);
+}
+
+struct ltq_cpufreq_module_info dp_coc_feature_fss = {
+	.name = "Datapath FSS",
+	.pmcuModule = DP_MODULE,
+	.pmcuModuleNr = DP_ID,
+	.powerFeatureStat = 1,
+	.ltq_cpufreq_state_get = dp_coc_stateget,
+	.ltq_cpufreq_pwr_feature_switch = dp_coc_fss_ena,
+};
+
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_DBG) && CONFIG_INTEL_DATAPATH_DBG
+static char *get_sub_module_str(uint32_t flag)
+{
+	if (flag == DP_COC_REQ_DP)
+		return "DP";
+	else if (flag == DP_COC_REQ_ETHERNET)
+		return "Ethernet";
+	else if (flag == DP_COC_REQ_VRX318)
+		return "VRX318";
+	else
+		return "Unknown";
+}
+#endif
+
+static char *get_coc_stat_string(enum ltq_cpufreq_state stat)
+{
+	if (stat == LTQ_CPUFREQ_PS_D0)
+		return "D0";
+	else if (stat == LTQ_CPUFREQ_PS_D1)
+		return "D1";
+	else if (stat == LTQ_CPUFREQ_PS_D2)
+		return "D2";
+	else if (stat == LTQ_CPUFREQ_PS_D3)
+		return "D3";
+	else if (stat == LTQ_CPUFREQ_PS_D0D3)
+		return "D0D3-NotCare";
+	else if (stat == LTQ_CPUFREQ_PS_BOOST)
+		return "Boost";
+	else
+		return "Undef";
+}
+
+static void dp_rmon_polling(unsigned long data)
+{
+}
+
+static int dp_coc_stateget(enum ltq_cpufreq_state *state)
+{
+}
+
+static int dp_coc_fss_ena(int ena)
+{
+}
+
+void update_rmon_last(void)
+{
+}
+
+int update_coc_rmon_timer(enum ltq_cpufreq_state new_state, uint32_t flag)
+{
+	return 0;
+}
+
+static int update_coc_cfg(enum ltq_cpufreq_state new_state,
+			  enum ltq_cpufreq_state old_state, uint32_t flag)
+{
+	return 0;
+}
+
+static int dp_coc_prechange(enum ltq_cpufreq_module module,
+			    enum ltq_cpufreq_state new_state,
+			    enum ltq_cpufreq_state old_state, u8 flags)
+{
+	return 0;
+}
+
+static int dp_coc_postchange(enum ltq_cpufreq_module module,
+			     enum ltq_cpufreq_state new_state,
+			     enum ltq_cpufreq_state old_state, u8 flags)
+{
+	return 0;
+}
+
+/* keep track of frequency transitions */
+static int dp_coc_cpufreq_notifier(struct notifier_block *nb,
+				   unsigned long val, void *data)
+{
+	return 0;
+}
+
+void proc_coc_read(struct seq_file *s)
+{
+	if (!capable(CAP_NET_ADMIN))
+		return;
+}
+
+int dp_set_rmon_threshold(struct ltq_cpufreq_threshold *threshold,
+			  uint32_t flags)
+{
+	return 0;
+}
+EXPORT_SYMBOL(dp_set_rmon_threshold);
+
+ssize_t proc_coc_write(struct file *file, const char *buf, size_t count,
+		       loff_t *ppos)
+{
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+	return count;
+}
+
+int clear_meter_interrupt(void)
+{
+	return 0;
+}
+
+int enable_meter_interrupt(void)
+{
+	return 0;
+}
+
+/* rate      0: disable meter
+ * -1: enable meter
+ * others: really change rate.
+ */
+int apply_meter_rate(u32 rate, enum ltq_cpufreq_state new_state)
+{
+	return 0;
+}
+
+int meter_set_default(void)
+{
+	return 0;
+}
+
+/* For Datapth's sub-module to request power state change, esp for
+ *  Ethernet/VRX318 driver to call it if there is state change needed.
+ *   The flag can be:
+ *     DP_COC_REQ_DP
+ *     DP_COC_REQ_ETHERNET
+ *     DP_COC_REQ_VRX318
+ */
+int dp_coc_new_stat_req(enum ltq_cpufreq_state new_state, uint32_t flag)
+{
+	return 0;
+}
+EXPORT_SYMBOL(dp_coc_new_stat_req);
+
+int dp_coc_cpufreq_init(void)
+{
+	return 0;
+}
+
+int dp_coc_cpufreq_exit(void)
+{
+	return 0;
+}
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_ext_vlan.c b/drivers/net/datapath/dpm/gswip32/datapath_ext_vlan.c
new file mode 100644
index 000000000000..8613354eaadd
--- /dev/null
+++ b/drivers/net/datapath/dpm/gswip32/datapath_ext_vlan.c
@@ -0,0 +1,623 @@
+/*
+ * Copyright (C) Intel Corporation
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#ifdef NON_LINUX
+#include <stdlib.h>
+#include <errno.h>
+#include <string.h>
+#include "lantiq_gsw.h"
+#include "lantiq_gsw_api.h"
+#include "gsw_flow_ops.h"
+/* Adaption */
+#define kfree(x)	free(x)
+#define kmalloc(x, y)	malloc(x)
+#else
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/etherdevice.h>
+#include <net/datapath_api.h>
+#include "../datapath.h"
+#endif
+
+/* Structure only used by function set_gswip_ext_vlan_32 */
+struct priv_ext_vlan {
+	/* number of bridge ports has VLAN */
+	u32 num_bp;
+	/* bridge port list (vlan1_list + vlan2_list) */
+	u32 bp[1];
+};
+
+/* Supporting Functions */
+static int update_vlan0(struct core_ops *ops, u32 bp,
+			GSW_EXTENDEDVLAN_config_t *vcfg)
+{
+	GSW_return_t ret;
+	u32 block = vcfg->nExtendedVlanBlockId;
+
+	memset(vcfg, 0, sizeof(*vcfg));
+	vcfg->nExtendedVlanBlockId = block;
+	vcfg->nEntryIndex = 0;
+	vcfg->sFilter.sOuterVlan.eType = GSW_EXTENDEDVLAN_FILTER_TYPE_NO_TAG;
+	vcfg->sFilter.eEtherType = GSW_EXTENDEDVLAN_FILTER_ETHERTYPE_NO_FILTER;
+	vcfg->sTreatment.bReassignBridgePort = LTQ_TRUE;
+	vcfg->sTreatment.nNewBridgePortId = bp;
+	ret = ops->gsw_extvlan_ops.ExtendedVlan_Set(ops, vcfg);
+
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	memset(vcfg, 0, sizeof(*vcfg));
+	vcfg->nExtendedVlanBlockId = block;
+	vcfg->nEntryIndex = 1;
+	vcfg->sFilter.sOuterVlan.eType = GSW_EXTENDEDVLAN_FILTER_TYPE_DEFAULT;
+	vcfg->sFilter.sInnerVlan.eType = GSW_EXTENDEDVLAN_FILTER_TYPE_NO_TAG;
+	vcfg->sFilter.eEtherType = GSW_EXTENDEDVLAN_FILTER_ETHERTYPE_NO_FILTER;
+	vcfg->sTreatment.bReassignBridgePort = LTQ_TRUE;
+	vcfg->sTreatment.nNewBridgePortId = bp;
+	ret = ops->gsw_extvlan_ops.ExtendedVlan_Set(ops, vcfg);
+
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	memset(vcfg, 0, sizeof(*vcfg));
+	vcfg->nExtendedVlanBlockId = block;
+	vcfg->nEntryIndex = 2;
+	vcfg->sFilter.sInnerVlan.eType = GSW_EXTENDEDVLAN_FILTER_TYPE_DEFAULT;
+	vcfg->sFilter.eEtherType = GSW_EXTENDEDVLAN_FILTER_ETHERTYPE_NO_FILTER;
+	vcfg->sTreatment.bReassignBridgePort = LTQ_TRUE;
+	vcfg->sTreatment.nNewBridgePortId = bp;
+	ret = ops->gsw_extvlan_ops.ExtendedVlan_Set(ops, vcfg);
+
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	return 0;
+}
+
+static int update_vlan1(struct core_ops *ops, int base, int num,
+			struct vlan1 *vlan_list, int drop,
+			GSW_EXTENDEDVLAN_config_t *vcfg)
+{
+	GSW_return_t ret;
+	u32 block = vcfg->nExtendedVlanBlockId;
+	int i;
+
+	for (i = 0; i < num; i++, base += 2) {
+		memset(vcfg, 0, sizeof(*vcfg));
+		vcfg->nExtendedVlanBlockId = block;
+		vcfg->nEntryIndex = base;
+
+		vcfg->sFilter.sOuterVlan.eType =
+			GSW_EXTENDEDVLAN_FILTER_TYPE_NORMAL;
+		vcfg->sFilter.sOuterVlan.bPriorityEnable = LTQ_FALSE;
+		vcfg->sFilter.sOuterVlan.bVidEnable = LTQ_TRUE;
+		vcfg->sFilter.sOuterVlan.nVidVal = vlan_list[i].outer_vlan.vid;
+		vcfg->sFilter.sOuterVlan.eTpid = vlan_list[i].outer_vlan.tpid;
+		vcfg->sFilter.sOuterVlan.eDei =
+			GSW_EXTENDEDVLAN_FILTER_DEI_NO_FILTER;
+
+		vcfg->sFilter.sInnerVlan.eType =
+			GSW_EXTENDEDVLAN_FILTER_TYPE_NO_TAG;
+
+		vcfg->sFilter.eEtherType = vlan_list[i].ether_type;
+
+		if (drop) {
+			vcfg->sTreatment.eRemoveTag =
+				GSW_EXTENDEDVLAN_TREATMENT_DISCARD_UPSTREAM;
+		} else {
+			vcfg->sTreatment.eRemoveTag =
+				GSW_EXTENDEDVLAN_TREATMENT_REMOVE_1_TAG;
+			vcfg->sTreatment.bReassignBridgePort = LTQ_TRUE;
+			vcfg->sTreatment.nNewBridgePortId = vlan_list[i].bp;
+		}
+
+		ret = ops->gsw_extvlan_ops.ExtendedVlan_Set(ops, vcfg);
+
+		if (ret != GSW_statusOk)
+			return -EIO;
+
+		memset(vcfg, 0, sizeof(*vcfg));
+		vcfg->nExtendedVlanBlockId = block;
+		vcfg->nEntryIndex = base + 1;
+
+		vcfg->sFilter.sOuterVlan.eType =
+			GSW_EXTENDEDVLAN_FILTER_TYPE_NORMAL;
+		vcfg->sFilter.sOuterVlan.bPriorityEnable = LTQ_FALSE;
+		vcfg->sFilter.sOuterVlan.bVidEnable = LTQ_TRUE;
+		vcfg->sFilter.sOuterVlan.nVidVal = vlan_list[i].outer_vlan.vid;
+		vcfg->sFilter.sOuterVlan.eTpid = vlan_list[i].outer_vlan.tpid;
+		vcfg->sFilter.sOuterVlan.eDei =
+			GSW_EXTENDEDVLAN_FILTER_DEI_NO_FILTER;
+
+		vcfg->sFilter.sInnerVlan.eType =
+			GSW_EXTENDEDVLAN_FILTER_TYPE_NO_FILTER;
+
+		vcfg->sFilter.eEtherType = vlan_list[i].ether_type;
+
+		if (drop) {
+			vcfg->sTreatment.eRemoveTag =
+				GSW_EXTENDEDVLAN_TREATMENT_DISCARD_UPSTREAM;
+		} else {
+			vcfg->sTreatment.eRemoveTag =
+				GSW_EXTENDEDVLAN_TREATMENT_REMOVE_1_TAG;
+			vcfg->sTreatment.bReassignBridgePort = LTQ_TRUE;
+			vcfg->sTreatment.nNewBridgePortId = vlan_list[i].bp;
+		}
+
+		ret = ops->gsw_extvlan_ops.ExtendedVlan_Set(ops, vcfg);
+
+		if (ret != GSW_statusOk)
+			return -EIO;
+	}
+
+	return 0;
+}
+
+static int update_vlan2(struct core_ops *ops, int base, int num,
+			struct vlan2 *vlan_list, int drop,
+			GSW_EXTENDEDVLAN_config_t *vcfg)
+{
+	GSW_return_t ret;
+	u32 block = vcfg->nExtendedVlanBlockId;
+	int i;
+
+	for (i = 0; i < num; i++, base++) {
+		memset(vcfg, 0, sizeof(*vcfg));
+		vcfg->nExtendedVlanBlockId = block;
+		vcfg->nEntryIndex = base;
+
+		vcfg->sFilter.sOuterVlan.eType =
+			GSW_EXTENDEDVLAN_FILTER_TYPE_NORMAL;
+		vcfg->sFilter.sOuterVlan.bPriorityEnable = LTQ_FALSE;
+		vcfg->sFilter.sOuterVlan.bVidEnable = LTQ_TRUE;
+		vcfg->sFilter.sOuterVlan.nVidVal = vlan_list[i].outer_vlan.vid;
+		vcfg->sFilter.sOuterVlan.eTpid = vlan_list[i].outer_vlan.tpid;
+		vcfg->sFilter.sOuterVlan.eDei =
+			GSW_EXTENDEDVLAN_FILTER_DEI_NO_FILTER;
+
+		vcfg->sFilter.sInnerVlan.eType =
+			GSW_EXTENDEDVLAN_FILTER_TYPE_NORMAL;
+		vcfg->sFilter.sInnerVlan.bPriorityEnable = LTQ_FALSE;
+		vcfg->sFilter.sInnerVlan.bVidEnable = LTQ_TRUE;
+		vcfg->sFilter.sInnerVlan.nVidVal = vlan_list[i].inner_vlan.vid;
+		vcfg->sFilter.sInnerVlan.eTpid = vlan_list[i].inner_vlan.tpid;
+		vcfg->sFilter.sInnerVlan.eDei =
+			GSW_EXTENDEDVLAN_FILTER_DEI_NO_FILTER;
+
+		vcfg->sFilter.eEtherType = vlan_list[i].ether_type;
+
+		if (drop) {
+			vcfg->sTreatment.eRemoveTag =
+				GSW_EXTENDEDVLAN_TREATMENT_DISCARD_UPSTREAM;
+		} else {
+			vcfg->sTreatment.eRemoveTag =
+				GSW_EXTENDEDVLAN_TREATMENT_REMOVE_2_TAG;
+			vcfg->sTreatment.bReassignBridgePort = LTQ_TRUE;
+			vcfg->sTreatment.nNewBridgePortId = vlan_list[i].bp;
+		}
+
+		ret = ops->gsw_extvlan_ops.ExtendedVlan_Set(ops, vcfg);
+
+		if (ret != GSW_statusOk)
+			return -EIO;
+	}
+
+	return 0;
+}
+
+static int update_ctp(struct core_ops *ops, struct ext_vlan_info *vlan)
+{
+	static GSW_CTP_portConfig_t ctp;
+	static GSW_EXTENDEDVLAN_config_t vcfg;
+	GSW_return_t ret;
+	ltq_bool_t enable;
+	u32 block;
+	GSW_EXTENDEDVLAN_alloc_t alloc;
+	int i;
+
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "bp=%d v1=%d v1_d=%d v2=%d v2_d=%d\n",
+		 vlan->bp, vlan->n_vlan1, vlan->n_vlan1_drop,
+		 vlan->n_vlan2, vlan->n_vlan2_drop);
+	memset(&ctp, 0, sizeof(ctp));
+	memset(&alloc, 0, sizeof(GSW_EXTENDEDVLAN_alloc_t));
+
+	ctp.nLogicalPortId = vlan->logic_port;
+	ctp.nSubIfIdGroup = vlan->subif_grp;
+	ctp.eMask = GSW_CTP_PORT_CONFIG_MASK_INGRESS_VLAN;
+	ret = ops->gsw_ctp_ops.CTP_PortConfigGet(ops, &ctp);
+
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	enable = ctp.bIngressExtendedVlanEnable;
+	block = ctp.nIngressExtendedVlanBlockId;
+
+	/* disable previous vlan block operation,if enabled and
+	 * free the previous allocated vlan blocks
+	 * so that new vlan block can be allocated and configured to ctp
+	 */
+	if (enable) {
+		ctp.bIngressExtendedVlanEnable = LTQ_FALSE;
+		ret = ops->gsw_ctp_ops.CTP_PortConfigSet(ops, &ctp);
+		if (ret != GSW_statusOk) {
+			PR_ERR("Fail:Ingress VLAN operate disable in ctp\n");
+			return -EIO;
+		}
+		DP_DEBUG(DP_DBG_FLAG_SWDEV,
+			 "ingress VLAN operation disabled in ctp\n");
+		alloc.nExtendedVlanBlockId = block;
+		ret = ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+		if (ret != GSW_statusOk) {
+			return -EIO;
+		}
+	}
+	memset(&alloc, 0, sizeof(GSW_EXTENDEDVLAN_alloc_t));
+
+	alloc.nNumberOfEntries += vlan->n_vlan1 * 2;
+	alloc.nNumberOfEntries += vlan->n_vlan2;
+	alloc.nNumberOfEntries += vlan->n_vlan1_drop * 2;
+	alloc.nNumberOfEntries += vlan->n_vlan2_drop;
+	if (alloc.nNumberOfEntries == 0) {
+		DP_DEBUG(DP_DBG_FLAG_SWDEV,"nNumberOfEntries == 0\n");
+		return 0;
+	}
+
+	alloc.nNumberOfEntries += 3;
+	ret = ops->gsw_extvlan_ops.ExtendedVlan_Alloc(ops, &alloc);
+
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	vcfg.nExtendedVlanBlockId = alloc.nExtendedVlanBlockId;
+	ret = update_vlan0(ops, vlan->bp, &vcfg);
+
+	if (ret < 0)
+		goto UPDATE_ERROR;
+
+	i = 3;
+	ret = update_vlan2(ops, i, vlan->n_vlan2, vlan->vlan2_list, 0, &vcfg);
+
+	if (ret < 0)
+		goto UPDATE_ERROR;
+
+	i += vlan->n_vlan2;
+	ret = update_vlan2(ops, i, vlan->n_vlan2_drop,
+			   vlan->vlan2_drop_list, 1, &vcfg);
+
+	if (ret < 0)
+		goto UPDATE_ERROR;
+
+	i += vlan->n_vlan2_drop;
+	ret = update_vlan1(ops, i, vlan->n_vlan1, vlan->vlan1_list, 0, &vcfg);
+
+	if (ret < 0)
+		goto UPDATE_ERROR;
+
+	i += vlan->n_vlan1;
+	ret = update_vlan1(ops, i, vlan->n_vlan1_drop,
+			   vlan->vlan1_drop_list, 1, &vcfg);
+
+	if (ret < 0)
+		goto UPDATE_ERROR;
+
+	ctp.bIngressExtendedVlanEnable = LTQ_TRUE;
+	ctp.nIngressExtendedVlanBlockId = alloc.nExtendedVlanBlockId;
+	ctp.nIngressExtendedVlanBlockSize = 0;
+	ret = ops->gsw_ctp_ops.CTP_PortConfigSet(ops, &ctp);
+
+	if (ret != GSW_statusOk) {
+		return -EIO;
+	}
+	return 0;
+UPDATE_ERROR:
+	ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+	return ret;
+}
+
+static int bp_add_vlan1(struct core_ops *ops, struct vlan1 *vlan,
+			GSW_BRIDGE_portConfig_t *bpcfg,
+			GSW_EXTENDEDVLAN_config_t *vcfg)
+{
+	int ret;
+	GSW_EXTENDEDVLAN_alloc_t alloc = {0};
+	GSW_ExtendedVlanFilterType_t types[6] = {
+		GSW_EXTENDEDVLAN_FILTER_TYPE_NO_TAG,
+		GSW_EXTENDEDVLAN_FILTER_TYPE_NO_FILTER,
+		GSW_EXTENDEDVLAN_FILTER_TYPE_DEFAULT,
+		GSW_EXTENDEDVLAN_FILTER_TYPE_NO_TAG,
+		GSW_EXTENDEDVLAN_FILTER_TYPE_NO_FILTER,
+		GSW_EXTENDEDVLAN_FILTER_TYPE_DEFAULT
+	};
+	u32 i;
+
+	memset(bpcfg, 0, sizeof(*bpcfg));
+	bpcfg->nBridgePortId = vlan->bp;
+	bpcfg->eMask = GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_VLAN;
+	ret = ops->gsw_brdgport_ops.BridgePort_ConfigGet(ops, bpcfg);
+
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	/* This bridge port should have no egress VLAN configured yet */
+	if (bpcfg->bEgressExtendedVlanEnable != LTQ_FALSE)
+		return -EINVAL;
+
+	alloc.nNumberOfEntries = ARRAY_SIZE(types) / 2;
+	ret = ops->gsw_extvlan_ops.ExtendedVlan_Alloc(ops, &alloc);
+
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	for (i = 0; i < alloc.nNumberOfEntries; i++) {
+		memset(vcfg, 0, sizeof(*vcfg));
+		vcfg->nExtendedVlanBlockId = alloc.nExtendedVlanBlockId;
+		vcfg->nEntryIndex = i;
+		vcfg->sFilter.sOuterVlan.eType = types[i * 2];
+		vcfg->sFilter.sInnerVlan.eType = types[i * 2 + 1];
+		/* filter ether_type as ingress */
+		vcfg->sFilter.eEtherType = vlan->ether_type;
+
+		vcfg->sTreatment.bAddOuterVlan = LTQ_TRUE;
+		vcfg->sTreatment.sOuterVlan.ePriorityMode =
+			GSW_EXTENDEDVLAN_TREATMENT_PRIORITY_VAL;
+		vcfg->sTreatment.sOuterVlan.ePriorityVal = 0;
+		vcfg->sTreatment.sOuterVlan.eVidMode =
+			GSW_EXTENDEDVLAN_TREATMENT_VID_VAL;
+		vcfg->sTreatment.sOuterVlan.eVidVal = vlan->outer_vlan.vid;
+		vcfg->sTreatment.sOuterVlan.eTpid = vlan->outer_vlan.tpid;
+		vcfg->sTreatment.sOuterVlan.eDei =
+			GSW_EXTENDEDVLAN_TREATMENT_DEI_0;
+
+		vcfg->sTreatment.bAddInnerVlan = LTQ_FALSE;
+
+		ret = ops->gsw_extvlan_ops.ExtendedVlan_Set(ops, vcfg);
+
+		if (ret != GSW_statusOk) {
+			PR_ERR("Fail updating Extended VLAN entry (%u, %u).\n",
+				alloc.nExtendedVlanBlockId, i);
+			ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+			return -EIO;
+		}
+	}
+
+	bpcfg->bEgressExtendedVlanEnable = LTQ_TRUE;
+	bpcfg->nEgressExtendedVlanBlockId = alloc.nExtendedVlanBlockId;
+	bpcfg->nEgressExtendedVlanBlockSize = 0;
+	ret = ops->gsw_brdgport_ops.BridgePort_ConfigSet(ops, bpcfg);
+
+	if (ret != GSW_statusOk) {
+		PR_ERR("Failed in attaching Extended VLAN to Bridge Port.\n");
+		ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+		return -EIO;
+	} else {
+		return 0;
+	}
+}
+
+static int bp_add_vlan2(struct core_ops *ops, struct vlan2 *vlan,
+			GSW_BRIDGE_portConfig_t *bpcfg,
+			GSW_EXTENDEDVLAN_config_t *vcfg)
+{
+	int ret;
+	GSW_EXTENDEDVLAN_alloc_t alloc = {0};
+	GSW_ExtendedVlanFilterType_t types[6] = {
+		GSW_EXTENDEDVLAN_FILTER_TYPE_NO_TAG,
+		GSW_EXTENDEDVLAN_FILTER_TYPE_NO_FILTER,
+		GSW_EXTENDEDVLAN_FILTER_TYPE_DEFAULT,
+		GSW_EXTENDEDVLAN_FILTER_TYPE_NO_TAG,
+		GSW_EXTENDEDVLAN_FILTER_TYPE_NO_FILTER,
+		GSW_EXTENDEDVLAN_FILTER_TYPE_DEFAULT
+	};
+	u32 i;
+
+	memset(bpcfg, 0, sizeof(*bpcfg));
+	bpcfg->nBridgePortId = vlan->bp;
+	bpcfg->eMask = GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_VLAN;
+	ret = ops->gsw_brdgport_ops.BridgePort_ConfigGet(ops, bpcfg);
+
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	/* This bridge port should have no egress VLAN configured yet */
+	if (bpcfg->bEgressExtendedVlanEnable != LTQ_FALSE)
+		return -EINVAL;
+
+	alloc.nNumberOfEntries = ARRAY_SIZE(types) / 2;
+	ret = ops->gsw_extvlan_ops.ExtendedVlan_Alloc(ops, &alloc);
+
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	for (i = 0; i < alloc.nNumberOfEntries; i++) {
+		memset(vcfg, 0, sizeof(*vcfg));
+		vcfg->nExtendedVlanBlockId = alloc.nExtendedVlanBlockId;
+		vcfg->nEntryIndex = i;
+		vcfg->sFilter.sOuterVlan.eType = types[i * 2];
+		vcfg->sFilter.sInnerVlan.eType = types[i * 2 + 1];
+		/* filter ether_type as ingress */
+		vcfg->sFilter.eEtherType = vlan->ether_type;
+
+		vcfg->sTreatment.bAddOuterVlan = LTQ_TRUE;
+		vcfg->sTreatment.sOuterVlan.ePriorityMode =
+			GSW_EXTENDEDVLAN_TREATMENT_PRIORITY_VAL;
+		vcfg->sTreatment.sOuterVlan.ePriorityVal = 0;
+		vcfg->sTreatment.sOuterVlan.eVidMode =
+			GSW_EXTENDEDVLAN_TREATMENT_VID_VAL;
+		vcfg->sTreatment.sOuterVlan.eVidVal = vlan->outer_vlan.vid;
+		vcfg->sTreatment.sOuterVlan.eTpid = vlan->outer_vlan.tpid;
+		vcfg->sTreatment.sOuterVlan.eDei =
+			GSW_EXTENDEDVLAN_TREATMENT_DEI_0;
+
+		vcfg->sTreatment.bAddInnerVlan = LTQ_TRUE;
+		vcfg->sTreatment.sInnerVlan.ePriorityMode =
+			GSW_EXTENDEDVLAN_TREATMENT_PRIORITY_VAL;
+		vcfg->sTreatment.sInnerVlan.ePriorityVal = 0;
+		vcfg->sTreatment.sInnerVlan.eVidMode =
+			GSW_EXTENDEDVLAN_TREATMENT_VID_VAL;
+		vcfg->sTreatment.sInnerVlan.eVidVal = vlan->inner_vlan.vid;
+		vcfg->sTreatment.sInnerVlan.eTpid = vlan->inner_vlan.tpid;
+		vcfg->sTreatment.sInnerVlan.eDei =
+			GSW_EXTENDEDVLAN_TREATMENT_DEI_0;
+
+		ret = ops->gsw_extvlan_ops.ExtendedVlan_Set(ops, vcfg);
+
+		if (ret != GSW_statusOk) {
+			PR_ERR("Fail updating Extended VLAN entry (%u, %u).\n",
+				alloc.nExtendedVlanBlockId, i);
+			ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+			return -EIO;
+		}
+	}
+
+	bpcfg->bEgressExtendedVlanEnable = LTQ_TRUE;
+	bpcfg->nEgressExtendedVlanBlockId = alloc.nExtendedVlanBlockId;
+	bpcfg->nEgressExtendedVlanBlockSize = 0;
+	ret = ops->gsw_brdgport_ops.BridgePort_ConfigSet(ops, bpcfg);
+
+	if (ret != GSW_statusOk) {
+		PR_ERR("Failed in attaching Extended VLAN to Bridge Port.\n");
+		ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+		return -EIO;
+	} else {
+		return 0;
+	}
+}
+
+static int bp_add_vlan(struct core_ops *ops, struct ext_vlan_info *vlan,
+		       int idx, GSW_BRIDGE_portConfig_t *bpcfg)
+{
+	static GSW_EXTENDEDVLAN_config_t vcfg;
+
+	if (idx < vlan->n_vlan1)
+		return bp_add_vlan1(ops, vlan->vlan1_list + idx, bpcfg, &vcfg);
+	else
+		return bp_add_vlan2(ops,
+				    vlan->vlan2_list + (idx - vlan->n_vlan1),
+				    bpcfg, &vcfg);
+}
+
+static int bp_rm_vlan(struct core_ops *ops, u32 bp,
+		      GSW_BRIDGE_portConfig_t *bpcfg)
+{
+	int ret;
+	GSW_EXTENDEDVLAN_alloc_t alloc = {0};
+
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "bp=%d\n", bp);
+	memset(bpcfg, 0, sizeof(*bpcfg));
+	bpcfg->nBridgePortId = bp;
+	bpcfg->eMask = GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_VLAN;
+	ret = ops->gsw_brdgport_ops.BridgePort_ConfigGet(ops, bpcfg);
+
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	if (bpcfg->bEgressExtendedVlanEnable == LTQ_FALSE)
+		return 0;
+
+	alloc.nExtendedVlanBlockId = bpcfg->nEgressExtendedVlanBlockId;
+	bpcfg->bEgressExtendedVlanEnable = LTQ_FALSE;
+	ret = ops->gsw_brdgport_ops.BridgePort_ConfigSet(ops, bpcfg);
+
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	ret = ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+
+	if (ret != GSW_statusOk)
+		return -EIO;
+	else
+		return 0;
+}
+
+static int bp_diff(u32 *bp0, u32 num_bp0, u32 *bp1)
+{
+	u32 i, j;
+
+	for (i = 0; i < num_bp0; i++) {
+		for (j = 0; j < num_bp0 && bp0[j] != bp1[i]; j++);
+		if (j >= num_bp0)
+			break;
+	}
+	return i;
+}
+
+/* Function for VLAN configure */
+int set_gswip_ext_vlan_32(struct core_ops *ops, struct ext_vlan_info *vlan,
+		       int flag)
+{
+	static GSW_BRIDGE_portConfig_t bpcfg;
+
+	int ret;
+	struct priv_ext_vlan *old_priv, *new_priv;
+	size_t new_priv_size;
+	int i, j;
+
+	/* Assumptions:
+	 * 1. Every time this function is called, one and only one "vlan" is
+	 *    added or removed. No replacement happens.
+	 * 2. The content of 2 vlan_list (not vlan_drop_list) keeps sequence.
+	 *    Only one new item is inserted or removed. No re-ordering happens.
+	 * 3. Bridge ports are not freed before this function is called. This
+	 *    is not compulsary. Just in case any resource free function
+	 *    such as free VLAN block, disable VLAN from bridge port, fails.
+	 * 4. In egress, assume packet at bridge port has no VLAN before
+	 *    Extended VLAN processing to save 2 Extended VLAN entries. This
+	 *    can be changed later if required.
+	 */
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "ext vlan info bp=%d logic=%d subif=%d\n",
+		 vlan->bp, vlan->logic_port, vlan->subif_grp);
+	ret = update_ctp(ops, vlan);
+
+	if (ret < 0)
+		return ret;
+
+	/* prepare new private data */
+	new_priv_size = sizeof(*new_priv);
+	new_priv_size += sizeof(new_priv->bp[0])
+			 * (vlan->n_vlan1 + vlan->n_vlan2);
+	new_priv = kmalloc(new_priv_size, GFP_KERNEL);
+
+	if (!new_priv)
+		return -ENOMEM;
+
+	new_priv->num_bp = (u32)(vlan->n_vlan1 + vlan->n_vlan2);
+
+	for (i = j = 0; i < vlan->n_vlan1; i++, j++)
+		new_priv->bp[j] = vlan->vlan1_list[i].bp;
+
+	for (i = 0; i < vlan->n_vlan2; i++, j++)
+		new_priv->bp[j] = vlan->vlan2_list[i].bp;
+
+	/* remember pointer to old private data */
+	old_priv = vlan->priv;
+
+	if (!old_priv) {
+		/* no old private data, vlan is added */
+		ret = bp_add_vlan(ops, vlan, 0, &bpcfg);
+	} else if (old_priv->num_bp < new_priv->num_bp) {
+		/* vlan added */
+		i = bp_diff(old_priv->bp, old_priv->num_bp, new_priv->bp);
+
+		ret = bp_add_vlan(ops, vlan, i, &bpcfg);
+	} else if (old_priv->num_bp > new_priv->num_bp) {
+		/* vlan removed */
+		i = bp_diff(new_priv->bp, new_priv->num_bp, old_priv->bp);
+
+		bp_rm_vlan(ops, old_priv->bp[i], &bpcfg);
+		ret = 0;
+	} else {
+		ret = 0;
+	}
+
+	/* return new private data in vlan structure */
+	vlan->priv = new_priv;
+
+	/* free old private data if any */
+	kfree(old_priv);
+	return ret;
+}
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_gswip.c b/drivers/net/datapath/dpm/gswip32/datapath_gswip.c
new file mode 100644
index 000000000000..35f363794424
--- /dev/null
+++ b/drivers/net/datapath/dpm/gswip32/datapath_gswip.c
@@ -0,0 +1,1720 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/etherdevice.h>
+#include <net/datapath_api.h>
+#include "../datapath.h"
+#include "datapath_misc.h"
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_DDR_SIMULATE_GSWIP32)
+#include "datapath_gswip_simulate.h"
+#endif
+
+#define GSW_CORE_API(_handle, a, b) ({ \
+	typeof(_handle) (handle) = (_handle); \
+	gsw_core_api((dp_gsw_cb)(handle)->a, (handle), (b)); })
+
+#define BP_CFG(bp_cfg, _index, bflag, id) ({ \
+	typeof(_index) (index) = (_index); \
+	(bp_cfg)->bEgressSubMeteringEnable[(index)] = bflag; \
+	(bp_cfg)->nEgressTrafficSubMeterId[(index)] = id; })
+
+#define BR_CFG(br_cfg, _index, bflag, id) ({\
+	typeof(_index) (index) = (_index); \
+	(br_cfg)->bSubMeteringEnable[(index)] = bflag; \
+	(br_cfg)->nTrafficSubMeterId[(index)] = id; })
+
+#define METER_CIR(cir)  ((cir) / 1000)
+#define METER_PIR(pir)  ((pir) / 1000)
+
+static struct ctp_assign ctp_assign_info[] = {
+	/*note: multiple flags must put first */
+	{DP_F_CPU, GSW_LOGICAL_PORT_8BIT_WLAN, 16, 8, 0xF, CQE_LU_MODE2, 8, 0},
+	{DP_F_GPON, GSW_LOGICAL_PORT_GPON, 256, 0, 0xFF, CQE_LU_MODE1, 128, 1},
+	{DP_F_EPON, GSW_LOGICAL_PORT_EPON, 256, 0, 0xFF, CQE_LU_MODE1, 128, 1},
+	{DP_F_GINT, GSW_LOGICAL_PORT_GINT, 8, 0, 0xFF, CQE_LU_MODE1, 8, 0},
+/*#define DP_ETH_TEST*/
+#ifndef DP_ETH_TEST
+	{DP_F_FAST_ETH_WAN, GSW_LOGICAL_PORT_8BIT_WLAN, 8, 8, 0xF,
+		CQE_LU_MODE2, 8, 0},
+	{DP_F_FAST_ETH_LAN | DP_F_ALLOC_EXPLICIT_SUBIFID,
+		GSW_LOGICAL_PORT_8BIT_WLAN, 8, 8, 0xF, CQE_LU_MODE2, 8, 0},
+	{DP_F_FAST_ETH_LAN, GSW_LOGICAL_PORT_8BIT_WLAN, 2, 8, 0xF,
+		CQE_LU_MODE2, 2, 0},
+#else /*testing only */
+	{DP_F_FAST_ETH_WAN, GSW_LOGICAL_PORT_OTHER, 1, 8, 0xF, CQE_LU_MODE2, 1},
+	{DP_F_FAST_ETH_LAN | DP_F_ALLOC_EXPLICIT_SUBIFID,
+		GSW_LOGICAL_PORT_OTHER, 1, 8, 0xF, CQE_LU_MODE2, 1},
+	{DP_F_FAST_ETH_LAN, GSW_LOGICAL_PORT_OTHER, 1, 8, 0xF, CQE_LU_MODE2, 1},
+#endif
+	{DP_F_FAST_WLAN, GSW_LOGICAL_PORT_8BIT_WLAN, 16, 8, 0xF, CQE_LU_MODE2,
+		16, 0},
+	{DP_F_FAST_WLAN_EXT, GSW_LOGICAL_PORT_9BIT_WLAN, 8, 9, 0x7,
+		CQE_LU_MODE2, 16, 0}
+};
+
+static struct ctp_assign ctp_assign_def = {0, GSW_LOGICAL_PORT_8BIT_WLAN, 8, 8,
+					   0xF, CQE_LU_MODE2, 8, 1};
+
+static struct gsw_itf itf_assign[PMAC_MAX_NUM] = {0};
+
+struct ctp_assign *get_ctp_assign(int flags)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(ctp_assign_info); i++) {
+		if ((ctp_assign_info[i].flag & flags) ==
+			ctp_assign_info[i].flag)
+			return &ctp_assign_info[i];
+	}
+	return NULL;
+}
+
+static char *ctp_mode_string(GSW_LogicalPortMode_t type)
+{
+	if (type == GSW_LOGICAL_PORT_8BIT_WLAN)
+		return "8BIT_WLAN";
+	if (type == GSW_LOGICAL_PORT_9BIT_WLAN)
+		return "9BIT_WLAN";
+	if (type == GSW_LOGICAL_PORT_GPON)
+		return "GPON";
+	if (type == GSW_LOGICAL_PORT_EPON)
+		return "EPON";
+	if (type == GSW_LOGICAL_PORT_GINT)
+		return "GINT";
+	if (type == GSW_LOGICAL_PORT_OTHER)
+		return "OTHER";
+	return "Undef";
+}
+
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_DDR_SIMULATE_GSWIP32)
+GSW_return_t gsw_core_api_ddr_simu32(dp_gsw_cb func, void *ops, void *param)
+{
+	if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_brdgport_ops.BridgePort_Alloc) {
+		return BridgePortAlloc(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_brdgport_ops.BridgePort_ConfigGet) {
+		return BridgePortConfigGet(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_brdgport_ops.BridgePort_ConfigSet) {
+		return BridgePortConfigSet(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_brdgport_ops.BridgePort_Free) {
+		return BridgePortFree(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_brdg_ops.Bridge_Alloc) {
+		return BridgeAlloc(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_brdg_ops.Bridge_ConfigSet) {
+		return BridgeConfigSet(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_brdg_ops.Bridge_ConfigGet) {
+		return BridgeConfigGet(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_brdg_ops.Bridge_Free) {
+		return BridgeFree(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_ctp_ops.CTP_PortAssignmentAlloc) {
+		return CTP_PortAssignmentAlloc
+			(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_ctp_ops.CTP_PortAssignmentFree) {
+		return CTP_PortAssignmentFree
+			(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_ctp_ops.CTP_PortAssignmentSet) {
+		return CTP_PortAssignmentSet
+			(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_ctp_ops.CTP_PortAssignmentGet) {
+		return CTP_PortAssignmentGet
+			(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_ctp_ops.CTP_PortConfigSet) {
+		return CtpPortConfigSet(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_ctp_ops.CTP_PortConfigGet) {
+		return CtpPortConfigGet(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+		   gsw_swmac_ops.MAC_TableEntryAdd) {
+		return MacTableAdd(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+		   gsw_swmac_ops.MAC_TableEntryRead) {
+		return MacTableRead(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+		   gsw_swmac_ops.MAC_TableEntryRemove) {
+		return MacTableRemove(param);
+	}  else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+		    gsw_swmac_ops.MAC_TableEntryQuery) {
+		return MacTableQuery(param);
+	}
+	return GSW_SIMUTE_DDR_NOT_MATCH;
+}
+#endif
+
+/*This API is only for GSWIP-R PMAC modification, not for GSWIP-L */
+int dp_pmac_set_32(int inst, u32 port, dp_pmac_cfg_t *pmac_cfg)
+{
+	GSW_PMAC_Eg_Cfg_t egcfg;
+	GSW_PMAC_Ig_Cfg_t igcfg;
+	int i, j, k;
+	u32 flag = 0;
+	cbm_dq_port_res_t dqport;
+	s32 ret;
+	u32 pmacid;
+	struct core_ops *gswr_r;
+	GSW_PMAC_Glbl_Cfg_t pmac_glb;
+
+	if (!pmac_cfg || !port) {
+		PR_ERR("dp_pmac_set:wrong parameter(pmac_cfg/port NULL)\n");
+		return -1;
+	}
+
+	if (!pmac_cfg->ig_pmac_flags && !pmac_cfg->eg_pmac_flags)
+		return 0;
+	if (port == 2)
+		pmacid = 1;
+	else
+		pmacid = 0;
+	memset(&dqport, 0, sizeof(cbm_dq_port_res_t));
+
+	/* Get GSWIP device handler */
+	gswr_r = dp_port_prop[inst].ops[0];
+
+	/*set ingress port via DMA tx channel */
+	if (pmac_cfg->ig_pmac_flags) {
+		/*Read back igcfg from gsw first */
+		ret = cbm_dequeue_port_resources_get(port, &dqport, flag);
+
+		if (ret == -1) {
+			PR_ERR("cbm_dequeue_port_resources_get failed\n");
+			return -1;
+		}
+
+		memset(&igcfg, 0, sizeof(GSW_PMAC_Ig_Cfg_t));
+
+		for (i = 0; i < dqport.num_deq_ports; i++) {
+			igcfg.nPmacId = pmacid;
+			igcfg.nTxDmaChanId = dqport.deq_info[i].dma_tx_chan;
+			if (igcfg.nTxDmaChanId == (u8)-1) {
+				igcfg.nTxDmaChanId =
+					pmac_cfg->ig_pmac.tx_dma_chan;
+			}
+			gsw_core_api((dp_gsw_cb)gswr_r->gsw_pmac_ops.
+				     Pmac_Ig_CfgGet, gswr_r, &igcfg);
+
+			/*update igcfg and write back to gsw */
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_ERR_DISC)
+				igcfg.bErrPktsDisc =
+					pmac_cfg->ig_pmac.err_disc;
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_PRESENT)
+				igcfg.bPmacPresent = pmac_cfg->ig_pmac.pmac;
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_SUBIF)
+				//igcfg.bSubIdDefault =
+				igcfg.eSubId =
+					pmac_cfg->ig_pmac.def_pmac_subifid;
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_SPID)
+				igcfg.bSpIdDefault =
+					pmac_cfg->ig_pmac.def_pmac_src_port;
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_CLASSENA)
+				igcfg.bClassEna =
+					pmac_cfg->ig_pmac.def_pmac_en_tc;
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_CLASS)
+				igcfg.bClassDefault =
+					pmac_cfg->ig_pmac.def_pmac_tc;
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_PMAPENA)
+				igcfg.bPmapEna =
+					pmac_cfg->ig_pmac.def_pmac_en_pmap;
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_PMAP)
+				igcfg.bPmapDefault =
+					pmac_cfg->ig_pmac.def_pmac_pmap;
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_PMACHDR1)
+				igcfg.defPmacHdr[0] =
+					pmac_cfg->ig_pmac.def_pmac_hdr[0];
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_PMACHDR2)
+				igcfg.defPmacHdr[1] =
+					pmac_cfg->ig_pmac.def_pmac_hdr[1];
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_PMACHDR3)
+				igcfg.defPmacHdr[2] =
+					pmac_cfg->ig_pmac.def_pmac_hdr[2];
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_PMACHDR4)
+				igcfg.defPmacHdr[3] =
+					pmac_cfg->ig_pmac.def_pmac_hdr[3];
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_PMACHDR5)
+				igcfg.defPmacHdr[4] =
+					pmac_cfg->ig_pmac.def_pmac_hdr[4];
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_PMACHDR6)
+				igcfg.defPmacHdr[5] =
+					pmac_cfg->ig_pmac.def_pmac_hdr[5];
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_PMACHDR7)
+				igcfg.defPmacHdr[6] =
+					pmac_cfg->ig_pmac.def_pmac_hdr[6];
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_PMACHDR8)
+				igcfg.defPmacHdr[7] =
+					pmac_cfg->ig_pmac.def_pmac_hdr[7];
+
+			DP_DEBUG(DP_DBG_FLAG_DBG,
+				 "\nPMAC %d igcfg configuration:\n", port);
+			DP_DEBUG(DP_DBG_FLAG_DBG, "igcfg.nPmacId=%d\n",
+				 igcfg.nPmacId);
+			DP_DEBUG(DP_DBG_FLAG_DBG, "igcfg.nTxDmaChanId=%d\n",
+				 igcfg.nTxDmaChanId);
+			DP_DEBUG(DP_DBG_FLAG_DBG, "igcfg.bErrPktsDisc=%d\n",
+				 igcfg.bErrPktsDisc);
+			DP_DEBUG(DP_DBG_FLAG_DBG, "igcfg.bPmapDefault=%d\n",
+				 igcfg.bPmapDefault);
+			DP_DEBUG(DP_DBG_FLAG_DBG, "igcfg.bPmapEna=%d\n",
+				 igcfg.bPmapEna);
+			DP_DEBUG(DP_DBG_FLAG_DBG, "igcfg.bClassDefault=%d\n",
+				 igcfg.bClassDefault);
+			DP_DEBUG(DP_DBG_FLAG_DBG, "igcfg.bClassEna=%d\n",
+				 igcfg.bClassEna);
+			DP_DEBUG(DP_DBG_FLAG_DBG, "igcfg.bSubIdDefault=%d\n",
+				 igcfg.eSubId);
+			DP_DEBUG(DP_DBG_FLAG_DBG, "igcfg.bSpIdDefault=%d\n",
+				 igcfg.bSpIdDefault);
+			DP_DEBUG(DP_DBG_FLAG_DBG, "igcfg.bPmacPresent=%d\n",
+				 igcfg.bPmacPresent);
+			DP_DEBUG(DP_DBG_FLAG_DBG, "igcfg.defPmacHdr=");
+
+			for (k = 0;
+					k <
+					sizeof(igcfg.defPmacHdr) /
+					sizeof(igcfg.defPmacHdr[0]); k++)
+				DP_DEBUG(DP_DBG_FLAG_DBG, "0x%x ",
+					 igcfg.defPmacHdr[k]);
+
+			DP_DEBUG(DP_DBG_FLAG_DBG, "\n");
+
+			gsw_core_api((dp_gsw_cb)gswr_r->gsw_pmac_ops
+				     .Pmac_Ig_CfgSet, gswr_r, &igcfg);
+		}
+
+			kfree(dqport.deq_info);
+	}
+
+	/*set egress port via pmac port id */
+	if (!pmac_cfg->eg_pmac_flags)
+		return 0;
+
+	for (i = 0; i <= 15; i++) {	/*traffic class */
+		for (j = 0; j <= 3; j++) {	/* flow */
+			/*read back egcfg first from gsw */
+			memset(&egcfg, 0, sizeof(GSW_PMAC_Eg_Cfg_t));
+			egcfg.nPmacId = pmacid;
+			egcfg.nDestPortId = port;
+			egcfg.nTrafficClass = i;
+			egcfg.nFlowIDMsb = j;
+			egcfg.nBslTrafficClass = i;
+
+			memset(&pmac_glb, 0, sizeof(pmac_glb));
+			gsw_core_api((dp_gsw_cb)gswr_r->gsw_pmac_ops
+				     .Pmac_Gbl_CfgGet, gswr_r, &pmac_glb);
+			egcfg.bProcFlagsSelect = pmac_glb.bProcFlagsEgCfgEna;
+			DP_DEBUG(DP_DBG_FLAG_DBG, "bProcFlagsSelect=%u\n",
+				 egcfg.bProcFlagsSelect);
+
+			/*update egcfg and write back to gsw */
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_FCS)
+				egcfg.bFcsEna = pmac_cfg->eg_pmac.fcs;
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_L2HDR_RM) {
+				egcfg.bRemL2Hdr = pmac_cfg->eg_pmac.rm_l2hdr;
+				egcfg.numBytesRem =
+				    pmac_cfg->eg_pmac.num_l2hdr_bytes_rm;
+			}
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_PMAC)
+				egcfg.bPmacEna = pmac_cfg->eg_pmac.pmac;
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_RXID)
+				egcfg.nRxDmaChanId =
+				    pmac_cfg->eg_pmac.rx_dma_chan;
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_REDIREN)
+				egcfg.bRedirEnable = pmac_cfg->eg_pmac.redir;
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_BSLSEG)
+				egcfg.bBslSegmentDisable =
+					pmac_cfg->eg_pmac.bsl_seg;
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_RESDW1EN)
+				egcfg.bResDW1Enable =
+					pmac_cfg->eg_pmac.res_endw1;
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_RESDW1)
+				egcfg.nResDW1 = pmac_cfg->eg_pmac.res_dw1;
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_RES1DW0EN)
+				egcfg.bRes1DW0Enable =
+					pmac_cfg->eg_pmac.res1_endw0;
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_RES1DW0)
+				egcfg.nRes1DW0 = pmac_cfg->eg_pmac.res1_dw0;
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_RES2DW0EN)
+				egcfg.bRes2DW0Enable =
+					pmac_cfg->eg_pmac.res2_endw0;
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_RES2DW0)
+				egcfg.nRes2DW0 = pmac_cfg->eg_pmac.res2_dw0;
+
+			/*if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_TCENA)
+			 *    egcfg.bTCEnable = pmac_cfg->eg_pmac.tc_enable;
+			 */
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_DECFLG)
+				egcfg.bDecFlag = pmac_cfg->eg_pmac.dec_flag;
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_ENCFLG)
+				egcfg.bEncFlag = pmac_cfg->eg_pmac.enc_flag;
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_MPE1FLG)
+				egcfg.bMpe1Flag = pmac_cfg->eg_pmac.mpe1_flag;
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_MPE2FLG)
+				egcfg.bMpe2Flag = pmac_cfg->eg_pmac.mpe2_flag;
+#if defined(CONFIG_INTEL_DATAPATH_DBG) && CONFIG_INTEL_DATAPATH_DBG
+			if (dp_dbg_flag) {
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "\nPMAC %d egcfg configuration:\n",
+					 port);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.nPmacId=%d\n",
+					 egcfg.nPmacId);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.nRxDmaChanId=%d\n",
+					 egcfg.nRxDmaChanId);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.bRemL2Hdr=%d\n",
+					 egcfg.bRemL2Hdr);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.numBytesRem=%d\n",
+					 egcfg.numBytesRem);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.bFcsEna=%d\n", egcfg.bFcsEna);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.bPmacEna=%d\n",
+					 egcfg.bPmacEna);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.bResDW1Ena=%d\n",
+					 egcfg.bResDW1Enable);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.nResDW1=%d\n", egcfg.nResDW1);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.bRes1DW0Ena=%d\n",
+					 egcfg.bRes1DW0Enable);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.nRes1DW0=%d\n",
+					 egcfg.nRes1DW0);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.bRes2DW0Ena=%d\n",
+					 egcfg.bRes2DW0Enable);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.nRes2DW0=%d\n",
+					 egcfg.nRes2DW0);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.nDestPortId=%d\n",
+					 egcfg.nDestPortId);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.bTCEnable=%d\n",
+					 egcfg.bTCEnable);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.nTrafficClass=%d\n",
+					 egcfg.nTrafficClass);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.nBslTrafficClass=%d\n",
+					 egcfg.nBslTrafficClass);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.nFlowIDMsb=%d\n",
+					 egcfg.nFlowIDMsb);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.bDecFlag=%d\n",
+					 egcfg.bDecFlag);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.bEncFlag=%d\n",
+					 egcfg.bEncFlag);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.bMpe1Flag=%d\n",
+					 egcfg.bMpe1Flag);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.bMpe2Flag=%d\n",
+					 egcfg.bMpe2Flag);
+			}
+#endif
+			gsw_core_api((dp_gsw_cb)gswr_r->gsw_pmac_ops
+				     .Pmac_Eg_CfgSet, gswr_r, &egcfg);
+			;
+		}
+	}
+
+	return 0;
+}
+
+/*flag: bit 0 for cpu
+ * bit 1 for mpe1,bit 2 for mpe2, bit 3 for mpe3;
+ */
+#define GSW_L_BASE_ADDR        (0xBC000000)
+#define GSW_R_BASE_ADDR        (0xBA000000)
+#define FDMA_PASR_ADDR         (0xA47)
+
+int dp_set_gsw_parser_32(u8 flag, u8 cpu, u8 mpe1,
+			 u8 mpe2, u8 mpe3)
+{
+	GSW_CPU_PortCfg_t param = {0};
+	struct core_ops *gsw_handle = dp_port_prop[0].ops[0];/*gswip o */
+
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_common_ops
+			 .CPU_PortCfgGet, gsw_handle, &param)) {
+		PR_ERR("Failed GSW_CPU_PORT_CFG_GET\n");
+		return -1;
+	}
+	DP_DEBUG(DP_DBG_FLAG_DBG, "old flag=0x%x cpu=%d mpe1/2/3=%d/%d/%d\n",
+		 flag, param.eNoMPEParserCfg,
+		 param.eMPE1ParserCfg, param.eMPE2ParserCfg,
+		 param.eMPE1MPE2ParserCfg);
+	DP_DEBUG(DP_DBG_FLAG_DBG, "new flag=0x%x cpu=%d mpe1/2/3=%d/%d/%d\n",
+		 flag, cpu, mpe1, mpe2, mpe3);
+	if (flag & F_MPE_NONE)
+		param.eNoMPEParserCfg = cpu;
+
+	if (flag & F_MPE1_ONLY)
+		param.eMPE1ParserCfg = mpe1;
+
+	if (flag & F_MPE2_ONLY)
+		param.eMPE2ParserCfg = mpe2;
+
+	if (flag & F_MPE1_MPE2)
+		param.eMPE1MPE2ParserCfg = mpe3;
+
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_common_ops
+			 .CPU_PortCfgSet, gsw_handle, &param)) {
+		PR_ERR("Failed GSW_CPU_PORT_CFG_SET\n");
+		return -1;
+	}
+	dp_parser_info_refresh(param.eNoMPEParserCfg,
+			       param.eMPE1ParserCfg,
+			       param.eMPE2ParserCfg,
+			       param.eMPE1MPE2ParserCfg, 0);
+	return 0;
+}
+
+int dp_get_gsw_parser_32(u8 *cpu, u8 *mpe1, u8 *mpe2,
+			 u8 *mpe3)
+{
+	GSW_CPU_PortCfg_t param = {0};
+	struct core_ops *gsw_handle = dp_port_prop[0].ops[0]; /*gswip 0*/
+
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_common_ops
+			 .CPU_PortCfgGet, gsw_handle, &param)) {
+		PR_ERR("Failed GSW_CPU_PORT_CFG_GET\n");
+		return -1;
+	}
+	dp_parser_info_refresh(param.eNoMPEParserCfg,
+			       param.eMPE1ParserCfg,
+			       param.eMPE2ParserCfg,
+			       param.eMPE1MPE2ParserCfg, 1);
+
+	if (cpu) {
+		*cpu = param.eNoMPEParserCfg;
+		DP_DEBUG(DP_DBG_FLAG_DBG, "  cpu=%d\n", *cpu);
+	}
+
+	if (mpe1) {
+		*mpe1 = param.eMPE1ParserCfg;
+		DP_DEBUG(DP_DBG_FLAG_DBG, "  mpe1=%d\n", *mpe1);
+	}
+
+	if (mpe2) {
+		*mpe2 = param.eMPE2ParserCfg;
+		DP_DEBUG(DP_DBG_FLAG_DBG, "  mpe2=%d\n", *mpe2);
+	}
+
+	if (mpe3) {
+		*mpe3 = param.eMPE1MPE2ParserCfg;
+		DP_DEBUG(DP_DBG_FLAG_DBG, "  mpe3=%d\n", *mpe3);
+	}
+	return 0;
+}
+
+int gsw_mib_reset_32(int dev, u32 flag)
+{
+	struct core_ops *gsw_handle;
+	GSW_return_t ret;
+	GSW_RMON_clear_t rmon_clear;
+
+	gsw_handle = dp_port_prop[0].ops[0];
+	rmon_clear.eRmonType = GSW_RMON_ALL_TYPE;
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_rmon_ops.RMON_Clear,
+			   gsw_handle, &rmon_clear);
+
+	if (ret != GSW_statusOk) {
+		PR_ERR("R:GSW_RMON_CLEAR failed for GSW_RMON_ALL_TYPE\n");
+		return -1;
+	}
+	return ret;
+}
+
+/* Return allocated ctp number */
+struct gsw_itf *ctp_port_assign_32(int inst, u8 ep, int bp_default,
+				u32 flags, struct dp_dev_data *data)
+{
+	GSW_CTP_portAssignment_t ctp_assign;
+	struct ctp_assign *assign = &ctp_assign_def;
+	int i, alloc_flag;
+	u16 num;
+	struct core_ops *gsw_handle;
+
+	memset(&ctp_assign, 0, sizeof(ctp_assign));
+	gsw_handle = dp_port_prop[inst].ops[GSWIP_L];
+	alloc_flag = get_dp_port_info(inst, ep)->alloc_flags;
+
+	if (flags & DP_F_DEREGISTER) {
+		PR_ERR("Need to Free CTP Port here for ep=%d\n", ep);
+		ctp_assign.nLogicalPortId = ep;
+		ctp_assign.eMode = itf_assign[ep].mode;
+		ctp_assign.nFirstCtpPortId = itf_assign[ep].start;
+		ctp_assign.nNumberOfCtpPort = itf_assign[ep].n;
+		if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_ctp_ops
+				 .CTP_PortAssignmentFree,
+				 gsw_handle,
+				 &ctp_assign) != 0) {
+			PR_ERR("Failed to allc CTP for ep=%d blk=%d mode=%d\n",
+			       ep, assign->num, assign->emode);
+			return NULL;
+		}
+		return NULL;
+	}
+
+	for (i = 0; i < ARRAY_SIZE(ctp_assign_info); i++) {
+		if ((ctp_assign_info[i].flag & alloc_flag) ==
+			ctp_assign_info[i].flag) {
+			assign = &ctp_assign_info[i];
+			break;
+		}
+	}
+	if (data->max_ctp)
+		num = data->max_ctp;
+	else
+		num = assign->num;
+	ctp_assign.nLogicalPortId = ep;
+	ctp_assign.eMode = assign->emode;
+	ctp_assign.nBridgePortId = bp_default;
+	ctp_assign.nFirstCtpPortId = 0;
+	ctp_assign.nNumberOfCtpPort = num;
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_ctp_ops
+			  .CTP_PortAssignmentAlloc,
+			  gsw_handle,
+			  &ctp_assign) != 0) {
+		PR_ERR("Failed CTP Assignment for ep=%d blk size=%d mode=%s\n",
+		       ep, num, ctp_mode_string(assign->emode));
+		return NULL;
+	}
+
+	DP_DEBUG(DP_DBG_FLAG_DBG, "assign ep=%d with eMode=%d ctp_max:%d\n",
+		 ep, assign->emode, ctp_assign.nNumberOfCtpPort);
+	itf_assign[ep].mode = assign->emode;
+	itf_assign[ep].n = ctp_assign.nNumberOfCtpPort;
+	itf_assign[ep].start = ctp_assign.nFirstCtpPortId;
+	itf_assign[ep].end = ctp_assign.nFirstCtpPortId +
+		 ctp_assign.nNumberOfCtpPort - 1;
+	itf_assign[ep].ep = ep;
+	get_dp_port_info(inst, ep)->ctp_max = ctp_assign.nNumberOfCtpPort;
+	get_dp_port_info(inst, ep)->vap_offset = assign->vap_offset;
+	get_dp_port_info(inst, ep)->vap_mask = assign->vap_mask;
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_SWITCHDEV)
+	get_dp_port_info(inst, ep)->swdev_en = assign->swdev_enable;
+#endif
+	return &itf_assign[ep];
+}
+
+int set_port_lookup_mode_32(int inst, u8 ep, u32 flags)
+{
+	int i, alloc_flag;
+	struct ctp_assign *assign = &ctp_assign_def;
+
+	alloc_flag = get_dp_port_info(inst, ep)->alloc_flags;
+	for (i = 0; i < ARRAY_SIZE(ctp_assign_info); i++) {
+		if ((ctp_assign_info[i].flag & alloc_flag) ==
+		    ctp_assign_info[i].flag) {
+			assign = &ctp_assign_info[i];
+			break;
+		}
+	}
+	get_dp_port_info(inst, ep)->cqe_lu_mode = assign->lookup_mode;
+	get_dp_port_info(inst, ep)->gsw_mode = (u32)assign->emode;
+	return	0;
+}
+
+/*Allocate a bridge port with specified FID and hardcoded CPU port member */
+int alloc_bridge_port_32(int inst, int port_id, int subif_ix,
+		      int fid, int bp_member)
+{
+	GSW_BRIDGE_portAlloc_t bp = {0};
+	GSW_BRIDGE_portConfig_t bp_cfg = {0};
+	struct core_ops *gsw_handle;
+	GSW_return_t ret;
+
+	gsw_handle = dp_port_prop[inst].ops[GSWIP_L];
+	/*allocate a free bridge port */
+	memset(&bp, 0, sizeof(bp));
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			   .BridgePort_Alloc, gsw_handle, &bp);
+	if ((ret != GSW_statusOk) ||
+	    (bp.nBridgePortId < 0)) {
+		PR_ERR("Failed to get a bridge port\n");
+		return -1;
+	}
+	/*set this new bridge port with specified bridge ID(fid)
+	 *and bridge port map
+	 */
+	bp_cfg.nBridgePortId = bp.nBridgePortId;
+	bp_cfg.nDestLogicalPortId = port_id;
+	bp_cfg.nDestSubIfIdGroup = subif_ix;
+	/* By default Disable src mac learning for registered
+	 * non CPU bridge port with DP
+	 */
+	if (get_dp_port_subif(get_dp_port_info(inst, port_id), subif_ix)->
+	    mac_learn_dis == DP_MAC_LEARNING_DIS)
+		bp_cfg.bSrcMacLearningDisable = 1;
+	else
+		bp_cfg.bSrcMacLearningDisable = 0;
+	bp_cfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID |
+		GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP |
+		GSW_BRIDGE_PORT_CONFIG_MASK_MC_SRC_MAC_LEARNING |
+		GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_CTP_MAPPING;
+	bp_cfg.nBridgeId = fid;
+
+	SET_BP_MAP(bp_cfg.nBridgePortMap, bp_member); /*CPU*/
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			     .BridgePort_ConfigSet, gsw_handle, &bp_cfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("Failed to set bridge id(%d) and port map for bp= %d\n",
+		       fid, bp_cfg.nBridgePortId);
+		gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			     .BridgePort_Free, gsw_handle, &bp);
+		return -1;
+	}
+
+	/*ADD this bridge port to CPU bridge port's member.
+	 *Need read back first
+	 */
+	bp_cfg.nBridgePortId = bp_member;
+	bp_cfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			   .BridgePort_ConfigGet, gsw_handle, &bp_cfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("Failed to get bridge port's member for bridgeport=%d\n",
+		       bp_cfg.nBridgePortId);
+		gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			     .BridgePort_Free, gsw_handle, &bp);
+		return -1;
+	}
+	SET_BP_MAP(bp_cfg.nBridgePortMap, bp.nBridgePortId);
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			   .BridgePort_ConfigSet, gsw_handle, &bp_cfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("Failed to set bridge port's member for bridgeport=%d\n",
+		       bp_cfg.nBridgePortId);
+		gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			     .BridgePort_Free, gsw_handle, &bp);
+		return -1;
+	}
+
+	return bp.nBridgePortId;
+}
+
+/*Free one GSWIP bridge port
+ *First read out its port member
+ *according to this port memeber, from this deleing bridge port
+ *from its member's member Free this bridge port
+ */
+int free_bridge_port_32(int inst, int bp)
+{
+	GSW_BRIDGE_portConfig_t *tmp = NULL, *tmp2 = NULL;
+	int i, j;
+	GSW_return_t ret;
+	struct core_ops *gsw_handle;
+
+	gsw_handle = dp_port_prop[inst].ops[GSWIP_L];
+	if (bp == CPU_BP)
+		return 0;
+
+	tmp = kzalloc(sizeof(*tmp), GFP_KERNEL);
+	tmp2 = kzalloc(sizeof(*tmp2), GFP_KERNEL);
+	if (!tmp || !tmp2)
+		goto FREE_EXIT;
+
+	/*read out this delting bridge port's member*/
+	tmp->nBridgePortId = bp;
+	tmp->eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			   .BridgePort_ConfigGet, gsw_handle, tmp);
+	if (ret != GSW_statusOk) {
+		PR_ERR("Failed GSW_BRIDGE_PORT_CONFIG_GET: %d\n", bp);
+		goto EXIT;
+	}
+	/*remove this delting bridgeport from other bridge port's member*/
+	for (i = 0; i < ARRAY_SIZE(tmp->nBridgePortMap); i++) {
+		for (j = 0; j < 16 /*u16*/; j++) {
+			if (!(tmp->nBridgePortMap[i] & (1 << j)))
+				continue; /*not memmber bit set */
+			memset(tmp2->nBridgePortMap, 0,
+			       sizeof(tmp2->nBridgePortMap));
+			tmp2->eMask =
+				GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
+			tmp2->nBridgePortId = i * 16 + j;
+			ret = gsw_core_api((dp_gsw_cb)gsw_handle
+					   ->gsw_brdgport_ops
+					   .BridgePort_ConfigGet, gsw_handle,
+					   tmp2);
+			if (ret != GSW_statusOk) {
+				PR_ERR("Failed GSW_BRIDGE_PORT_CONFIG_GET\n");
+				goto EXIT;
+			}
+			UNSET_BP_MAP(tmp2->nBridgePortMap, bp);
+			ret = gsw_core_api((dp_gsw_cb)gsw_handle
+					   ->gsw_brdgport_ops
+					   .BridgePort_ConfigSet, gsw_handle,
+					   tmp2);
+			if (ret != GSW_statusOk) {
+				PR_ERR("Failed GSW_BRIDGE_PORT_CONFIG_SET\n");
+				goto EXIT;
+			}
+		}
+	}
+EXIT:
+	/*FRee thie bridge port */
+	memset(tmp, 0, sizeof(*tmp));
+	tmp->nBridgePortId = bp;
+	tmp->eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			   .BridgePort_Free, gsw_handle, tmp);
+	if (ret != GSW_statusOk)
+		PR_ERR("Failed to GSW_BRIDGE_PORT_FREE:%d\n", bp);
+FREE_EXIT:
+	kfree(tmp);
+	kfree(tmp2);
+	return 0;
+}
+
+int dp_gswip_mac_entry_add_32(int bport, int fid, int inst, u8 *addr)
+{
+	GSW_MAC_tableAdd_t tmp;
+	GSW_return_t ret;
+	struct core_ops *gsw_handle;
+
+	gsw_handle = dp_port_prop[inst].ops[GSWIP_L];
+	memset(&tmp, 0, sizeof(tmp));
+	tmp.bStaticEntry = 1;
+	tmp.nFId = fid;
+	tmp.nPortId = bport;
+	SET_BP_MAP(tmp.nPortMap, bport);
+	tmp.nSubIfId = 0;
+	memcpy(tmp.nMAC, addr, GSW_MAC_ADDR_LEN);
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_swmac_ops.
+			   MAC_TableEntryAdd, gsw_handle, &tmp);
+	if (ret != GSW_statusOk) {
+		PR_ERR("fail in setting MAC table static add entry\r\n");
+		return -1;
+	}
+	return 0;
+}
+
+int dp_gswip_mac_entry_del_32(int bport, int fid, int inst, u8 *addr)
+{
+	GSW_MAC_tableRemove_t tmp;
+	GSW_MAC_tableQuery_t mac_query;
+	GSW_return_t ret;
+	struct core_ops *gsw_handle;
+
+	gsw_handle = dp_port_prop[inst].ops[GSWIP_L];
+	memset(&tmp, 0, sizeof(tmp));
+	memset(&mac_query, 0, sizeof(mac_query));
+	mac_query.nFId = fid;
+	memcpy(mac_query.nMAC, addr, GSW_MAC_ADDR_LEN);
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_swmac_ops.
+			   MAC_TableEntryQuery, gsw_handle, &tmp);
+	if (ret != GSW_statusOk) {
+		PR_ERR("fail in getting MAC query entry\r\n");
+		return -1;
+	}
+	tmp.nFId = fid;
+	memcpy(tmp.nMAC, addr, GSW_MAC_ADDR_LEN);
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_swmac_ops.
+			   MAC_TableEntryRemove, gsw_handle, &tmp);
+	if (ret != GSW_statusOk) {
+		PR_ERR("fail in setting MAC static entry remove\r\n");
+		return -1;
+	}
+	return 0;
+}
+
+int cpu_vlan_mod_dis_32(int inst)
+{
+	GSW_QoS_portRemarkingCfg_t cfg = {0};
+	struct core_ops *ops;
+	int ret;
+
+	ops = dp_port_prop[inst].ops[GSWIP_L];
+
+	cfg.nPortId = 0;
+	ret = gsw_core_api((dp_gsw_cb)ops->gsw_qos_ops.
+			   QoS_PortRemarkingCfgGet, ops, &cfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("QoS_PortRemarkingCfgGet failed\n");
+		return -1;
+	}
+
+	cfg.bPCP_EgressRemarkingEnable = LTQ_FALSE;
+	ret = gsw_core_api((dp_gsw_cb)ops->gsw_qos_ops.
+			   QoS_PortRemarkingCfgSet, ops, &cfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("QoS_PortRemarkingCfgSet failed\n");
+		return -1;
+	}
+
+	return 0;
+}
+
+int dp_set_gsw_pmapper_32(int inst, int bport, int lport,
+			  struct dp_pmapper *mapper, u32 flag)
+{
+	GSW_BRIDGE_portConfig_t bp_cfg = {0};
+	struct core_ops *gsw_handle;
+	GSW_return_t ret;
+	int i, index;
+	int ctp;
+	struct pmac_port_info *port_info = get_dp_port_info(inst, lport);
+
+	gsw_handle = dp_port_prop[inst].ops[GSWIP_L];
+
+	DP_DEBUG(DP_DBG_FLAG_DBG,
+		 "set pmapper bport %d inst %d lport %d\n",
+		 bport, inst, lport);
+	bp_cfg.nBridgePortId = bport;
+	bp_cfg.nDestLogicalPortId = lport;
+	bp_cfg.bPmapperEnable = 1;
+	bp_cfg.ePmapperMappingMode = mapper->mode;
+
+	/* copy the sub if information in all pmapper list*/
+	if (mapper->def_ctp != DP_PMAPPER_DISCARD_CTP) {
+		ctp = GET_VAP(mapper->def_ctp, port_info->vap_offset,
+			      port_info->vap_mask);
+	} else {
+		ctp = PMAPPER_DISC_CTP;
+	}
+	bp_cfg.sPmapper.nDestSubIfIdGroup[0] = ctp;
+
+	for (i = 0; i < DP_PMAP_PCP_NUM; i++) {
+		if (mapper->pcp_map[i] != DP_PMAPPER_DISCARD_CTP) {
+			ctp = GET_VAP(mapper->pcp_map[i],
+				      port_info->vap_offset,
+				      port_info->vap_mask);
+		} else {
+			ctp = PMAPPER_DISC_CTP;
+		}
+		bp_cfg.sPmapper.nDestSubIfIdGroup[1 + i] = ctp;
+	}
+	for (i = 0; i < DP_PMAP_DSCP_NUM; i++) {
+		if (mapper->dscp_map[i] != DP_PMAPPER_DISCARD_CTP) {
+			ctp = GET_VAP(mapper->dscp_map[i],
+				      port_info->vap_offset,
+				      port_info->vap_mask);
+		} else {
+			ctp = PMAPPER_DISC_CTP;
+		}
+		index = 1 + DP_PMAP_PCP_NUM + i;
+		bp_cfg.sPmapper.nDestSubIfIdGroup[index] = ctp;
+	}
+	bp_cfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_CTP_MAPPING;
+
+	DP_DEBUG(DP_DBG_FLAG_DBG,
+		 "call switch api mode %d enable %d eMask 0x%x\n",
+		 bp_cfg.ePmapperMappingMode, bp_cfg.bPmapperEnable,
+		 bp_cfg.eMask);
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			     .BridgePort_ConfigSet, gsw_handle, &bp_cfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("fail in setting pmapper\r\n");
+		return -1;
+	}
+	return 0;
+}
+
+int dp_get_gsw_pmapper_32(int inst, int bport, int lport,
+			  struct dp_pmapper *mapper, u32 flag)
+{
+	GSW_BRIDGE_portConfig_t bp_cfg = {0};
+	struct core_ops *gsw_handle;
+	GSW_return_t ret;
+	int i, index;
+	struct hal_priv *priv;
+	u16 dest;
+	struct pmac_port_info *info = get_dp_port_info(inst, lport);
+
+	priv = (struct hal_priv *)dp_port_prop[inst].priv_hal;
+	gsw_handle = dp_port_prop[inst].ops[GSWIP_L];
+
+	DP_DEBUG(DP_DBG_FLAG_DBG,
+		 "get bport %d inst %d lport %d\n",
+		 bport, inst, lport);
+	bp_cfg.nBridgePortId = bport;
+	bp_cfg.nDestLogicalPortId = lport;
+	bp_cfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_CTP_MAPPING;
+
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			     .BridgePort_ConfigGet, gsw_handle, &bp_cfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("fail in getting pmapper\r\n");
+		return -1;
+	}
+
+	DP_DEBUG(DP_DBG_FLAG_DBG,
+		 "after call switch api mode %d enable %d\n",
+		 bp_cfg.ePmapperMappingMode, bp_cfg.bPmapperEnable);
+
+	if (!bp_cfg.bPmapperEnable) {
+		PR_ERR("pmapper not enabled\r\n");
+		return -1;
+	}
+	mapper->pmapper_id = bp_cfg.sPmapper.nPmapperId;
+	mapper->mode = bp_cfg.ePmapperMappingMode;
+
+	dest = bp_cfg.sPmapper.nDestSubIfIdGroup[0];
+	if (dest == PMAPPER_DISC_CTP)
+		mapper->def_ctp = DP_PMAPPER_DISCARD_CTP;
+	else
+		mapper->def_ctp = SET_VAP(dest, info->vap_offset,
+					  info->vap_mask);
+	for (i = 0; i < DP_PMAP_PCP_NUM; i++) {
+		dest = bp_cfg.sPmapper.nDestSubIfIdGroup[1 + i];
+		if (dest == PMAPPER_DISC_CTP)
+			mapper->pcp_map[i] = DP_PMAPPER_DISCARD_CTP;
+		else
+			mapper->pcp_map[i] = SET_VAP(dest, info->vap_offset,
+						     info->vap_mask);
+	}
+	for (i = 0; i < DP_PMAP_DSCP_NUM; i++) {
+		index = 1 + DP_PMAP_PCP_NUM + i;
+		dest = bp_cfg.sPmapper.nDestSubIfIdGroup[index];
+		if (dest == PMAPPER_DISC_CTP)
+			mapper->dscp_map[i] = DP_PMAPPER_DISCARD_CTP;
+		else
+			mapper->dscp_map[i] = SET_VAP(dest, info->vap_offset,
+						      info->vap_mask);
+	}
+	return 0;
+}
+
+int dp_meter_alloc_32(int inst, int *meterid, int flag)
+{
+	struct core_ops *gsw_handle;
+	GSW_QoS_meterCfg_t meter_cfg = {0};
+	GSW_return_t ret;
+
+	if (inst < 0) {
+		PR_ERR("inst invalid\n");
+		return -1;
+	}
+	if (!meterid) {
+		PR_ERR("meterid NULL\n");
+		return -1;
+	}
+	gsw_handle = dp_port_prop[inst].ops[GSWIP_L];
+	if (!gsw_handle) {
+		PR_ERR("gsw_handle NULL\n");
+		return -1;
+	}
+	if (flag == DP_F_DEREGISTER && *meterid >= 0) {
+		meter_cfg.nMeterId = *meterid;
+		ret = GSW_CORE_API(gsw_handle, gsw_qos_ops.QOS_MeterAlloc,
+				   &meter_cfg);
+		if (ret != GSW_statusOk) {
+			PR_ERR("Meter dealloc failed: %d\n", ret);
+			return -1;
+		}
+		return 0;
+	}
+	memset(&meter_cfg, 0, sizeof(meter_cfg));
+	ret = GSW_CORE_API(gsw_handle, gsw_qos_ops.QOS_MeterAlloc,
+			   &meter_cfg);
+	if ((ret != GSW_statusOk) || (meter_cfg.nMeterId < 0)) {
+		PR_ERR("Failed to get a meter alloc\n");
+		*meterid = -1;
+		return -1;
+	}
+	*meterid = meter_cfg.nMeterId;
+	return 0;
+}
+
+static int dp_set_col_mark(struct net_device *dev, struct dp_meter_cfg  *meter,
+			   int flag, struct dp_meter_subif *mtr_subif)
+{
+	struct core_ops *gsw_handle;
+	GSW_BRIDGE_portConfig_t bp_cfg;
+	GSW_CTP_portConfig_t ctp_cfg;
+	GSW_return_t ret;
+	struct pmac_port_info *port_info;
+
+	if (!mtr_subif) {
+		PR_ERR("mtr_subif NULL\n");
+		return -1;
+	}
+	memset(&bp_cfg, 0, sizeof(GSW_BRIDGE_portConfig_t));
+	memset(&ctp_cfg, 0, sizeof(GSW_CTP_portConfig_t));
+	gsw_handle = dp_port_prop[mtr_subif->inst].ops[GSWIP_L];
+	if (!gsw_handle)
+		return -1;
+	if (flag & DP_METER_ATTACH_CTP) {/* CTP port Flag */
+		if (mtr_subif->subif.flag_pmapper) {
+			PR_ERR("can't use CTP,pmapper is enable\n");
+			return -1;
+		}
+		port_info = get_dp_port_info(mtr_subif->subif.inst,
+					     mtr_subif->subif.port_id);
+		ctp_cfg.nLogicalPortId = mtr_subif->subif.port_id;
+		ctp_cfg.nSubIfIdGroup  = GET_VAP(mtr_subif->subif.subif,
+						 port_info->vap_offset,
+						 port_info->vap_mask);
+		ret = GSW_CORE_API(gsw_handle, gsw_ctp_ops.CTP_PortConfigGet,
+				   &ctp_cfg);
+		if (ret != GSW_statusOk) {
+			PR_ERR("PortConfigGet API failed :%d\n", ret);
+			return -1;
+		}
+		if (meter->dir == DP_DIR_INGRESS) {
+			ctp_cfg.eMask = GSW_CTP_PORT_CONFIG_INGRESS_MARKING;
+			ctp_cfg.eIngressMarkingMode = meter->mode;
+		} else if (meter->dir == DP_DIR_EGRESS) {
+			if (meter->mode != DP_INTERNAL) {
+				ctp_cfg.eMask =
+				GSW_CTP_PORT_CONFIG_EGRESS_MARKING_OVERRIDE;
+				ctp_cfg.bEgressMarkingOverrideEnable = 1;
+				ctp_cfg.eEgressMarkingModeOverride =
+					meter->mode;
+			} else {
+				DP_DEBUG(DP_DBG_FLAG_PAE,
+					 "mode:internal invalid for egress\n");
+			}
+		} else {
+			return -1;
+		}
+		ret = GSW_CORE_API(gsw_handle, gsw_ctp_ops.CTP_PortConfigSet,
+				   &ctp_cfg);
+		if (ret != GSW_statusOk) {
+			PR_ERR("PortConfigSet API failed :%d\n", ret);
+			return -1;
+		}
+	}
+	if (flag & DP_METER_ATTACH_BRPORT) {/*BRIDGE port Flag*/
+		if (!mtr_subif->subif.flag_bp) {
+			PR_ERR("flag_bp value 0\n");
+			return -1;
+		}
+		bp_cfg.nBridgePortId = mtr_subif->subif.bport;
+		ret = GSW_CORE_API(gsw_handle,
+				   gsw_brdgport_ops.BridgePort_ConfigGet,
+				   &bp_cfg);
+		if (ret != GSW_statusOk) {
+			PR_ERR("BridgePort_ConfigGet API failed :%d\n", ret);
+			return -1;
+		}
+		if (meter->dir == DP_DIR_EGRESS) {
+			PR_ERR("No Egress color marking for bridge port\n");
+			return -1;
+		} else if (meter->dir == DP_DIR_INGRESS) {
+			bp_cfg.eMask =
+				GSW_BRIDGE_PORT_CONFIG_MASK_INGRESS_MARKING;
+			bp_cfg.eIngressMarkingMode = meter->mode;
+		} else {
+			PR_ERR(" invalid color mark dir\n");
+			return -1;
+		}
+		ret = GSW_CORE_API(gsw_handle,
+				   gsw_brdgport_ops.BridgePort_ConfigSet,
+				   &bp_cfg);
+		if (ret != GSW_statusOk) {
+			PR_ERR("BridgePort_ConfigSet API failed :%d\n", ret);
+			return -1;
+		}
+	}
+	return 0;
+}
+
+int dp_meter_add_32(struct net_device *dev,  struct dp_meter_cfg  *meter,
+		    int flag, struct dp_meter_subif *mtr_subif)
+{
+	struct core_ops *gsw_handle;
+	GSW_QoS_meterCfg_t meter_cfg;
+	GSW_BRIDGE_portConfig_t *bp_cfg = NULL;
+	GSW_PCE_rule_t *pce_rule = NULL;
+	GSW_CTP_portConfig_t *ctp_cfg = NULL;
+	GSW_BRIDGE_config_t *br_cfg = NULL;
+	GSW_return_t ret;
+	int bret = 0;
+
+	if (!mtr_subif) {
+		PR_ERR("mtr_subif NULL\n");
+		return -1;
+	}
+	gsw_handle = dp_port_prop[mtr_subif->inst].ops[GSWIP_L];
+	if (!gsw_handle)
+		return -1;
+
+	if (flag & DP_COL_MARKING)
+		return dp_set_col_mark(dev, meter, flag, mtr_subif);
+	memset(&meter_cfg, 0, sizeof(GSW_QoS_meterCfg_t));
+	meter_cfg.nCbs = meter->cbs;
+	meter_cfg.nRate = METER_CIR(meter->cir);
+	meter_cfg.nEbs = meter->pbs;
+	meter_cfg.nPiRate = METER_PIR(meter->pir);
+	meter_cfg.bEnable = 1;
+	if (meter->type == srTCM) {
+		meter_cfg.eMtrType = GSW_QOS_Meter_srTCM;
+	} else if (meter->type == trTCM) {
+		meter_cfg.eMtrType = GSW_QOS_Meter_trTCM;
+	} else {
+		PR_ERR(" invalid meter type\n");
+		return -1;
+	}
+	meter_cfg.nMeterId = meter->meter_id;
+	meter_cfg.nColourBlindMode = meter->col_mode;
+	ret = GSW_CORE_API(gsw_handle, gsw_qos_ops.QoS_MeterCfgSet,
+			   &meter_cfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("MeterCfgSet API failed:%d\n", ret);
+		return -1;
+	}
+
+	if (flag & DP_METER_ATTACH_PCE) {
+		/* pattern setting */
+		pce_rule = kzalloc(sizeof(GSW_PCE_rule_t), GFP_KERNEL);
+		if (!pce_rule) {
+			PR_ERR("ctp_cfg alloc failed\n");
+			bret = -1;
+			goto err;
+		}
+		pce_rule->pattern.nIndex = meter->dp_pce.pce_idx;
+		pce_rule->pattern.bEnable = 1;
+		/* action setting */
+		pce_rule->action.eMeterAction = GSW_PCE_ACTION_METER_1;
+		pce_rule->action.nMeterId =  meter->meter_id;
+		ret = GSW_CORE_API(gsw_handle, gsw_tflow_ops.TFLOW_PceRuleWrite,
+				   pce_rule);
+		if (ret != GSW_statusOk) {
+			PR_ERR("PceRule Write API failed :%d\n", ret);
+			goto err;
+		}
+	}
+	if (flag & DP_METER_ATTACH_CTP) {/* CTP port Flag */
+		struct pmac_port_info *port_info;
+
+		if (mtr_subif->subif.flag_pmapper) {
+			PR_ERR("can't use CTP,pmapper is enable\n");
+			bret = -1;
+			goto err;
+		}
+		ctp_cfg = kzalloc(sizeof(GSW_CTP_portConfig_t), GFP_KERNEL);
+		if (!ctp_cfg) {
+			PR_ERR("ctp_cfg alloc failed\n");
+			bret = -1;
+			goto err;
+		}
+		port_info =
+		get_dp_port_info(mtr_subif->subif.inst, mtr_subif->subif.port_id);
+		if (!port_info) {
+			PR_ERR(" port_info is NULL\n");
+			bret = -1;
+			goto err;
+		}
+		ctp_cfg->nLogicalPortId = mtr_subif->subif.port_id;
+		ctp_cfg->nSubIfIdGroup  = GET_VAP(mtr_subif->subif.subif,
+						 port_info->vap_offset,
+						 port_info->vap_mask);
+		ret = GSW_CORE_API(gsw_handle, gsw_ctp_ops.CTP_PortConfigGet,
+				   ctp_cfg);
+		if (ret != GSW_statusOk) {
+			PR_ERR("PortConfigGet API failed :%d\n", ret);
+			bret = -1;
+			goto err;
+		}
+		if (meter->dir == DP_DIR_EGRESS) {
+			ctp_cfg->eMask = GSW_CTP_PORT_CONFIG_EGRESS_METER;
+			ctp_cfg->bEgressMeteringEnable = 1;
+			ctp_cfg->nEgressTrafficMeterId =  meter->meter_id;
+		} else if (meter->dir == DP_DIR_INGRESS) {
+			ctp_cfg->eMask = GSW_CTP_PORT_CONFIG_INGRESS_METER;
+			ctp_cfg->bIngressMeteringEnable = 1;
+			ctp_cfg->nIngressTrafficMeterId =  meter->meter_id;
+		} else {
+			PR_ERR(" invalid meter dir\n");
+			return -1;
+		}
+		ret = GSW_CORE_API(gsw_handle, gsw_ctp_ops.CTP_PortConfigSet,
+				   ctp_cfg);
+		if (ret != GSW_statusOk) {
+			PR_ERR("PortConfigSet API failed :%d\n", ret);
+			bret = -1;
+			goto err;
+		}
+	}
+	if (flag & DP_METER_ATTACH_BRPORT) {/*BRIDGE port Flag*/
+		if (!mtr_subif->subif.flag_bp) {
+			PR_ERR("flag_bp value 0\n");
+			bret = -1;
+			goto err;
+		}
+		bp_cfg = kzalloc(sizeof(GSW_BRIDGE_portConfig_t), GFP_KERNEL);
+		if (!bp_cfg) {
+			PR_ERR("bp_cfg alloc failed\n");
+			bret = -1;
+			goto err;
+		}
+		bp_cfg->nBridgePortId = mtr_subif->subif.bport;
+		ret = GSW_CORE_API(gsw_handle,
+				   gsw_brdgport_ops.BridgePort_ConfigGet,
+				   bp_cfg);
+		if (ret != GSW_statusOk) {
+			PR_ERR("BridgePort_ConfigGet API failed :%d\n", ret);
+			bret = -1;
+			goto err;
+		}
+		if (meter->dir == DP_DIR_EGRESS) {
+			bp_cfg->eMask =
+				GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_SUB_METER;
+			if (meter->dp_pce.flow == DP_UKNOWN_UNICAST)
+				BP_CFG(bp_cfg,
+				       GSW_BRIDGE_PORT_EGRESS_METER_UNKNOWN_UC,
+				       meter->meter_id, 1);
+			else if (meter->dp_pce.flow == DP_MULTICAST)
+				BP_CFG(bp_cfg,
+				       GSW_BRIDGE_PORT_EGRESS_METER_MULTICAST,
+				       meter->meter_id, 1);
+			else if (meter->dp_pce.flow == DP_BROADCAST)
+				BP_CFG(bp_cfg,
+				       GSW_BRIDGE_PORT_EGRESS_METER_BROADCAST,
+				       meter->meter_id, 1);
+			else
+				BP_CFG(bp_cfg,
+				       GSW_BRIDGE_PORT_EGRESS_METER_OTHERS,
+				       meter->meter_id, 1);
+		} else if (meter->dir == DP_DIR_INGRESS) {
+			bp_cfg->eMask =
+				GSW_BRIDGE_PORT_CONFIG_MASK_INGRESS_METER;
+			bp_cfg->bIngressMeteringEnable = 1;
+			bp_cfg->nIngressTrafficMeterId = meter->meter_id;
+		} else {
+			PR_ERR(" invalid meter dir\n");
+			return -1;
+		}
+		ret = GSW_CORE_API(gsw_handle,
+				   gsw_brdgport_ops.BridgePort_ConfigSet,
+				   bp_cfg);
+		if (ret != GSW_statusOk) {
+			PR_ERR("BridgePort_ConfigSet API failed :%d\n", ret);
+			bret = -1;
+			goto err;
+		}
+	} else if (flag & DP_METER_ATTACH_BRIDGE) {
+		br_cfg = kzalloc(sizeof(GSW_BRIDGE_config_t), GFP_KERNEL);
+		if (!br_cfg) {
+			PR_ERR("br_cfg alloc failed\n");
+			bret = -1;
+			goto err;
+		}
+		br_cfg->nBridgeId = mtr_subif->fid;
+		ret = GSW_CORE_API(gsw_handle,
+				   gsw_brdg_ops.Bridge_ConfigGet,
+				   br_cfg);
+		if (ret != GSW_statusOk) {
+			PR_ERR("Bridge_ConfigGet API failed :%d\n", ret);
+			bret = -1;
+			goto err;
+		}
+		br_cfg->eMask = GSW_BRIDGE_CONFIG_MASK_SUB_METER;
+		if (meter->dp_pce.flow == DP_UKNOWN_UNICAST)
+			BR_CFG(br_cfg,
+			       GSW_BRIDGE_PORT_EGRESS_METER_UNKNOWN_UC,
+			       meter->meter_id, 1);
+		else if (meter->dp_pce.flow == DP_MULTICAST)
+			BR_CFG(br_cfg,
+			       GSW_BRIDGE_PORT_EGRESS_METER_MULTICAST,
+			       meter->meter_id, 1);
+		else if (meter->dp_pce.flow == DP_BROADCAST)
+			BR_CFG(br_cfg,
+			       GSW_BRIDGE_PORT_EGRESS_METER_BROADCAST,
+			       meter->meter_id, 1);
+		else {
+			PR_ERR("Meter flow invalid\n");
+			bret = -1;
+			goto err;
+		}
+		ret = GSW_CORE_API(gsw_handle, gsw_brdg_ops.Bridge_ConfigSet,
+				   br_cfg);
+		if (ret != GSW_statusOk) {
+			PR_ERR("Bridge_ConfigSet API failed :%d\n", ret);
+			bret = -1;
+			goto err;
+		}
+	}
+err:
+	kfree(br_cfg);
+	kfree(bp_cfg);
+	kfree(ctp_cfg);
+	kfree(pce_rule);
+	return bret;
+}
+
+int dp_meter_del_32(struct net_device *dev,  struct dp_meter_cfg  *meter,
+		    int flag, struct dp_meter_subif *mtr_subif)
+{
+	struct core_ops *gsw_handle;
+	GSW_BRIDGE_portConfig_t *bp_cfg = NULL;
+	GSW_PCE_rule_t *pce_rule = NULL;
+	GSW_CTP_portConfig_t *ctp_cfg = NULL;
+	GSW_BRIDGE_config_t *br_cfg = NULL;
+	GSW_return_t ret;
+	int bret = 0;
+
+	if (!mtr_subif) {
+		PR_ERR(" mtr_subif NULL\n");
+		return -1;
+	}
+	gsw_handle = dp_port_prop[mtr_subif->inst].ops[GSWIP_L];
+	if (!gsw_handle)
+		return -1;
+
+	if (meter->dir & DP_METER_ATTACH_PCE) {
+		pce_rule = kzalloc(sizeof(GSW_PCE_rule_t), GFP_KERNEL);
+		if (!pce_rule) {
+			PR_ERR("ctp_cfg alloc failed\n");
+			bret = -1;
+			goto err;
+		}
+		/* pattern setting */
+		pce_rule->pattern.nIndex = meter->dp_pce.pce_idx;
+		pce_rule->pattern.bEnable = 0;
+		ret = GSW_CORE_API(gsw_handle, gsw_tflow_ops.TFLOW_PceRuleWrite,
+				   pce_rule);
+		if (ret != GSW_statusOk) {
+			PR_ERR("PceRule Write API failed :%d\n", ret);
+			bret = -1;
+			goto err;
+		}
+	}
+	if (flag & DP_METER_ATTACH_CTP) {
+		struct pmac_port_info *port_info;
+
+		if (mtr_subif->subif.flag_pmapper) {
+			PR_ERR("flag_pmapper is set\n");
+			bret = -1;
+			goto err;
+		}
+		ctp_cfg = kzalloc(sizeof(GSW_CTP_portConfig_t), GFP_KERNEL);
+		if (!ctp_cfg) {
+			PR_ERR("ctp_cfg alloc failed\n");
+			bret = -1;
+			goto err;
+		}
+		port_info = get_dp_port_info(mtr_subif->subif.inst,
+					     mtr_subif->subif.port_id);
+		if (!port_info) {
+			PR_ERR(" port_info is NULL\n");
+			bret = -1;
+			goto err;
+		}
+		ctp_cfg->nLogicalPortId = mtr_subif->subif.port_id;
+		ctp_cfg->nSubIfIdGroup = GET_VAP(mtr_subif->subif.subif,
+							port_info->vap_offset,
+							port_info->vap_mask);
+		if (meter->dir == DP_DIR_EGRESS)
+			ctp_cfg->nEgressTrafficMeterId =  meter->meter_id;
+		else if (meter->dir == DP_DIR_INGRESS)
+			ctp_cfg->nIngressTrafficMeterId =  meter->meter_id;
+		ret = GSW_CORE_API(gsw_handle, gsw_ctp_ops.CTP_PortConfigGet,
+				   ctp_cfg);
+		if (ret != GSW_statusOk) {
+			bret = -1;
+			goto err;
+		}
+		if (meter->dir == DP_DIR_EGRESS) {
+			ctp_cfg->eMask = GSW_CTP_PORT_CONFIG_EGRESS_METER;
+			ctp_cfg->bEgressMeteringEnable = 0;
+		} else if (meter->dir == DP_DIR_INGRESS) {
+			ctp_cfg->eMask = GSW_CTP_PORT_CONFIG_INGRESS_METER;
+			ctp_cfg->bIngressMeteringEnable = 0;
+		}
+		ret = GSW_CORE_API(gsw_handle, gsw_ctp_ops.CTP_PortConfigSet,
+				   ctp_cfg);
+		if (ret != GSW_statusOk) {
+			bret = -1;
+			goto err;
+		}
+	}
+	if (flag & DP_METER_ATTACH_BRPORT) {
+		if (!mtr_subif->subif.flag_bp) {
+			PR_ERR("flag_bp is 0\n");
+			bret = -1;
+			goto err;
+		}
+		bp_cfg = kzalloc(sizeof(GSW_BRIDGE_portConfig_t), GFP_KERNEL);
+		if (!bp_cfg) {
+			PR_ERR("bp_cfg alloc failed\n");
+			bret = -1;
+			goto err;
+		}
+		bp_cfg->nBridgePortId = mtr_subif->subif.bport;
+		ret = GSW_CORE_API(gsw_handle,
+				   gsw_brdgport_ops.BridgePort_ConfigGet,
+				   bp_cfg);
+		if (ret != GSW_statusOk) {
+			PR_ERR("BridgePort_ConfigGet API failed\n");
+			bret = -1;
+			goto err;
+		}
+		if (meter->dir == DP_DIR_EGRESS) {
+			bp_cfg->eMask =
+				GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_SUB_METER;
+			if (meter->dp_pce.flow == DP_UKNOWN_UNICAST)
+				BP_CFG(bp_cfg,
+				       GSW_BRIDGE_PORT_EGRESS_METER_UNKNOWN_UC,
+				       meter->meter_id, 0);
+			else if (meter->dp_pce.flow == DP_MULTICAST)
+				BP_CFG(bp_cfg,
+				       GSW_BRIDGE_PORT_EGRESS_METER_MULTICAST,
+				       meter->meter_id, 0);
+			else if (meter->dp_pce.flow == DP_BROADCAST)
+				BP_CFG(bp_cfg,
+				       GSW_BRIDGE_PORT_EGRESS_METER_BROADCAST,
+				       meter->meter_id, 0);
+			else
+				BP_CFG(bp_cfg,
+				       GSW_BRIDGE_PORT_EGRESS_METER_OTHERS,
+				       meter->meter_id, 0);
+		} else {
+			bp_cfg->eMask =
+				GSW_BRIDGE_PORT_CONFIG_MASK_INGRESS_METER;
+			bp_cfg->bIngressMeteringEnable = 0;
+		}
+		ret = GSW_CORE_API(gsw_handle,
+				   gsw_brdgport_ops.BridgePort_ConfigSet,
+				   bp_cfg);
+		if (ret != GSW_statusOk) {
+			PR_ERR("BridgePort_ConfigSet API failed\n");
+			bret = -1;
+			goto err;
+		}
+	}
+	if (flag & DP_METER_ATTACH_BRIDGE) {
+		br_cfg = kzalloc(sizeof(GSW_BRIDGE_config_t), GFP_KERNEL);
+		if (!br_cfg) {
+			PR_ERR("br_cfg alloc failed\n");
+			bret = -1;
+			goto err;
+		}
+		br_cfg->nBridgeId = mtr_subif->fid;
+		ret = GSW_CORE_API(gsw_handle, gsw_brdg_ops.Bridge_ConfigGet,
+				   br_cfg);
+		if (ret != GSW_statusOk) {
+			PR_ERR("Bridge_ConfigGet API failed :%d\n", ret);
+			bret = -1;
+			goto err;
+		}
+		br_cfg->eMask = GSW_BRIDGE_CONFIG_MASK_SUB_METER;
+		if (meter->dp_pce.flow == DP_UKNOWN_UNICAST)
+			BR_CFG(br_cfg,
+			       GSW_BRIDGE_PORT_EGRESS_METER_UNKNOWN_UC,
+			       meter->meter_id, 0);
+		else if (meter->dp_pce.flow == DP_MULTICAST)
+			BR_CFG(br_cfg,
+			       GSW_BRIDGE_PORT_EGRESS_METER_MULTICAST,
+			       meter->meter_id, 0);
+		else if (meter->dp_pce.flow == DP_BROADCAST)
+			BR_CFG(br_cfg,
+			       GSW_BRIDGE_PORT_EGRESS_METER_BROADCAST,
+			       meter->meter_id, 0);
+		else {
+			PR_ERR("Meter flow invalid\n");
+			bret = -1;
+			goto err;
+		}
+		ret = GSW_CORE_API(gsw_handle, gsw_brdg_ops.Bridge_ConfigSet,
+				   br_cfg);
+		if (ret != GSW_statusOk) {
+			PR_ERR("Bridge_ConfigSet API failed :%d\n", ret);
+			bret = -1;
+			goto err;
+		}
+	}
+err:
+	kfree(br_cfg);
+	kfree(bp_cfg);
+	kfree(ctp_cfg);
+	kfree(pce_rule);
+	return bret;
+}
+
+int gpid_port_assign(int inst, u8 ep, u32 flags)
+{
+	GSW_LPID_to_GPID_Assignment_t lp_gp_assign;
+	GSW_GPID_to_LPID_Assignment_t gp_lp_assign;
+	int gpid_base, gpid_num, i, gpid_spl = -1;
+	struct core_ops *gsw_handle = dp_port_prop[inst].ops[GSWIP_L];
+	struct ctp_assign *assign;
+	struct cbm_gpid_lpid cbm_gpid = {0};
+	struct hal_priv *priv;
+
+	priv = (struct hal_priv *)dp_port_prop[inst].priv_hal;
+	if (!priv) {
+		PR_ERR("priv NULL ?\n");
+		return DP_FAILURE;
+	}
+	if (!gsw_handle) {
+		PR_ERR("gsw_handle NULL ?\n");
+		return DP_FAILURE;
+	}
+	memset(&lp_gp_assign, 0, sizeof(GSW_LPID_to_GPID_Assignment_t));
+	memset(&gp_lp_assign, 0, sizeof(GSW_GPID_to_LPID_Assignment_t));
+
+	if (flags & DP_F_DEREGISTER) {
+		lp_gp_assign.nLogicalPortId = ep;
+		gpid_base = get_dp_port_info(inst, ep)->gpid_base;
+		gpid_num = get_dp_port_info(inst, ep)->gpid_num;
+
+		if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_gpid_ops
+				 .LpidToGpid_AssignmentSet,
+				 gsw_handle,
+				 &lp_gp_assign) != 0) {
+			PR_ERR("Fail to assign Lpid->Gpid table %d in GSWIP\n",
+			       ep);
+			return DP_FAILURE;
+		}
+
+		gp_lp_assign.nLogicalPortId = ep;
+		gp_lp_assign.nGlobalPortId = gpid_base;
+
+		if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_gpid_ops
+				 .GpidToLpid_AssignmentSet,
+				 gsw_handle,
+				 &gp_lp_assign) != 0) {
+			PR_ERR("Fail to assign GPID->LPID table %d in GSWIP\n",
+			       ep);
+			return DP_FAILURE;
+		}
+		free_gpid(inst, gpid_base, gpid_num);
+		return DP_FAILURE;
+	}
+	assign = get_ctp_assign(get_dp_port_info(inst, ep)->alloc_flags);
+	if (!assign) {
+		PR_ERR("assign NULL:ep=%d, alloc_f=0x%x\n",
+		       ep, get_dp_port_info(inst, ep)->alloc_flags);
+		return DP_FAILURE;
+	}
+	gpid_num = assign->max_gpid;
+	gpid_base = alloc_gpid(inst, DP_DYN_GPID, gpid_num, ep);
+	if (gpid_base == DP_FAILURE) {
+		PR_ERR("Allocate %d GPID failed for Ep %d\n", gpid_num, ep);
+		return DP_FAILURE;
+	}
+	gpid_spl = alloc_gpid(inst, DP_SPL_GPID, 1, ep);
+	if (gpid_spl == DP_FAILURE) {
+		DP_ERR("Fail to alloc special GPID for dpid=%d\n", ep);
+		return DP_FAILURE;
+	}
+	DP_DEBUG(DP_DBG_FLAG_DBG, "Lpid->Gpid table ep=%d \n", ep);
+	PR_INFO("Alloc GPID=%d num=%d spl_GPID=%d\n", gpid_base, gpid_num,
+		gpid_spl);
+	lp_gp_assign.nLogicalPortId = ep;
+	lp_gp_assign.nFirstGlobalPortId = gpid_base;
+	lp_gp_assign.nNumberOfGlobalPort = gpid_num;
+
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_gpid_ops
+			 .LpidToGpid_AssignmentSet,
+			 gsw_handle,
+			 &lp_gp_assign) != 0) {
+#if 0
+		PR_ERR("Fail to assign Lpid->Gpid table %d in GSWIP\n",
+		       ep);
+#endif
+		return DP_FAILURE;
+	}
+
+	DP_DEBUG(DP_DBG_FLAG_DBG, "Gpid->Lpid table ep=%d \n", ep);
+
+	gp_lp_assign.nLogicalPortId = ep;
+	gp_lp_assign.nGlobalPortId = gpid_base;
+
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_gpid_ops
+			 .GpidToLpid_AssignmentSet,
+			 gsw_handle,
+			 &gp_lp_assign) != 0) {
+		PR_ERR("Fail to assign Gpid->Lpid table %d in GSWIP\n",
+		       ep);
+#if 0
+		return DP_FAILURE;
+#endif
+	}
+
+	/* Store for later use */
+	cbm_gpid.cbm_inst = dp_port_prop[inst].cbm_inst;
+	cbm_gpid.lpid = ep;
+	cbm_gpid.gpid = gpid_base;
+	if (!gpid_num)
+		gpid_num = 1;
+	for (i = 0; i < gpid_num; i++) {
+		cbm_gpid.gpid = gpid_base + i;
+		if (cbm_gpid_lpid_map(&cbm_gpid)) {
+			DP_ERR("Fail to set CBM GPID(%d)<->LPID(%d)\n",
+			       cbm_gpid.gpid, cbm_gpid.lpid);
+#if 0
+			return DP_FAILURE;
+#endif
+		}
+		DP_INFO("cbm_gpid:lpid=%d gpid=%d\n", cbm_gpid.lpid,
+			cbm_gpid.gpid);
+	}
+	get_dp_port_info(inst, ep)->gpid_base = gpid_base;
+	get_dp_port_info(inst, ep)->gpid_num = gpid_num;
+	get_dp_port_info(inst, ep)->gpid_spl = gpid_spl;
+	if (gpid_spl >= 0)
+		priv->gp_dp_map[gpid_spl].dpid = ep;
+	for (i = 0; i < gpid_num; i++) {
+		priv->gp_dp_map[gpid_base + i].dpid = ep;
+	}
+	priv->gp_dp_map[gpid_spl].dpid = ep;
+
+	/* alloc/configure speical GPID for this dp port */
+	PR_INFO("Try to add gpid_spl=%d\n", get_dp_port_info(inst, ep)->gpid_spl);
+	if (dp_add_pp_gpid(inst, ep, 0, get_dp_port_info(inst, ep)->gpid_spl, 1)) {
+		DP_ERR("dp_add_pp_gpid for dport/gpid=%d/%d\n", ep,
+		       get_dp_port_info(inst, ep)->gpid_spl);
+		return DP_FAILURE;
+	}
+	return DP_SUCCESS;
+}
+
+
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_gswip_simulate.c b/drivers/net/datapath/dpm/gswip32/datapath_gswip_simulate.c
new file mode 100644
index 000000000000..2195b295426a
--- /dev/null
+++ b/drivers/net/datapath/dpm/gswip32/datapath_gswip_simulate.c
@@ -0,0 +1,983 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/etherdevice.h>
+#include <net/datapath_api.h>
+#include "../datapath.h"
+#include "datapath_misc.h"
+
+#define MAX_CPT_PORT 288
+u8 ctp_assign_f[MAX_CPT_PORT] = {0};
+GSW_CTP_portAssignment_t ctp_assign[MAX_CPT_PORT] = {0};
+
+#define MAX_PMAPPER 2336
+u8 pmapper_f[MAX_PMAPPER] = {0};
+
+u32  Pmapper_Alloc(int size)
+{
+	u32 i, j;
+
+	for (i = 1; i < MAX_PMAPPER; i++) {
+		if (pmapper_f[i])
+			continue;
+		for (j = 0; j < size; j++)
+			if (pmapper_f[i + j])
+				continue;
+
+		for (j = 0; j < size; j++)
+			pmapper_f[i] = 1;
+		return i;
+	}
+	return -1;
+}
+
+int Pmapper_Free(u32 offset, int size)
+{
+	int i;
+
+	for (i = 0; i < size; i++)
+		pmapper_f[i + offset] = 0;
+	return -1;
+}
+
+int CTP_PortAssignmentAlloc(GSW_CTP_portAssignment_t *param)
+{
+	int i, j;
+
+	for (i = 1; i < MAX_CPT_PORT; i++) {
+		if (ctp_assign_f[i])
+			continue;
+		for (j = 0; j < param->nNumberOfCtpPort; j++)
+			if (ctp_assign_f[i + j])
+				continue;
+		param->nFirstCtpPortId = i;
+		for (j = 0; j < param->nNumberOfCtpPort; j++) {
+			ctp_assign_f[i + j] = 1;
+			ctp_assign[i + j].eMode = param->eMode;
+			ctp_assign[i + j].nBridgePortId =
+				param->nBridgePortId;
+			ctp_assign[i + j].nFirstCtpPortId =
+				param->nFirstCtpPortId;
+			ctp_assign[i + j].nLogicalPortId =
+				param->nLogicalPortId;
+			ctp_assign[i + j].nNumberOfCtpPort =
+				param->nNumberOfCtpPort;
+		}
+		return 0;
+	}
+	return -1;
+}
+
+int CTP_PortAssignmentFree(GSW_CTP_portAssignment_t *param)
+{
+	int i;
+
+	for (i = 0; i < param->nNumberOfCtpPort; i++)
+		ctp_assign_f[i + param->nNumberOfCtpPort] = 0;
+	return 0;
+}
+
+int CTP_PortAssignmentSet(GSW_CTP_portAssignment_t *param)
+{
+	u32 idx = param->nFirstCtpPortId;
+	int i;
+
+	for (i = 0; i < param->nNumberOfCtpPort; i++) {
+		ctp_assign[idx + i].eMode = param->eMode;
+		ctp_assign[idx + i].nBridgePortId = param->nBridgePortId;
+		ctp_assign[idx + i].nFirstCtpPortId = param->nFirstCtpPortId;
+		ctp_assign[idx + i].nLogicalPortId = param->nLogicalPortId;
+		ctp_assign[idx + i].nNumberOfCtpPort = param->nNumberOfCtpPort;
+	}
+	return 0;
+}
+
+int CTP_PortAssignmentGet(GSW_CTP_portAssignment_t *param)
+{
+	int i;
+
+	for (i = 0; i < MAX_CPT_PORT; i++) {
+		if (!ctp_assign_f[i])
+			continue;
+		if (ctp_assign[i].nLogicalPortId != param->nLogicalPortId)
+			continue;
+
+		param->eMode = ctp_assign[i].eMode;
+		param->nBridgePortId = ctp_assign[i].nBridgePortId;
+		param->nFirstCtpPortId = ctp_assign[i].nFirstCtpPortId;
+		param->nNumberOfCtpPort = ctp_assign[i].nNumberOfCtpPort;
+		return 0;
+	}
+	return -1;
+}
+
+GSW_CTP_portConfig_t ctp_port_cfg[MAX_CPT_PORT] = {0};
+int CtpPortConfigSet(GSW_CTP_portConfig_t *param)
+{
+	GSW_CTP_portAssignment_t ctp_get;
+	int ret;
+	u32 ctp_port;
+
+	ctp_get.nLogicalPortId = param->nLogicalPortId;
+	ret = CTP_PortAssignmentGet(&ctp_get);
+	if (ret) {
+		PR_ERR("CTP_PortAssignmentGet fail for bp=%d\n",
+		       ctp_get.nLogicalPortId);
+		return GSW_statusErr;
+	}
+	ctp_port = ctp_get.nFirstCtpPortId  + param->nSubIfIdGroup;
+	if ((ctp_port >= MAX_CPT_PORT) ||
+	    (ctp_port < ctp_get.nFirstCtpPortId)) {
+		PR_ERR("CtpPortConfigSet wrong ctp_port %d (%d ~ %d)\n",
+		       ctp_port, ctp_get.nFirstCtpPortId, MAX_CPT_PORT);
+		return GSW_statusErr;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_BRIDGE_PORT_ID)
+		ctp_port_cfg[ctp_port].nBridgePortId = param->nBridgePortId;
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_FORCE_TRAFFIC_CLASS) {
+		ctp_port_cfg[ctp_port].nDefaultTrafficClass =
+			param->nDefaultTrafficClass;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_INGRESS_VLAN) {
+		ctp_port_cfg[ctp_port].bIngressExtendedVlanEnable =
+			param->bIngressExtendedVlanEnable;
+		ctp_port_cfg[ctp_port].nIngressExtendedVlanBlockId =
+			param->nIngressExtendedVlanBlockId;
+	}
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_INGRESS_VLAN_IGMP) {
+		ctp_port_cfg[ctp_port].bIngressExtendedVlanIgmpEnable = param->
+			bIngressExtendedVlanIgmpEnable;
+		ctp_port_cfg[ctp_port].nIngressExtendedVlanBlockIdIgmp = param->
+			nIngressExtendedVlanBlockIdIgmp;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_EGRESS_VLAN) {
+		ctp_port_cfg[ctp_port].bEgressExtendedVlanEnable = param->
+			bEgressExtendedVlanEnable;
+		ctp_port_cfg[ctp_port].nEgressExtendedVlanBlockId = param->
+			nEgressExtendedVlanBlockId;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_EGRESS_VLAN_IGMP) {
+		ctp_port_cfg[ctp_port].bEgressExtendedVlanIgmpEnable = param->
+			bEgressExtendedVlanIgmpEnable;
+		ctp_port_cfg[ctp_port].nEgressExtendedVlanBlockIdIgmp = param->
+			nEgressExtendedVlanBlockIdIgmp;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_INRESS_NTO1_VLAN)
+		ctp_port_cfg[ctp_port].bIngressNto1VlanEnable = param->
+			bIngressNto1VlanEnable;
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_EGRESS_NTO1_VLAN)
+		ctp_port_cfg[ctp_port].eMask = param->eMask;
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_INGRESS_METER) {
+		ctp_port_cfg[ctp_port].bIngressMeteringEnable = param->
+			bIngressMeteringEnable;
+		ctp_port_cfg[ctp_port].nIngressTrafficMeterId = param->
+			nIngressTrafficMeterId;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_EGRESS_METER) {
+		ctp_port_cfg[ctp_port].bEgressMeteringEnable =
+			param->bEgressMeteringEnable;
+		ctp_port_cfg[ctp_port].nEgressTrafficMeterId =
+			param->nEgressTrafficMeterId;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_BRIDGING_BYPASS) {
+		ctp_port_cfg[ctp_port].bBridgingBypass =
+			param->bBridgingBypass;
+		ctp_port_cfg[ctp_port].nDestLogicalPortId =
+			param->nDestLogicalPortId;
+		ctp_port_cfg[ctp_port].ePmapperMappingMode =
+			param->ePmapperMappingMode;
+		ctp_port_cfg[ctp_port].nDestSubIfIdGroup =
+			param->nDestSubIfIdGroup;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_INGRESS_MARKING) {
+		ctp_port_cfg[ctp_port].eIngressMarkingMode =
+			param->eIngressMarkingMode;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_EGRESS_MARKING) {
+		ctp_port_cfg[ctp_port].eEgressMarkingMode =
+			param->eEgressMarkingMode;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_EGRESS_REMARKING) {
+		ctp_port_cfg[ctp_port].eEgressRemarkingMode =
+			param->eEgressRemarkingMode;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_EGRESS_MARKING_OVERRIDE) {
+		ctp_port_cfg[ctp_port].bEgressMarkingOverrideEnable = param->
+			bEgressMarkingOverrideEnable;
+		ctp_port_cfg[ctp_port].eEgressMarkingModeOverride = param->
+			eEgressMarkingModeOverride;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_FLOW_ENTRY) {
+		ctp_port_cfg[ctp_port].nFirstFlowEntryIndex =
+			param->nFirstFlowEntryIndex;
+		ctp_port_cfg[ctp_port].nNumberOfFlowEntries =
+			param->nNumberOfFlowEntries;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_LOOPBACK_AND_MIRROR) {
+		ctp_port_cfg[ctp_port].bIngressLoopbackEnable = param->
+			bIngressLoopbackEnable;
+		ctp_port_cfg[ctp_port].bEgressLoopbackEnable =
+			param->bEgressLoopbackEnable;
+		ctp_port_cfg[ctp_port].bIngressMirrorEnable =
+			param->bIngressMirrorEnable;
+		ctp_port_cfg[ctp_port].bEgressMirrorEnable =
+			param->bEgressMirrorEnable;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_LOOPBACK_AND_MIRROR) {
+		ctp_port_cfg[ctp_port].bIngressDaSaSwapEnable = param->
+			bIngressDaSaSwapEnable;
+		ctp_port_cfg[ctp_port].bEgressDaSaSwapEnable =
+			param->bEgressDaSaSwapEnable;
+	}
+
+	return 0;
+}
+
+int CtpPortConfigGet(GSW_CTP_portConfig_t *param)
+{
+	GSW_CTP_portAssignment_t ctp_get;
+	int ret;
+	u32 ctp_port;
+
+	ctp_get.nLogicalPortId = param->nLogicalPortId;
+	ret = CTP_PortAssignmentGet(&ctp_get);
+	if (ret) {
+		PR_ERR("CtpPortConfigGet returns ERROR\n");
+		return GSW_statusErr;
+	}
+	ctp_port = ctp_get.nFirstCtpPortId  + param->nSubIfIdGroup;
+	if ((ctp_port >= MAX_CPT_PORT) ||
+	    (ctp_port < ctp_get.nFirstCtpPortId)) {
+		PR_ERR("CtpPortConfigGet wrong ctp_port %d (%d ~ %d)\n",
+		       ctp_port, ctp_get.nFirstCtpPortId, MAX_CPT_PORT);
+		return GSW_statusErr;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_BRIDGE_PORT_ID)
+		param->nBridgePortId = ctp_port_cfg[ctp_port].nBridgePortId;
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_FORCE_TRAFFIC_CLASS) {
+		param->nDefaultTrafficClass =
+			ctp_port_cfg[ctp_port].nDefaultTrafficClass;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_INGRESS_VLAN) {
+		param->bIngressExtendedVlanEnable = ctp_port_cfg[ctp_port].
+			bIngressExtendedVlanEnable;
+		param->nIngressExtendedVlanBlockId = ctp_port_cfg[ctp_port].
+			nIngressExtendedVlanBlockId;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_INGRESS_VLAN_IGMP) {
+		param->bIngressExtendedVlanIgmpEnable = ctp_port_cfg[ctp_port].
+			bIngressExtendedVlanIgmpEnable;
+		param->nIngressExtendedVlanBlockIdIgmp = ctp_port_cfg[ctp_port].
+			nIngressExtendedVlanBlockIdIgmp;
+		/*param->nIngressExtendedVlanBlockSizeIgmp =
+		 *ctp_port_cfg[ctp_port].nIngressExtendedVlanBlockSizeIgmp;
+		 */
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_EGRESS_VLAN) {
+		param->bEgressExtendedVlanEnable = ctp_port_cfg[ctp_port].
+			bEgressExtendedVlanEnable;
+		param->nEgressExtendedVlanBlockId = ctp_port_cfg[ctp_port].
+			nEgressExtendedVlanBlockId;
+		/*param->nEgressExtendedVlanBlockSize = ctp_port_cfg[ctp_port].
+		 *nEgressExtendedVlanBlockSize;
+		 */
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_EGRESS_VLAN_IGMP) {
+		param->bEgressExtendedVlanIgmpEnable = ctp_port_cfg[ctp_port].
+			bEgressExtendedVlanIgmpEnable;
+		param->nEgressExtendedVlanBlockIdIgmp = ctp_port_cfg[ctp_port].
+			nEgressExtendedVlanBlockIdIgmp;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_INRESS_NTO1_VLAN)
+		param->bIngressNto1VlanEnable = ctp_port_cfg[ctp_port].
+			bIngressNto1VlanEnable;
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_EGRESS_NTO1_VLAN)
+		param->eMask = ctp_port_cfg[ctp_port].eMask;
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_INGRESS_METER) {
+		param->bIngressMeteringEnable = ctp_port_cfg[ctp_port].
+			bIngressMeteringEnable;
+		param->nIngressTrafficMeterId = ctp_port_cfg[ctp_port].
+			nIngressTrafficMeterId;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_EGRESS_METER) {
+		param->bEgressMeteringEnable =
+			ctp_port_cfg[ctp_port].bEgressMeteringEnable;
+		param->nEgressTrafficMeterId =
+			ctp_port_cfg[ctp_port].nEgressTrafficMeterId;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_BRIDGING_BYPASS) {
+		param->bBridgingBypass =
+			ctp_port_cfg[ctp_port].bBridgingBypass;
+		param->nDestLogicalPortId =
+			ctp_port_cfg[ctp_port].nDestLogicalPortId;
+		param->ePmapperMappingMode =
+			ctp_port_cfg[ctp_port].ePmapperMappingMode;
+		param->nDestSubIfIdGroup =
+			ctp_port_cfg[ctp_port].nDestSubIfIdGroup;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_INGRESS_MARKING) {
+		param->eIngressMarkingMode =
+			ctp_port_cfg[ctp_port].eIngressMarkingMode;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_EGRESS_MARKING) {
+		param->eEgressMarkingMode =
+			ctp_port_cfg[ctp_port].eEgressMarkingMode;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_EGRESS_REMARKING) {
+		param->eEgressRemarkingMode =
+			ctp_port_cfg[ctp_port].eEgressRemarkingMode;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_EGRESS_MARKING_OVERRIDE) {
+		param->bEgressMarkingOverrideEnable = ctp_port_cfg[ctp_port].
+			bEgressMarkingOverrideEnable;
+		param->eEgressMarkingModeOverride = ctp_port_cfg[ctp_port].
+			eEgressMarkingModeOverride;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_FLOW_ENTRY) {
+		param->nFirstFlowEntryIndex =
+			ctp_port_cfg[ctp_port].nFirstFlowEntryIndex;
+		param->nNumberOfFlowEntries =
+			ctp_port_cfg[ctp_port].nNumberOfFlowEntries;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_LOOPBACK_AND_MIRROR) {
+		param->bIngressLoopbackEnable = ctp_port_cfg[ctp_port].
+			bIngressLoopbackEnable;
+		param->bEgressLoopbackEnable = ctp_port_cfg[ctp_port].
+			bEgressLoopbackEnable;
+		param->bIngressMirrorEnable = ctp_port_cfg[ctp_port].
+			bIngressMirrorEnable;
+		param->bEgressMirrorEnable = ctp_port_cfg[ctp_port].
+			bEgressMirrorEnable;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_LOOPBACK_AND_MIRROR) {
+		param->bIngressDaSaSwapEnable = ctp_port_cfg[ctp_port].
+			bIngressDaSaSwapEnable;
+		param->bEgressDaSaSwapEnable = ctp_port_cfg[ctp_port].
+			bEgressDaSaSwapEnable;
+	}
+	return 0;
+}
+
+#define MAX_BRIDGE_PORT 120
+u8 bridge_port_f[MAX_BRIDGE_PORT];
+GSW_BRIDGE_portConfig_t bridge_port[MAX_BRIDGE_PORT];
+
+#define MAX_BRIDGE	64
+u8 bridge_f[MAX_BRIDGE_PORT];
+GSW_BRIDGE_config_t bridge[MAX_BRIDGE];
+
+int BridgePortAlloc(GSW_BRIDGE_portConfig_t *param)
+{
+	int i;
+
+	for (i = 1; i < MAX_BRIDGE_PORT; i++) {
+		if (bridge_port_f[i])
+			continue;
+		bridge_port_f[i] = 1;
+		param->nBridgePortId = i;
+		return 0;
+	}
+
+	return -1;
+}
+
+int BridgePortFree(GSW_BRIDGE_portConfig_t *param)
+{
+	bridge_port_f[param->nBridgePortId] = 0;
+	return 0;
+}
+
+int BridgeAlloc(GSW_BRIDGE_alloc_t *param)
+{
+	int i;
+
+	for (i = 1; i < MAX_BRIDGE_PORT; i++) {
+		if (bridge_f[i])
+			continue;
+		bridge_f[i] = 1;
+		param->nBridgeId = i;
+		return 0;
+	}
+
+	return -1;
+}
+
+int BridgeFree(GSW_BRIDGE_alloc_t *param)
+{
+	bridge_f[param->nBridgeId] = 0;
+	return 0;
+}
+
+int BridgePortConfigSet(GSW_BRIDGE_portConfig_t *param)
+{
+	int i = param->nBridgePortId;
+	//int k;
+	bridge_port[i].nBridgePortId = param->nBridgePortId;
+
+	/*If Bridge Port ID is invalid ,find a free Bridge
+	 *port configuration table
+	 *index and allocate
+	 *New Bridge Port configuration table index
+	 */
+	if (param->nBridgePortId >= MAX_BRIDGE_PORT) {
+		PR_ERR("wrong bridge port %d\n", param->nBridgePortId);
+		return GSW_statusErr;
+	}
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID)
+		bridge_port[i].nBridgeId = param->nBridgeId;
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_INGRESS_VLAN) {
+		bridge_port[i].bIngressExtendedVlanEnable = param->
+			bIngressExtendedVlanEnable;
+		if (param->bIngressExtendedVlanEnable)
+			bridge_port[i].nIngressExtendedVlanBlockId = param->
+				nIngressExtendedVlanBlockId;
+		/*bridge_port[i].nIngressExtendedVlanBlockSize = param->
+		 *nIngressExtendedVlanBlockSize;
+		 */
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_VLAN) {
+		bridge_port[i].bEgressExtendedVlanEnable = param->
+			bEgressExtendedVlanEnable;
+		bridge_port[i].nEgressExtendedVlanBlockId = param->
+			nEgressExtendedVlanBlockId;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_INGRESS_MARKING)
+		bridge_port[i].eIngressMarkingMode = param->eIngressMarkingMode;
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_REMARKING) {
+		bridge_port[i].eEgressRemarkingMode = param->
+			eEgressRemarkingMode;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_INGRESS_METER) {
+		bridge_port[i].bIngressMeteringEnable = param->
+			bIngressMeteringEnable;
+		bridge_port[i].nIngressTrafficMeterId = param->
+			nIngressTrafficMeterId;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_SUB_METER) {
+		memcpy(bridge_port[i].bEgressSubMeteringEnable, param->
+		       bEgressSubMeteringEnable,
+		       sizeof(bridge_port[i].bEgressSubMeteringEnable));
+		memcpy(bridge_port[i].nEgressTrafficSubMeterId, param->
+		       nEgressTrafficSubMeterId,
+		       sizeof(bridge_port[i].nEgressTrafficSubMeterId));
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_CTP_MAPPING) {
+#define PMAPPER_BLOCK_SIZE 36
+		bridge_port[i].bPmapperEnable = param->bPmapperEnable;
+		bridge_port[i].nDestLogicalPortId = param->nDestLogicalPortId;
+		bridge_port[i].ePmapperMappingMode = param->ePmapperMappingMode;
+		if (bridge_port[i].bPmapperEnable) {
+			if (!bridge_port[i].sPmapper.nPmapperId) {
+				bridge_port[i].sPmapper.nPmapperId =
+					Pmapper_Alloc(PMAPPER_BLOCK_SIZE);
+				param->sPmapper.nPmapperId =
+					bridge_port[i].sPmapper.nPmapperId;
+			}
+		} else if (bridge_port[i].sPmapper.nPmapperId)
+			Pmapper_Free(bridge_port[i].sPmapper.nPmapperId,
+				     PMAPPER_BLOCK_SIZE);
+	}
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP)
+		memcpy(bridge_port[i].nBridgePortMap, param->nBridgePortMap,
+		       sizeof(bridge_port[i].nBridgePortMap));
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_MC_DEST_IP_LOOKUP) {
+		bridge_port[i].bMcDestIpLookupDisable = param->
+			bMcDestIpLookupDisable;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_MC_SRC_IP_LOOKUP) {
+		bridge_port[i].bMcSrcIpLookupEnable = param->
+			bMcSrcIpLookupEnable;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_MC_DEST_MAC_LOOKUP) {
+		bridge_port[i].bDestMacLookupDisable = param->
+			bDestMacLookupDisable;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_MC_SRC_MAC_LEARNING) {
+		bridge_port[i].bSrcMacLearningDisable = param->
+			bSrcMacLearningDisable;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_MAC_SPOOFING) {
+		bridge_port[i].bMacSpoofingDetectEnable = param->
+			bMacSpoofingDetectEnable;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_PORT_LOCK)
+		bridge_port[i].bPortLockEnable = param->bPortLockEnable;
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_INGRESS_VLAN_FILTER) {
+		bridge_port[i].bBypassEgressVlanFilter1 = param->
+			bBypassEgressVlanFilter1;
+		bridge_port[i].bIngressVlanFilterEnable = param->
+			bIngressVlanFilterEnable;
+		bridge_port[i].nIngressVlanFilterBlockId = param->
+			nIngressVlanFilterBlockId;
+		/*bridge_port[i].nIngressVlanFilterBlockSize = param->
+		 *nIngressVlanFilterBlockSize;
+		 */
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_VLAN_FILTER1) {
+		bridge_port[i].bEgressVlanFilter1Enable = param->
+			bEgressVlanFilter1Enable;
+		bridge_port[i].nEgressVlanFilter1BlockId = param->
+			nEgressVlanFilter1BlockId;
+		/*bridge_port[i].nEgressVlanFilter1BlockSize = param->
+		 *nEgressVlanFilter1BlockSize;
+		 */
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_VLAN_FILTER2) {
+		bridge_port[i].bEgressVlanFilter2Enable = param->
+			bEgressVlanFilter2Enable;
+		bridge_port[i].nEgressVlanFilter2BlockId = param->
+			nEgressVlanFilter2BlockId;
+		/*bridge_port[i].nEgressVlanFilter2BlockSize = param->
+		 *nEgressVlanFilter2BlockSize;
+		 */
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_MAC_LEARNING_LIMIT) {
+		bridge_port[i].bMacLearningLimitEnable = param->
+			bMacLearningLimitEnable;
+		bridge_port[i].nMacLearningLimit = param->nMacLearningLimit;
+	}
+	return 0;
+}
+
+int BridgePortConfigGet(GSW_BRIDGE_portConfig_t *param)
+{
+	int i = param->nBridgePortId;
+	//int k;
+
+	if (param->nBridgePortId >= MAX_BRIDGE_PORT) {
+		PR_ERR("wrong bridge port %d\n", param->nBridgePortId);
+		return GSW_statusErr;
+	}
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID)
+		param->nBridgeId = bridge_port[i].nBridgeId;
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_INGRESS_VLAN) {
+		param->bIngressExtendedVlanEnable = bridge_port[i].
+			bIngressExtendedVlanEnable;
+
+		param->nIngressExtendedVlanBlockId = bridge_port[i].
+			nIngressExtendedVlanBlockId;
+		/*param->nIngressExtendedVlanBlockSize = bridge_port[i].
+		 *nIngressExtendedVlanBlockSize;
+		 */
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_VLAN) {
+		param->bEgressExtendedVlanEnable = bridge_port[i].
+			bEgressExtendedVlanEnable;
+		param->nEgressExtendedVlanBlockId = bridge_port[i].
+			nEgressExtendedVlanBlockId;
+		/*param->nEgressExtendedVlanBlockSize = bridge_port[i].
+		 *nEgressExtendedVlanBlockSize;
+		 */
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_INGRESS_MARKING)
+		param->eIngressMarkingMode = bridge_port[i].eIngressMarkingMode;
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_REMARKING) {
+		param->eEgressRemarkingMode = bridge_port[i].
+			eEgressRemarkingMode;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_INGRESS_METER) {
+		param->bIngressMeteringEnable = bridge_port[i].
+			bIngressMeteringEnable;
+		param->nIngressTrafficMeterId = bridge_port[i].
+			nIngressTrafficMeterId;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_SUB_METER) {
+		memcpy(param->bEgressSubMeteringEnable, bridge_port[i].
+		       bEgressSubMeteringEnable,
+		       sizeof(bridge_port[i].bEgressSubMeteringEnable));
+		memcpy(param->nEgressTrafficSubMeterId, bridge_port[i].
+		       nEgressTrafficSubMeterId,
+		       sizeof(bridge_port[i].nEgressTrafficSubMeterId));
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_CTP_MAPPING) {
+#define PMAPPER_BLOCK_SIZE 36
+		param->bPmapperEnable = bridge_port[i].bPmapperEnable;
+		param->nDestLogicalPortId = bridge_port[i].nDestLogicalPortId;
+		param->ePmapperMappingMode = bridge_port[i].ePmapperMappingMode;
+		memcpy(&param->sPmapper, &bridge_port[i].sPmapper,
+		       sizeof(param->sPmapper));
+	}
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP) {
+		memcpy(param->nBridgePortMap, bridge_port[i].nBridgePortMap,
+		       sizeof(bridge_port[i].nBridgePortMap));
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_MC_DEST_IP_LOOKUP) {
+		param->bMcDestIpLookupDisable = bridge_port[i].
+			bMcDestIpLookupDisable;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_MC_SRC_IP_LOOKUP) {
+		param->bMcSrcIpLookupEnable = bridge_port[i].
+			bMcSrcIpLookupEnable;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_MC_DEST_MAC_LOOKUP) {
+		param->bDestMacLookupDisable = bridge_port[i].
+			bDestMacLookupDisable;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_MC_SRC_MAC_LEARNING) {
+		param->bSrcMacLearningDisable = bridge_port[i].
+			bSrcMacLearningDisable;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_MAC_SPOOFING) {
+		param->bMacSpoofingDetectEnable = bridge_port[i].
+			bMacSpoofingDetectEnable;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_PORT_LOCK)
+		param->bPortLockEnable = bridge_port[i].bPortLockEnable;
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_INGRESS_VLAN_FILTER) {
+		param->bBypassEgressVlanFilter1 = bridge_port[i].
+			bBypassEgressVlanFilter1;
+		param->bIngressVlanFilterEnable = bridge_port[i].
+			bIngressVlanFilterEnable;
+		param->nIngressVlanFilterBlockId = bridge_port[i].
+			nIngressVlanFilterBlockId;
+		/*param->nIngressVlanFilterBlockSize = bridge_port[i].
+		 *nIngressVlanFilterBlockSize;
+		 */
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_VLAN_FILTER1) {
+		param->bEgressVlanFilter1Enable = bridge_port[i].
+			bEgressVlanFilter1Enable;
+		param->nEgressVlanFilter1BlockId = bridge_port[i].
+			nEgressVlanFilter1BlockId;
+		/*param->nEgressVlanFilter1BlockSize = bridge_port[i].
+		 *nEgressVlanFilter1BlockSize;
+		 */
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_VLAN_FILTER2) {
+		param->bEgressVlanFilter2Enable = bridge_port[i].
+			bEgressVlanFilter2Enable;
+		param->nEgressVlanFilter2BlockId = bridge_port[i].
+			nEgressVlanFilter2BlockId;
+		/*param->nEgressVlanFilter2BlockSize = bridge_port[i].
+		 *nEgressVlanFilter2BlockSize;
+		 */
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_MAC_LEARNING_LIMIT) {
+		param->bMacLearningLimitEnable = bridge_port[i].
+			bMacLearningLimitEnable;
+		param->nMacLearningLimit = bridge_port[i].nMacLearningLimit;
+	}
+	return 0;
+}
+
+int BridgeConfigSet(GSW_BRIDGE_config_t *param)
+{
+	int i = param->nBridgeId;
+
+	if (param->eMask & GSW_BRIDGE_CONFIG_MASK_MAC_LEARNING_LIMIT) {
+		bridge[i].bMacLearningLimitEnable = param->
+			bMacLearningLimitEnable;
+	}
+
+	if (param->eMask & GSW_BRIDGE_CONFIG_MASK_SUB_METER) {
+		memcpy(bridge[i].bSubMeteringEnable, param->bSubMeteringEnable,
+		       sizeof(bridge[i].bSubMeteringEnable));
+		memcpy(bridge[i].nTrafficSubMeterId, param->nTrafficSubMeterId,
+		       sizeof(bridge[i].nTrafficSubMeterId));
+	}
+
+	if (param->eMask & GSW_BRIDGE_CONFIG_MASK_FORWARDING_MODE) {
+		bridge[i].eForwardBroadcast = param->eForwardBroadcast;
+		bridge[i].eForwardUnknownUnicast = param->
+			eForwardUnknownUnicast;
+		bridge[i].eForwardUnknownMulticastNonIp = param->
+			eForwardUnknownMulticastNonIp;
+	}
+	return 0;
+}
+
+int BridgeConfigGet(GSW_BRIDGE_config_t *param)
+{
+	int i = param->nBridgeId;
+
+	if (param->eMask & GSW_BRIDGE_CONFIG_MASK_MAC_LEARNING_LIMIT) {
+		param->bMacLearningLimitEnable = bridge[i].
+			bMacLearningLimitEnable;
+	}
+
+	if (param->eMask & GSW_BRIDGE_CONFIG_MASK_SUB_METER) {
+		memcpy(param->bSubMeteringEnable, bridge[i].bSubMeteringEnable,
+		       sizeof(bridge[i].bSubMeteringEnable));
+		memcpy(param->nTrafficSubMeterId, bridge[i].nTrafficSubMeterId,
+		       sizeof(bridge[i].nTrafficSubMeterId));
+	}
+
+	if (param->eMask & GSW_BRIDGE_CONFIG_MASK_FORWARDING_MODE) {
+		param->eForwardBroadcast = bridge[i].eForwardBroadcast;
+		param->eForwardUnknownUnicast = bridge[i].
+			eForwardUnknownUnicast;
+		param->eForwardUnknownMulticastNonIp = bridge[i].
+			eForwardUnknownMulticastNonIp;
+	}
+	return 0;
+}
+
+#define MAX_EXTENDVLAN  512
+u8 extvlan_f[MAX_EXTENDVLAN] = {0};
+GSW_EXTENDEDVLAN_alloc_t extvlan_alloc[MAX_EXTENDVLAN] = {0};
+GSW_EXTENDEDVLAN_config_t extvlan[MAX_EXTENDVLAN] = {0};
+GSW_return_t ExtendedVlanAlloc(GSW_EXTENDEDVLAN_alloc_t *param)
+{
+	int i, j;
+
+	for (i = 1; i < MAX_EXTENDVLAN; i++) {
+		if (extvlan_f[i])
+			continue;
+		for (j = 0; j < param->nNumberOfEntries; j++)
+			if (extvlan_f[i + j])
+				continue;
+		param->nExtendedVlanBlockId = i;
+		for (j = 0; j < param->nNumberOfEntries; j++) {
+			extvlan_f[i + j] = 1;
+			extvlan_alloc[i + j].nExtendedVlanBlockId = i;
+			extvlan_alloc[i + j].nNumberOfEntries =  param->
+				nNumberOfEntries;
+		}
+		return 0;
+	}
+	return -1;
+}
+
+int ExtendedVlanFree(GSW_EXTENDEDVLAN_alloc_t *param)
+{
+	int i;
+
+	for (i = 0; i < extvlan_alloc[param->nExtendedVlanBlockId].
+	     nNumberOfEntries ; i++)
+		extvlan_f[i + param->nExtendedVlanBlockId] = 0;
+	return 0;
+}
+
+GSW_return_t ExtendedVlanSet(GSW_EXTENDEDVLAN_config_t *param)
+{ /*Note: not support pmapper here */
+	u32 idx = param->nExtendedVlanBlockId + param->nEntryIndex;
+
+	if (idx >= MAX_EXTENDVLAN) {
+		PR_ERR("ERROR : idx %d >= %d\n", idx, MAX_EXTENDVLAN);
+		return -1;
+	}
+
+	memcpy(&extvlan[idx].sFilter, &param->sFilter,
+	       sizeof(extvlan[idx].sFilter));
+	memcpy(&extvlan[idx].sTreatment, &param->sTreatment,
+	       sizeof(extvlan[idx].sTreatment));
+
+	return 0;
+}
+
+GSW_return_t ExtendedVlanGet(GSW_EXTENDEDVLAN_config_t *param)
+{ /*Note: not support pmapper here */
+	u32 idx = param->nExtendedVlanBlockId + param->nEntryIndex;
+
+	if (idx >= MAX_EXTENDVLAN) {
+		PR_ERR("ERROR: idx %d >= %d\n", idx, MAX_EXTENDVLAN);
+		return -1;
+	}
+	memcpy(&param->sFilter, &extvlan[idx].sFilter,
+	       sizeof(extvlan[idx].sFilter));
+	memcpy(&param->sTreatment, &extvlan[idx].sTreatment,
+	       sizeof(extvlan[idx].sTreatment));
+
+	return 0;
+}
+
+//TODO Yet to test below simulation code completely
+//#define MAC_MAX_ENTRY 4096
+#define MAC_MAX_ENTRY 10
+u8 mac_f[MAC_MAX_ENTRY];
+GSW_MAC_tableAdd_t MacAdd[MAC_MAX_ENTRY];
+
+int MacTableAdd(GSW_MAC_tableAdd_t *param)
+{
+	int i;
+
+	for (i = 0; i < MAC_MAX_ENTRY; i++) {
+		if (mac_f[i]) {
+			continue;
+		} else {
+			PR_ERR("MAC add i(%d) value\n", i);
+			mac_f[i] = 1;
+			break;
+		}
+	}
+
+	if (param->nPortId >= MAX_BRIDGE_PORT) {
+		PR_ERR("wrong bridge port for MAC add%d\n", param->nPortId);
+		return GSW_statusErr;
+	}
+
+	if (param->nPortMap) {
+		memcpy(MacAdd[i].nPortMap, param->nPortMap,
+		       sizeof(param->nPortMap));
+	}
+	MacAdd[i].nFId = param->nFId;
+	MacAdd[i].bStaticEntry = param->bStaticEntry;
+	MacAdd[i].nPortId = param->nPortId;
+	MacAdd[i].nSubIfId = param->nSubIfId;
+	memcpy(MacAdd[i].nMAC, param->nMAC, sizeof(param->nMAC));
+	PR_ERR("MAC add for entry:%d %02x:%02x:%02x:%02x:%02x:%02x\n", i,
+	       MacAdd[i].nMAC[0], MacAdd[i].nMAC[1], MacAdd[i].nMAC[2],
+	       MacAdd[i].nMAC[3], MacAdd[i].nMAC[4], MacAdd[i].nMAC[5]);
+	return 0;
+}
+
+int MacTableRemove(GSW_MAC_tableRemove_t *param)
+{
+	int i;
+
+	for (i = 0; i < MAC_MAX_ENTRY; i++) {
+		if (!mac_f[i])
+			continue;
+		if (MacAdd[i].nFId != param->nFId)
+			continue;
+		if (memcmp(MacAdd[i].nMAC, param->nMAC, sizeof(param->nMAC))) {
+			memset(&MacAdd[i], 0, sizeof(MacAdd[i]));
+			PR_ERR("MAC remove for entry:%d\n", i);
+		}
+		mac_f[i] = 0;
+		return 0;
+	}
+	return -1;
+}
+
+int MacTableQuery(GSW_MAC_tableQuery_t *param)
+{
+	int i;
+
+	for (i = 0; i < MAC_MAX_ENTRY; i++) {
+		if (!mac_f[i])
+			continue;
+		if (MacAdd[i].nFId != param->nFId)
+			continue;
+		if (memcmp(MacAdd[i].nMAC, param->nMAC, sizeof(param->nMAC))) {
+			param->nFId = MacAdd[i].nFId;
+			param->bFound = 1;
+			param->nPortId = MacAdd[i].nPortId;
+			param->nSubIfId = MacAdd[i].nSubIfId;
+			param->bStaticEntry = MacAdd[i].bStaticEntry;
+			memcpy(param->nPortMap,
+			       MacAdd[i].nPortMap,
+			       sizeof(MacAdd[i].nPortMap));
+			memcpy(param->nMAC, MacAdd[i].nMAC,
+			       sizeof(MacAdd[i].nMAC));
+
+		PR_ERR("%d    %d    %d	%02x:%02x:%02x:%02x:%02x:%02x\n",
+		       param->nFId, param->nPortId,
+		       param->bStaticEntry,
+		       param->nMAC[0], param->nMAC[1], param->nMAC[2],
+		       param->nMAC[3], param->nMAC[4], param->nMAC[5]);
+			return 0;
+		} else if (MacAdd[i].nPortId == param->nPortId) {
+			param->nFId = MacAdd[i].nFId;
+			param->bFound = 1;
+			param->nPortId = MacAdd[i].nPortId;
+			param->nSubIfId = MacAdd[i].nSubIfId;
+			memcpy(param->nPortMap,
+			       MacAdd[i].nPortMap,
+			       sizeof(MacAdd[i].nPortMap));
+			memcpy(param->nMAC, MacAdd[i].nMAC,
+			       sizeof(MacAdd[i].nMAC));
+			return 0;
+		}
+	}
+	PR_ERR("MAC Tbl query not match\n");
+	return -1;
+}
+
+int MacTableRead(GSW_MAC_tableRead_t *param)
+{
+	int i;
+
+	PR_ERR("MAC Tbl read\n");
+	for (i = 0; i < MAC_MAX_ENTRY; i++) {
+		if (!mac_f[i])
+			return -1;
+		param->nFId = MacAdd[i].nFId;
+		param->nPortId = MacAdd[i].nPortId;
+		param->nSubIfId = MacAdd[i].nSubIfId;
+		param->bStaticEntry = MacAdd[i].bStaticEntry;
+		memcpy(param->nPortMap,
+		       MacAdd[i].nPortMap,
+		       sizeof(MacAdd[i].nPortMap));
+		memcpy(param->nMAC, MacAdd[i].nMAC,
+		       sizeof(MacAdd[i].nMAC));
+	}
+	return 0;
+}
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_gswip_simulate.h b/drivers/net/datapath/dpm/gswip32/datapath_gswip_simulate.h
new file mode 100644
index 000000000000..a69fabc46301
--- /dev/null
+++ b/drivers/net/datapath/dpm/gswip32/datapath_gswip_simulate.h
@@ -0,0 +1,39 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#ifndef DATAPATH_GSWIP_SIMULATE_H_
+#define DATAPATH_GSWIP_SIMULATE_H_
+
+u32  Pmapper_Alloc(int size);
+int Pmapper_Free(u32 offset, int size);
+int CTP_PortAssignmentAlloc(GSW_CTP_portAssignment_t *param);
+int CTP_PortAssignmentFree(GSW_CTP_portAssignment_t *param);
+int CTP_PortAssignmentSet(GSW_CTP_portAssignment_t *param);
+int CTP_PortAssignmentGet(GSW_CTP_portAssignment_t *param);
+int CtpPortConfigSet(GSW_CTP_portConfig_t *param);
+int CtpPortConfigGet(GSW_CTP_portConfig_t *param);
+int BridgePortAlloc(GSW_BRIDGE_portConfig_t *param);
+int BridgePortFree(GSW_BRIDGE_portConfig_t *param);
+int BridgeAlloc(GSW_BRIDGE_alloc_t *param);
+int BridgeFree(GSW_BRIDGE_alloc_t *param);
+int BridgePortConfigSet(GSW_BRIDGE_portConfig_t *param);
+int BridgePortConfigGet(GSW_BRIDGE_portConfig_t *param);
+int BridgeConfigSet(GSW_BRIDGE_config_t *param);
+int BridgeConfigGet(GSW_BRIDGE_config_t *param);
+int MacTableAdd(GSW_MAC_tableAdd_t *param);
+int MacTableRemove(GSW_MAC_tableRemove_t *param);
+int MacTableQuery(GSW_MAC_tableQuery_t *param);
+int MacTableRead(GSW_MAC_tableAdd_t *param);
+
+GSW_return_t ExtendedVlanAlloc(GSW_EXTENDEDVLAN_alloc_t *param);
+int ExtendedVlanFree(GSW_EXTENDEDVLAN_alloc_t *param);
+GSW_return_t ExtendedVlanSet(GSW_EXTENDEDVLAN_config_t *param);
+GSW_return_t ExtendedVlanGet(GSW_EXTENDEDVLAN_config_t *param);
+#endif
+
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_lookup_proc.c b/drivers/net/datapath/dpm/gswip32/datapath_lookup_proc.c
new file mode 100644
index 000000000000..65d7e25e9fb9
--- /dev/null
+++ b/drivers/net/datapath/dpm/gswip32/datapath_lookup_proc.c
@@ -0,0 +1,738 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/version.h>
+#include <linux/if_ether.h>
+#include <linux/ethtool.h>
+#include <linux/proc_fs.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/clk.h>
+#include <linux/if_ether.h>
+#include <linux/if_vlan.h>
+
+#include <linux/clk.h>
+#include <linux/ip.h>
+#include <net/ip.h>
+#define DATAPATH_HAL_LAYER   /*must put before include datapath_api.h in
+			      *order to avoid include another platform's
+			      *DMA descriptor and pmac header files
+			      */
+#include <net/datapath_api.h>
+#include <net/datapath_api_gswip32.h>
+#include "../datapath.h"
+#include "datapath_proc.h"
+#include "datapath_ppv4.h"
+#include "datapath_misc.h"
+#include <net/datapath_proc_api.h>
+
+#define SEQ_PRINTF seq_printf
+
+#define proc_printf(s, fmt, arg...) \
+	do { \
+		if (!s) \
+			PR_INFO(fmt, ##arg); \
+		else \
+			seq_printf(s, fmt, ##arg); \
+	} while (0)
+
+#define CARE_FLAG      0
+#define CARE_NOT_FLAG  1
+
+#define LIST_ALL_CASES(t, mask, not_care)  \
+	for (t[0] = 0;  t[0] < ((mask[0] == not_care) ? 2 : 1); t[0]++) \
+	for (t[1] = 0;  t[1] < ((mask[1] == not_care) ? 2 : 1); t[1]++) \
+	for (t[2] = 0;  t[2] < ((mask[2] == not_care) ? 2 : 1); t[2]++) \
+	for (t[3] = 0;  t[3] < ((mask[3] == not_care) ? 2 : 1); t[3]++) \
+	for (t[4] = 0;  t[4] < ((mask[4] == not_care) ? 2 : 1); t[4]++) \
+	for (t[5] = 0;  t[5] < ((mask[5] == not_care) ? 2 : 1); t[5]++) \
+	for (t[6] = 0;  t[6] < ((mask[6] == not_care) ? 2 : 1); t[6]++) \
+	for (t[7] = 0;  t[7] < ((mask[7] == not_care) ? 2 : 1); t[7]++) \
+	for (t[8] = 0;  t[8] < 1; t[8]++) \
+	for (t[9] = 0;  t[9] < 1; t[9]++) \
+	for (t[10] = 0; t[10] < 1; t[10]++) \
+	for (t[11] = 0; t[11] < 1; t[11]++) \
+	for (t[12] = 0; t[12] < ((mask[12] == not_care) ? 2 : 1); t[12]++)
+
+/* The purpose of this file is to find the CBM lookup pattern and
+ * print it in the simple way.
+ * Otherway it may print up to 16K lines in the console to get lookup setting
+ *  Lookup table: flow[1] flow[0] dec end mpe2 mpe1 ep(4) class(4)
+ * Idea: We fixed the EP value during finding lookup setting pattern.
+ * method:
+ *     1st: to find the possible don't care bit from flow[2]/dec/enc/mpe2/mpe1
+ *	 and class(4), excluding ep, ie total 10 bits
+ *	       API: c_not_care_walkthrought
+ *		   Note: from big don't care bit number (ie, maximum don't
+ *		   care case) to 1 (minimal don't care case)
+ *
+ *	  2nd: generate tmp_index based on care bits
+ *	       API: list_care_combination
+ *
+ *    3rd: based on tmp_index, check whether there is really pattern which meet
+ *    don't care, ie, mapping to same qid.
+ *
+ */
+#define LOOKUP_FIELD_BITS 13
+#define EGFLAG_BIT  12 /* egflag bit */
+#define EP_BIT      8  /* EP port starting bit */
+
+static int lookup_mask_n;
+
+/* match state */
+#define PATTERN_MATCH_INIT  0
+#define PATTERN_MATCH_START 1
+#define PATTERN_MATCH_FAIL  2
+#define PATTERN_MATCH_PASS  3
+
+#define ENTRY_FILLED 0
+#define ENTRY_USED   1
+
+static int pattern_match_flag;	/*1--start matching  2--failed, 3---match 0k */
+static unsigned char lookup_mask1[LOOKUP_FIELD_BITS];
+
+#define C_ARRAY_SIZE  20
+static int c_tmp_data[C_ARRAY_SIZE];
+
+/*store result */
+#define MAX_PATTERN_NUM 1024
+static int lookup_match_num;
+static unsigned short lookup_match_mask[MAX_PATTERN_NUM];
+/*save tmp_index */
+static unsigned short lookup_match_index[MAX_PATTERN_NUM];
+/*save tmp_index */
+static unsigned char lookup_match_qid[MAX_PATTERN_NUM];
+
+static int tmp_pattern_port_id;
+
+static int left_n;
+/*10 bits lookup table except 4 bits EP */
+static unsigned char lookup_tbl_flags[BIT(LOOKUP_FIELD_BITS)];
+
+static void combine_util(int *arr, int *data, int start, int end, int index,
+			 int r);
+static int check_pattern(int *data, int r);
+static void lookup_table_via_qid(int qid);
+static void lookup_table_remap(int old_q, int new_q);
+static int find_pattern(int port_id, struct seq_file *s, int qid);
+static int get_dont_care_lookup(char *s);
+static void lookup_table_recursive(int k, int tmp_index, int set_flag, int qid);
+
+int dp_get_lookup_qid_via_index(struct cbm_lookup *info)
+{
+	int qid;
+
+	if (!info)
+		return 0;
+	if (info->index & BIT(EGFLAG_BIT))
+		info->egflag = 1;
+	else
+		info->egflag = 0;
+	info->index &= BIT(EGFLAG_BIT) - 1;
+	qid = get_lookup_qid_via_index(info);
+	DP_DEBUG(DP_DBG_FLAG_LOOKUP, "get egflag=%d index=0x%x qid=%d\n",
+		 info->egflag, info->index, qid);
+	return qid;
+}
+
+void dp_set_lookup_qid_via_index(struct cbm_lookup *info)
+{
+	if (!info)
+		return;
+	if (info->index & BIT(EGFLAG_BIT))
+		info->egflag = 1;
+	else
+		info->egflag = 0;
+	info->index &= BIT(EGFLAG_BIT) - 1;
+	set_lookup_qid_via_index(info);
+	DP_DEBUG(DP_DBG_FLAG_LOOKUP, "set egflag=%d index=0x%x qid=%d\n",
+		 info->egflag, info->index, info->qid);
+}
+
+/* The main function that prints all combinations of size r*/
+/* in arr[] of size n. This function mainly uses combine_util()*/
+static void c_not_care_walkthrought(int *arr, int n, int r)
+{
+	/* A temporary array data[] to store all combination one by one */
+
+	/* Print all combination using temprary array 'data[]' */
+	combine_util(arr, c_tmp_data, 0, n - 1, 0, r);
+}
+
+/* arr[]  ---> Input Array
+ * data[] ---> Temporary array to store current combination
+ * start & end ---> Staring and Ending indexes in arr[]
+ * index  ---> Current index in data[]
+ * r ---> Size of a combination to be printed
+ *
+ */
+static void combine_util(int *arr, int *data, int start, int end, int index,
+			 int r)
+{
+	int i;
+
+	/* Current combination is ready to be printed, print it */
+	if (left_n <= 0)
+		return;
+	if (index == r) {/*Find one pattern with specified don't care flag */
+
+		check_pattern(data, r);
+		/*find a don't care case and need further check */
+
+		return;
+	}
+	/* replace index with all possible elements. The condition */
+	/* "end-i+1 >= r-index" makes sure that including one element */
+	/* at index will make a combination with remaining elements */
+	/* at remaining positions */
+	for (i = start; i <= end && end - i + 1 >= r - index; i++) {
+		data[index] = arr[i];
+		if (left_n <= 0)
+			break;
+		combine_util(arr, data, i + 1, end, index + 1, r);
+	}
+}
+
+/*Note: when call this API, for those cared bits,
+ * its value already set in tmp_index.
+ */
+static void lookup_pattern_match(int tmp_index)
+{
+	int i;
+	int qid;
+	static int first_qid;
+	int t[LOOKUP_FIELD_BITS] = { 0 };
+	int index;
+	struct cbm_lookup lookup;
+
+	DP_DEBUG(DP_DBG_FLAG_LOOKUP,
+		 "trying with tmp_index=0x%x with lookup_match_num=%d\n",
+		 tmp_index, lookup_match_num);
+	pattern_match_flag = PATTERN_MATCH_INIT;
+	lookup_match_index[lookup_match_num] = tmp_index;
+
+	LIST_ALL_CASES(t, lookup_mask1, CARE_NOT_FLAG) {
+		index = tmp_index;
+		for (i = 0; i < LOOKUP_FIELD_BITS; i++)
+			index |= (t[i] << i);
+		DP_DEBUG(DP_DBG_FLAG_LOOKUP, "don't care[14]=");
+		for (i = 0; i < LOOKUP_FIELD_BITS; i++)
+			DP_DEBUG(DP_DBG_FLAG_LOOKUP, "%d ", t[i]);
+		DP_DEBUG(DP_DBG_FLAG_LOOKUP, "\n");
+
+		DP_DEBUG(DP_DBG_FLAG_LOOKUP, "don't care index=%x\n", index);
+
+		if (lookup_tbl_flags[index] == ENTRY_USED) {
+			pattern_match_flag = PATTERN_MATCH_FAIL;
+			goto END;
+		}
+		lookup.index = index;
+		qid = dp_get_lookup_qid_via_index(&lookup);
+
+		if (pattern_match_flag == PATTERN_MATCH_INIT) {
+			pattern_match_flag = PATTERN_MATCH_START;
+			first_qid = qid;
+		} else if (first_qid != qid) {
+			pattern_match_flag = PATTERN_MATCH_FAIL;
+			DP_DEBUG(DP_DBG_FLAG_LOOKUP,
+				 "first_qid(%d) != qid(%d)\n",
+				 first_qid, qid);
+			goto END;
+		}
+	}
+
+END:
+	/*save the result if necessary here */
+	if (pattern_match_flag == PATTERN_MATCH_START) {
+		/*pass since still not fail yet */
+		pattern_match_flag = PATTERN_MATCH_PASS;
+
+		/*mark the entries */
+		LIST_ALL_CASES(t, lookup_mask1, CARE_NOT_FLAG) {
+			index = tmp_index;
+			for (i = 0; i < LOOKUP_FIELD_BITS; i++)
+				index |= (t[i] << i);
+			if (lookup_tbl_flags[index] == ENTRY_USED)
+				PR_ERR("why already used\n");
+			else
+				lookup_tbl_flags[index] = ENTRY_USED;
+		}
+		/*save status */
+		lookup_match_qid[lookup_match_num] = first_qid;
+		lookup_match_mask[lookup_match_num] = 0;
+		for (i = 0; i < LOOKUP_FIELD_BITS; i++)
+			if (lookup_mask1[i])
+				lookup_match_mask[lookup_match_num] |= 1 << i;
+		lookup_match_num++;
+		DP_DEBUG(DP_DBG_FLAG_LOOKUP,
+			 "left_n=%d lookup_mask_n=%d. Need reduce=%d\n",
+			 left_n, lookup_mask_n, (1 << lookup_mask_n));
+		left_n -= (1 << lookup_mask_n);
+	} else {
+		/*failed */
+	}
+}
+
+/*k--number of don't care flags
+ */
+static int list_care_combination(int tmp_index)
+{
+	int i, k, index;
+	int t[14] = { 0 };
+
+	LIST_ALL_CASES(t, lookup_mask1, CARE_FLAG) {
+		index = tmp_index;
+		for (i = 0; i < LOOKUP_FIELD_BITS; i++)
+			index |= (t[i] << i);
+		DP_DEBUG(DP_DBG_FLAG_LOOKUP, "care index=%x\n", index);
+		DP_DEBUG(DP_DBG_FLAG_LOOKUP, "care t[14]=");
+		for (k = 0; k < LOOKUP_FIELD_BITS; k++)
+			DP_DEBUG(DP_DBG_FLAG_LOOKUP, "%d ", t[k]);
+		DP_DEBUG(DP_DBG_FLAG_LOOKUP, "\n");
+		lookup_pattern_match(index);
+	}
+
+	return 0;
+}
+
+/*based on the don't care list, we try to find the all possible pattern:
+ *for example: bit 13 and bit 11 don't care.
+ *data---the flag index list which is don't care
+ *r -- the flag index length
+ */
+static int check_pattern(int *data, int r)
+{
+	int i;
+
+	memset(lookup_mask1, 0, sizeof(lookup_mask1));
+	DP_DEBUG(DP_DBG_FLAG_LOOKUP, "data:");
+	for (i = 0; i < r; i++) {
+		DP_DEBUG(DP_DBG_FLAG_LOOKUP, "%d ", data[i]);
+		lookup_mask1[data[i]] = CARE_NOT_FLAG;
+	}
+	lookup_mask_n = r;
+	pattern_match_flag = 0;
+	DP_DEBUG(DP_DBG_FLAG_LOOKUP, "\n");
+
+	DP_DEBUG(DP_DBG_FLAG_LOOKUP, "Don't care flag: ");
+	for (i = 0; i < LOOKUP_FIELD_BITS; i++)
+		DP_DEBUG(DP_DBG_FLAG_LOOKUP, "%c ",
+			 lookup_mask1[i] ? 'x' : '0');
+	DP_DEBUG(DP_DBG_FLAG_LOOKUP, "\n");
+
+	list_care_combination(tmp_pattern_port_id << EP_BIT);
+	return 0;
+}
+
+static void print_title(struct seq_file *s, int ep, int mode)
+{
+	proc_printf(s,
+		    "EP%-2d:%5s%5s%5s%5s%5s%5s%5s%5s%5s%5s%5s%5s%5s%5s%5s%s%d\n",
+		    ep,
+		    "EG", "EP3",
+		    "EP2", "EP1", "EP0",
+		    "S7", "S6", "S5", "S4", "S3", "S2", "S1", "S0",
+		    "qid", "mode", "=", mode);
+	if (mode == 0)
+		proc_printf(s,
+			    "%7s subif[13-8] class[1-0]\n", "");
+	else if (mode == 1)
+		proc_printf(s,
+			    "%7s subif[7-0]\n", "");
+	else if (mode == 2)
+		proc_printf(s,
+			    "%7s subif[11-8] class[3-0]\n", "");
+	else if (mode == 3)
+		proc_printf(s,
+			    "%7s subif[4-0] class[2-0]\n", "");
+	else if (mode == 4)
+		proc_printf(s,
+			    "%7s class[1-0] subif[5-0]\n", "");
+	else if (mode == 5)
+		proc_printf(s,
+			    "%7s subif[15-8]\n", "");
+	else if (mode == 6)
+		proc_printf(s,
+			    "%7s subif[1-0] class[3-0] color[1-0]\n", "");
+	else if (mode == 7)
+		proc_printf(s,
+			    "%7s subif[14-8] class[0]\n", "");
+}
+
+/*qid: -1: match all queues
+ *      >=0: only match the specified queue
+ */
+int find_pattern(int port_id, struct seq_file *s, int qid)
+{
+	int r, i, j, n;
+	int f = 0;
+	char *flag_s;
+	char flag_buf[40];
+	int deq_port;
+	int arr[] = {12, /*11, 10, 9, 8,*/ 7, 6, 5, 4, 3, 2, 1, 0 };
+	int inst = 0;
+	struct hal_priv *priv = (struct hal_priv *)dp_port_prop[inst].priv_hal;
+	u32 mode = get_dp_port_info(inst, port_id)->cqe_lu_mode;
+
+	left_n = 1 << (LOOKUP_FIELD_BITS - 4); /* maximum lookup entries */
+	lookup_match_num = 0;
+	tmp_pattern_port_id = port_id;
+	memset(lookup_tbl_flags, 0, sizeof(lookup_tbl_flags));
+	n = ARRAY_SIZE(arr);
+	/*list all pattern, ie, don't care numbers from 10 to 1 */
+	for (r = n; r >= 0; r--) {
+		if (left_n <= 0)
+			break;
+		c_not_care_walkthrought(arr, n, r);
+		DP_DEBUG(DP_DBG_FLAG_LOOKUP, "left_n=%d\n", left_n);
+		if (!left_n)
+			break;
+	}
+
+	for (i = 0; i < lookup_match_num; i++) {
+		if ((qid >= 0) && (qid != lookup_match_qid[i]))
+			continue;
+		if (!f) {
+			f = 1;
+			print_title(s, tmp_pattern_port_id, mode);
+		}
+		deq_port = priv->qos_queue_stat[lookup_match_qid[i]].deq_port;
+		flag_s = get_dma_flags_str32(deq_port, flag_buf,
+					     sizeof(flag_buf));
+
+		if (lookup_match_qid[i] != priv->ppv4_drop_q) {
+			proc_printf(s, "    ");
+			for (j = LOOKUP_FIELD_BITS- 1; j >= 0; j--) {
+				if ((lookup_match_mask[i] >> j) & 1)
+					proc_printf(s, "%5c", 'x');
+				else
+					proc_printf(s, "%5d",
+						    (lookup_match_index[i] >> j)
+									& 1);
+			}
+
+			proc_printf(s, "  ->%-3d(0x%04x)%s\n",
+				    lookup_match_qid[i],
+				    lookup_match_index[i], flag_s);
+		}
+	}
+	if (s && seq_has_overflowed(s))
+		return -1;
+
+	return 0;
+}
+
+int lookup_start32(void)
+{
+	return 0;
+}
+
+int lookup_dump32(struct seq_file *s, int pos)
+{
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+	if (find_pattern(pos, s, -1) < 0)
+		return pos;
+	pos++;
+	if (pos >= 16)
+		pos = -1;
+	return pos;
+}
+
+ssize_t proc_get_qid_via_index32(struct file *file, const char *buf,
+				 size_t count, loff_t *ppos)
+{
+	int err = 0, len = 0;
+	char data[100];
+	unsigned int lookup_index;
+	unsigned int qid = 0;
+	char *param_list[10];
+	int num;
+	struct cbm_lookup lookup;
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+	len = (count >= sizeof(data)) ? (sizeof(data) - 1) : count;
+	DP_DEBUG(DP_DBG_FLAG_LOOKUP, "len=%d\n", len);
+
+	if (len <= 0) {
+		err = -EFAULT;
+		PR_ERR("Wrong len value (%d)\n", len);
+		return count;
+	}
+
+	if (copy_from_user(data, buf, len)) {
+		err = -EFAULT;
+		PR_ERR("copy_from_user fail");
+		return count;
+	}
+
+	data[len - 1] = 0; /* Make string */
+	num = dp_split_buffer(data, param_list, ARRAY_SIZE(param_list));
+
+	if (num <= 1)
+		goto help;
+	if (!param_list[1])
+		goto help;
+
+	lookup_index = dp_atoi(param_list[1]);
+
+	if ((dp_strncmpi(param_list[0], "set", strlen("set")) == 0) ||
+	    (dp_strncmpi(param_list[0], "write", strlen("write")) == 0)) {
+		if (!param_list[2]) {
+			PR_ERR("wrong command\n");
+			return count;
+		}
+		qid = dp_atoi(param_list[2]);
+		/*workaround for mask support */
+		if (get_dont_care_lookup(param_list[1]) == 0) {
+			lookup_table_recursive(LOOKUP_FIELD_BITS - 1, 0, 1,
+					       qid);
+			return count;
+		}
+		PR_INFO("Set to queue[%u] done\n", qid);
+		lookup.index = lookup_index;
+		lookup.qid = qid;
+		dp_set_lookup_qid_via_index(&lookup);
+		return count;
+	} else if ((dp_strncmpi(param_list[0], "get", strlen("get")) == 0) ||
+		   (dp_strncmpi(param_list[0], "read", strlen("read")) == 0)) {
+		if (get_dont_care_lookup(param_list[1]) == 0) {
+			lookup_table_recursive(LOOKUP_FIELD_BITS - 1, 0, 0,
+					       0);
+			return count;
+		}
+		lookup.index = lookup_index;
+		qid = dp_get_lookup_qid_via_index(&lookup);
+		PR_INFO("Get lookup[%05u 0x%04x] ->     queue[%u]\n",
+			lookup_index, lookup_index, qid);
+		return count;
+	} else if (dp_strncmpi(param_list[0], "find", strlen("find") + 1) == 0) {
+		/*read out its all flags for specified qid */
+		int i;
+
+		qid = dp_atoi(param_list[1]);
+		for (i = 0; i < 16; i++)
+			find_pattern(i, NULL, qid);
+		return count;
+	} else if (dp_strncmpi(param_list[0], "find2",
+		   strlen("find2")+ 1) == 0) {
+		/*read out its all flags for specified qid */
+		qid = dp_atoi(param_list[1]);
+		lookup_table_via_qid(qid);
+		return count;
+	} else if (dp_strncmpi(param_list[0], "remap", strlen("remap")) == 0) {
+		int old_q = dp_atoi(param_list[1]);
+		int new_q = dp_atoi(param_list[2]);
+
+		lookup_table_remap(old_q, new_q);
+		PR_INFO("remap queue[%d] to queue[%d] done\n",
+			old_q, new_q);
+		return count;
+	}  else if (dp_strncmpi(param_list[0], "test",
+		   strlen("test")+ 1) == 0) {
+		cbm_queue_map_entry_t lookup = {0};
+		int inst = 0;
+		int qid = dp_atoi(param_list[1]);
+		u32 flag = CBM_QUEUE_MAP_F_FLOWID_L_DONTCARE |
+			   CBM_QUEUE_MAP_F_FLOWID_H_DONTCARE |
+			   CBM_QUEUE_MAP_F_SUBIF_DONTCARE |
+			   CBM_QUEUE_MAP_F_EN_DONTCARE |
+			   CBM_QUEUE_MAP_F_DE_DONTCARE |
+			   CBM_QUEUE_MAP_F_MPE1_DONTCARE |
+			   CBM_QUEUE_MAP_F_MPE2_DONTCARE |
+			   CBM_QUEUE_MAP_F_TC_DONTCARE |
+			   CBM_QUEUE_MAP_F_COLOR_DONTCARE;
+
+		lookup.egflag = 0;
+		lookup.ep = 3;
+		cbm_queue_map_set(dp_port_prop[inst].cbm_inst, qid,
+			  &lookup, flag);
+		DP_INFO("cbm_queue_map_set ep=%d egflag=%d qid=%d flag=0x%x\n",
+			lookup.ep, lookup.egflag, qid, flag);
+		lookup.egflag = 1;
+		cbm_queue_map_set(dp_port_prop[inst].cbm_inst, qid,
+			  &lookup, flag);
+		DP_INFO("cbm_queue_map_set ep=%d egflag=%d qid=%d flag=0x%x\n",
+			lookup.ep, lookup.egflag, qid, flag);
+		return count;
+	}
+	goto help;
+help:
+	PR_INFO("Usage: echo set lookup_flags queue_id > /proc/dp/lookup\n");
+	PR_INFO("     : echo get lookup_flags > /proc/dp/lookup\n");
+	PR_INFO("     : echo find  <x> > /proc/dp/lookup\n");
+	PR_INFO("     : echo find2 <x> > /proc/dp/lookup\n");
+	PR_INFO("     : echo remap <old_q> <new_q> > /proc/dp/lookup\n");
+	PR_INFO("     : echo test <qid> > /proc/dp/lookup\n");
+	PR_INFO("  Hex example: echo set 0x10 10 > /proc/dp/lookup\n");
+	PR_INFO("  Dec:example: echo set 16 10 > /proc/dp/lookup\n");
+	PR_INFO("  Bin:example: echo set b10000 10 > /proc/dp/lookup\n");
+
+	PR_INFO("%s: echo set b1xxxx 10 > /proc/dp/lookup\n",
+		"Special for BIN(Don't care bit)");
+	PR_INFO("Lookup format:\n");
+	PR_INFO("  Bits Index: | %s\n",
+		"13   12 |  11  |  10  |  9   |  8   |7   4 | 3   0 |");
+	PR_INFO("  Fields:     | %s\n",
+		"Flow ID | DEC  | ENC  | MPE2 | MPE1 |  EP  | CLASS |");
+	return count;
+}
+
+void lookup_table_via_qid(int qid)
+{
+	u32 index, tmp, i, j, k, f = 0;
+	struct cbm_lookup lookup;
+
+	DP_DEBUG(DP_DBG_FLAG_LOOKUP,
+		 "Try to find all lookup flags mapped to qid %d\n", qid);
+	for (i = 0; i < 16; i++) { /*ep: 4 bits */
+		for (j = 0; j < 256; j++) { /*selected fields: 8 bits */
+			for (k = 0; k < 2; k++) {/*eg flag: 1 bit */
+				index = (k << EGFLAG_BIT) | (i << EP_BIT) | j;
+				lookup.index = index;
+				tmp = dp_get_lookup_qid_via_index(&lookup);
+				if (tmp != qid)
+					continue;
+				f = 1;
+				PR_INFO("Get lookup[%05u 0x%04x]%s[%d]\n",
+					index, index,
+					" ->     queue", qid);
+			}
+		}
+	}
+	if (!f)
+		PR_ERR("No mapping to queue id %d yet ?\n", qid);
+}
+
+void lookup_table_remap(int old_q, int new_q)
+{
+	u32 index, tmp, i, j, k, f = 0;
+	struct cbm_lookup lookup;
+
+	DP_DEBUG(DP_DBG_FLAG_LOOKUP,
+		 "Try to remap lookup flags mapped from old_q %d to new_q %d\n",
+		 old_q, new_q);
+	for (i = 0; i < 16; i++) { /* ep */
+		for (j = 0; j < 256; j++) { /*  select fields */
+			for (k = 0; k < 2; k++) { /* eg flag */
+				index = (k << EGFLAG_BIT) | (i << EP_BIT) | j;
+				lookup.index = index;
+				tmp = dp_get_lookup_qid_via_index(&lookup);
+				if (tmp != old_q)
+					continue;
+				lookup.index = index;
+				lookup.qid = new_q;
+				dp_set_lookup_qid_via_index(&lookup);
+				f = 1;
+				DP_DEBUG(DP_DBG_FLAG_LOOKUP,
+					 "Remap lookup[%05u 0x%04x] %s[%d]\n",
+					 index, index,
+					 "->     queue", new_q);
+			}
+		}
+	}
+	if (!f)
+		PR_INFO("No mapping to queue id %d yet\n", new_q);
+}
+
+static u8 lookup_flags2[LOOKUP_FIELD_BITS];
+static u8 lookup_mask2[LOOKUP_FIELD_BITS];
+
+/*return 0: get correct bit mask
+ * -1: no
+ */
+int get_dont_care_lookup(char *s)
+{
+	int len, i, j;
+	int flag = 0;
+
+	if (!s)
+		return -1;
+	len = strlen(s);
+	dp_replace_ch(s, strlen(s), ' ', 0);
+	dp_replace_ch(s, strlen(s), '\r', 0);
+	dp_replace_ch(s, strlen(s), '\n', 0);
+	if (s[0] == 0)
+		return -1;
+	memset(lookup_flags2, 0, sizeof(lookup_flags2));
+	memset(lookup_mask2, 0, sizeof(lookup_mask2));
+	if ((s[0] != 'b') && (s[0] != 'B'))
+		return -1;
+
+	if (len >= LOOKUP_FIELD_BITS + 1)
+		len = LOOKUP_FIELD_BITS + 1;
+	for (i = len - 1, j = 0; i >= 1; i--, j++) {
+		if ((s[i] == 'x') || (s[i] == 'X')) {
+			lookup_mask2[j] = 1;
+			flag = 1;
+		} else if (('0' <= s[i]) && (s[i] <= '9')) {
+			lookup_flags2[j] = s[i] - '0';
+		} else if (('A' <= s[i]) && (s[i] <= 'F')) {
+			lookup_flags2[j] = s[i] - 'A' + 10;
+		} else if (('a' <= s[i]) && (s[i] <= 'f')) {
+			lookup_flags2[j] = s[i] - '1' + 10;
+		} else {
+			return -1;
+		}
+	}
+	if (flag) {
+		DP_DEBUG(DP_DBG_FLAG_LOOKUP, "\nGet lookup flag: ");
+		for (i = LOOKUP_FIELD_BITS - 1; i >= 0; i--) {
+			if (lookup_mask2[i])
+				DP_DEBUG(DP_DBG_FLAG_LOOKUP, "x");
+			else
+				DP_DEBUG(DP_DBG_FLAG_LOOKUP, "%d",
+					 lookup_flags2[i]);
+		}
+		DP_DEBUG(DP_DBG_FLAG_LOOKUP, "\n");
+
+		return 0;
+	} else {
+		return -1;
+	}
+}
+
+void lookup_table_recursive(int k, int tmp_index, int set_flag, int qid)
+{
+	int i;
+	struct cbm_lookup lookup = {0};
+	
+	if (k < 0) {	/*finish recursive and start real read/set action */
+		if (set_flag) {
+			lookup.index = tmp_index;
+			lookup.qid = qid;
+			dp_set_lookup_qid_via_index(&lookup);
+			DP_DEBUG(DP_DBG_FLAG_LOOKUP,
+				 "Set lookup[%05u/0x%04x] ->     queue[%d]\n",
+				 tmp_index, tmp_index, qid);
+		} else {
+			lookup.index = tmp_index;
+			qid = dp_get_lookup_qid_via_index(&lookup);
+			PR_INFO("Get lookup[%05u/0x%04x] ->     queue[%d]\n",
+				tmp_index, tmp_index, qid);
+		}
+		return;
+	}
+
+	if (lookup_mask2[k]) {
+		for (i = 0; i < 2; i++)
+			lookup_table_recursive(k - 1, tmp_index + (i << k),
+					       set_flag, qid);
+		return;
+	}
+
+	lookup_table_recursive(k - 1, tmp_index + (lookup_flags2[k] << k),
+			       set_flag, qid);
+}
+
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_mib.c b/drivers/net/datapath/dpm/gswip32/datapath_mib.c
new file mode 100644
index 000000000000..64b3d82d1efe
--- /dev/null
+++ b/drivers/net/datapath/dpm/gswip32/datapath_mib.c
@@ -0,0 +1,384 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+#include<linux/init.h>
+#include<linux/module.h>
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/version.h>
+#include <linux/timer.h>
+#include <linux/skbuff.h>
+#include <linux/if_ether.h>
+#include <linux/ethtool.h>
+#include <linux/proc_fs.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/clk.h>
+#include <linux/if_ether.h>
+#include <linux/clk.h>
+#include <net/datapath_api.h>
+#include <net/datapath_proc_api.h>
+#include "../datapath.h"
+
+#define WRAPAROUND32   0xFFFFFFFF
+/*timer interval for mib wraparound handling:
+ * Most mib counter is 32 bits, ie, maximu ix 0xFFFFFFFF
+ * one pmac port maximum (cpu port) can support less than 3G, ie,
+ * 1488096 * 3 packets for 64 bytes case. so the time to have 1 wrapround is:
+ * 0xFFFFFFFF / (1488096 * 3) = 962 seconds
+ * If each timer check one port and its subif, then 962/16 = 60 sec.
+ */
+#define POLL_INTERVAL (60 * HZ)
+#define WAN_EP          15	/*WAN Interface's EP value */
+#define MAX_RMON_ITF    256	/*maximum 256 GSW RMON interface supported */
+
+struct mibs_port {
+	u64 rx_good_bytes;
+	u64 rx_bad_bytes;
+	u64 rx_good_pkts;
+	u64 rx_drop_pkts;
+	/*For eth0_x only, for all others, must keep it
+	 * to zero in order to share same algo
+	 */
+	u64 rx_drop_pkts_pae;
+	u64 rx_disc_pkts_redir;	/*for eth1 only */
+	u64 rx_fcs_err_pkts;
+	u64 rx_undersize_good_pkts;
+	u64 rx_oversize_good_pkts;
+	u64 rx_undersize_err_pkts;
+	u64 rx_oversize_err_pkts;
+	u64 rx_align_err_pkts;
+	u64 rx_filter_pkts;
+
+	u64 tx_good_bytes;
+	u64 tx_good_pkts;
+	u64 tx_drop_pkts;
+	/*For eth0_x only, for all others, must keep it
+	 *to zero in order to share same algo
+	 */
+	u64 tx_drop_pkts_pae;
+	u64 tx_acm_drop_pkts;
+	u64 tx_acm_drop_pkts_pae;	/*for eth0_x only */
+	u64 tx_disc_pkts_redir;	/*for eth1 only */
+	u64 tx_coll_pkts;
+	u64 tx_coll_pkts_pae;	/*for eth0_x only */
+	u64 tx_pkts_redir;	/*for eth1 only */
+	u64 tx_bytes_redir;	/*for eth1 only */
+	u64 rx_fcs_err_pkts_pae;
+	u64 rx_undersize_err_pkts_pae;
+	u64 rx_oversize_err_pkts_pae;
+	u64 rx_align_err_pkts_pae;
+
+	/*driver related */
+	u64 rx_drv_drop_pkts;
+	u64 rx_drv_error_pkts;
+	u64 tx_drv_drop_pkts;
+	u64 tx_drv_error_pkts;
+
+	/*for DSL ATM only */
+	u64 tx_drv_pkts;
+	u64 rx_drv_pkts;
+	u64 tx_drv_bytes;
+	u64 rx_drv_bytes;
+};
+
+struct mib_vap {
+	u64 rx_pkts_itf;
+	u64 rx_disc_pkts_itf;
+	u64 rx_disc_pkts_drv;
+	u64 tx_pkts_itf;
+	u64 tx_disc_pkts_itf;
+	u64 tx_disc_pkts_drv;
+};
+
+struct port_mib {
+	struct mibs_port curr;/*tmp variable used for mib counter calculation */
+	struct mib_vap curr_vap[MAX_SUBIF_PER_PORT];	/*for future */
+};
+
+struct mibs_low_lvl_port {
+	GSW_RMON_Port_cnt_t l;          /*only for ethernet LAN ports */
+	GSW_RMON_Port_cnt_t r;
+	GSW_RMON_Redirect_cnt_t redir; /*only for ethernet WAN port */
+	dp_drv_mib_t drv;
+};
+
+struct mibs_low_lvl_vap {
+	GSW_RMON_If_cnt_t gsw_if; /*for pae only since GSW-L not
+				   *support interface mib
+				   */
+	dp_drv_mib_t drv;
+};
+
+static unsigned int proc_mib_vap_start_id = 1;
+static unsigned int proc_mib_vap_end_id = PMAC_MAX_NUM - 1;
+static spinlock_t dp_mib_lock;
+static unsigned long poll_interval = POLL_INTERVAL;
+
+/*save port based lower level last mib counter
+ * for wraparound checking
+ */
+struct mibs_low_lvl_port last[PMAC_MAX_NUM];
+/*save vap/sub interface based lower level last mib counter
+ * for wraparound checking
+ */
+struct mibs_low_lvl_vap last_vap[PMAC_MAX_NUM][MAX_SUBIF_PER_PORT];
+/*Save all necessary aggregated basic MIB */
+static struct port_mib aggregate_mib[PMAC_MAX_NUM];
+/*For PAE CPU port only */
+static struct port_mib aggregate_mib_r[1];
+
+#define THREAD_MODE
+
+#ifdef THREAD_MODE
+#include <linux/kthread.h>
+struct task_struct *thread;
+#else
+static struct timer_list exp_timer;	/*timer setting */
+#endif
+
+/*internal API: update local net mib counters periodically */
+static int update_port_mib_lower_lvl(dp_subif_t *subif, u32 flag);
+static int update_vap_mib_lower_lvl(dp_subif_t *subif, u32 flag);
+
+/* ----- API implementation ------- */
+static u64 wraparound(u64 curr, u64 last, u32 size)
+{
+#define WRAPAROUND_MAX_32 0xFFFFFFFF
+
+	if ((size > 4) || /*for 8 bytes(64bit mib),no need to do wraparound*/
+	    (curr >= last))
+		return curr - last;
+
+	return ((u64)WRAPAROUND_MAX_32) + (u64)curr - last;
+}
+
+static int port_mib_wraparound(u32 ep, struct mibs_low_lvl_port *curr,
+			       struct mibs_low_lvl_port *last)
+{
+	return 0;
+}
+
+static int vap_mib_wraparound(dp_subif_t *subif,
+			      struct mibs_low_lvl_vap *curr,
+			      struct mibs_low_lvl_vap *last)
+{
+	return 0;
+}
+
+static int get_gsw_port_rmon(u32 ep, char *gsw_drv_name,
+			     GSW_RMON_Port_cnt_t *mib)
+{
+	return 0;
+}
+
+static int get_gsw_redirect_rmon(u32 ep, int index,
+				 GSW_RMON_Redirect_cnt_t *mib)
+{
+	return 0;
+}
+
+static int get_gsw_itf_rmon(u32 index, int index,
+			    GSW_RMON_If_cnt_t *mib)
+{
+	return 0;
+}
+
+int get_gsw_interface_base(int port_id)
+{
+	return 0;
+}
+
+/* if ethernet WAN redirect is enabled, return 1,
+ * else return 0
+ */
+int gsw_eth_wan_redirect_status(void)
+{
+	return 0;
+}
+
+/*Note:
+ * Update mib counter for physical port only
+ * flag so far no much use only
+ */
+static int update_port_mib_lower_lvl(dp_subif_t *subif, u32 flag)
+{
+	return 0;
+}
+
+static void mib_wraparound_timer_poll(unsigned long data)
+{
+}
+
+static int update_vap_mib_lower_lvl(dp_subif_t *subif, u32 flag)
+{
+	return 0;
+}
+
+int dp_reset_sys_mib(u32 flag)
+{
+	return 0;
+}
+
+void proc_mib_timer_read(struct seq_file *s)
+{
+	seq_printf(s, "\nMib timer interval is %u sec\n",
+		   (unsigned int)poll_interval / HZ);
+}
+
+ssize_t proc_mib_timer_write(struct file *file, const char *buf, size_t count,
+			     loff_t *ppos)
+{
+	int len, num;
+	char str[64];
+	char *param_list[2];
+#define MIN_POLL_TIME 2
+	len = (sizeof(str) > count) ? count : sizeof(str) - 1;
+	len -= copy_from_user(str, buf, len);
+	str[len] = 0;
+	num = dp_split_buffer(str, param_list, ARRAY_SIZE(param_list));
+	poll_interval = dp_atoi(param_list[0]);
+
+	if (poll_interval < MIN_POLL_TIME)
+		poll_interval = MIN_POLL_TIME;
+
+	poll_interval *= HZ;
+#ifndef THREAD_MODE
+	mod_timer(&exp_timer, jiffies + poll_interval);
+#endif
+	DP_DEBUG(DP_DBG_FLAG_MIB, "new poll_interval=%u sec\n",
+		 (unsigned int)poll_interval / HZ);
+	return count;
+}
+
+static unsigned int proc_mib_port_start_id = 1;
+static unsigned int proc_mib_port_end_id = PMAC_MAX_NUM - 1;
+int proc_mib_inside_dump(struct seq_file *s, int pos)
+{
+	return -1;
+}
+
+int proc_mib_inside_start(void)
+{
+	return proc_mib_port_start_id;
+}
+
+ssize_t proc_mib_inside_write(struct file *file, const char *buf,
+			      size_t count, loff_t *ppos)
+{
+	return count;
+}
+
+/*Note:
+ *if (flag & DP_F_STATS_SUBIF), get sub-interface/vap mib only
+ *otherwise, get physical port's mib
+ */
+int dp_get_port_vap_mib_32(dp_subif_t *subif, void *priv,
+			   struct rtnl_link_stats64 *net_mib, u32 flag)
+{
+	return -1;
+}
+
+/*Clear GSW Interface MIB: only for sub interface/vap only  */
+int clear_gsw_itf_mib(dp_subif_t *subif, u32 flag)
+{
+	return 0;
+}
+
+int dp_clear_netif_mib_32(dp_subif_t *subif, void *priv, u32 flag)
+{
+	return 0;
+}
+
+int proc_mib_port_start(void)
+{
+	return 0;
+}
+
+int proc_mib_port_dump(struct seq_file *s, int pos)
+{
+	return -1;
+}
+
+ssize_t proc_mib_port_write(struct file *file, const char *buf, size_t count,
+			    loff_t *ppos)
+{
+	return count;
+}
+
+int proc_mib_vap_dump(struct seq_file *s, int pos)
+{
+	return -1;
+}
+
+int proc_mib_vap_start(void)
+{
+	return proc_mib_vap_start_id;
+}
+
+ssize_t proc_mib_vap_write(struct file *file, const char *buf, size_t count,
+			   loff_t *ppos)
+{
+	return count;
+}
+
+#ifdef THREAD_MODE
+int mib_wraparound_thread(void *data)
+{
+	while (1) {
+		mib_wraparound_timer_poll(0);
+		msleep(poll_interval / HZ * 1000 / PMAC_MAX_NUM / 2);
+		DP_DEBUG(DP_DBG_FLAG_MIB, "mib_wraparound_thread\n");
+	}
+}
+#endif
+
+int adjust_itf(void)
+{
+	return 0;
+}
+
+int set_gsw_itf(u8 ep, u8 ena, int start)
+{
+	return 0;
+}
+
+int reset_gsw_itf(u8 ep)
+{
+}
+
+int dp_mib_init(u32 flag)
+{
+	spin_lock_init(&dp_mib_lock);
+	memset(&aggregate_mib, 0, sizeof(aggregate_mib));
+	memset(&last, 0, sizeof(last));
+	memset(&last_vap, 0, sizeof(last_vap));
+	adjust_itf();
+
+#ifdef THREAD_MODE
+	thread = kthread_run(mib_wraparound_thread, 0, "dp_mib");
+#else
+	init_timer_on_stack(&exp_timer);
+	exp_timer.expires = jiffies + poll_interval;
+	exp_timer.data = 0;
+	exp_timer.function = mib_wraparound_timer_poll;
+	add_timer(&exp_timer);
+	DP_DEBUG(DP_DBG_FLAG_MIB, "dp_mib_init done\n");
+#endif
+	return 0;
+}
+
+void dp_mib_exit(void)
+{
+#ifdef THREAD_MODE
+	if (thread)
+		kthread_stop(thread);
+#else
+	del_timer(&exp_timer);
+#endif
+}
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_mib.h b/drivers/net/datapath/dpm/gswip32/datapath_mib.h
new file mode 100644
index 000000000000..d9ba07f5fb08
--- /dev/null
+++ b/drivers/net/datapath/dpm/gswip32/datapath_mib.h
@@ -0,0 +1,19 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#ifndef DATAPATH_MIB_H
+#define DATAPATH_MIB_H
+
+int dp_reset_mib(u32 flag);
+int set_gsw_itf(u8 ep, u8 ena, int start);
+int reset_gsw_itf(u8 ep);
+int dp_get_port_vap_mib_32(dp_subif_t *subif, void *priv,
+			   struct rtnl_link_stats64 *net_mib, u32 flag);
+
+#endif
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_misc.c b/drivers/net/datapath/dpm/gswip32/datapath_misc.c
new file mode 100644
index 000000000000..313df55c5f79
--- /dev/null
+++ b/drivers/net/datapath/dpm/gswip32/datapath_misc.c
@@ -0,0 +1,2240 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/version.h>
+#include <linux/if_ether.h>
+#include <linux/ethtool.h>
+#include <linux/proc_fs.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/clk.h>
+#include <linux/if_ether.h>
+#include <linux/if_vlan.h>
+
+#include <linux/clk.h>
+#include <linux/ip.h>
+#include <net/ip.h>
+#include "datapath_ppv4_session.h"
+
+#define DATAPATH_HAL_LAYER   /*must put before include datapath_api.h in
+			      *order to avoid include another platform's
+			      *DMA descriptor and pmac header files
+			      */
+#include <net/datapath_api.h>
+#include <net/datapath_api_gswip32.h>
+#include <linux/skbuff.h>
+#include "../datapath.h"
+#include "../datapath_instance.h"
+#include "datapath_proc.h"
+#include "datapath_ppv4.h"
+#include "datapath_misc.h"
+
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_SWITCHDEV)
+#include "datapath_switchdev.h"
+#endif
+
+static void init_dma_desc_mask(void)
+{
+	/*mask 0: to remove the bit, 1 -- keep the bit */
+	dma_rx_desc_mask1.all = 0xFFFFFFFF;
+	dma_rx_desc_mask3.all = 0xFFFFFFFF;
+	dma_rx_desc_mask3.field.own = 0; /*remove owner bit */
+	dma_rx_desc_mask3.field.c = 0;
+	dma_rx_desc_mask3.field.sop = 0;
+	dma_rx_desc_mask3.field.eop = 0;
+	dma_rx_desc_mask3.field.dic = 0;
+	dma_rx_desc_mask2.field.byte_offset = 0;
+	/*mask to keep some value via 1 set by
+	 * top application all others set to 0
+	 */
+	dma_tx_desc_mask0.all = 0;
+	dma_tx_desc_mask1.all = 0;
+	dma_tx_desc_mask0.field.flow_id = 0xFF;
+	dma_tx_desc_mask0.field.dest_sub_if_id = 0x7FFF;
+	dma_tx_desc_mask1.field.lro_type = 0x1;
+	dma_tx_desc_mask1.field.color = 0x3;
+	dma_tx_desc_mask1.field.ep = 0xF;
+}
+
+static void init_dma_pmac_template(int portid, u32 flags)
+{
+	int i;
+	struct pmac_port_info *dp_info = get_dp_port_info(0, portid);
+	//struct pmac_port_info *port = get_dp_port_info(0, portid);
+
+	/*Note:
+	 * MASK = 0, Datapath control
+	 * MASK = 1, Application control
+	 * final tx_dma0 = (tx_dma0 & dma0_mask_template) | dma0_template
+	 * final tx_dma1 = (tx_dma1 & dma1_mask_template) | dma1_template
+	 * final tx_pmac = pmac_template
+	 */
+	memset(dp_info->pmac_template, 0, sizeof(dp_info->pmac_template));
+	memset(dp_info->dma0_template, 0, sizeof(dp_info->dma0_template));
+	memset(dp_info->dma1_template, 0, sizeof(dp_info->dma1_template));
+
+	for (i = 0; i < MAX_TEMPLATE; i++) {
+		dp_info->dma0_mask_template[i].all = 0xFFFFFFFF;
+		dp_info->dma1_mask_template[i].all = 0xFFFFFFFF;
+		dp_info->dma3_mask_template[i].all = 0xFFFFFFFF;
+	}
+	if ((flags & DP_F_FAST_ETH_LAN) || (flags & DP_F_FAST_ETH_WAN) ||
+	    (flags & DP_F_GPON) || (flags & DP_F_EPON)|| (flags & DP_F_GINT)) {
+		/*always with pmac */
+		for (i = 0; i < MAX_TEMPLATE; i++) {
+			/* Pmac Template */
+			dp_info->pmac_template[i].class_en = 1;
+			SET_PMAC_IGP_EGP(&dp_info->pmac_template[i], portid);
+
+			/* DMA descriptor template */
+			dp_info->dma1_template[i].field.pmac = 1;
+			dp_info->dma1_mask_template[i].field.pmac = 0;
+			dp_info->dma1_template[i].field.redir = 1;
+			dp_info->dma1_mask_template[i].field.redir = 0;
+			dp_info->dma3_mask_template[i].field.own = 0;
+			dp_info->dma3_mask_template[i].field.c = 0;
+			dp_info->dma3_mask_template[i].field.sop = 0;
+			dp_info->dma3_mask_template[i].field.eop = 0;
+			dp_info->dma3_mask_template[i].field.haddr = 0;
+		}
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_PTP1588)
+		dp_info->pmac_template[TEMPL_PTP].ptp = 1;
+#endif
+	} else if (flags & DP_F_DIRECTLINK) { /*always with pmac*/
+		/*normal dirctpath without checksum support
+		 *but with pmac to Switch for accelerate
+		 */
+		dp_info->pmac_template[TEMPL_NORMAL].class_en = 1;
+		SET_PMAC_IGP_EGP(&dp_info->pmac_template[TEMPL_NORMAL], portid);
+
+		/*dirctpath with checksum support */
+		dp_info->dma1_template[TEMPL_CHECKSUM].field.redir = 1;
+		dp_info->dma1_mask_template[TEMPL_CHECKSUM].field.redir = 0;
+		dp_info->pmac_template[TEMPL_CHECKSUM].tcp_chksum = 1;
+		dp_info->pmac_template[TEMPL_CHECKSUM].class_en = 1;
+		SET_PMAC_IGP_EGP(&dp_info->pmac_template[TEMPL_CHECKSUM],
+				 portid);
+
+		/*dirctpath without checksum support but send packet to MPE
+		 * DL FW
+		 */
+		dp_info->dma1_template[TEMPL_OTHERS].field.redir = 1;
+		dp_info->dma1_mask_template[TEMPL_OTHERS].field.redir = 0;
+		dp_info->pmac_template[TEMPL_OTHERS].class_en = 1;
+		SET_PMAC_IGP_EGP(&dp_info->pmac_template[TEMPL_OTHERS], portid);
+	} else if (flags & DP_F_FAST_DSL) { /*sometimes with pmac*/
+		/* For normal single DSL upstream, there is no pmac at all*/
+
+		/*DSL  with checksum support */
+		dp_info->dma1_template[TEMPL_CHECKSUM].field.redir = 1;
+		dp_info->dma1_mask_template[TEMPL_CHECKSUM].field.redir = 0;
+		/*checksum*/
+		dp_info->pmac_template[TEMPL_CHECKSUM].tcp_chksum = 1;
+		dp_info->pmac_template[TEMPL_CHECKSUM].class_en = 1;
+		SET_PMAC_IGP_EGP(&dp_info->pmac_template[TEMPL_CHECKSUM],
+				 portid);
+
+		/*Bonding DSL  FCS Support via GSWIP */
+		dp_info->dma1_template[TEMPL_OTHERS].field.redir = 1;
+		dp_info->dma1_mask_template[TEMPL_OTHERS].field.redir = 0;
+		/*dp_info->pmac_template[TEMPL_OTHERS].tcp_chksum = 1; */
+		dp_info->pmac_template[TEMPL_OTHERS].class_en = 1;
+		SET_PMAC_IGP_EGP(&dp_info->pmac_template[TEMPL_CHECKSUM],
+				 portid);
+	} else if (flags & DP_F_FAST_WLAN) {
+		/* WLAN block must put after DSL/DIRECTLINK block
+		 * since all ACA device in GRX500 is using WLAN flag wrongly
+		 * and here must make sure still be back-compatible for it.
+		 * normal fast_wlan without pmac except checksum offload support
+		 * So no need to configure pmac_tmplate[0]
+		 *
+		 * Need to add real stuff later
+		 */
+	} else /*if(flags & DP_F_DIRECT ) */{/*always with pmac*/
+		/*normal dirctpath without checksum support */
+		dp_info->pmac_template[TEMPL_NORMAL].class_en = 1;
+		SET_PMAC_IGP_EGP(&dp_info->pmac_template[TEMPL_NORMAL], portid);
+
+		/*dirctpath with checksum support */
+		dp_info->dma1_template[TEMPL_CHECKSUM].field.redir = 1;
+		dp_info->dma1_mask_template[TEMPL_CHECKSUM].field.redir = 0;
+		dp_info->pmac_template[TEMPL_CHECKSUM].tcp_chksum = 1;
+		dp_info->pmac_template[TEMPL_CHECKSUM].class_en = 1;
+		SET_PMAC_IGP_EGP(&dp_info->pmac_template[TEMPL_CHECKSUM],
+				 portid);
+	}
+}
+
+void dump_rx_dma_desc_32(struct dma_rx_desc_0 *desc_0,
+		      struct dma_rx_desc_1 *desc_1,
+		      struct dma_rx_desc_2 *desc_2,
+		      struct dma_rx_desc_3 *desc_3)
+{
+	if (!desc_0 || !desc_1 || !desc_2 || !desc_3) {
+		PR_ERR("rx desc_0/1/2/3 NULL\n");
+		return;
+	}
+
+	PR_INFO(" DMA Descriptor:D0=0x%08x D1=0x%08x D2=0x%08x D3=0x%08x\n",
+		*(u32 *)desc_0, *(u32 *)desc_1,
+		*(u32 *)desc_2, *(u32 *)desc_3);
+	PR_INFO("  DW0: dw0bit31=%d flow_id=%d dw0bit16=%d %s=0x%04x\n",
+		desc_0->field.dw0bit31,
+		desc_0->field.flow_id, desc_0->field.dw0bit16,
+		"subif", desc_0->field.dest_sub_if_id);
+	PR_INFO("  DW1: redir=%d header_mode=%d pmac=%d ts=%d pre_l2=%d"
+		" classen=%d fcs=%d pkt_type=%d src_pool=%d dec=%d enc=%d"
+		" lro_type=%d color=%d port=%d classid=%d\n",
+		desc_1->field.redir, desc_1->field.header_mode,
+		desc_1->field.pmac, desc_1->field.ts, desc_1->field.pre_l2,
+		desc_1->field.classen, desc_1->field.fcs, desc_1->field.pkt_type,
+		desc_1->field.src_pool, desc_1->field.dec, desc_1->field.enc,
+		desc_1->field.lro_type, desc_1->field.color, desc_1->field.ep,
+		desc_1->field.classid);
+	PR_INFO("  DW2: data_ptr=0x%08x ByteOffset=%d\n", desc_2->field.data_ptr,
+		desc_2->field.byte_offset);
+	PR_INFO("  DW3: own=%d c=%d sop=%d eop=%d dic=%d haddr=0x%08x"
+		" sp=%d pool_policy=%d data_len=%d\n",
+		desc_3->field.own, desc_3->field.c, desc_3->field.sop,
+		desc_3->field.eop, desc_3->field.dic, desc_3->field.haddr,
+		desc_3->field.sp, desc_3->field.pool_policy,
+		desc_3->field.data_len);
+}
+
+void dump_tx_dma_desc_32(struct dma_tx_desc_0 *desc_0,
+		      struct dma_tx_desc_1 *desc_1,
+		      struct dma_tx_desc_2 *desc_2,
+		      struct dma_tx_desc_3 *desc_3)
+{
+	int lookup;
+	int inst = 0;
+	int dp_port;
+	struct hal_priv *priv = HAL(inst);
+	struct cbm_lookup cbm_lookup;
+
+	if (!desc_0 || !desc_1 || !desc_2 || !desc_3) {
+		PR_ERR("tx desc_0/1/2/3 NULL\n");
+		return;
+	}
+	PR_INFO(" DMA Descripotr:D0=0x%08x D1=0x%08x D2=0x%08x D3=0x%08x\n",
+		*(u32 *)desc_0, *(u32 *)desc_1,
+		*(u32 *)desc_2, *(u32 *)desc_3);
+	PR_INFO("  DW0:dw0bit31=%d flow_id=%d dw0bit16=%d %s=0x%04x\n",
+		desc_0->field.dw0bit31,
+		desc_0->field.flow_id, desc_0->field.dw0bit16,
+		"subif", desc_0->field.dest_sub_if_id);
+	PR_INFO("  DW1:redir=%d header_mode=%d pmac=%d ts=%d pre_l2=%d"
+		"classen=%d fcs=%d pkt_type=%d src_pool=%d dec=%d enc=%d"
+		"lro_type=%d color=%d port=%d classid=%d\n",
+		desc_1->field.redir, desc_1->field.header_mode,
+		desc_1->field.pmac, desc_1->field.ts, desc_1->field.pre_l2,
+		desc_1->field.classen, desc_1->field.fcs, desc_1->field.pkt_type,
+		desc_1->field.src_pool, desc_1->field.dec, desc_1->field.enc,
+		desc_1->field.lro_type, desc_1->field.color, desc_1->field.ep,
+		desc_1->field.classid);
+	PR_INFO("  DW2:data_ptr=0x%08x ByteOffset=%d\n", desc_2->field.data_ptr,
+		desc_2->field.byte_offset);
+	PR_INFO("  DW3:own=%d c=%d sop=%d eop=%d dic=%d haddr=0x%08x"
+		"sp=%d pool_policy=%d data_len=%d\n",
+		desc_3->field.own, desc_3->field.c, desc_3->field.sop,
+		desc_3->field.eop, desc_3->field.dic, desc_3->field.haddr,
+		desc_3->field.sp, desc_3->field.pool_policy,
+		desc_3->field.data_len);
+	dp_port = priv->gp_dp_map[desc_1->field.ep].dpid; /* get lpid */
+	if (get_dp_port_info(inst, dp_port)->cqe_lu_mode == CQE_LU_MODE0)
+		/* Eg Lpid[3:0] Sub_If_Id[13:8] Class[1:0] */
+		lookup = ((desc_0->field.dest_sub_if_id >> 8) << 2) |
+			 ((desc_1->field.classid & 0x3)) |
+			 ((dp_port & 0x0F) << 8) |
+			 ((desc_1->field.redir) << 12);
+	else if (get_dp_port_info(inst, dp_port)->cqe_lu_mode == CQE_LU_MODE1)
+		/* Eg Lpid[3:0] Sub_If_Id[7:0] */
+		lookup = (desc_0->field.dest_sub_if_id) |
+			 ((dp_port & 0x0F) << 8) |
+			 ((desc_1->field.redir) << 12);
+	else if (get_dp_port_info(inst, dp_port)->cqe_lu_mode == CQE_LU_MODE2)
+		/* Eg Lpid[3:0] Sub_If_Id[11:8] Class[3:0] */
+		lookup = ((desc_0->field.dest_sub_if_id >> 8) << 4) |
+			 (desc_1->field.classid) |
+			 ((dp_port & 0x0F) << 8) |
+			 ((desc_1->field.redir) << 12);
+	else if (get_dp_port_info(inst, dp_port)->cqe_lu_mode == CQE_LU_MODE3)
+		/* Eg Lpid[3:0] Sub_If_Id[4:0] Class[2:0] */
+		lookup = ((desc_0->field.dest_sub_if_id & 0x1F) << 3) |
+			 (desc_1->field.classid & 0x7) |
+			 ((dp_port & 0x0F) << 8) |
+			 ((desc_1->field.redir) << 12);
+	else if (get_dp_port_info(inst, dp_port)->cqe_lu_mode == CQE_LU_MODE4)
+		/* Eg Lpid[3:0] Class[1:0] Sub_If_Id[5:0] */
+		lookup = ((desc_0->field.dest_sub_if_id & 0x1F) << 3) |
+			 (desc_1->field.classid & 0x7) |
+			 ((dp_port & 0x0F) << 8) |
+			 ((desc_1->field.redir) << 12);
+	else if (get_dp_port_info(inst, dp_port)->cqe_lu_mode == CQE_LU_MODE5)
+		/* Eg Lpid[3:0] Sub_If_Id[15:8] */
+		lookup = ((desc_0->field.dest_sub_if_id & 0xFF00) >> 8) |
+			 ((dp_port & 0x0F) << 8) |
+			 ((desc_1->field.redir) << 12);
+	else if (get_dp_port_info(inst, dp_port)->cqe_lu_mode == CQE_LU_MODE6)
+		/* Eg Lpid[3:0] Sub_If_Id[1:0] Class[3:0] Color[1:0] */
+		lookup = ((desc_0->field.dest_sub_if_id & 0x2) << 6) |
+			 ((desc_1->field.classid) << 2) |
+			 ((dp_port & 0x0F) << 8) |
+			 ((desc_1->field.redir) << 12);
+	else if (get_dp_port_info(inst, dp_port)->cqe_lu_mode == CQE_LU_MODE7)
+		/* Eg Lpid[3:0] Sub_If_Id[14:8] Class[0] */
+		lookup = ((desc_0->field.dest_sub_if_id >> 8) << 1) |
+			 (desc_1->field.classid & 0x1) |
+			 ((dp_port & 0x0F) << 8) |
+			 ((desc_1->field.redir) << 12);
+	else {
+		PR_INFO("Invalid Lookup Mode: %d\n",
+			get_dp_port_info(inst, dp_port)->cqe_lu_mode);
+		return;
+	}
+	cbm_lookup.index = lookup;
+	cbm_lookup.egflag = desc_1->field.redir;
+	PR_INFO("  lookup index=0x%x qid=%d for gpid=%u\n", lookup,
+		dp_get_lookup_qid_via_index(&cbm_lookup), desc_1->field.ep);
+}
+
+static void dump_rx_pmac(struct pmac_rx_hdr *pmac)
+{
+	int i, l;
+	unsigned char *p = (char *)pmac;
+	unsigned char buf[100];
+
+	if (!pmac) {
+		PR_ERR(" pmac NULL ??\n");
+		return;
+	}
+
+	l = sprintf(buf, "PMAC at 0x%px: ", p);
+	for (i = 0; i < 8; i++)
+		l += sprintf(buf + l, "0x%02x ", p[i]);
+	l += sprintf(buf + l, "\n");
+	PR_INFO("%s", buf);
+
+	/*byte 0 */
+	PR_INFO("  byte 0:ver_done=%d ip_offset=%d\n",
+		pmac->ver_done, pmac->ip_offset);
+	/*byte 1 */
+	PR_INFO("  byte 1:tcp_h_offset=%d tcp_type=%d\n", pmac->tcp_h_offset,
+		pmac->tcp_type);
+	/*byte 2 */
+	PR_INFO("  byte 2:class=%d res=%d src_dst_subif_id_14_12=%d\n",
+	pmac->class, pmac->res2, pmac->src_dst_subif_id_14_12);
+	/*byte 3 */
+	PR_INFO("  byte 3:pkt_type=%d ext=%d ins=%d pre_l2=%d oam=%d res32=%d\n",
+		pmac->pkt_type, pmac->ext, pmac->ins, pmac->pre_l2,
+		pmac->oam, pmac->res32);
+	/*byte 4 */
+	PR_INFO("  byte 4:fcs=%d ptp=%d one_step=%d src_dst_subif_id_msb=%d\n",
+		pmac->fcs, pmac->ptp, pmac->one_step,
+		pmac->src_dst_subif_id_msb);
+	/*byte 5 */
+	PR_INFO("  byte 5:src_sub_inf_id2=%d\n", pmac->src_dst_subif_id_lsb);
+	/*byte 6 */
+	PR_INFO("  byte 6:record_id_msb=%d\n", pmac->record_id_msb);
+	/*byte 7 */
+	PR_INFO("  byte 7:record_id_lsb=%d igp_egp=%d\n",
+		pmac->record_id_lsb, pmac->igp_egp);
+}
+
+static void dump_tx_pmac(struct pmac_tx_hdr *pmac)
+{
+	int i, l;
+	unsigned char *p = (char *)pmac;
+	unsigned char buf[100];
+
+	if (!pmac) {
+		PR_ERR("dump_tx_pmac pmac NULL ??\n");
+		return;
+	}
+
+	l = sprintf(buf, "PMAC at 0x%px: ", p);
+	for (i = 0; i < 8; i++)
+		l += sprintf(buf + l, "0x%02x ", p[i]);
+	sprintf(buf + l, "\n");
+	PR_INFO("%s", buf);
+
+	/*byte 0 */
+	PR_INFO("  byte 0:tcp_chksum=%d ip_offset=%d\n",
+		pmac->tcp_chksum, pmac->ip_offset);
+	/*byte 1 */
+	PR_INFO("  byte 1:tcp_h_offset=%d tcp_type=%d\n", pmac->tcp_h_offset,
+		pmac->tcp_type);
+	/*byte 2 */
+	PR_INFO("  byte 2:igp_msb=%d res=%d\n", pmac->src_dst_subif_id_14_12,
+	pmac->res2);
+	/*byte 3 */
+	PR_INFO("  byte 3:%s=%d %s=%d %s=%d %s=%d %s=%d %s=%d %s=%d\n",
+		"pkt_type", pmac->pkt_type,
+		"ext", pmac->ext,
+		"ins", pmac->ins,
+		"res3", pmac->res3,
+		"oam", pmac->oam,
+		"lrnmd", pmac->lrnmd,
+		"class_en", pmac->class_en);
+	/*byte 4 */
+	PR_INFO("  byte 4:%s=%d ptp=%d one_step=%d src_dst_subif_id_msb=%d\n",
+		"fcs_ins_dis", pmac->fcs_ins_dis,
+		pmac->ptp, pmac->one_step,
+		pmac->src_dst_subif_id_msb);
+	/*byte 5 */
+	PR_INFO("  byte 5:src_dst_subif_id_lsb=%d\n",
+		pmac->src_dst_subif_id_lsb);
+	/*byte 6 */
+	PR_INFO("  byte 6:record_id_msb=%d\n", pmac->record_id_msb);
+	/*byte 7 */
+	PR_INFO("  byte 7:record_id_lsb=%d igp_egp=%d\n", pmac->record_id_lsb,
+		pmac->igp_egp);
+}
+
+static void mib_init(u32 flag)
+{
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_MIB)
+	dp_mib_init(0);
+#endif
+	gsw_mib_reset_32(0, 0); /* GSW O */
+}
+
+void dp_sys_mib_reset_32(u32 flag)
+{
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_MIB)
+	dp_reset_sys_mib(0);
+#else
+	gsw_mib_reset_32(0, 0); /* GSW L */
+	gsw_mib_reset_32(1, 0); /* GSW R */
+	dp_clear_all_mib_inside(0);
+#endif
+}
+
+#ifndef CONFIG_INTEL_DATAPATH_QOS_HAL
+int alloc_q_to_port_32(struct ppv4_q_sch_port *info, u32 flag)
+{
+	struct ppv4_queue q;
+	struct ppv4_port port;
+	int inst = info->inst;
+	struct hal_priv *priv = HAL(inst);
+	struct dp_subif_info *subif;
+
+	if (!priv) {
+		PR_ERR("why priv NULL ???\n");
+		return -1;
+	}
+
+	if (priv->deq_port_stat[info->cqe_deq].flag == PP_NODE_FREE) {
+		port.cqm_deq_port = info->cqe_deq;
+		port.tx_pkt_credit = info->tx_pkt_credit;
+		port.tx_ring_addr = info->tx_ring_addr_push;
+		port.tx_ring_size = info->tx_ring_size;
+		port.inst = inst;
+		port.dp_port = info->dp_port;
+		if (dp_pp_alloc_port_32(&port)) {
+			PR_ERR("%s fail for deq_port=%d qos_deq_port=%d\n",
+			       "dp_pp_alloc_port_32",
+			       port.cqm_deq_port, port.qos_deq_port);
+			return -1;
+		}
+		info->port_node = port.node_id;
+		info->f_deq_port_en = 1;
+	} else {
+		port.node_id = priv->deq_port_stat[info->cqe_deq].node_id;
+	}
+	q.qid = -1;
+	q.parent = port.node_id;
+	q.inst = inst;
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_DUMMY_QOS)
+	q.dq_port = info->cqe_deq; /*for qos slim driver only */
+#endif
+	if (dp_pp_alloc_queue_32(&q)) {
+		PR_ERR("%s fail\n",
+		       "dp_pp_alloc_queue_32");
+		return -1;
+	}
+	subif = get_dp_port_subif(get_dp_port_info(info->inst, info->dp_port),
+			   info->ctp);
+	subif->qid = q.qid;
+	subif->q_node = q.qid;
+	info->qid = q.qid;
+	info->q_node = q.node_id;
+	priv->qos_queue_stat[q.qid].deq_port = info->cqe_deq;
+	priv->qos_queue_stat[q.qid].node_id = q.node_id;
+	priv->qos_queue_stat[q.qid].flag = PP_NODE_ALLOC;
+	DP_DEBUG(DP_DBG_FLAG_QOS, "alloc_q_to_port_32 done\n");
+	return 0;
+}
+#else
+int alloc_q_to_port_32(struct ppv4_q_sch_port *info, u32 flag)
+{
+	struct dp_node_link link = {0};
+
+	link.cqm_deq_port.cqm_deq_port = info->cqe_deq;
+	link.dp_port = info->dp_port; /*in case for qos node alloc resv pool*/
+	link.inst = info->inst;
+	link.node_id.q_id = DP_NODE_AUTO_ID;
+	link.node_type = DP_NODE_QUEUE;
+	link.p_node_id.cqm_deq_port = info->cqe_deq;
+	link.p_node_type = DP_NODE_PORT;
+	link.arbi = ARBITRATION_WRR;
+	link.prio_wfq = 0;
+
+	if (dp_node_link_add(&link, 0)) {
+		PR_ERR("dp_node_link_add_32 fail: cqm_deq_port=%d\n",
+		       info->cqe_deq);
+		return -1;
+	}
+	//info->f_deq_port_en = 1;
+	info->qid = link.node_id.q_id;
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "qid=%d p_node_id=%d for cqm port=%d\n",
+		 link.node_id.q_id,
+		 link.p_node_id.cqm_deq_port, info->cqe_deq);
+	return 0;
+}
+#endif /*CONFIG_INTEL_DATAPATH_QOS_HAL*/
+#define PRIO0	0
+#define PRIO1	1
+#define PRIO2	2
+#define PRIO3	3
+#define PRIO4	4
+#define PRIO5	5
+#define PRIO6	6
+#define PRIO7	7
+
+static int dp_gswip_remark_8P0D_set(int mode, int inst)
+{
+	struct core_ops *gsw_handle;
+	GSW_QoS_colorRemarkingEntry_t color_remark = {0};
+
+	gsw_handle = dp_port_prop[inst].ops[0];
+	color_remark.eMode = mode;
+	color_remark.nVal[0] = 0;
+	color_remark.nVal[1] = 2;
+	color_remark.nVal[2] = 4;
+	color_remark.nVal[3] = 6;
+	color_remark.nVal[4] = 8;
+	color_remark.nVal[5] = 10;
+	color_remark.nVal[6] = 12;
+	color_remark.nVal[7] = 14;
+	color_remark.nVal[8] = 1;
+	color_remark.nVal[9] = 3;
+	color_remark.nVal[10] = 5;
+	color_remark.nVal[11] = 7;
+	color_remark.nVal[12] = 9;
+	color_remark.nVal[13] = 11;
+	color_remark.nVal[14] = 13;
+	color_remark.nVal[15] = 15;
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
+				.QOS_ColorReMarkingTableSet,
+				gsw_handle, &color_remark)) {
+		PR_ERR("GSW_QOS_COLOR_REMARKING_CFG_SET failed\n");
+		return -1;
+	}
+	return 0;
+}
+
+static int dp_gswip_remark_7P1D_set(int mode, int inst)
+{
+	struct core_ops *gsw_handle;
+	GSW_QoS_colorRemarkingEntry_t color_remark = {0};
+
+	gsw_handle = dp_port_prop[inst].ops[0];
+	color_remark.eMode = mode;
+	color_remark.nVal[0] = 0;
+	color_remark.nVal[1] = 2;
+	color_remark.nVal[2] = 4;
+	color_remark.nVal[3] = 6;
+	color_remark.nVal[4] = 10;
+	color_remark.nVal[5] = 10;
+	color_remark.nVal[6] = 12;
+	color_remark.nVal[7] = 14;
+	color_remark.nVal[8] = 1;
+	color_remark.nVal[9] = 3;
+	color_remark.nVal[10] = 5;
+	color_remark.nVal[11] = 7;
+	color_remark.nVal[12] = 9;
+	color_remark.nVal[13] = 9;
+	color_remark.nVal[14] = 13;
+	color_remark.nVal[15] = 15;
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
+				.QOS_ColorReMarkingTableSet,
+				gsw_handle, &color_remark)) {
+		PR_ERR("GSW_QOS_COLOR_REMARKING_CFG_SET failed\n");
+		return -1;
+	}
+	return 0;
+}
+
+static int dp_gswip_remark_6P2D_set(int mode, int inst)
+{
+	struct core_ops *gsw_handle;
+	GSW_QoS_colorRemarkingEntry_t color_remark = {0};
+
+	gsw_handle = dp_port_prop[inst].ops[0];
+	color_remark.eMode = mode;
+	color_remark.nVal[0] = 0;
+	color_remark.nVal[1] = 2;
+	color_remark.nVal[2] = 6;
+	color_remark.nVal[3] = 6;
+	color_remark.nVal[4] = 10;
+	color_remark.nVal[5] = 10;
+	color_remark.nVal[6] = 12;
+	color_remark.nVal[7] = 14;
+	color_remark.nVal[8] = 1;
+	color_remark.nVal[9] = 3;
+	color_remark.nVal[10] =	5;
+	color_remark.nVal[11] = 5;
+	color_remark.nVal[12] = 9;
+	color_remark.nVal[13] = 9;
+	color_remark.nVal[14] = 13;
+	color_remark.nVal[15] = 15;
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
+				.QOS_ColorReMarkingTableSet,
+				gsw_handle, &color_remark)) {
+		PR_ERR("GSW_QOS_COLOR_REMARKING_CFG_SET failed\n");
+		return -1;
+	}
+	return 0;
+}
+
+static int dp_gswip_remark_5P3D_set(int mode, int inst)
+{
+	struct core_ops *gsw_handle;
+	GSW_QoS_colorRemarkingEntry_t color_remark = {0};
+
+	gsw_handle = dp_port_prop[inst].ops[0];
+	color_remark.eMode = mode;
+	color_remark.nVal[0] = 2;
+	color_remark.nVal[1] = 2;
+	color_remark.nVal[2] = 6;
+	color_remark.nVal[3] = 6;
+	color_remark.nVal[4] = 10;
+	color_remark.nVal[5] = 10;
+	color_remark.nVal[6] = 12;
+	color_remark.nVal[7] = 14;
+	color_remark.nVal[8] = 1;
+	color_remark.nVal[9] = 1;
+	color_remark.nVal[10] = 5;
+	color_remark.nVal[11] = 5;
+	color_remark.nVal[12] = 9;
+	color_remark.nVal[13] = 9;
+	color_remark.nVal[14] = 13;
+	color_remark.nVal[15] = 15;
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
+				.QOS_ColorReMarkingTableSet,
+				gsw_handle, &color_remark)) {
+		PR_ERR("GSW_QOS_COLOR_REMARKING_CFG_SET failed\n");
+		return -1;
+	}
+	return 0;
+}
+
+static int dp_gswip_remark_dscp_set(int mode, int inst)
+{
+	struct core_ops *gsw_handle;
+	GSW_QoS_colorRemarkingEntry_t color_remark = {0};
+
+	gsw_handle = dp_port_prop[inst].ops[0];
+	color_remark.eMode = mode;
+	color_remark.nVal[0] = 10;
+	color_remark.nVal[1] = 18;
+	color_remark.nVal[2] = 26;
+	color_remark.nVal[3] = 34;
+	color_remark.nVal[4] = 34;
+	color_remark.nVal[5] = 34;
+	color_remark.nVal[6] = 34;
+	color_remark.nVal[7] = 34;
+	color_remark.nVal[8] = 12;
+	color_remark.nVal[9] = 20;
+	color_remark.nVal[10] = 28;
+	color_remark.nVal[11] = 36;
+	color_remark.nVal[12] = 36;
+	color_remark.nVal[13] = 36;
+	color_remark.nVal[14] = 36;
+	color_remark.nVal[15] = 36;
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
+				.QOS_ColorReMarkingTableSet,
+				gsw_handle, &color_remark)) {
+		PR_ERR("GSW_QOS_COLOR_REMARKING_CFG_SET failed\n");
+		return -1;
+	}
+	return 0;
+}
+
+static int dp_gswip_color_dscp_set(int mode, int inst)
+{
+	struct core_ops *gsw_handle;
+	GSW_QoS_colorMarkingEntry_t color_mark = {0};
+
+	gsw_handle = dp_port_prop[inst].ops[0];
+	color_mark.eMode = mode;
+	color_mark.nPriority[0] = PRIO0;
+	color_mark.nPriority[1] = PRIO0;
+	color_mark.nPriority[2] = PRIO0;
+	color_mark.nPriority[3] = PRIO0;
+	color_mark.nPriority[4] = PRIO0;
+	color_mark.nPriority[5] = PRIO0;
+	color_mark.nPriority[6] = PRIO0;
+	color_mark.nPriority[7] = PRIO0;
+	color_mark.nPriority[8] = PRIO0;
+	color_mark.nPriority[9] = PRIO0;
+	color_mark.nPriority[10] = PRIO0;
+	color_mark.nPriority[11] = PRIO0;
+	color_mark.nPriority[12] = PRIO0;
+	color_mark.nPriority[13] = PRIO0;
+	color_mark.nPriority[14] = PRIO0;
+	color_mark.nPriority[15] = PRIO0;
+	color_mark.nPriority[16] = PRIO0;
+	color_mark.nPriority[17] = PRIO0;
+	color_mark.nPriority[18] = PRIO1;
+	color_mark.nPriority[19] = PRIO0;
+	color_mark.nPriority[20] = PRIO1;
+	color_mark.nPriority[21] = PRIO0;
+	color_mark.nPriority[22] = PRIO1;
+	color_mark.nPriority[23] = PRIO0;
+	color_mark.nPriority[24] = PRIO0;
+	color_mark.nPriority[25] = PRIO0;
+	color_mark.nPriority[26] = PRIO2;
+	color_mark.nPriority[27] = PRIO0;
+	color_mark.nPriority[28] = PRIO2;
+	color_mark.nPriority[29] = PRIO0;
+	color_mark.nPriority[30] = PRIO2;
+	color_mark.nPriority[31] = PRIO0;
+	color_mark.nPriority[32] = PRIO0;
+	color_mark.nPriority[33] = PRIO0;
+	color_mark.nPriority[34] = PRIO3;
+	color_mark.nPriority[35] = PRIO0;
+	color_mark.nPriority[36] = PRIO3;
+	color_mark.nPriority[37] = PRIO0;
+	color_mark.nPriority[38] = PRIO3;
+	color_mark.nPriority[39] = PRIO0;
+	color_mark.nPriority[40] = PRIO0;
+	color_mark.nPriority[41] = PRIO0;
+	color_mark.nPriority[42] = PRIO0;
+	color_mark.nPriority[43] = PRIO0;
+	color_mark.nPriority[44] = PRIO0;
+	color_mark.nPriority[45] = PRIO0;
+	color_mark.nPriority[46] = PRIO0;
+	color_mark.nPriority[47] = PRIO0;
+	color_mark.nPriority[48] = PRIO0;
+	color_mark.nPriority[49] = PRIO0;
+	color_mark.nPriority[50] = PRIO0;
+	color_mark.nPriority[51] = PRIO0;
+	color_mark.nPriority[52] = PRIO0;
+	color_mark.nPriority[53] = PRIO0;
+	color_mark.nPriority[54] = PRIO0;
+	color_mark.nPriority[55] = PRIO0;
+	color_mark.nPriority[56] = PRIO0;
+	color_mark.nPriority[57] = PRIO0;
+	color_mark.nPriority[58] = PRIO0;
+	color_mark.nPriority[59] = PRIO0;
+	color_mark.nPriority[60] = PRIO0;
+	color_mark.nPriority[61] = PRIO0;
+	color_mark.nPriority[62] = PRIO0;
+	color_mark.nPriority[63] = PRIO0;
+	color_mark.nColor[0] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[1] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[2] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[3] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[4] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[5] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[6] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[7] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[8] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[9] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[10] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[11] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[12] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[13] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[14] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[15] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[16] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[17] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[18] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[19] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[20] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[21] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[22] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[23] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[24] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[25] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[26] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[27] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[28] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[29] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[30] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[31] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[32] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[33] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[34] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[35] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[36] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[37] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[38] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[39] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[40] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[41] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[42] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[43] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[44] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[45] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[46] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[47] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[48] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[49] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[50] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[51] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[52] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[53] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[54] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[55] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[56] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[57] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[58] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[59] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[60] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[61] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[62] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[63] = GSW_DROP_PRECEDENCE_YELLOW;
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
+				.QOS_ColorMarkingTableSet,
+				gsw_handle, &color_mark)) {
+		PR_ERR("GSW_QOS_COLOR_MARKING_CFG_SET failed\n");
+		return -1;
+	}
+	return 0;
+}
+
+static int dp_gswip_color_5P3D_set(int mode, int inst)
+{
+	struct core_ops *gsw_handle;
+	GSW_QoS_colorMarkingEntry_t color_mark = {0};
+
+	gsw_handle = dp_port_prop[inst].ops[0];
+	color_mark.eMode = mode;
+	color_mark.nPriority[0] = PRIO0;
+	color_mark.nPriority[1] = PRIO0;
+	color_mark.nPriority[2] = PRIO0;
+	color_mark.nPriority[3] = PRIO0;
+	color_mark.nPriority[4] = PRIO2;
+	color_mark.nPriority[5] = PRIO2;
+	color_mark.nPriority[6] = PRIO2;
+	color_mark.nPriority[7] = PRIO2;
+	color_mark.nPriority[8] = PRIO4;
+	color_mark.nPriority[9] = PRIO4;
+	color_mark.nPriority[10] = PRIO4;
+	color_mark.nPriority[11] = PRIO4;
+	color_mark.nPriority[12] = PRIO6;
+	color_mark.nPriority[13] = PRIO6;
+	color_mark.nPriority[14] = PRIO7;
+	color_mark.nPriority[15] = PRIO7;
+	color_mark.nColor[0] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[1] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[2] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[3] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[4] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[5] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[6] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[7] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[8] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[9] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[10] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[11] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[12] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[13] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[14] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[15] = GSW_DROP_PRECEDENCE_YELLOW;
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
+				.QOS_ColorMarkingTableSet,
+				gsw_handle, &color_mark)) {
+		PR_ERR("GSW_QOS_COLOR_MARKING_CFG_SET failed\n");
+		return -1;
+	}
+	return 0;
+}
+
+static int dp_gswip_color_6P2D_set(int mode, int inst)
+{
+	struct core_ops *gsw_handle;
+	GSW_QoS_colorMarkingEntry_t color_mark = {0};
+
+	gsw_handle = dp_port_prop[inst].ops[0];
+	color_mark.eMode = mode;
+	color_mark.nPriority[0] = PRIO0;
+	color_mark.nPriority[1] = PRIO0;
+	color_mark.nPriority[2] = PRIO1;
+	color_mark.nPriority[3] = PRIO1;
+	color_mark.nPriority[4] = PRIO2;
+	color_mark.nPriority[5] = PRIO2;
+	color_mark.nPriority[6] = PRIO2;
+	color_mark.nPriority[7] = PRIO2;
+	color_mark.nPriority[8] = PRIO4;
+	color_mark.nPriority[9] = PRIO4;
+	color_mark.nPriority[10] = PRIO4;
+	color_mark.nPriority[11] = PRIO4;
+	color_mark.nPriority[12] = PRIO6;
+	color_mark.nPriority[13] = PRIO6;
+	color_mark.nPriority[14] = PRIO7;
+	color_mark.nPriority[15] = PRIO7;
+	color_mark.nColor[0] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[1] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[2] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[3] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[4] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[5] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[6] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[7] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[8] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[9] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[10] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[11] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[12] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[13] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[14] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[15] = GSW_DROP_PRECEDENCE_YELLOW;
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
+				.QOS_ColorMarkingTableSet,
+				gsw_handle, &color_mark)) {
+		PR_ERR("GSW_QOS_COLOR_MARKING_CFG_SET failed\n");
+		return -1;
+	}
+	return 0;
+}
+
+static int dp_gswip_color_7P1D_set(int mode, int inst)
+{
+	struct core_ops *gsw_handle;
+	GSW_QoS_colorMarkingEntry_t color_mark = {0};
+
+	gsw_handle = dp_port_prop[inst].ops[0];
+	color_mark.eMode = mode;
+	color_mark.nPriority[0] = PRIO0;
+	color_mark.nPriority[1] = PRIO0;
+	color_mark.nPriority[2] = PRIO1;
+	color_mark.nPriority[3] = PRIO1;
+	color_mark.nPriority[4] = PRIO2;
+	color_mark.nPriority[5] = PRIO2;
+	color_mark.nPriority[6] = PRIO3;
+	color_mark.nPriority[7] = PRIO3;
+	color_mark.nPriority[8] = PRIO4;
+	color_mark.nPriority[9] = PRIO4;
+	color_mark.nPriority[10] = PRIO4;
+	color_mark.nPriority[11] = PRIO4;
+	color_mark.nPriority[12] = PRIO6;
+	color_mark.nPriority[13] = PRIO6;
+	color_mark.nPriority[14] = PRIO7;
+	color_mark.nPriority[15] = PRIO7;
+	color_mark.nColor[0] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[1] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[2] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[3] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[4] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[5] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[6] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[7] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[8] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[9] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[10] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[11] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[12] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[13] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[14] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[15] = GSW_DROP_PRECEDENCE_YELLOW;
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
+				.QOS_ColorMarkingTableSet,
+				gsw_handle, &color_mark)) {
+		PR_ERR("GSW_QOS_COLOR_MARKING_CFG_SET failed\n");
+		return -1;
+	}
+	return 0;
+}
+
+static int dp_gswip_color_8P0D_set(int mode, int inst)
+{
+	struct core_ops *gsw_handle;
+	GSW_QoS_colorMarkingEntry_t color_mark = {0};
+
+	gsw_handle = dp_port_prop[inst].ops[0];
+	color_mark.eMode = mode;
+	color_mark.nPriority[0] = PRIO0;
+	color_mark.nPriority[1] = PRIO0;
+	color_mark.nPriority[2] = PRIO1;
+	color_mark.nPriority[3] = PRIO1;
+	color_mark.nPriority[4] = PRIO2;
+	color_mark.nPriority[5] = PRIO2;
+	color_mark.nPriority[6] = PRIO3;
+	color_mark.nPriority[7] = PRIO3;
+	color_mark.nPriority[8] = PRIO4;
+	color_mark.nPriority[9] = PRIO4;
+	color_mark.nPriority[10] = PRIO5;
+	color_mark.nPriority[11] = PRIO5;
+	color_mark.nPriority[12] = PRIO6;
+	color_mark.nPriority[13] = PRIO6;
+	color_mark.nPriority[14] = PRIO7;
+	color_mark.nPriority[15] = PRIO7;
+	color_mark.nColor[0] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[1] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[2] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[3] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[4] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[5] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[6] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[7] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[8] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[9] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[10] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[11] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[12] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[13] = GSW_DROP_PRECEDENCE_YELLOW;
+	color_mark.nColor[14] = GSW_DROP_PRECEDENCE_GREEN;
+	color_mark.nColor[15] = GSW_DROP_PRECEDENCE_YELLOW;
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
+				.QOS_ColorMarkingTableSet,
+				gsw_handle, &color_mark)) {
+		PR_ERR("GSW_QOS_COLOR_MARKING_CFG_SET failed\n");
+		return -1;
+	}
+	return 0;
+}
+
+static int dp_platform_color_table_set(int inst)
+{
+	dp_gswip_color_8P0D_set(GSW_MARKING_PCP_8P0D, inst);
+	dp_gswip_color_7P1D_set(GSW_MARKING_PCP_7P1D, inst);
+	dp_gswip_color_6P2D_set(GSW_MARKING_PCP_6P2D, inst);
+	dp_gswip_color_5P3D_set(GSW_MARKING_PCP_5P3D, inst);
+	dp_gswip_color_dscp_set(GSW_MARKING_DSCP_AF, inst);
+	dp_gswip_remark_8P0D_set(GSW_MARKING_PCP_8P0D, inst);
+	dp_gswip_remark_7P1D_set(GSW_MARKING_PCP_7P1D, inst);
+	dp_gswip_remark_6P2D_set(GSW_MARKING_PCP_6P2D, inst);
+	dp_gswip_remark_5P3D_set(GSW_MARKING_PCP_5P3D, inst);
+	dp_gswip_remark_dscp_set(GSW_MARKING_DSCP_AF, inst);
+
+	return 0;
+}
+#define PER_CPU_PORTS 2 /* cqm dequeue ports per CPU*/
+
+int dp_platform_queue_set_32(int inst, u32 flag)
+{
+	int ret, i, j, vap, gpid_base;
+	u32 mode;
+	struct ppv4_queue q = {0};
+	cbm_queue_map_entry_t lookup = {0};
+	struct cbm_cpu_port_data cpu = {0};
+	struct ppv4_q_sch_port q_port = {0};
+	u8 f_cpu_q = 0;
+	struct cbm_dp_en_data en_data = {0};
+	struct hal_priv *priv = (struct hal_priv *)dp_port_prop[inst].priv_hal;
+	struct pmac_port_info *port_info;
+	struct dp_subif_info *subif_info;
+	struct dp_dflt_hostif hostif = {0};
+	int vap_num = 0;
+	struct ctp_assign *cpu_assign = get_ctp_assign(DP_F_CPU);
+
+	if (!cpu_assign) {
+		DP_ERR("cpu_assign NULL\n");
+		return 0;
+	}
+	port_info = get_dp_port_info(inst, CPU_PORT);
+	if ((flag & DP_PLATFORM_DE_INIT) == DP_PLATFORM_DE_INIT) {
+		PR_ERR("Need to free resoruce in the future\n");
+		return 0;
+	}
+
+	/*Allocate a drop queue*/
+	if (priv->ppv4_drop_q < 0) {
+		q.parent = 0;
+		q.inst = inst;
+		if (dp_pp_alloc_queue_32(&q)) {
+			PR_ERR("%s fail to alloc a drop queue ??\n",
+			       "dp_pp_alloc_queue_32");
+			return -1;
+		}
+		priv->ppv4_drop_q = q.qid;
+	} else {
+		PR_INFO("drop queue/port: %d/%d\n", priv->ppv4_drop_q,
+			priv->cqm_drop_p);
+	}
+	/*Map all lookup entry to drop queue at the beginning*/
+	lookup.egflag = 0;
+	cbm_queue_map_set(dp_port_prop[inst].cbm_inst, priv->ppv4_drop_q,
+			  &lookup,
+			  CBM_QUEUE_MAP_F_FLOWID_L_DONTCARE |
+			  CBM_QUEUE_MAP_F_FLOWID_H_DONTCARE |
+			  CBM_QUEUE_MAP_F_SUBIF_DONTCARE |
+			  CBM_QUEUE_MAP_F_EN_DONTCARE |
+			  CBM_QUEUE_MAP_F_DE_DONTCARE |
+			  CBM_QUEUE_MAP_F_MPE1_DONTCARE |
+			  CBM_QUEUE_MAP_F_MPE2_DONTCARE |
+			  CBM_QUEUE_MAP_F_EP_DONTCARE |
+			  CBM_QUEUE_MAP_F_TC_DONTCARE |
+			  CBM_QUEUE_MAP_F_COLOR_DONTCARE);
+	lookup.egflag = 1;
+	cbm_queue_map_set(dp_port_prop[inst].cbm_inst, priv->ppv4_drop_q,
+			  &lookup,
+			  CBM_QUEUE_MAP_F_FLOWID_L_DONTCARE |
+			  CBM_QUEUE_MAP_F_FLOWID_H_DONTCARE |
+			  CBM_QUEUE_MAP_F_SUBIF_DONTCARE |
+			  CBM_QUEUE_MAP_F_EN_DONTCARE |
+			  CBM_QUEUE_MAP_F_DE_DONTCARE |
+			  CBM_QUEUE_MAP_F_MPE1_DONTCARE |
+			  CBM_QUEUE_MAP_F_MPE2_DONTCARE |
+			  CBM_QUEUE_MAP_F_EP_DONTCARE |
+			  CBM_QUEUE_MAP_F_TC_DONTCARE |
+			  CBM_QUEUE_MAP_F_COLOR_DONTCARE);
+
+	/*Set CPU port to Mode0 only*/
+	mode = cpu_assign->lookup_mode;
+	get_dp_port_info(inst, 0)->cqe_lu_mode = mode;
+	get_dp_port_info(inst, 0)->status = PORT_DEV_REGISTERED;
+	lookup.ep = PMAC_CPU_ID;
+	cqm_mode_table_set(dp_port_prop[inst].cbm_inst, &lookup,
+			   mode,
+			  CBM_QUEUE_MAP_F_FLOWID_L_DONTCARE |
+			  CBM_QUEUE_MAP_F_FLOWID_H_DONTCARE |
+			  CBM_QUEUE_MAP_F_SUBIF_DONTCARE |
+			  CBM_QUEUE_MAP_F_EN_DONTCARE |
+			  CBM_QUEUE_MAP_F_DE_DONTCARE |
+			  CBM_QUEUE_MAP_F_MPE1_DONTCARE |
+			  CBM_QUEUE_MAP_F_MPE2_DONTCARE |
+			  CBM_QUEUE_MAP_F_TC_DONTCARE |
+			  CBM_QUEUE_MAP_F_COLOR_DONTCARE);
+
+	/*Alloc queue/scheduler/port per CPU port */
+	cpu.dp_inst = inst;
+	cpu.cbm_inst = dp_port_prop[inst].cbm_inst;
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_DDR_SIMULATE_GSWIP32)
+	cpu.dq_tx_push_info[0].deq_port = 0;
+	cpu.dq_tx_push_info[1].deq_port = -1;
+	cpu.dq_tx_push_info[2].deq_port = -1;
+	cpu.dq_tx_push_info[3].deq_port = -1;
+#else
+	ret = cbm_cpu_port_get(&cpu, 0);
+#endif
+	if (ret == -1) {
+		PR_ERR("%s fail for CPU Port. Why ???\n",
+		       "cbm_cpu_port_get");
+		return -1;
+	}
+	gpid_base= alloc_gpid(inst, DP_DYN_GPID,
+			      CPU_GPID_NUM, CPU_PORT);
+	if (gpid_base == DP_FAILURE) {
+		DP_ERR("alloc_gpid fail for CPU: %d\n", CPU_PORT);
+		return -1;
+	}
+	port_info->gpid_base = gpid_base;
+	port_info->gpid_num = CPU_GPID_NUM;
+	for (i = 0; i < CPU_GPID_NUM; i++) { /* update gpid status table */
+		priv->gp_dp_map[gpid_base + i].dpid = CPU_PORT;
+		priv->gp_dp_map[gpid_base + i].subif = SET_VAP(i,
+			port_info->vap_offset, port_info->vap_mask);
+		get_dp_port_subif(port_info, i)->gpid = gpid_base + i;
+	}
+	port_info->deq_port_base = 0;
+	for (i = 0; i < CQM_MAX_CPU; i++) {
+		for (j = 0; j < PER_CPU_PORTS; j++) {
+			if (cpu.dq_tx_push_info[i][j].deq_port == (s32)-1)
+				continue;
+			vap_num++;
+			DP_DEBUG(DP_DBG_FLAG_QOS, "cpu(%d)(%d) deq_port=%d",
+				 i, j, cpu.dq_tx_push_info[i][j].deq_port);
+			q_port.cqe_deq = cpu.dq_tx_push_info[i][j].deq_port;
+			q_port.tx_pkt_credit = cpu.dq_tx_push_info[i][j].
+								tx_pkt_credit;
+			q_port.tx_ring_addr = cpu.dq_tx_push_info[i][j].
+								txpush_addr_qos;
+			q_port.tx_ring_addr_push = cpu.dq_tx_push_info[i][j].
+								txpush_addr;
+			q_port.tx_ring_size = cpu.dq_tx_push_info[i][j].tx_ring_size;
+
+			/*Sotre Ring Info */
+			dp_deq_port_tbl[inst][q_port.cqe_deq].tx_pkt_credit =
+				cpu.dq_tx_push_info[i][j].tx_pkt_credit;
+			dp_deq_port_tbl[inst][q_port.cqe_deq].txpush_addr =
+				cpu.dq_tx_push_info[i][j].txpush_addr;
+			dp_deq_port_tbl[inst][q_port.cqe_deq].txpush_addr_qos =
+				cpu.dq_tx_push_info[i][j].txpush_addr_qos;
+			dp_deq_port_tbl[inst][q_port.cqe_deq].tx_ring_size =
+				cpu.dq_tx_push_info[i][j].tx_ring_size;
+			dp_deq_port_tbl[inst][q_port.cqe_deq].dp_port =
+				CPU_PORT;
+			DP_DEBUG(DP_DBG_FLAG_QOS, "Store CPU ring info\n");
+			DP_DEBUG(DP_DBG_FLAG_QOS, "  ring_address[%d]=0x%px\n",
+				 q_port.cqe_deq,
+				 dp_deq_port_tbl[inst][q_port.cqe_deq].txpush_addr);
+			DP_DEBUG(DP_DBG_FLAG_QOS, "  ring_address_push[%d]=0x%px\n",
+				 q_port.cqe_deq,
+				 dp_deq_port_tbl[inst][q_port.cqe_deq].txpush_addr_qos);
+			DP_DEBUG(DP_DBG_FLAG_QOS, "  ring_size[%d]=%d\n",
+				 q_port.cqe_deq,
+				 dp_deq_port_tbl[inst][q_port.cqe_deq].tx_ring_size);
+			DP_DEBUG(DP_DBG_FLAG_QOS, "  credit[%d]=%d\n",
+				 q_port.cqe_deq,
+				 dp_deq_port_tbl[inst][q_port.cqe_deq].tx_pkt_credit);
+			q_port.inst = inst;
+			q_port.dp_port = PMAC_CPU_ID;
+			DP_DEBUG(DP_DBG_FLAG_QOS, "CPU[%d] ring addr=%px\n", i,
+				 cpu.dq_tx_push_info[i][j].txpush_addr);
+			DP_DEBUG(DP_DBG_FLAG_QOS, "CPU[%d] ring addr push=%px\n", i,
+				 cpu.dq_tx_push_info[i][j].txpush_addr_qos);
+			q_port.ctp = i; /*fake CTP for CPU port to store its qid*/
+			DP_DEBUG(DP_DBG_FLAG_QOS, "alloc_q_to_port_32...\n");
+			if (alloc_q_to_port_32(&q_port, 0)) { /* q_port.qid */
+				PR_ERR("alloc_q_to_port_32 fail for dp_port=%d\n",
+				       q_port.dp_port);
+				return -1;
+			}
+			port_info->deq_port_num++;
+			vap = PER_CPU_PORTS * i + j;
+			subif_info = get_dp_port_subif(port_info, vap);
+			subif_info->flags = PORT_DEV_REGISTERED;
+			subif_info->qid = q_port.qid;
+			subif_info->q_node = q_port.q_node;
+			subif_info->qos_deq_port = q_port.port_node;
+			subif_info->cqm_deq_port = q_port.cqe_deq;
+			subif_info->policy_base = cpu.policy_base[i][j];
+			subif_info->policy_num = cpu.policy_num[i][j];
+			if(dp_add_pp_gpid(inst, CPU_PORT, vap,
+					  gpid_base + vap, 0)) {
+				DP_ERR("dp_alloc_pp_gpid fail for CPU VAP=%d\n",
+				       vap);
+				return -1;
+			}
+			if (!f_cpu_q && (i == cpu.default_cpu)) {
+				f_cpu_q = 1;
+				/*Map all CPU port's lookup to its 1st queue only */
+				lookup.ep = PMAC_CPU_ID;
+				lookup.egflag = 0;
+				cbm_queue_map_set(dp_port_prop[inst].cbm_inst,
+						  q_port.qid,
+						  &lookup,
+						  CBM_QUEUE_MAP_F_FLOWID_L_DONTCARE |
+						  CBM_QUEUE_MAP_F_FLOWID_H_DONTCARE |
+						  CBM_QUEUE_MAP_F_SUBIF_DONTCARE |
+						  CBM_QUEUE_MAP_F_EN_DONTCARE |
+						  CBM_QUEUE_MAP_F_DE_DONTCARE |
+						  CBM_QUEUE_MAP_F_MPE1_DONTCARE |
+						  CBM_QUEUE_MAP_F_MPE2_DONTCARE |
+						  CBM_QUEUE_MAP_F_TC_DONTCARE |
+						  CBM_QUEUE_MAP_F_COLOR_DONTCARE);
+				lookup.egflag = 1;
+				cbm_queue_map_set(dp_port_prop[inst].cbm_inst,
+						  q_port.qid,
+						  &lookup,
+						  CBM_QUEUE_MAP_F_FLOWID_L_DONTCARE |
+						  CBM_QUEUE_MAP_F_FLOWID_H_DONTCARE |
+						  CBM_QUEUE_MAP_F_SUBIF_DONTCARE |
+						  CBM_QUEUE_MAP_F_EN_DONTCARE |
+						  CBM_QUEUE_MAP_F_DE_DONTCARE |
+						  CBM_QUEUE_MAP_F_MPE1_DONTCARE |
+						  CBM_QUEUE_MAP_F_MPE2_DONTCARE |
+						  CBM_QUEUE_MAP_F_TC_DONTCARE |
+						  CBM_QUEUE_MAP_F_COLOR_DONTCARE);
+				hostif.inst = inst;
+				hostif.gpid = gpid_base + vap;
+				hostif.qid = q_port.qid;
+				hostif.color = PP_COLOR_GREEN;
+				if (dp_add_dflt_hostif(&hostif, 0)) {
+					DP_ERR("dp_add_dflt_hostif fail for CPU VAP=%d\n",
+					       vap);
+					return -1;
+				}
+			}
+			/*Note:
+			 *    CPU port no DMA and don't set en_data.dma_chnl_init to 1
+			 */
+			en_data.cbm_inst = dp_port_prop[inst].cbm_inst;
+			en_data.dp_inst = inst;
+			en_data.deq_port = cpu.dq_tx_push_info[i][j].deq_port;
+			if (cbm_dp_enable(NULL, PMAC_CPU_ID, &en_data, 0, 0)) {
+				PR_ERR("Fail to enable CPU[%d]\n", en_data.deq_port);
+				return -1;
+			}
+		}
+	}
+	DP_INFO("CPU VAP %d are enabled now \n", vap_num);
+
+	return 0;
+}
+
+static int dp_platform_set(int inst, u32 flag)
+{
+	GSW_QoS_portRemarkingCfg_t port_remark;
+	struct core_ops *gsw_handle;
+	struct hal_priv *priv;
+
+	/* For initialize */
+	if ((flag & DP_PLATFORM_INIT) == DP_PLATFORM_INIT) {
+		struct dp_subif_info *sif;
+
+		dp_port_prop[inst].priv_hal =
+			kzalloc(sizeof(*priv), GFP_KERNEL);
+		if (!dp_port_prop[inst].priv_hal) {
+			PR_ERR("kmalloc failed: %zu bytes\n",
+			       sizeof(struct hal_priv));
+			return -1;
+		}
+		priv = (struct hal_priv *)dp_port_prop[inst].priv_hal;
+		priv->ppv4_drop_q = MAX_QUEUE - 1; /*Need change later */
+		gsw_handle = dp_port_prop[inst].ops[0];
+		if (!inst)/*only inst zero need DMA descriptor */
+			init_dma_desc_mask();
+		if (!dp_port_prop[inst].ops[0] ||
+		    !dp_port_prop[inst].ops[1]) {
+			PR_ERR("Why GSWIP handle Zero\n");
+			return -1;
+		}
+		if (!inst)
+			dp_sub_proc_install_32();
+		init_qos_fn_32();
+		/*just for debugging purpose */
+		sif = get_dp_port_subif(get_dp_port_info(inst, 0), 0);
+		sif->bp = CPU_BP;
+		get_dp_port_info(inst, 0)->alloc_flags = DP_F_CPU;
+		sif->mac_learn_dis = DP_MAC_LEARNING_DIS;
+		INIT_LIST_HEAD(&sif->logic_dev);
+
+		priv->bp_def = alloc_bridge_port_32(inst, CPU_PORT, CPU_SUBIF,
+						 CPU_FID, CPU_BP);
+		DP_DEBUG(DP_DBG_FLAG_DBG, "bp_def[%d]=%d\n",
+			 inst, priv->bp_def);
+
+		if (!inst) /*only inst zero will support mib feature */
+			mib_init(0);
+		dp_get_gsw_parser_32(NULL, NULL, NULL, NULL);
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_CPUFREQ)
+	if (!inst)
+		dp_coc_cpufreq_init();
+#endif
+		/*disable egress VLAN modification for CPU port*/
+		port_remark.nPortId = 0;
+		if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
+				 .QoS_PortRemarkingCfgGet,
+				 gsw_handle, &port_remark)) {
+			PR_ERR("GSW_QOS_PORT_REMARKING_CFG_GET failed\n");
+			return -1;
+		}
+		port_remark.bPCP_EgressRemarkingEnable = 0;
+		if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
+				 .QoS_PortRemarkingCfgGet,
+				 gsw_handle, &port_remark)) {
+			PR_ERR("GSW_QOS_PORT_REMARKING_CFG_GET failed\n");
+			return -1;
+		}
+
+		if (init_ppv4_qos_32(inst, flag)) {
+			PR_ERR("init_ppv4_qos_32 fail\n");
+			return -1;
+		}
+		if (dp_platform_queue_set_32(inst, flag)) {
+			PR_ERR("dp_platform_queue_set_32 fail\n");
+
+			return -1;
+		}
+		if (dp_platform_color_table_set(inst)) {
+			PR_ERR("dp_platform_color_table_set fail\n");
+
+			return -1;
+		}
+		if (cpu_vlan_mod_dis_32(inst)) {
+			PR_ERR("cpu_vlan_mod_dis_32 fail\n");
+			return -1;
+		}
+		return 0;
+	}
+
+	/* De-initialize */
+	priv = (struct hal_priv *)dp_port_prop[inst].priv_hal;
+	if (priv->bp_def)
+		free_bridge_port_32(inst, priv->bp_def);
+	init_ppv4_qos_32(inst, flag); /*de-initialize qos */
+	kfree(priv);
+	free_gpid(inst, get_dp_port_info(inst, CPU_PORT)->gpid_base,
+		  get_dp_port_info(inst, CPU_PORT)->gpid_num);
+	get_dp_port_info(inst, CPU_PORT)->gpid_num = 0;
+	dp_port_prop[inst].priv_hal = NULL;
+
+	return 0;
+}
+
+#define DP_GSWIP_CRC_DISABLE 1
+#define DP_GSWIP_FLOW_CTL_DISABLE 4
+static int pon_config(int inst, int ep, struct dp_port_data *data, u32 flags)
+{
+	struct core_ops *gsw_handle;
+	GSW_return_t ret;
+	struct mac_ops *mac_ops;
+	GSW_CPU_PortCfg_t cpu_port_cfg;
+
+	mac_ops = dp_port_prop[inst].mac_ops[ep];
+	gsw_handle = dp_port_prop[inst].ops[GSWIP_L];
+	memset((void *)&cpu_port_cfg, 0x00, sizeof(cpu_port_cfg));
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_common_ops.CPU_PortCfgGet,
+			   gsw_handle, &cpu_port_cfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("fail in getting CPU port config\r\n");
+		return -1;
+	}
+	/* Enable the Egress and Ingress Special Tag */
+	cpu_port_cfg.nPortId = ep;
+	cpu_port_cfg.bSpecialTagIngress = 1;
+	cpu_port_cfg.bSpecialTagEgress = 1;
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_common_ops.CPU_PortCfgSet,
+			   gsw_handle, &cpu_port_cfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("Fail in configuring CPU port\n");
+		return -1;
+	}
+
+	/* Disable Rx CRC check. Value '0'-enable, '1'-disable */
+	mac_ops->set_rx_crccheck(mac_ops, DP_GSWIP_CRC_DISABLE);
+
+	/* TX FCS generation disable, Padding Insertion disable. */
+	if (data->flag_ops & DP_F_DATA_FCS_DISABLE)
+		mac_ops->set_fcsgen(mac_ops, GSW_CRC_PAD_INS_DIS);
+
+	/* Disables RX/TX Flow control */
+	mac_ops->set_flow_ctl(mac_ops, DP_GSWIP_FLOW_CTL_DISABLE);
+
+	/* Replace Tx Special Tag Byte 2 & Byte 3 with packet length */
+	mac_ops->mac_op_cfg(mac_ops, TX_SPTAG_REPLACE);
+
+	/* Indicate GSWIP that packet coming from PON have timestamp
+	 * In acceleration path, GSWIP can remove the timestamp
+	 */
+	mac_ops->mac_op_cfg(mac_ops, RX_TIME_NO_INSERT);
+
+	return 0;
+}
+
+static int dp_port_spl_cfg(int inst, int ep, struct dp_port_data *data,
+			   u32 flags)
+{
+	if (flags & (DP_F_GPON | DP_F_EPON))
+		pon_config(inst, ep, data, flags);
+	return 0;
+}
+
+static int dev_platform_set(int inst, u8 ep, struct dp_dev_data *data,
+			    uint32_t flags)
+{
+	struct gsw_itf *itf;
+	struct hal_priv *priv = (struct hal_priv *)dp_port_prop[inst].priv_hal;
+
+	if (!priv) {
+		PR_ERR("priv is NULL\n");
+		return DP_FAILURE;
+	}
+	if (flags & DP_F_DEREGISTER) {
+		DP_INFO("Need implement in the future\n");
+		return DP_SUCCESS;
+	}
+
+	/* Register */
+	itf = ctp_port_assign_32(inst, ep, priv->bp_def, flags, data);
+	get_dp_port_info(inst, ep)->itf_info = itf;
+	if (gpid_port_assign(inst, ep, flags)) {
+		DP_ERR("gpid_port_assign failed\n");
+		return DP_FAILURE;
+	}
+
+	return DP_SUCCESS;
+}
+
+static int port_platform_set(int inst, u8 ep, struct dp_port_data *data,
+			     u32 flags)
+{
+	int idx, i;
+	u32 mode;
+	cbm_queue_map_entry_t lookup = {0};
+	struct hal_priv *priv = (struct hal_priv *)dp_port_prop[inst].priv_hal;
+	struct pmac_port_info *port_info = get_dp_port_info(inst, ep);
+	u32 dma_chan, dma_ch_base;
+
+	if (!priv) {
+		PR_ERR("priv is NULL\n");
+		return DP_FAILURE;
+	}
+
+	set_port_lookup_mode_32(inst, ep, flags);
+	if (flags & DP_F_DEREGISTER) {
+		dp_node_reserve_32(inst, ep, NULL, flags);
+		return 0;
+	}
+
+	DP_DEBUG(DP_DBG_FLAG_QOS, "priv=%px deq_port_stat=%px qdev=%px\n",
+		 priv,
+		 priv ? priv->deq_port_stat : NULL,
+		 priv ? priv->qdev : NULL);
+	idx = port_info->deq_port_base;
+	dma_chan =  port_info->dma_chan;
+	dma_ch_base = port_info->dma_ch_base;
+	for (i = 0; i < port_info->deq_port_num; i++) {
+		dp_deq_port_tbl[inst][i + idx].txpush_addr =
+			port_info->txpush_addr +
+			(port_info->tx_ring_offset * i);
+		dp_deq_port_tbl[inst][i + idx].txpush_addr_qos =
+			port_info->txpush_addr_qos +
+			(port_info->tx_ring_offset * i);
+		dp_deq_port_tbl[inst][i + idx].tx_ring_size =
+			port_info->tx_ring_size;
+		dp_deq_port_tbl[inst][i + idx].tx_pkt_credit =
+			port_info->tx_pkt_credit;
+		dp_deq_port_tbl[inst][i + idx].dp_port = ep;
+		if (port_info->num_dma_chan > 1) {
+			dp_deq_port_tbl[inst][i	+ idx].dma_chan = dma_chan++;
+			dp_deq_port_tbl[inst][i + idx].dma_ch_offset =
+								dma_ch_base + i;
+		} else if (port_info->num_dma_chan == 1) {
+			dp_deq_port_tbl[inst][i	+ idx].dma_chan	= dma_chan;
+			dp_deq_port_tbl[inst][i + idx].dma_ch_offset =
+								dma_ch_base;
+		}
+		DP_DEBUG(DP_DBG_FLAG_DBG, "deq_port_tbl[%d][%d].dma_chan=%x\n",
+			 inst, (i + idx), dma_chan);
+	}
+	mode = get_dp_port_info(inst, ep)->cqe_lu_mode;
+	lookup.ep = ep;
+	/*Set all mode based on MPE1/2 to same single mode as specified */
+	cqm_mode_table_set(dp_port_prop[inst].cbm_inst, &lookup,
+			   mode,
+			   CBM_QUEUE_MAP_F_MPE1_DONTCARE |
+			   CBM_QUEUE_MAP_F_MPE2_DONTCARE);
+
+	dp_node_reserve_32(inst, ep, data, flags);
+	dp_port_spl_cfg(inst, ep, data, flags);
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_DBG)
+	if (DP_DBG_FLAG_QOS & dp_dbg_flag) {
+		for (i = 0; i < port_info->deq_port_num; i++) {
+			PR_INFO("cqm[%d]: addr/push=%px/%px credit=%d size==%d\n",
+				i + idx,
+				dp_deq_port_tbl[inst][i + idx].txpush_addr,
+				dp_deq_port_tbl[inst][i + idx].txpush_addr_qos,
+				dp_deq_port_tbl[inst][i + idx].tx_pkt_credit,
+				dp_deq_port_tbl[inst][i + idx].tx_ring_size);
+		}
+	}
+#endif
+	return 0;
+}
+
+static int set_ctp_bp(int inst, int ctp, int portid, int bp)
+{
+	GSW_CTP_portConfig_t tmp;
+	struct core_ops *gsw_handle;
+
+	gsw_handle = dp_port_prop[inst].ops[GSWIP_L];
+	memset(&tmp, 0, sizeof(tmp));
+	tmp.nLogicalPortId = portid;
+	tmp.nSubIfIdGroup = ctp;
+	tmp.eMask = GSW_CTP_PORT_CONFIG_MASK_BRIDGE_PORT_ID;
+	tmp.nBridgePortId = bp;
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_ctp_ops.CTP_PortConfigSet,
+			 gsw_handle, &tmp) != 0) {
+		PR_ERR("Failed to CTP(%d)'s bridge port=%d for ep=%d\n",
+		       ctp, bp, portid);
+		return -1;
+	}
+
+	return 0;
+}
+
+static int reset_ctp_bp(int inst, int ctp, int portid, int bp)
+{
+	GSW_CTP_portConfig_t tmp;
+	struct core_ops *gsw_handle;
+
+	gsw_handle = dp_port_prop[inst].ops[GSWIP_L];
+	memset(&tmp, 0, sizeof(tmp));
+	tmp.nLogicalPortId = portid;
+	tmp.nSubIfIdGroup = ctp;
+	tmp.nBridgePortId = bp;
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_ctp_ops.CTP_PortConfigReset,
+			 gsw_handle, &tmp) != 0) {
+		PR_ERR("Failed to reset CTP(%d)'s bridge port=%d for ep=%d\n",
+		       ctp, bp, portid);
+		return -1;
+	}
+
+	return 0;
+}
+
+static char *q_flag_str(int q_flag)
+{
+	if (q_flag == DP_SUBIF_AUTO_NEW_Q)
+		return "auto_new_queue";
+	if (q_flag == DP_SUBIF_SPECIFIC_Q)
+		return "specified_queue";
+	return "sharing_queue";
+}
+
+static int subif_hw_set(int inst, int portid, int subif_ix,
+			struct subif_platform_data *data, u32 flags)
+{
+	struct ppv4_q_sch_port q_port = {0};
+	static cbm_queue_map_entry_t lookup = {0};
+	u32 lookup_f = CBM_QUEUE_MAP_F_FLOWID_L_DONTCARE |
+		CBM_QUEUE_MAP_F_FLOWID_H_DONTCARE |
+		CBM_QUEUE_MAP_F_EN_DONTCARE |
+		CBM_QUEUE_MAP_F_DE_DONTCARE |
+		CBM_QUEUE_MAP_F_MPE1_DONTCARE |
+		CBM_QUEUE_MAP_F_MPE2_DONTCARE |
+		CBM_QUEUE_MAP_F_TC_DONTCARE |
+		CBM_QUEUE_MAP_F_COLOR_DONTCARE;
+	int subif, deq_port_idx = 0, bp = -1;
+	int dma_ch_offset = 0;
+	struct pmac_port_info *port_info;
+	struct hal_priv *priv = HAL(inst);
+	int q_flag = 0;
+	struct dp_subif_info *sif;
+
+	if (!data || !data->subif_data) {
+		PR_ERR("data NULL or subif_data NULL\n");
+		return -1;
+	}
+	if (!dp_dma_chan_tbl[inst]) {
+		PR_ERR("dp_dma_chan_tbl[%d] NULL\n", inst);
+		return DP_FAILURE;
+	}
+	port_info = get_dp_port_info(inst, portid);
+	subif = SET_VAP(subif_ix, port_info->vap_offset,
+			port_info->vap_mask);
+	sif = get_dp_port_subif(port_info, subif_ix);
+
+	if (data->subif_data->ctp_dev) /* for pmapper later */
+		bp = bp_pmapper_dev_get(inst, data->dev);
+	if (bp >= 0) {
+		DP_DEBUG(DP_DBG_FLAG_DBG, "%s Reuse BP(%d) ctp_dev=%s\n",
+			 data->dev ? data->dev->name : "NULL",
+			 bp,
+			 data->subif_data->ctp_dev->name);
+	} else {
+		DP_DEBUG(DP_DBG_FLAG_DBG, "need alloc BP for %s ctp_dev=%s\n",
+			 data->dev ? data->dev->name : "NULL",
+			 data->subif_data->ctp_dev ?
+				data->subif_data->ctp_dev->name : "NULL");
+		sif->mac_learn_dis = data->subif_data->mac_learn_disable;
+		bp = alloc_bridge_port_32(inst, portid,
+				       subif_ix, CPU_FID, CPU_BP);
+		if (bp < 0) {
+			PR_ERR("Fail to alloc bridge port\n");
+			return -1;
+		}
+	}
+	sif->bp = bp;
+	set_ctp_bp(inst, subif_ix, portid,
+		   sif->bp);
+	data->act = 0;
+	if (flags & DP_F_SUBIF_LOGICAL) {
+		PR_ERR("need more for logical dev??\n");
+		return 0;
+	}
+	if (data->subif_data->ctp_dev) {
+		DP_DEBUG(DP_DBG_FLAG_DBG,
+			 "dp_bp_dev_tbl[%d][%d]=%s current reg_cnt=%d\n",
+			 inst, bp, data->dev->name,
+			 dp_bp_dev_tbl[inst][bp].ref_cnt);
+		dp_bp_dev_tbl[inst][bp].dev = data->dev;
+		dp_bp_dev_tbl[inst][bp].ref_cnt++;
+		dp_bp_dev_tbl[inst][bp].flag = 1;
+		sif->ctp_dev =
+			data->subif_data->ctp_dev;
+	}
+	DP_DEBUG(DP_DBG_FLAG_DBG,
+		 "inst=%d portid=%d dp numsubif=%d subif_ix=%d pmapper.cnt=%d\n",
+		 inst, portid,
+		 port_info->num_subif, subif_ix,
+		 dp_bp_dev_tbl[inst][bp].ref_cnt);
+	if (data->subif_data)
+		deq_port_idx = data->subif_data->deq_port_idx;
+	if (port_info->deq_port_num < deq_port_idx + 1) {
+		PR_ERR("Wrong deq_port_idx(%d), should < %d\n",
+		       deq_port_idx, port_info->deq_port_num);
+		return -1;
+	}
+	/*QUEUE_CFG if needed */
+	q_port.cqe_deq = port_info->deq_port_base + deq_port_idx;
+	if (!priv) {
+		PR_ERR("priv NULL\n");
+		return -1;
+	}
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_DBG)
+	if (unlikely(dp_dbg_flag & DP_DBG_FLAG_QOS)) {
+		DP_DEBUG(DP_DBG_FLAG_QOS, "cqe_deq=%d\n", q_port.cqe_deq);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "priv=%px deq_port_stat=%px qdev=%px\n",
+			 priv,
+			 priv ? priv->deq_port_stat : NULL,
+			 priv ? priv->qdev : NULL);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "cqe_deq=%d inst=%d\n",
+			 q_port.cqe_deq, inst);
+	}
+#endif
+	q_port.tx_pkt_credit =
+		dp_deq_port_tbl[inst][q_port.cqe_deq].tx_pkt_credit;
+	q_port.tx_ring_addr =
+		dp_deq_port_tbl[inst][q_port.cqe_deq].txpush_addr_qos;
+	q_port.tx_ring_addr_push =
+		dp_deq_port_tbl[inst][q_port.cqe_deq].txpush_addr;
+	q_port.tx_ring_size =
+		dp_deq_port_tbl[inst][q_port.cqe_deq].tx_ring_size;
+	q_port.inst = inst;
+	q_port.dp_port = portid;
+	q_port.ctp = subif_ix;
+
+	dma_ch_offset = dp_deq_port_tbl[inst][q_port.cqe_deq].dma_ch_offset;
+	if (data->subif_data->flag_ops & DP_SUBIF_SPECIFIC_Q) {
+		q_flag = DP_SUBIF_SPECIFIC_Q;
+	} else if (data->subif_data->flag_ops & DP_SUBIF_AUTO_NEW_Q) {
+		q_flag = DP_SUBIF_AUTO_NEW_Q;
+	}  else { /*sharing mode (default)*/
+		if (!dp_deq_port_tbl[inst][q_port.cqe_deq].f_first_qid)
+			q_flag = DP_SUBIF_AUTO_NEW_Q; /*no queue created yet*/
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS, "Queue decision:%s\n", q_flag_str(q_flag));
+	if (q_flag == DP_SUBIF_AUTO_NEW_Q) {
+		int cqe_deq;
+
+		if (alloc_q_to_port_32(&q_port, 0)) {
+			PR_ERR("alloc_q_to_port_32 fail for dp_port=%d\n",
+			       q_port.dp_port);
+			return -1;
+		}
+		if (dp_q_tbl[inst][q_port.qid].flag) {
+			PR_ERR("Why dp_q_tbl[%d][%d].flag =%d:expect 0?\n",
+			       inst, q_port.qid,
+			       dp_q_tbl[inst][q_port.qid].flag);
+			return -1;
+		}
+		if (dp_q_tbl[inst][q_port.qid].ref_cnt) {
+			PR_ERR("Why dp_q_tbl[%d][%d].ref_cnt =%d:expect 0?\n",
+			       inst, q_port.qid,
+			       dp_q_tbl[inst][q_port.qid].ref_cnt);
+			return -1;
+		}
+		/*update queue table */
+		dp_q_tbl[inst][q_port.qid].flag = 1;
+		dp_q_tbl[inst][q_port.qid].need_free = 1;
+		dp_q_tbl[inst][q_port.qid].ref_cnt = 1;
+		dp_q_tbl[inst][q_port.qid].q_node_id = q_port.q_node;
+		dp_q_tbl[inst][q_port.qid].cqm_dequeue_port = q_port.cqe_deq;
+
+		/*update port table */
+		cqe_deq = q_port.cqe_deq;
+		dp_deq_port_tbl[inst][cqe_deq].ref_cnt++;
+		if (port_info->num_dma_chan)
+			atomic_inc(&(dp_dma_chan_tbl[inst] +
+				   dma_ch_offset)->ref_cnt);
+		dp_deq_port_tbl[inst][cqe_deq].qos_port = q_port.port_node;
+		if (!dp_deq_port_tbl[inst][cqe_deq].f_first_qid) {
+			dp_deq_port_tbl[inst][cqe_deq].first_qid = q_port.qid;
+			dp_deq_port_tbl[inst][cqe_deq].f_first_qid = 1;
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "dp_deq_port_tbl[%d][%d].first_qid=%d\n",
+				 inst, q_port.cqe_deq,
+				 dp_deq_port_tbl[inst][cqe_deq].first_qid);
+		}
+		/*update scheduler table later */
+
+	} else if (q_flag == DP_SUBIF_SPECIFIC_Q) { /*specified queue */
+		if (!dp_q_tbl[inst][q_port.qid].flag) {
+			/*1st time to use it
+			 *In this case, normally this queue is created by caller
+			 */
+			dp_q_tbl[inst][q_port.qid].flag = 1;
+			dp_q_tbl[inst][q_port.qid].need_free = 0; /*caller q*/
+			dp_q_tbl[inst][q_port.qid].ref_cnt = 1;
+
+			/*update port table
+			 *Note: since this queue is created by caller itself
+			 *      we need find way to get cqm_dequeue_port
+			 *      and qos_port later
+			 */
+			/* need set cqm_dequeue_port/qos_port since not fully
+			 * tested
+			 */
+			dp_q_tbl[inst][q_port.qid].cqm_dequeue_port =
+				q_port.cqe_deq;
+			dp_deq_port_tbl[inst][q_port.cqe_deq].qos_port = -1;
+			dp_deq_port_tbl[inst][q_port.cqe_deq].ref_cnt++;
+			if (port_info->num_dma_chan)
+				atomic_inc(&(dp_dma_chan_tbl[inst] +
+					   dma_ch_offset)->ref_cnt);
+		} else {
+			/*note: don't change need_free in this case */
+			dp_q_tbl[inst][q_port.cqe_deq].ref_cnt++;
+			dp_deq_port_tbl[inst][q_port.cqe_deq].ref_cnt++;
+			if (port_info->num_dma_chan)
+				atomic_inc(&(dp_dma_chan_tbl[inst] +
+					   dma_ch_offset)->ref_cnt);
+		}
+
+		/*get already stored q_node_id/qos_port id to q_port
+		 */
+		q_port.q_node = dp_q_tbl[inst][q_port.qid].q_node_id;
+		q_port.port_node =
+			dp_deq_port_tbl[inst][q_port.cqe_deq].qos_port;
+
+		/* need to further set q_port.q_node/port_node
+		 * via special internal QOS HAL API to get it
+		 * since it is created by caller itself\n");
+		 */
+
+	} else { /*auto sharing queue: if go to here, it means sharing queue
+		  *is ready and it is created by previous dp_register_subif_ext
+		  */
+
+		/*get already stored q_node_id/qos_port id to q_port
+		 */
+		q_port.qid = dp_deq_port_tbl[inst][q_port.cqe_deq].first_qid;
+		q_port.q_node = dp_deq_port_tbl[inst][q_port.cqe_deq].q_node;
+		q_port.port_node =
+			dp_deq_port_tbl[inst][q_port.cqe_deq].qos_port;
+		dp_q_tbl[inst][q_port.qid].ref_cnt++;
+		dp_deq_port_tbl[inst][q_port.cqe_deq].ref_cnt++;
+		if (port_info->num_dma_chan)
+			atomic_inc(&(dp_dma_chan_tbl[inst] +
+				   dma_ch_offset)->ref_cnt);
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "%s:%s=%d %s=%d q[%d].cnt=%d cqm_p[%d].cnt=%d tx_dma_chan ref=%d\n",
+		 "subif_hw_set",
+		 "dp_port", portid,
+		 "vap", subif_ix,
+		 q_port.qid, dp_q_tbl[inst][q_port.qid].ref_cnt,
+		 q_port.cqe_deq, dp_deq_port_tbl[inst][q_port.cqe_deq].ref_cnt,
+		 atomic_read(&(dp_dma_chan_tbl[inst] +
+			     dma_ch_offset)->ref_cnt));
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_QOS_HAL)
+	if (dp_deq_port_tbl[inst][q_port.cqe_deq].ref_cnt == 1) /*first CTP*/
+		data->act = TRIGGER_CQE_DP_ENABLE;
+#else
+	if (q_port.f_deq_port_en)
+		data->act = TRIGGER_CQE_DP_ENABLE;
+#endif
+	/* update caller dp_subif_data.q_id with allocated queue number */
+	data->subif_data->q_id = q_port.qid;
+	/*update subif table */
+	sif->qid = q_port.qid;
+	sif->q_node = q_port.q_node;
+	sif->qos_deq_port = q_port.port_node;
+	sif->cqm_deq_port = q_port.cqe_deq;
+	sif->cqm_port_idx = deq_port_idx;
+	sif->policy_num = port_info->policy_num;
+	sif->policy_base = port_info->policy_base;
+	if (dp_subif_pp_set(inst, portid, subif_ix, data, flags)) {
+		DP_ERR("dp_subif_pp_set fail for dpid/vap=%d/%d\n",
+		       portid, subif_ix);
+	}
+	if (subif_ix < port_info->gpid_num)
+		sif->gpid = port_info->gpid_base +
+						       subif_ix;
+	else
+		sif->gpid = port_info->gpid_base +
+						       port_info->gpid_num - 1;
+
+	/* Map this port's lookup to its 1st queue only */
+	//lookup.mode = get_dp_port_info(inst, portid)->cqe_lu_mode; /*no need */
+	lookup.ep = portid;
+	lookup.sub_if_id = subif; /* Note:CQM API need full subif(15bits) */
+	/* For 1st subif and mode 0, use CBM_QUEUE_MAP_F_SUBIF_DONTCARE,
+	 * otherwise, don't use this flag
+	 */
+	if (!get_dp_port_info(inst, portid)->num_subif &&
+	    (get_dp_port_info(inst, portid)->cqe_lu_mode == CQE_LU_MODE0))
+		lookup_f |= CBM_QUEUE_MAP_F_SUBIF_DONTCARE;
+	lookup.egflag = 0;
+	cbm_queue_map_set(dp_port_prop[inst].cbm_inst,
+			  q_port.qid,
+			  &lookup,
+			  lookup_f);
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "%s %s=%d %s=%d %s=%d %s=%d %s=0x%x %s=0x%x(%d) %s=%d\n",
+		 "subif_hw_set",
+		 "qid", q_port.qid,
+		 "for dp_port", lookup.ep,
+		 "num_subif", get_dp_port_info(inst, portid)->num_subif,
+		 "lu_mode", get_dp_port_info(inst, portid)->cqe_lu_mode,
+		 "flag", lookup_f,
+		 "subif", subif, subif_ix,
+		 "egflag", lookup.egflag);
+	lookup.egflag = 1;
+	cbm_queue_map_set(dp_port_prop[inst].cbm_inst,
+			  q_port.qid,
+			  &lookup,
+			  lookup_f);
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "%s %s=%d %s=%d %s=%d %s=%d %s=0x%x %s=0x%x(%d) %s=%d\n",
+		 "subif_hw_set",
+		 "qid", q_port.qid,
+		 "for dp_port", lookup.ep,
+		 "num_subif", get_dp_port_info(inst, portid)->num_subif,
+		 "lu_mode", get_dp_port_info(inst, portid)->cqe_lu_mode,
+		 "flag", lookup_f,
+		 "subif", subif, subif_ix,
+		 "egflag", lookup.egflag);
+	return 0;
+}
+
+static int subif_hw_reset(int inst, int portid, int subif_ix,
+			  struct subif_platform_data *data, u32 flags)
+{
+	int qid;
+	int cqm_deq_port;
+	int dma_ch_offset;
+	struct pmac_port_info *port_info = get_dp_port_info(inst, portid);
+	struct dp_node_alloc node;
+	struct dp_subif_info *sif = get_dp_port_subif(port_info, subif_ix);
+	int bp = sif->bp;
+
+	qid = sif->qid;
+	cqm_deq_port = sif->cqm_deq_port;
+	dma_ch_offset = dp_deq_port_tbl[inst][cqm_deq_port].dma_ch_offset;
+	bp = sif->bp;
+
+	if (!dp_dma_chan_tbl[inst]) {
+		PR_ERR("dp_dma_chan_tbl[%d] NULL\n", inst);
+		return DP_FAILURE;
+	}
+
+	/* santity check table */
+	if (!dp_q_tbl[inst][qid].ref_cnt) {
+		PR_ERR("Why dp_q_tbl[%d][%d].ref_cnt Zero: expect > 0\n",
+		       inst, qid);
+		return DP_FAILURE;
+	}
+	if (!dp_deq_port_tbl[inst][cqm_deq_port].ref_cnt) {
+		PR_ERR("Why dp_deq_port_tbl[%d][%d].ref_cnt Zero\n",
+		       inst, cqm_deq_port);
+		return DP_FAILURE;
+	}
+	if ((sif->ctp_dev) &&
+	    !dp_bp_dev_tbl[inst][bp].ref_cnt) {
+		PR_ERR("Why dp_bp_dev_tbl[%d][%d].ref_cnt =%d\n",
+		       inst, bp, dp_bp_dev_tbl[inst][bp].ref_cnt);
+		return DP_FAILURE;
+	}
+
+	/* update queue/port/sched/bp_pmapper table's ref_cnt */
+	dp_q_tbl[inst][qid].ref_cnt--;
+	dp_deq_port_tbl[inst][cqm_deq_port].ref_cnt--;
+	if (port_info->num_dma_chan)
+		atomic_dec(&(dp_dma_chan_tbl[inst] + dma_ch_offset)->ref_cnt);
+	if (sif->ctp_dev) { /* pmapper */
+		sif->ctp_dev = NULL;
+		dp_bp_dev_tbl[inst][bp].ref_cnt--;
+		if (!dp_bp_dev_tbl[inst][bp].ref_cnt) {
+			dp_bp_dev_tbl[inst][bp].dev = NULL;
+			dp_bp_dev_tbl[inst][bp].flag = 0;
+			DP_DEBUG(DP_DBG_FLAG_REG,
+				 "ctp ref_cnt becomes zero:%s\n",
+				 sif->device_name);
+		}
+	}
+
+	reset_ctp_bp(inst, subif_ix, portid, bp);
+	if (!dp_bp_dev_tbl[inst][bp].dev) /*NULL already, then free it */ {
+		DP_DEBUG(DP_DBG_FLAG_PAE, "Free BP[%d] vap=%d\n",
+			 bp, subif_ix);
+		free_bridge_port_32(inst, bp);
+	}
+	qid = sif->qid;
+	cqm_deq_port = dp_q_tbl[inst][qid].cqm_dequeue_port;
+
+	if (dp_q_tbl[inst][qid].flag &&
+	    !dp_q_tbl[inst][qid].ref_cnt &&/*no one is using */
+	    dp_q_tbl[inst][qid].need_free) {
+		DP_DEBUG(DP_DBG_FLAG_QOS, "Free qid %d\n", qid);
+		node.id.q_id = qid;
+		/*if no subif using this queue, need to delete it*/
+		node.inst = inst;
+		node.dp_port = portid;
+		node.type = DP_NODE_QUEUE;
+		dp_node_free(&node, 0);
+
+		/*update dp_q_tbl*/
+		dp_q_tbl[inst][qid].flag = 0;
+		dp_q_tbl[inst][qid].need_free = 0;
+		if (dp_deq_port_tbl[inst][cqm_deq_port].f_first_qid &&
+		    (dp_deq_port_tbl[inst][cqm_deq_port].first_qid
+		     == qid)) {
+			dp_deq_port_tbl[inst][cqm_deq_port].f_first_qid = 0;
+			dp_deq_port_tbl[inst][cqm_deq_port].first_qid = 0;
+			DP_DEBUG(DP_DBG_FLAG_QOS, "q_id[%d] is freed\n", qid);
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "dp_deq_port_tbl[%d][%d].f_first_qid reset\n",
+				 inst, cqm_deq_port);
+		}
+	} else {
+		DP_DEBUG(DP_DBG_FLAG_QOS, "q_id[%d] dont need freed\n", qid);
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "%s:%s=%d %s=%d q[%d].cnt=%d cqm_p[%d].cnt=%d tx_dma_chan: (ref=%d)\n",
+		 "subif_hw_reset",
+		 "dp_port", portid,
+		 "vap", subif_ix,
+		 qid, dp_q_tbl[inst][qid].ref_cnt,
+		 cqm_deq_port, dp_deq_port_tbl[inst][cqm_deq_port].ref_cnt,
+		 atomic_read(&(dp_dma_chan_tbl[inst] +
+			     dma_ch_offset)->ref_cnt));
+
+	if (!port_info->num_subif &&
+	    dp_deq_port_tbl[inst][cqm_deq_port].ref_cnt) {
+		PR_ERR("num_subif(%d) not match dp_deq_port[%d][%d].ref_cnt\n",
+		       port_info->num_subif,
+		       inst, cqm_deq_port);
+		return DP_FAILURE;
+	}
+
+	return DP_SUCCESS;
+}
+
+/*Set basic BP/CTP */
+static int subif_platform_set(int inst, int portid, int subif_ix,
+			      struct subif_platform_data *data, u32 flags)
+{
+	if (flags & DP_F_DEREGISTER)
+		return subif_hw_reset(inst, portid, subif_ix, data, flags);
+	return subif_hw_set(inst, portid, subif_ix, data, flags);
+}
+
+static int supported_logic_dev(int inst, struct net_device *dev,
+			       char *subif_name)
+{
+	return is_vlan_dev(dev);
+}
+
+static int subif_platform_set_unexplicit(int inst, int port_id,
+					 struct logic_dev *logic_dev,
+					 u32 flags)
+{
+	if (flags & DP_F_DEREGISTER) {
+		free_bridge_port_32(inst, logic_dev->bp);
+	} else {
+		logic_dev->bp =
+			alloc_bridge_port_32(inst, port_id, logic_dev->ctp,
+					  CPU_FID, CPU_BP);
+	}
+
+	return 0;
+}
+
+static int dp_ctp_tc_map_set_32(struct dp_tc_cfg *tc, int flag,
+				struct dp_meter_subif *mtr_subif)
+{
+	struct core_ops *gsw_handle;
+	GSW_CTP_portConfig_t ctp_tc_cfg;
+
+	memset(&ctp_tc_cfg, 0, sizeof(ctp_tc_cfg));
+
+	if (!mtr_subif) {
+		PR_ERR("mtr_subif struct NULL\n");
+		return -1;
+	}
+	if (mtr_subif->subif.flag_pmapper) {
+		PR_ERR("Cannot support ctp tc set for pmmapper dev(%s)\n",
+		       tc->dev ? tc->dev->name : "NULL");
+		return -1;
+	}
+	gsw_handle = dp_port_prop[mtr_subif->inst].ops[GSWIP_L];
+	ctp_tc_cfg.nLogicalPortId = mtr_subif->subif.port_id;
+	ctp_tc_cfg.nSubIfIdGroup = mtr_subif->subif.subif;
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_ctp_ops.CTP_PortConfigGet,
+			 gsw_handle, &ctp_tc_cfg) != 0) {
+		PR_ERR("Failed to get CTP info for ep=%d subif=%d\n",
+		       mtr_subif->subif.port_id, mtr_subif->subif.subif);
+		return -1;
+	}
+	ctp_tc_cfg.eMask = GSW_CTP_PORT_CONFIG_MASK_FORCE_TRAFFIC_CLASS;
+	ctp_tc_cfg.nDefaultTrafficClass = tc->tc;
+	if (tc->force)
+		ctp_tc_cfg.bForcedTrafficClass = tc->force;
+	else
+		ctp_tc_cfg.bForcedTrafficClass = 0;
+
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_ctp_ops.CTP_PortConfigSet,
+			 gsw_handle, &ctp_tc_cfg) != 0) {
+		PR_ERR("CTP tc set fail for ep=%d subif=%d tc=%d force=%d\n",
+		       mtr_subif->subif.port_id, mtr_subif->subif.subif,
+		       tc->tc, tc->force);
+		return -1;
+	}
+	return 0;
+}
+
+static int not_valid_rx_ep(int ep)
+{
+	return (((ep >= 3) && (ep <= 6)) || (ep == 2) || (ep > 15));
+}
+
+static void set_pmac_subif(struct pmac_tx_hdr *pmac, int32_t subif)
+{
+	pmac->src_dst_subif_id_lsb = subif & 0xff;
+	pmac->src_dst_subif_id_msb =  (subif >> 8) & 0x0f;
+	pmac->src_dst_subif_id_msb |= (((subif & 0x8000) >> 15) << 4);
+	pmac->src_dst_subif_id_14_12 =(subif & 0x7000) >> 12;
+}
+
+static void get_pmac_subif(struct pmac_rx_hdr *pmac, int32_t *subif)
+{
+	*subif = pmac->src_dst_subif_id_lsb + ((pmac->src_dst_subif_id_msb & 0x0F) << 8);
+	*subif |= ((pmac->src_dst_subif_id_msb & 0x10 >> 4) << 15);
+	*subif |= (pmac->src_dst_subif_id_14_12 >> 12);
+}
+
+static void update_port_vap(int inst, u32 *ep, int *vap,
+			    struct sk_buff *skb,
+			    struct pmac_rx_hdr *pmac, char *decryp)
+{
+	struct pmac_port_info *pi;
+	int32_t subif;
+#if defined(DP_SKB_HACK)
+	*ep = (skb->DW1 >> 4) & 0xF; /*get the port_id DMA descriptor */
+#endif
+	pi = get_dp_port_info(inst, *ep);
+	if (pi->alloc_flags & DP_F_LOOPBACK) {
+		/*get the real source port from VAP for ipsec */
+		/* related tunnel decap case */
+		if (pmac) {
+			get_pmac_subif(pmac, &subif);
+			*ep = GET_VAP(subif, pi->vap_offset, pi->vap_mask);
+		} else {
+			PR_ERR("Pmac Header is not present\n");
+		}
+		*vap = 0;
+		*decryp = 1;
+	} else {
+		struct dma_rx_desc_0 *desc_0;
+#if defined(DP_SKB_HACK)
+		desc_0 = (struct dma_rx_desc_0 *)&skb->DW0;
+#else
+	//error "Please add proper logic here"
+	return;
+#endif
+	*vap = desc_0->field.dest_sub_if_id;
+	}
+}
+
+static void get_dma_pmac_templ(int index, struct pmac_tx_hdr *pmac,
+			       struct dma_tx_desc_0 *desc_0,
+			       struct dma_tx_desc_1 *desc_1,
+			       struct pmac_port_info *dp_info)
+{
+	if (likely(pmac))
+		memcpy(pmac, &dp_info->pmac_template[index], sizeof(*pmac));
+	desc_0->all = (desc_0->all & dp_info->dma0_mask_template[index].all) |
+				dp_info->dma0_template[index].all;
+	desc_1->all = (desc_1->all & dp_info->dma1_mask_template[index].all) |
+				dp_info->dma1_template[index].all;
+}
+
+static int check_csum_cap(void)
+{
+	return 0;
+}
+
+static int get_itf_start_end(struct gsw_itf *itf_info, u16 *start, u16 *end)
+{
+	if (!itf_info)
+		return -1;
+	if (start)
+		*start = itf_info->start;
+	if (end)
+		*end = itf_info->end;
+
+	return 0;
+}
+
+int register_dp_cap_gswip32(int flag)
+{
+	struct dp_hw_cap cap;
+
+	memset(&cap, 0, sizeof(cap));
+	cap.info.type = GSWIP32_TYPE;
+	cap.info.ver = GSWIP32_VER;
+
+	cap.info.dp_platform_set = dp_platform_set;
+	cap.info.port_platform_set = port_platform_set;
+	cap.info.dev_platform_set = dev_platform_set;
+	cap.info.subif_platform_set_unexplicit = subif_platform_set_unexplicit;
+	cap.info.proc_print_ctp_bp_info = proc_print_ctp_bp_info_32;
+	cap.info.init_dma_pmac_template = init_dma_pmac_template;
+	//dp.info.port_platform_set_unexplicit = port_platform_set_unexplicit;
+	cap.info.subif_platform_set = subif_platform_set;
+	cap.info.init_dma_pmac_template = init_dma_pmac_template;
+	cap.info.not_valid_rx_ep = not_valid_rx_ep;
+	cap.info.set_pmac_subif = set_pmac_subif;
+	cap.info.update_port_vap = update_port_vap;
+	cap.info.check_csum_cap = check_csum_cap;
+	cap.info.get_dma_pmac_templ = get_dma_pmac_templ;
+	cap.info.get_itf_start_end = get_itf_start_end;
+	cap.info.dump_rx_dma_desc = dump_rx_dma_desc_32;
+	cap.info.dump_tx_dma_desc = dump_tx_dma_desc_32;
+	cap.info.dump_rx_pmac = dump_rx_pmac;
+	cap.info.dump_tx_pmac = dump_tx_pmac;
+	cap.info.supported_logic_dev = supported_logic_dev;
+	cap.info.dp_pmac_set = dp_pmac_set_32;
+	cap.info.dp_set_gsw_parser = dp_set_gsw_parser_32;
+	cap.info.dp_get_gsw_parser = dp_get_gsw_parser_32;
+	cap.info.dp_qos_platform_set = qos_platform_set_32;
+	cap.info.dp_set_gsw_pmapper = dp_set_gsw_pmapper_32;
+	cap.info.dp_get_gsw_pmapper = dp_get_gsw_pmapper_32;
+	cap.info.dp_ctp_tc_map_set = dp_ctp_tc_map_set_32;
+	cap.info.dp_meter_alloc = dp_meter_alloc_32;
+	cap.info.dp_meter_add = dp_meter_add_32;
+	cap.info.dp_meter_del = dp_meter_del_32;
+	cap.info.dp_rx = dp_rx_32;
+	cap.info.dp_tx = dp_xmit_32;
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_HAL_GSWIP31_MIB)
+	cap.info.dp_get_port_vap_mib = dp_get_port_vap_mib_32;
+	cap.info.dp_clear_netif_mib = dp_clear_netif_mib_32;
+#endif
+
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_SWITCHDEV)
+	cap.info.swdev_flag = 1;
+	cap.info.swdev_alloc_bridge_id = dp_swdev_alloc_bridge_id_32;
+	cap.info.swdev_free_brcfg = dp_swdev_free_brcfg_32;
+	cap.info.swdev_bridge_cfg_set = dp_swdev_bridge_cfg_set_32;
+	cap.info.swdev_bridge_port_cfg_reset = dp_swdev_bridge_port_cfg_reset_32;
+	cap.info.swdev_bridge_port_cfg_set = dp_swdev_bridge_port_cfg_set_32;
+	cap.info.dp_mac_set = dp_gswip_mac_entry_add_32;
+	cap.info.dp_mac_reset = dp_gswip_mac_entry_del_32;
+	cap.info.dp_cfg_vlan = dp_gswip_ext_vlan_32; /*for symmetric VLAN */
+	cap.info.dp_tc_vlan_set = tc_vlan_set_32;
+#endif
+	cap.info.cap.tx_hw_chksum = 0;
+	cap.info.cap.rx_hw_chksum = 0;
+	cap.info.cap.hw_tso = 0;
+	cap.info.cap.hw_gso = 0;
+	cap.info.cap.hw_ptp = 1;
+	strncpy(cap.info.cap.qos_eng_name, "ppv4",
+		sizeof(cap.info.cap.qos_eng_name));
+	strncpy(cap.info.cap.pkt_eng_name, "mpe",
+		sizeof(cap.info.cap.pkt_eng_name));
+	cap.info.cap.max_num_queues = MAX_QUEUE;
+	cap.info.cap.max_num_scheds = MAX_SCHD;
+	cap.info.cap.max_num_deq_ports = MAX_CQM_DEQ;
+	cap.info.cap.max_num_qos_ports = MAX_PPV4_PORT;
+	cap.info.cap.max_num_dp_ports = PMAC_MAX_NUM;
+	cap.info.cap.max_num_subif_per_port = MAX_SUBIF_PER_PORT;
+	cap.info.cap.max_num_subif = 288;
+	cap.info.cap.max_num_bridge_port = 128;
+
+	if (register_dp_hw_cap(&cap, flag)) {
+		PR_ERR("Why register_dp_hw_cap fail\n");
+		return -1;
+	}
+
+	return 0;
+}
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_misc.h b/drivers/net/datapath/dpm/gswip32/datapath_misc.h
new file mode 100644
index 000000000000..75840485b276
--- /dev/null
+++ b/drivers/net/datapath/dpm/gswip32/datapath_misc.h
@@ -0,0 +1,269 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#ifndef DATAPATH_MISC32_H
+#define DATAPATH_MISC32_H
+#include <linux/pp_api.h>
+#include <linux/pp_buffer_mgr_api.h>
+#include <linux/pp_qos_api.h>
+#include <linux/notifier.h>
+#include <linux/netdevice.h>
+#include "datapath_ppv4.h"
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+#include "datapath_ppv4_session.h"
+
+#define PMAC_MAX_NUM  16
+#define PAMC_LAN_MAX_NUM 7
+#define VAP_OFFSET 8
+#define VAP_MASK  0xF
+#define VAP_DSL_OFFSET 3
+#define NEW_CBM_API 1
+#define PMAPPER_DISC_CTP 255
+#define MAX_PPV4_PORTS 38
+#define PPV4_PORT_BASE 90
+
+#define GSWIP_O_DEV_NAME 1
+#define GSWIP_L GSWIP_O_DEV_NAME
+#define GSWIP_R GSWIP_O_DEV_NAME
+#define MAX_SUBIF_PER_PORT 256
+#define MAX_CTP 288
+#define MAX_BP_NUM 128
+#define MAX_GPID 256
+#define CPU_PORT 0
+#define CPU_GPID_NUM 16
+#define CPU_SUBIF 0 /*cpu default subif ID*/
+#define CPU_BP 0 /*cpu default bridge port ID */
+#define CPU_FID 0 /*cpu default bridge ID */
+#define SET_BP_MAP(x, ix) (x[(ix) / 16] |= 1 << ((ix) % 16))
+#define GET_BP_MAP(x, ix) ((x[(ix) / 16] >> ((ix) % 16)) & 1)
+#define UNSET_BP_MAP(x, ix) (x[(ix) / 16] &= ~(1 << ((ix) % 16)))
+
+enum CQE_LOOKUP_MODE {
+	CQE_LU_MODE0 = 0, /* subif_id[13:8] + class[1:0] */
+	CQE_LU_MODE1, /* subif_id[7:0]*/
+	CQE_LU_MODE2, /* subif_id[11:8] + class[3:0] */
+	CQE_LU_MODE3, /* subif_id[4:0] + class[1:0] */
+	CQE_LU_MODE4,
+	CQE_LU_MODE5,
+	CQE_LU_MODE6,
+	CQE_LU_MODE7,
+	CQE_LU_MODE_NOT_VALID,
+};
+
+struct gsw_itf {
+	u8 ep; /*-1 means no assigned yet for dynamic case */
+	u8 fixed; /*fixed (1) or dynamically allocate (0)*/
+	u16 start;
+	u16 end;
+	u16 n;
+	u8 mode;
+	u8 cqe_mode; /*CQE look up mode */
+	u8 gpid_start;	/* First GPID port */
+	u8 gpid_num;	/* Number of GPID port allocated */
+};
+
+struct cqm_deq_stat;
+struct pp_queue_stat;
+
+struct resv_q {
+	int flag;
+	int id;
+	int physical_id;
+};
+
+struct resv_sch {
+	int flag;
+	int id;
+};
+
+struct resv_info {
+	int num_resv_q; /*!< input:reserve the required number of queues*/
+	int num_resv_sched; /*!< input:reserve required number of schedulers*/
+	struct resv_q  *resv_q; /*!< reserved queues info*/
+	struct resv_sch *resv_sched; /*!< reserved schedulers info */
+};
+
+struct dp_gpid_map_table {
+	int alloc_flags;
+	int dpid;
+	int subif;
+	int ref_cnt; /* reference counter */
+};
+
+struct pp_qos_dev;
+struct hal_priv {
+	struct cqm_deq_stat deq_port_stat[MAX_CQM_DEQ];
+	struct pp_queue_stat qos_queue_stat[MAX_QUEUE];
+	struct pp_sch_stat qos_sch_stat[QOS_MAX_NODES];
+	struct resv_info resv[MAX_DP_PORTS];
+	int bp_def;
+	struct pp_qos_dev *qdev; /* ppv4 qos dev */
+	s32 ppv4_drop_q;  /* drop queue: physical id */
+	int cqm_drop_p; /* cqm drop/flush port id*/
+	u32 ppv4_drop_p;  /* drop qos port(logical node_id):workaround for
+			   * PPV4 API issue to get physical queue id
+			   * before pp_qos_queue_set
+			   */
+	u32 ppv4_tmp_p; /* workaround for ppv4 queue allocate to
+			 * to get physical queue id
+			 */
+	int ppv4_flag[MAX_PPV4_PORTS];
+	struct dp_gpid_map_table gp_dp_map[MAX_GPID]; /* Map Table GPID <-> DPID */
+};
+
+struct datapath_ctrl {
+	struct dentry *debugfs;
+	const char *name;
+};
+
+struct ctp_assign {
+	u32 flag; /*Datapath Device Flag */
+	GSW_LogicalPortMode_t emode; /*mapped GSWIP CTP flag */
+	u16 num; /*Max CTP allowed for that GSWIP logical port*/
+	u32 vap_offset; /*VAP offset */
+	u32 vap_mask;  /*VAP Mask after shift vap_offset bits */
+	u32 lookup_mode; /*CQE lookup mode  */
+	u32 max_gpid; /*Max GPID allowed for that GSWIP logical port*/
+	u16 swdev_enable; /* To enable or disable switchdev feature */
+};
+
+
+#define SET_PMAC_IGP_EGP(pmac, port_id) ((pmac)->igp_egp = (port_id) & 0xF)
+
+#define SET_PMAC_SUBIF(pmac, subif) do { \
+	(pmac)->src_dst_subif_id_lsb = (subif) & 0xff; \
+	(pmac)->src_dst_subif_id_msb =  ((subif) >> 8) & 0x1f; \
+} while (0)
+
+int alloc_bridge_port_32(int inst, int portid, int subif, int fid, int bp_member);
+int free_bridge_port_32(int inst, int bp);
+struct gsw_itf *ctp_port_assign_32(int inst, u8 ep, int bp_default,
+				u32 flags, struct dp_dev_data *data);
+int gpid_port_assign(int inst, u8 ep, u32 flags);
+void dp_sys_mib_reset_32(u32 flag);
+int dp_pmac_set_32(int inst, u32 port, dp_pmac_cfg_t *pmac_cfg);
+int dp_set_gsw_parser_32(u8 flag, u8 cpu, u8 mpe1, u8 mpe2, u8 mpe3);
+int dp_get_gsw_parser_32(u8 *cpu, u8 *mpe1, u8 *mpe2, u8 *mpe3);
+int gsw_mib_reset_32(int dev, u32 flag);
+int proc_print_ctp_bp_info_32(struct seq_file *s, int inst,
+			   struct pmac_port_info *port,
+			   int subif_index, u32 flag);
+
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_SWITCHDEV)
+int dp_gswip_mac_entry_add_32(int bport, int fid, int inst, u8 *addr);
+int dp_gswip_mac_entry_del_32(int bport, int fid, int inst, u8 *addr);
+int set_gswip_ext_vlan_32(struct core_ops *ops, struct ext_vlan_info *vlan,
+		       int flag);
+#endif
+int qos_platform_set_32(int cmd_id, void *node, int flag);
+int dp_node_alloc_32(struct dp_node_alloc *node, int flag);
+int dp_node_free_32(struct dp_node_alloc *node, int flag);
+int dp_deq_port_res_get_32(struct dp_dequeue_res *res, int flag);
+int dp_node_link_en_get_32(struct dp_node_link_enable *en, int flag);
+int dp_node_link_en_set_32(struct dp_node_link_enable *en, int flag);
+int dp_qos_link_prio_set_32(struct dp_node_prio *info, int flag);
+int dp_qos_link_prio_get_32(struct dp_node_prio *info, int flag);
+int dp_node_link_add_32(struct dp_node_link *info, int flag);
+int dp_link_add_32(struct dp_qos_link *cfg, int flag);
+int dp_link_get_32(struct dp_qos_link *cfg, int flag);
+int dp_node_unlink_32(struct dp_node_link *info, int flag);
+int dp_node_link_get_32(struct dp_node_link *info, int flag);
+int dp_queue_conf_set_32(struct dp_queue_conf *cfg, int flag);
+int dp_queue_conf_get_32(struct dp_queue_conf *cfg, int flag);
+int dp_shaper_conf_set_32(struct dp_shaper_conf *cfg, int flag);
+int dp_shaper_conf_get_32(struct dp_shaper_conf *cfg, int flag);
+int dp_queue_map_get_32(struct dp_queue_map_get *cfg, int flag);
+int dp_queue_map_set_32(struct dp_queue_map_set *cfg, int flag);
+int dp_counter_mode_set_32(struct dp_counter_conf *cfg, int flag);
+int dp_counter_mode_get_32(struct dp_counter_conf *cfg, int flag);
+int dp_get_queue_logic_32(struct dp_qos_q_logic *cfg, int flag);
+int dp_set_gsw_pmapper_32(int inst, int bport, int lport,
+			  struct dp_pmapper *mapper, u32 flag);
+int dp_get_gsw_pmapper_32(int inst, int bport, int lport,
+			  struct dp_pmapper *mapper, u32 flag);
+int dp_children_get_32(struct dp_node_child *cfg, int flag);
+int dp_free_children_via_parent_32(struct dp_node_alloc *node, int flag);
+int dp_node_reserve_32(int inst, int ep, struct dp_port_data *data, int flags);
+int dp_qos_level_get_32(struct dp_qos_level *dp, int flag);
+int dp_meter_alloc_32(int inst, int *meterid, int flag);
+int dp_meter_add_32(struct net_device *dev,
+		    struct dp_meter_cfg *meter,
+		    int flag, struct dp_meter_subif *mtr_subif);
+int dp_meter_del_32(struct net_device *dev,
+		    struct dp_meter_cfg *meter,
+		    int flag, struct dp_meter_subif *mtr_subif);
+int dp_qos_global_info_get_32(struct dp_qos_cfg_info *info, int flag);
+int32_t dp_rx_32(struct sk_buff *skb, u32 flags);
+int32_t dp_xmit_32(struct net_device *rx_if, dp_subif_t *rx_subif,
+		struct sk_buff *skb, int32_t len, uint32_t flags);
+void set_chksum(struct pmac_tx_hdr *pmac, u32 tcp_type,
+		       u32 ip_offset, int ip_off_hw_adjust,
+		       u32 tcp_h_offset);
+
+int dp_lan_wan_bridging(int port_id, struct sk_buff *skb);
+
+
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_DDR_SIMULATE_GSWIP32)
+GSW_return_t gsw_core_api_ddr_simu32(dp_gsw_cb func, void *ops, void *param);
+#define GSW_SIMUTE_DDR_NOT_MATCH  0x1234
+#endif
+
+static inline GSW_return_t gsw_core_api(dp_gsw_cb func, void *ops, void *param)
+{
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_DDR_SIMULATE_GSWIP32)
+	{
+		GSW_return_t res;
+
+		res = gsw_core_api_ddr_simu32(func, ops, param);
+		if (res != GSW_SIMUTE_DDR_NOT_MATCH)
+			return res;
+	}
+#endif /*CONFIG_INTEL_DATAPATH_DDR_SIMULATE_GSWIP32*/
+	return func(ops, param);
+}
+
+static inline char *parser_flag_str(u8 f)
+{
+	if (f == DP_PARSER_F_DISABLE)
+		return "No Parser";
+	else if (f == DP_PARSER_F_HDR_ENABLE)
+		return "Parser Flag only";
+	else if (f == DP_PARSER_F_HDR_OFFSETS_ENABLE)
+		return "Parser Full";
+	else
+		return "Reserved";
+}
+
+int dp_sub_proc_install_32(void);
+char *get_dma_flags_str32(u32 epn, char *buf, int buf_len);
+int lookup_dump32(struct seq_file *s, int pos);
+int lookup_start32(void);
+ssize_t proc_get_qid_via_index32(struct file *file, const char *buf,
+				 size_t count, loff_t *ppos);
+ssize_t proc_get_qid_via_index(struct file *file, const char *buf,
+			       size_t count, loff_t *ppos);
+int datapath_debugfs_init(struct datapath_ctrl *pctrl);
+int get_q_mib_32(int inst, int qid,
+	      u32 *total_accept,
+	      u32 *total_drop,
+	      u32 *red_drop);
+int get_p_mib_32(int inst, int pid,
+	      u32 *green /* bytes*/,
+	      u32 *yellow /*bytes*/);
+int cpu_vlan_mod_dis_32(int inst);
+int set_port_lookup_mode_32(int inst, u8 ep, u32 flags);
+int tc_vlan_set_32(struct core_ops *ops, struct dp_tc_vlan *vlan,
+		   struct dp_tc_vlan_info *info,
+		   int flag);
+struct ctp_assign *get_ctp_assign(int flags);
+int dp_get_lookup_qid_via_index(struct cbm_lookup *info);
+void dp_set_lookup_qid_via_index(struct cbm_lookup *info);
+
+#endif
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_ppv4.c b/drivers/net/datapath/dpm/gswip32/datapath_ppv4.c
new file mode 100644
index 000000000000..aacda08ad2b0
--- /dev/null
+++ b/drivers/net/datapath/dpm/gswip32/datapath_ppv4.c
@@ -0,0 +1,682 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include <net/datapath_api.h>
+#include "../datapath.h"
+#include "datapath_misc.h"
+
+int (*qos_queue_remove_32)(struct pp_qos_dev *qos_dev, unsigned int id);
+int (*qos_queue_allocate_32)(struct pp_qos_dev *qos_dev, unsigned int *id);
+int (*qos_queue_info_get_32)(struct pp_qos_dev *qos_dev, unsigned int id,
+			  struct pp_qos_queue_info *info);
+int (*qos_port_remove_32)(struct pp_qos_dev *qos_dev, unsigned int id);
+int (*qos_sched_allocate_32)(struct pp_qos_dev *qos_dev, unsigned int *id);
+int (*qos_sched_remove_32)(struct pp_qos_dev *qos_dev, unsigned int id);
+int (*qos_port_allocate_32)(struct pp_qos_dev *qos_dev,
+			 unsigned int physical_id,
+			 unsigned int *id);
+int (*qos_port_set_32)(struct pp_qos_dev *qos_dev, unsigned int id,
+		    const struct pp_qos_port_conf *conf);
+void (*qos_port_conf_set_default_32)(struct pp_qos_port_conf *conf);
+void (*qos_queue_conf_set_default_32)(struct pp_qos_queue_conf *conf);
+int (*qos_queue_set_32)(struct pp_qos_dev *qos_dev, unsigned int id,
+		     const struct pp_qos_queue_conf *conf);
+void (*qos_sched_conf_set_default_32)(struct pp_qos_sched_conf *conf);
+int (*qos_sched_set_32)(struct pp_qos_dev *qos_dev, unsigned int id,
+		     const struct pp_qos_sched_conf *conf);
+int (*qos_queue_conf_get_32)(struct pp_qos_dev *qos_dev, unsigned int id,
+			  struct pp_qos_queue_conf *conf);
+int (*qos_queue_flush_32)(struct pp_qos_dev *qos_dev, unsigned int id);
+int (*qos_sched_conf_get_32)(struct pp_qos_dev *qos_dev, unsigned int id,
+			  struct pp_qos_sched_conf *conf);
+int (*qos_sched_get_queues_32)(struct pp_qos_dev *qos_dev, unsigned int id,
+			    u16 *queue_ids, unsigned int size,
+			    unsigned int *queues_num);
+int (*qos_port_get_queues_32)(struct pp_qos_dev *qos_dev, unsigned int id,
+			   u16 *queue_ids, unsigned int size,
+				  unsigned int *queues_num);
+int (*qos_port_conf_get_32)(struct pp_qos_dev *qdev, unsigned int id,
+			 struct pp_qos_port_conf *conf);
+int (*qos_port_info_get_32)(struct pp_qos_dev *qdev, unsigned int id,
+			 struct pp_qos_port_info *info);
+struct pp_qos_dev *(*qos_dev_open_32)(unsigned int id);
+
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_DUMMY_QOS)
+struct fixed_q_port {
+	int deq_port; /*cqm dequeue port */
+	int queue_id; /*queue physical id*/
+	int port_node; /*qos dequeue port node id */
+	int queue_node; /*queue node id */
+	int q_used;    /*flag to indicate used or free*/
+};
+
+struct fixed_q_port q_port[] = {
+	{0, 14, 0, 2, 0},
+	{0, 74, 0, 4, 0},
+	{0, 30, 0, 6, 0},
+	{0, 87, 0, 8, 0},
+	{7, 235, 7, 10, 0},
+	{7, 42, 7, 12, 0},
+	{7, 242, 7, 14, 0},
+	{26, 190, 26, 16, 0},
+	{26, 119, 26, 18, 0},
+	{26, 103, 26, 20, 0}
+};
+
+static struct pp_qos_dev qdev;
+
+int test_qos_queue_remove(struct pp_qos_dev *qos_dev, unsigned int id)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(q_port); i++) {
+		if (q_port[i].queue_id == id) {
+			q_port[i].q_used = PP_NODE_FREE;
+			DP_DEBUG(DP_DBG_FLAG_DBG, "to free qid=%d\n", id);
+			return 0;
+		}
+	}
+	return -1;
+}
+
+int test_qos_queue_allocate(struct pp_qos_dev *qos_dev, unsigned int *id)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(q_port); i++) {
+		if ((q_port[i].deq_port == qos_dev->dq_port) &&
+		    (q_port[i].q_used == PP_NODE_FREE)) {
+			q_port[i].q_used = PP_NODE_ALLOC;
+			DP_DEBUG(DP_DBG_FLAG_DBG, "allocate qnode_id=%d\n",
+				 q_port[i].queue_node);
+			*id = q_port[i].queue_node;
+			return 0;
+		}
+	}
+	return -1;
+}
+
+int test_qos_queue_info_get(struct pp_qos_dev *qos_dev, unsigned int id,
+			    struct pp_qos_queue_info *info)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(q_port); i++) {
+		if (q_port[i].queue_node == id) {
+			DP_DEBUG(DP_DBG_FLAG_DBG, "q[%d]'s qid=%d\n",
+				 id, q_port[i].queue_id);
+			info->physical_id = q_port[i].queue_id;
+			return 0;
+		}
+	}
+	return -1;
+}
+
+int test_qos_port_remove(struct pp_qos_dev *qos_dev, unsigned int id)
+{
+	return 0;
+}
+
+int test_qos_sched_allocate(struct pp_qos_dev *qos_dev, unsigned int *id)
+{
+	return 0;
+}
+
+int test_qos_sched_remove(struct pp_qos_dev *qos_dev, unsigned int id)
+{
+	return 0;
+}
+
+int test_qos_port_allocate(struct pp_qos_dev *qos_dev, unsigned int physical_id,
+			   unsigned int *id)
+{
+	int i;
+
+	if (!id)
+		return -1;
+
+	for (i = 0; i < ARRAY_SIZE(q_port); i++) {
+		if (physical_id == q_port[i].deq_port) {
+			*id = q_port[i].port_node;
+			DP_DEBUG(DP_DBG_FLAG_DBG,
+				 "Ok: Deq_port/Node_id=%d/%d\n",
+				 physical_id, *id);
+			return 0;
+		}
+	}
+	return -1;
+}
+
+int test_qos_port_set(struct pp_qos_dev *qos_dev, unsigned int id,
+		      const struct pp_qos_port_conf *conf)
+{
+	return 0;
+}
+
+void test_qos_port_conf_set_default(struct pp_qos_port_conf *conf)
+{
+}
+
+void test_qos_queue_conf_set_default(struct pp_qos_queue_conf *conf)
+{
+}
+
+int test_qos_queue_set(struct pp_qos_dev *qos_dev, unsigned int id,
+		       const struct pp_qos_queue_conf *conf)
+{
+	return 0;
+}
+
+void test_qos_sched_conf_set_default(struct pp_qos_sched_conf *conf)
+{
+}
+
+int test_qos_sched_set(struct pp_qos_dev *qos_dev, unsigned int id,
+		       const struct pp_qos_sched_conf *conf)
+{
+	return 0;
+}
+
+int test_qos_queue_conf_get(struct pp_qos_dev *qos_dev, unsigned int id,
+			    struct pp_qos_queue_conf *conf)
+{
+	int i;
+
+	if (!conf)
+		return -1;
+	memset(conf, 0, sizeof(*conf));
+	for (i = 0; i < ARRAY_SIZE(q_port); i++) {
+		if (id == q_port[i].queue_node) {
+			conf->queue_child_prop.parent = q_port[i].port_node;
+			conf->common_prop.bandwidth_limit = 0;
+			conf->blocked = 0;
+			return 0;
+		}
+	}
+	return -1;
+}
+
+int test_qos_queue_flush(struct pp_qos_dev *qos_dev, unsigned int id)
+{
+	return 0;
+}
+
+int test_qos_sched_conf_get(struct pp_qos_dev *qos_dev, unsigned int id,
+			    struct pp_qos_sched_conf *conf)
+{
+	return -1;
+}
+
+int test_qos_sched_get_queues(struct pp_qos_dev *qos_dev, unsigned int id,
+			      u16 *queue_ids, unsigned int size,
+			    unsigned int *queues_num)
+{
+	return 0;
+}
+
+int test_qos_port_get_queues(struct pp_qos_dev *qos_dev, unsigned int id,
+			     u16 *queue_ids, unsigned int size,
+			   unsigned int *queues_num)
+{
+	int i, num = 0;
+
+	for (i = 0; i < ARRAY_SIZE(q_port); i++) {
+		if (q_port[i].port_node != id)
+			continue;
+		if (queue_ids && (num < size)) {
+			queue_ids[num] = q_port[i].queue_node;
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "saved[%d] qid[%d/%d] for cqm[%d/%d]\n",
+				 num,
+				 q_port[i].queue_id,
+				 q_port[i].queue_node,
+				 q_port[i].deq_port,
+				 q_port[i].port_node);
+		} else {
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "unsaved[%d]: qid[%d/%d] for cqm[%d/%d]\n",
+				 num,
+				 q_port[i].queue_id,
+				 q_port[i].queue_node,
+				 q_port[i].deq_port,
+				 q_port[i].port_node);
+		}
+		num++;
+	}
+	if (queues_num)
+		*queues_num = num;
+	return 0;
+}
+
+int test_qos_port_conf_get(struct pp_qos_dev *qdev, unsigned int id,
+			   struct pp_qos_port_conf *conf)
+{
+	return 0;
+}
+
+int test_qos_port_info_get(struct pp_qos_dev *qdev, unsigned int id,
+			   struct pp_qos_port_info *info)
+{
+	return 0;
+}
+
+/*this test code is only support one instance */
+struct pp_qos_dev *test_qos_dev_open(unsigned int id)
+{
+	return &qdev;
+}
+
+int test_qos_dev_init(struct pp_qos_dev *qos_dev,
+		      struct pp_qos_init_param *conf)
+{
+	return 0;
+}
+#endif /*CONFIG_INTEL_DATAPATH_DUMMY_QOS*/
+
+void init_qos_fn_32(void)
+{
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_DUMMY_QOS)
+	qos_queue_remove_32 = test_qos_queue_remove;
+	qos_queue_allocate_32 = test_qos_queue_allocate;
+	qos_queue_info_get_32 = test_qos_queue_info_get;
+	qos_port_remove_32 = test_qos_port_remove;
+	qos_sched_allocate_32 = test_qos_sched_allocate;
+	qos_sched_remove_32 = test_qos_sched_remove;
+	qos_port_allocate_32 = test_qos_port_allocate;
+	qos_port_set_32 = test_qos_port_set;
+	qos_port_conf_set_default_32 = test_qos_port_conf_set_default;
+	qos_queue_conf_set_default_32 = test_qos_queue_conf_set_default;
+	qos_queue_set_32 = test_qos_queue_set;
+	qos_sched_conf_set_default_32 = test_qos_sched_conf_set_default;
+	qos_sched_set_32 = test_qos_sched_set;
+	qos_queue_conf_get_32 = test_qos_queue_conf_get;
+	qos_queue_flush_32 = test_qos_queue_flush;
+	qos_sched_conf_get_32 = test_qos_sched_conf_get;
+	qos_sched_get_queues_32 = test_qos_sched_get_queues;
+	qos_port_get_queues_32 = test_qos_port_get_queues;
+	qos_port_conf_get_32 = test_qos_port_conf_get;
+	qos_dev_open_32 = test_qos_dev_open;
+#elif (IS_ENABLED(CONFIG_LTQ_PPV4_QOS) || IS_ENABLED(CONFIG_PPV4))
+	qos_queue_remove_32 = pp_qos_queue_remove;
+	qos_queue_allocate_32 = pp_qos_queue_allocate;
+	qos_queue_info_get_32 = pp_qos_queue_info_get;
+	qos_port_remove_32 = pp_qos_port_remove;
+	qos_sched_allocate_32 = pp_qos_sched_allocate;
+	qos_sched_remove_32 = pp_qos_sched_remove;
+	qos_port_allocate_32 = pp_qos_port_allocate;
+	qos_port_set_32 = pp_qos_port_set;
+	qos_port_conf_set_default_32 = pp_qos_port_conf_set_default;
+	qos_queue_conf_set_default_32 = pp_qos_queue_conf_set_default;
+	qos_queue_set_32 = pp_qos_queue_set;
+	qos_sched_conf_set_default_32 = pp_qos_sched_conf_set_default;
+	qos_sched_set_32 = pp_qos_sched_set;
+	qos_queue_conf_get_32 = pp_qos_queue_conf_get;
+	qos_queue_flush_32 = pp_qos_queue_flush;
+	qos_sched_conf_get_32 = pp_qos_sched_conf_get;
+	qos_sched_get_queues_32 = pp_qos_sched_get_queues;
+	qos_port_get_queues_32 = pp_qos_port_get_queues;
+	qos_port_conf_get_32 = pp_qos_port_conf_get;
+	qos_dev_open_32 = pp_qos_dev_open;
+#else
+	/*all NULL function pointer */
+	DP_DEBUG(DP_DBG_FLAG_QOS, "call QOS function pointer set to NULL\n");
+#endif /*CONFIG_INTEL_DATAPATH_DUMMY_QOS*/
+}
+
+
+int dp_pp_alloc_port_32(struct ppv4_port *info)
+{
+	int qos_p_id = 0;
+	struct pp_qos_port_conf conf;
+	struct hal_priv *priv = HAL(info->inst);
+	struct pp_qos_dev *qos_dev = priv->qdev;
+
+	if (qos_port_allocate_32(qos_dev,
+			      info->cqm_deq_port,
+			      &qos_p_id)) {
+		PR_ERR("Failed to alloc QoS for deq_port=%d\n",
+		       info->cqm_deq_port);
+		return -1;
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "qos_port_allocate_32 ok with port(cqm/qos)=%d/%d\n",
+		 info->cqm_deq_port, qos_p_id);
+
+	qos_port_conf_set_default_32(&conf);
+	conf.port_parent_prop.arbitration = PP_QOS_ARBITRATION_WRR;
+	conf.ring_address = (unsigned long)info->tx_ring_addr_push;
+	conf.ring_size = info->tx_ring_size;
+	conf.packet_credit_enable = 1;
+	conf.credit = info->tx_pkt_credit;
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_DBG)
+	if (dp_dbg_flag & DP_DBG_FLAG_QOS) {
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "qos_port_set_32 info for p[%d/%d] dp_port=%d:\n",
+			 info->cqm_deq_port, qos_p_id,
+			 info->dp_port);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  arbitration=%d\n",
+			 conf.port_parent_prop.arbitration);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  ring_address=0x%lx\n",
+			 (unsigned long)conf.ring_address);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  ring_size=%d\n",
+			 conf.ring_size);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  packet_credit_enable=%d\n",
+			 conf.packet_credit_enable);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  credit=%d\n",
+			 conf.credit);
+	}
+#endif
+	if (qos_port_set_32(qos_dev, qos_p_id, &conf)) {
+		PR_ERR("qos_port_set_32 fail for port(cqm/qos) %d/%d\n",
+		       info->cqm_deq_port, qos_p_id);
+		qos_port_remove_32(qos_dev, qos_p_id);
+		return -1;
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "qos_port_set_32 ok for port(cqm/qos) %d/%d\n",
+		       info->cqm_deq_port, qos_p_id);
+	info->node_id = qos_p_id;
+	priv->deq_port_stat[info->cqm_deq_port].flag = PP_NODE_ALLOC;
+	priv->deq_port_stat[info->cqm_deq_port].node_id = qos_p_id;
+	info->node_id = qos_p_id;
+	return 0;
+}
+
+int dp_pp_alloc_queue_32(struct ppv4_queue *info)
+{
+	struct pp_qos_queue_conf conf;
+	int q_node_id;
+	struct pp_qos_queue_info q_info;
+	struct hal_priv *priv = HAL(info->inst);
+	struct pp_qos_dev *qos_dev = priv->qdev;
+
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_DUMMY_QOS)
+	qos_dev->dq_port = info->dq_port;
+#endif
+	if (qos_queue_allocate_32(qos_dev, &q_node_id)) {
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_DUMMY_QOS)
+		PR_ERR("qos_queue_allocate_32 fail for dq_port %d\n",
+		       info->dq_port);
+#else
+		PR_ERR("qos_queue_allocate_32 fail\n");
+#endif
+		return -1;
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS, "qos_queue_allocate_32 ok q_node=%d\n",
+		 q_node_id);
+
+	qos_queue_conf_set_default_32(&conf);
+	conf.wred_enable = 0;
+	conf.wred_max_allowed = 0x400; /*max qocc in pkt */
+	conf.queue_child_prop.parent = info->parent;
+	if (qos_queue_set_32(qos_dev, q_node_id, &conf)) {
+		PR_ERR("qos_queue_set_32 fail for node_id=%d to parent=%d\n",
+		       q_node_id, info->parent);
+		return -1;
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS, "To attach q_node=%d to parent_node=%d\n",
+		 q_node_id, conf.queue_child_prop.parent);
+	if (qos_queue_info_get_32(qos_dev, q_node_id, &q_info)) {
+		PR_ERR("qos_queue_info_get_32 fail for queue node_id=%d\n",
+		       q_node_id);
+		return -1;
+	}
+	info->qid = q_info.physical_id;
+	info->node_id = q_node_id;
+	DP_DEBUG(DP_DBG_FLAG_QOS, "Attached q[%d/%d] to parent_node=%d\n",
+		 q_info.physical_id, q_node_id,
+		 info->parent);
+	return 0;
+}
+
+int init_ppv4_qos_32(int inst, int flag)
+{
+	union qos_init {
+		struct pp_qos_port_conf p_conf;
+		struct pp_qos_queue_conf q_conf;
+		struct pp_qos_queue_info q_info;
+	};
+	union qos_init *t = NULL;
+	int res = DP_FAILURE;
+	struct hal_priv *priv = HAL(inst);
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_QOS_HAL)
+	unsigned int q, idx;
+	struct cbm_tx_push *flush_port;
+	struct cbm_cpu_port_data cpu_data = {0};
+#endif
+	if (!priv) {
+		PR_ERR("priv is NULL\n");
+		return DP_FAILURE;
+	}
+	if (!(flag & DP_PLATFORM_INIT)) {
+		/*need to implement de-initialization for qos later*/
+		priv->qdev = NULL;
+		return DP_SUCCESS;
+	}
+	priv->qdev = qos_dev_open_32(dp_port_prop[inst].qos_inst);
+	if (!priv->qdev) {
+		PR_ERR("Could not open qos instance %d\n",
+		       dp_port_prop[inst].qos_inst);
+		return DP_FAILURE;
+	}
+	PR_INFO("qos_dev_open_32 qdev=%px\n", priv->qdev);
+	t = kzalloc(sizeof(*t), GFP_ATOMIC);
+	if (!t) {
+		PR_ERR("kzalloc fail: %zd bytes\n", sizeof(*t));
+		return DP_FAILURE;
+	}
+	if (cbm_cpu_port_get(&cpu_data, 0)) {
+		PR_ERR("cbm_cpu_port_get for CPU port?\n");
+		goto EXIT;
+	}
+	/* Sotre drop/flush port's info */
+	flush_port = &cpu_data.dq_tx_flush_info;
+	idx = flush_port->deq_port;
+	if ((idx == 0) || (idx >= ARRAY_SIZE(dp_deq_port_tbl[inst]))) {
+		PR_ERR("Wrog DP Flush port[%d]\n", idx);
+		goto EXIT;
+	}
+	priv->cqm_drop_p = idx;
+	dp_deq_port_tbl[inst][idx].tx_pkt_credit = flush_port->tx_pkt_credit;
+	dp_deq_port_tbl[inst][idx].txpush_addr = flush_port->txpush_addr;
+	dp_deq_port_tbl[inst][idx].txpush_addr_qos =
+						flush_port->txpush_addr_qos;
+	dp_deq_port_tbl[inst][idx].tx_ring_size = flush_port->tx_ring_size;
+	dp_deq_port_tbl[inst][idx].dp_port = 0;/* dummy one */
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "DP Flush port[%d]: ring addr/push=0x%px/0x%px size=%d pkt_credit=%d\n",
+			 priv->cqm_drop_p,
+			 dp_deq_port_tbl[inst][idx].txpush_addr_qos,
+		dp_deq_port_tbl[inst][idx].txpush_addr,
+		dp_deq_port_tbl[inst][idx].tx_ring_size,
+		dp_deq_port_tbl[inst][idx].tx_pkt_credit);
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_QOS_HAL)
+	DP_DEBUG(DP_DBG_FLAG_DBG, "priv=%px deq_port_stat=%px q_dev=%px\n",
+		 priv, priv ? priv->deq_port_stat : NULL,
+		 priv ? priv->qdev : NULL);
+	if (qos_port_allocate_32(priv->qdev,
+			      priv->cqm_drop_p,
+			      &priv->ppv4_drop_p)) {
+		PR_ERR("Failed to alloc  qos drop port=%d\n",
+		       priv->cqm_drop_p);
+		goto EXIT;
+	}
+	qos_port_conf_set_default_32(&t->p_conf);
+	t->p_conf.port_parent_prop.arbitration = PP_QOS_ARBITRATION_WRR;
+	t->p_conf.ring_address =
+	(unsigned long)dp_deq_port_tbl[inst][idx].txpush_addr_qos;
+	t->p_conf.ring_size = dp_deq_port_tbl[inst][idx].tx_ring_size;
+	t->p_conf.packet_credit_enable = 1;
+	t->p_conf.credit = dp_deq_port_tbl[inst][idx].tx_pkt_credit;
+	t->p_conf.disable = 1; /*not allowed for dequeue*/
+
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_DBG)
+	if (dp_dbg_flag & DP_DBG_FLAG_QOS) {
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "qos_port_set_32 param: %d/%d for drop pot:\n",
+			 priv->cqm_drop_p, priv->ppv4_drop_p);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  arbitration=%d\n",
+			 t->p_conf.port_parent_prop.arbitration);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  ring_address=0x%x\n",
+			 (unsigned int)t->p_conf.ring_address);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  ring_size=%d\n",
+			 t->p_conf.ring_size);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  packet_credit_enable=%d\n",
+			 t->p_conf.packet_credit_enable);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  credit=%d\n",
+			 t->p_conf.credit);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  disabled=%d\n",
+			 t->p_conf.disable);
+	}
+#endif
+	if (qos_port_set_32(priv->qdev, priv->ppv4_drop_p, &t->p_conf)) {
+		PR_ERR("qos_port_set_32 fail for port(cqm/qos) %d/%d\n",
+		       priv->cqm_drop_p, priv->ppv4_drop_p);
+		qos_port_remove_32(priv->qdev, priv->ppv4_drop_p);
+		goto EXIT;
+	}
+
+	if (qos_queue_allocate_32(priv->qdev, &q)) {
+		PR_ERR("qos_queue_allocate_32 fail\n");
+		qos_port_remove_32(priv->qdev, q);
+		goto EXIT;
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS, "ppv4_drop_q alloc ok q_node=%d\n", q);
+
+	qos_queue_conf_set_default_32(&t->q_conf);
+	t->q_conf.blocked = 1; /*drop mode */
+	t->q_conf.wred_enable = 0;
+	t->q_conf.wred_max_allowed = 0; /*max qocc in pkt */
+	t->q_conf.queue_child_prop.parent = priv->ppv4_drop_p;
+	if (qos_queue_set_32(priv->qdev, q, &t->q_conf)) {
+		PR_ERR("qos_queue_set_32 fail for node_id=%d to parent=%d\n",
+		       q, t->q_conf.queue_child_prop.parent);
+		goto EXIT;
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS, "To attach q_node=%d to parent_node=%d\n",
+		 q, priv->ppv4_drop_p);
+	if (qos_queue_info_get_32(priv->qdev, q, &t->q_info)) {
+		PR_ERR("qos_queue_info_get_32 fail for queue node_id=%d\n",
+		       q);
+		goto EXIT;
+	}
+	priv->ppv4_drop_q = t->q_info.physical_id;
+	DP_DEBUG(DP_DBG_FLAG_QOS, "Drop queue q[%d/%d] to parent=%d/%d\n",
+		 priv->ppv4_drop_q, q,
+		 priv->cqm_drop_p,
+		 priv->ppv4_drop_p);
+#endif /* end of CONFIG_INTEL_DATAPATH_QOS_HAL */
+	DP_DEBUG(DP_DBG_FLAG_DBG, "init_ppv4_qos_32 done\n");
+	res = DP_SUCCESS;
+
+EXIT:
+	kfree(t);
+	t = NULL;
+	return res;
+}
+
+/**
+ * __mark_ppv4_port - Mark the allocated ppv4 ports.
+ * @inst:	DP instance ID.
+ * @*priv:	hal private structure info.
+ * @base:	base of the continuous allocated ports.
+ * @mark:	no of ports allocated to be marked as 1.
+ **/
+static void __mark_ppv4_port(int inst, int base, int mark)
+{
+	int tmp;
+	struct hal_priv *priv = HAL(inst);
+
+	for (tmp = base; (tmp < mark) && (tmp < MAX_PPV4_PORTS); tmp++)
+		priv->ppv4_flag[tmp] = 1;
+}
+
+/**
+ * ppv4_alloc_port_32 -	Allocate continuous requested deq_ports.
+ * @inst:			DP instance ID
+ * @deq_port_num:	no'of continuous PPV4 ports to be allocated.
+ *
+ * Returns the base of the continuous allocated ports.
+ * else -ERROR.
+ **/
+int ppv4_alloc_port_32(int inst, int deq_port_num)
+{
+	u32 base, match;
+	struct hal_priv *priv = HAL(inst);
+
+	for (base = 0; base < MAX_PPV4_PORTS; base++) {
+		for (match = 0; (match < deq_port_num) && ((base + match)
+		     < MAX_PPV4_PORTS); match++) {
+			if (priv->ppv4_flag[base + match])
+				break;
+		}
+		if (match == deq_port_num) {
+			__mark_ppv4_port(inst, base, (base + match));
+			return (base + PPV4_PORT_BASE);
+		}
+	}
+	return DP_FAILURE; /* port not found */
+}
+
+/**
+ * ppv4_port_free_32 - Unmark the ppv4 ports for inst.
+ * @inst:			DP instance ID.
+ * @base:			base of continuous allocated PPV4 ports.
+ * @deq_port_num:	no'of continuous PPV4 ports allocated.
+ *
+ * Free the ports allocated by ppv4_alloc_port_32() by marking zero.
+ **/
+int ppv4_port_free_32(int inst, int base, int deq_port_num)
+{
+	u32 tmp;
+	struct hal_priv *priv = HAL(inst);
+
+	/* Expecting base always greater than PPV4_PORT_BASE */
+	if (base < PPV4_PORT_BASE)
+		return DP_FAILURE;
+
+	base = base - PPV4_PORT_BASE;
+	for (tmp = base; (tmp < MAX_PPV4_PORTS) && deq_port_num; tmp++) {
+		priv->ppv4_flag[tmp] = 0;
+		deq_port_num--;
+	}
+
+	return DP_SUCCESS;
+}
+
+/**
+ * ppv4_alloc_ring_32 - Allocate ring buffer for port
+ * @size:	size of the descriptor.
+ * @phy:	Phy addr of ring.
+ * @virt:	Virt addr of the ring.
+ *
+ * Allocate the ring buffer of @size(*DP_TXIN_RING_SIZE_DEF) requested by
+ * caller.
+ **/
+int ppv4_alloc_ring_32(int size, void **phy, void **virt)
+{
+	*virt = kmalloc(DP_TXIN_RING_SIZE_DEF * size, GFP_KERNEL);
+	if (!*virt)
+		return DP_FAILURE;
+
+	*phy = virt_to_phys(*virt);
+
+	return DP_SUCCESS;
+}
+
+/**
+ * ppv4_ring_free_32 - Free ring buffer.
+ * @ptr:	virt addr of the ring.
+ *
+ * Free the ring buffer allocated by ppv4_alloc_ring_32().
+ **/
+void ppv4_ring_free_32(void *ptr)
+{
+	kfree(ptr);
+}
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_ppv4.h b/drivers/net/datapath/dpm/gswip32/datapath_ppv4.h
new file mode 100644
index 000000000000..ec07373ee960
--- /dev/null
+++ b/drivers/net/datapath/dpm/gswip32/datapath_ppv4.h
@@ -0,0 +1,216 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#ifndef DATAPATH_PPV4_H
+#define DATAPATH_PPV4_H
+
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_DUMMY_QOS)
+struct pp_qos_dev {
+	int dq_port;
+};
+#endif
+
+#define MAX_PPV4_PORT 256
+#define MAX_CQM_DEQ   138
+#define MAX_QUEUE   512
+#define MAX_PP_CHILD_PER_NODE  8 /* Maximum queue per scheduler */
+#define MAX_Q_PER_PORT 32 /* Maximum queue per port */
+#define QOS_MAX_NODES  2048 /* Maximum PPV4 nodes. ?? need further check */
+#define MAX_SCHD 128 /* Need further check ?? need further check*/
+#define INV_RESV_IDX 0xFFFF  /* Invalid reserved resource index */
+#define MAX_LOOKUP_TBL_SIZE (8 * 1024) /* lookup table size:12bits + 1bit eg */
+#define DEF_QRED_MAX_ALLOW 0x400  /* max qocc in queue */
+#define DEF_QRED_MIN_ALLOW 0x40 /* minqocc in queue */
+#define DEF_QRED_SLOP_GREEN 30 /* green slop in queue */
+#define DEF_QRED_SLOP_YELLOW 70 /* yellow slop in queue */
+#define DEF_WRED_RATIO       5
+
+
+#define HAL(inst) ((struct hal_priv *)dp_port_prop[inst].priv_hal)
+#define PARENT(x) (x.queue_child_prop.parent)
+#define PARENT_S(x) (x.sched_child_prop.parent)
+#define CHILD(x, idx) (priv->qos_sch_stat[x].child[idx])
+#define DP_PORT(p) (dp_deq_port_tbl[p->inst][p->cqm_deq_port.cqm_deq_port])
+
+enum flag {
+	DP_NODE_DEC = BIT(0), /* flag to reduce node counter */
+	DP_NODE_INC = BIT(1), /* flag to increase node counter */
+	DP_NODE_RST = BIT(2), /* flag to reset node counter */
+	C_FLAG = BIT(8), /* scheduler flag linked to node */
+	P_FLAG = BIT(9) /* scheduler flag linked to parent */
+};
+
+struct ppv4_queue {
+	int inst;  /* dp instance */
+	u16 qid;  /* -1 means dynammic, otherwise already configured */
+	u16 node_id; /*output */
+	u16 sch;  /* -1 means dynammic, otherwise already configured */
+	u16 parent; /* -1 means no parent.
+		     * it is used for shared dropping queueu purpose
+		     */
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_DUMMY_QOS)
+	int dq_port; /* cqm dequeue port for qos slim driver queue alloc */
+#endif
+};
+
+struct ppv4_scheduler {
+	u16 sch;  /* -1 means dynammic, otherwise already configured */
+	u16 parent; /* input */
+	u16 node_id; /* output */
+};
+
+struct ppv4_port {
+	int inst;
+	u16 dp_port;
+	u16 qos_deq_port; /* -1 means dynammic, otherwise already specified.
+			   * Remove in new datapath lib
+			   */
+	u16 cqm_deq_port;  /* rename in new datapath lib */
+	u16 node_id; /* output */
+
+	u32 tx_pkt_credit;  /* PP port tx bytes credit */
+	void *tx_ring_addr;  /* PP port ring address */
+	void *tx_ring_addr_push;  /* PP port ring address */
+	u32 tx_ring_size; /* PP ring size */
+};
+
+struct ppv4_q_sch_port {
+	/* input */
+	int inst;
+	int dp_port; /* for storing q/scheduler */
+	int ctp; /* for storing q/scheduler: masked subifid. */
+	u32 cqe_deq; /* CQE dequeue port */
+	u32 tx_pkt_credit;  /* PP port tx bytes credit */
+	void *tx_ring_addr;  /* PP port ring address. */
+	void *tx_ring_addr_push;  /* PP port ring address. */
+	u32 tx_ring_size; /* PP ring size */
+
+	/* output of PP */
+	u32 qid;
+	u32 q_node;
+	u32 schd_node;
+	u32 port_node; /* qos port node id */
+	u32 f_deq_port_en:1; /* flag to trigger cbm_dp_enable */
+};
+
+struct pp_sch_list {
+	u32 flag:1; /* 0: valid 1-used 2-reserved */
+	u16 node;
+	u16 parent_type;  /* scheduler/port */
+	u16 parent;
+};
+
+enum PP_NODE_STAT {
+	PP_NODE_FREE = 0, /* Free and not allocated yet */
+	PP_NODE_ALLOC = BIT(0), /* allocated */
+	PP_NODE_ACTIVE = BIT(1), /* linked */
+	PP_NODE_RESERVE = BIT(2) /* reserved */
+};
+
+struct pp_node {
+	enum PP_NODE_STAT flag; /* 0: FREE 1-used/alloc */
+	u16 type; /* node type */
+	u16 node_id;  /* node id */
+};
+
+struct pp_queue_stat {
+	enum PP_NODE_STAT flag; /* 0: valid 1-used 2-reserved */
+	u16 deq_port; /* cqm dequeue port id */
+	u16 node_id;  /* queue node id */
+	u16 resv_idx; /* index of reserve table */
+	u16 dp_port; /* datapath port id */
+};
+
+struct pp_sch_stat {
+	enum PP_NODE_STAT c_flag; /* sch flag linked to child */
+	enum PP_NODE_STAT p_flag; /* sch flag linked to parent */
+	u16 resv_idx; /* index of reserve table */
+	struct pp_node child[MAX_PP_CHILD_PER_NODE];
+	u16 child_num; /* Number of child */
+	int type; /* Node type for queue/sch/port:
+		   * sched table is not just for scheduler, also for queue/port
+		   * It is table index is based on logical node id,
+		   * not just scheduler id
+		   */
+	struct pp_node parent; /* valid for node type queue/sch */
+	u16 dp_port; /* datapath port id */
+};
+
+struct cqm_deq_stat {
+	enum PP_NODE_STAT flag; /* 0: valid 1-used 2-reserved */
+	u16 deq_id; /* qos dequeue port physical id. Maybe no need */
+	u16 node_id; /* qos dequeue port's node id */
+	u16 child_num; /* Number of child */
+};
+
+struct limit_map {
+	int pp_limit; /* pp shaper limit */
+	int dp_limit; /* dp shaper limit */
+};
+
+struct arbi_map {
+	int pp_arbi; /* pp arbitrate */
+	int dp_arbi; /* dp arbitrate */
+};
+
+struct dp_lookup_entry {
+	int entry[MAX_LOOKUP_TBL_SIZE];
+	int num; /* num of valid lookup entries save in entry[] array */
+};
+
+void init_qos_fn_32(void);
+extern int (*qos_queue_remove_32)(struct pp_qos_dev *qos_dev, unsigned int id);
+extern int (*qos_queue_allocate_32)(struct pp_qos_dev *qos_dev, unsigned int *id);
+extern int (*qos_queue_info_get_32)(struct pp_qos_dev *qos_dev, unsigned int id,
+				 struct pp_qos_queue_info *info);
+extern int (*qos_port_remove_32)(struct pp_qos_dev *qos_dev, unsigned int id);
+extern int (*qos_sched_allocate_32)(struct pp_qos_dev *qos_dev, unsigned int *id);
+extern int (*qos_sched_remove_32)(struct pp_qos_dev *qos_dev, unsigned int id);
+extern int (*qos_port_allocate_32)(struct pp_qos_dev *qos_dev,
+				unsigned int physical_id,
+				unsigned int *id);
+extern int (*qos_port_set_32)(struct pp_qos_dev *qos_dev, unsigned int id,
+			   const struct pp_qos_port_conf *conf);
+extern void (*qos_port_conf_set_default_32)(struct pp_qos_port_conf *conf);
+extern void (*qos_queue_conf_set_default_32)(struct pp_qos_queue_conf *conf);
+extern int (*qos_queue_set_32)(struct pp_qos_dev *qos_dev, unsigned int id,
+			    const struct pp_qos_queue_conf *conf);
+extern void (*qos_sched_conf_set_default_32)(struct pp_qos_sched_conf *conf);
+extern int (*qos_sched_set_32)(struct pp_qos_dev *qos_dev, unsigned int id,
+			    const struct pp_qos_sched_conf *conf);
+extern int (*qos_queue_conf_get_32)(struct pp_qos_dev *qos_dev, unsigned int id,
+				 struct pp_qos_queue_conf *conf);
+extern int (*qos_queue_flush_32)(struct pp_qos_dev *qos_dev, unsigned int id);
+extern int (*qos_sched_conf_get_32)(struct pp_qos_dev *qos_dev, unsigned int id,
+				 struct pp_qos_sched_conf *conf);
+extern int (*qos_sched_get_queues_32)(struct pp_qos_dev *qos_dev, unsigned int id,
+				   u16 *queue_ids, unsigned int size,
+				   unsigned int *queues_num);
+extern int (*qos_port_get_queues_32)(struct pp_qos_dev *qos_dev, unsigned int id,
+				  u16 *queue_ids, unsigned int size,
+				  unsigned int *queues_num);
+extern int (*qos_port_conf_get_32)(struct pp_qos_dev *qdev, unsigned int id,
+				struct pp_qos_port_conf *conf);
+extern int (*qos_port_info_get_32)(struct pp_qos_dev *qdev, unsigned int id,
+				struct pp_qos_port_info *info);
+extern struct pp_qos_dev *(*qos_dev_open_32)(unsigned int id);
+int dp_map_to_drop_q_32(int inst, int q_id, struct dp_lookup_entry *lookup);
+int dp_pp_alloc_port_32(struct ppv4_port *info);
+int dp_pp_alloc_sched(struct ppv4_scheduler *info);
+int dp_pp_alloc_queue_32(struct ppv4_queue *info);
+int alloc_q_to_port_32(struct ppv4_q_sch_port *info, u32 flag);
+extern struct cqm_deq_stat deq_port_stat[MAX_CQM_DEQ];
+extern struct pp_queue_stat qos_queue_stat[MAX_QUEUE];
+int init_ppv4_qos_32(int inst, int flag);
+int ppv4_alloc_port_32(int inst, int deq_port_num);
+int ppv4_port_free_32(int inst, int base, int deq_port_num);
+int ppv4_alloc_ring_32(int size, void **phy, void **virt);
+void ppv4_ring_free_32(void *ptr);
+#endif /* DATAPATH_PPV4_H */
+
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_ppv4_api.c b/drivers/net/datapath/dpm/gswip32/datapath_ppv4_api.c
new file mode 100644
index 000000000000..c7212963ce48
--- /dev/null
+++ b/drivers/net/datapath/dpm/gswip32/datapath_ppv4_api.c
@@ -0,0 +1,4598 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include <net/datapath_api.h>
+#include <net/datapath_api_qos.h>
+#include <linux/pp_qos_api.h>
+#include "../datapath.h"
+#include "datapath_misc.h"
+
+#define FLUSH_RESTORE_LOOKUP BIT(0)
+
+static struct limit_map limit_maps[] = {
+	{QOS_NO_BANDWIDTH_LIMIT, DP_NO_SHAPER_LIMIT},
+	{QOS_MAX_BANDWIDTH_LIMIT, DP_MAX_SHAPER_LIMIT}
+};
+
+static struct arbi_map arbi_maps[] = {
+	{PP_QOS_ARBITRATION_WSP, ARBITRATION_WSP},
+	{PP_QOS_ARBITRATION_WRR, ARBITRATION_WRR},
+	{PP_QOS_ARBITRATION_WFQ, ARBITRATION_WFQ},
+	{PP_QOS_ARBITRATION_WRR, ARBITRATION_NULL}
+};
+
+static void dp_wred_def(struct pp_qos_queue_conf *conf)
+{
+	if (!conf)
+		return;
+
+#if 1
+
+	conf->wred_enable = 0;
+	conf->wred_min_guaranteed = DEF_QRED_MIN_ALLOW;
+	conf->wred_max_allowed = DEF_QRED_MAX_ALLOW;
+#else
+	conf->wred_enable = 1;
+	conf->wred_fixed_drop_prob_enable = 0;
+	conf->wred_min_guaranteed = DEF_QRED_MIN_ALLOW;
+	conf->wred_max_allowed = DEF_QRED_MAX_ALLOW;
+
+	conf->wred_fixed_drop_prob_yellow = 0;
+	conf->wred_min_avg_yellow = DEF_QRED_MIN_ALLOW/DEF_WRED_RATIO;
+	conf->wred_max_avg_yellow = DEF_QRED_MAX_ALLOW/DEF_WRED_RATIO;
+	conf->wred_slope_yellow = DEF_QRED_SLOP_YELLOW;
+
+	conf->wred_fixed_drop_prob_green = 0;
+	conf->wred_min_avg_green = (DEF_WRED_RATIO - 1) * DEF_QRED_MIN_ALLOW /
+				    DEF_WRED_RATIO;
+	conf->wred_max_avg_green = (DEF_WRED_RATIO - 1) * DEF_QRED_MAX_ALLOW /
+				    DEF_WRED_RATIO;
+	conf->wred_slope_green = DEF_QRED_SLOP_GREEN;
+#endif
+}
+
+
+int qos_platform_set_32(int cmd_id, void *node, int flag)
+{
+	struct dp_node_link *node_link = (struct dp_node_link *)node;
+	int inst;
+	struct hal_priv *priv;
+	int res = DP_FAILURE;
+
+	if (!node)
+		return DP_FAILURE;
+	inst = node_link->inst;
+	priv = HAL(inst);
+	if (!priv->qdev) {
+		PR_ERR("qdev NULL with inst=%d\n", inst);
+		return DP_FAILURE;
+	}
+
+	switch (cmd_id) {
+	case NODE_LINK_ADD:
+		res = dp_node_link_add_32((struct dp_node_link *)node, flag);
+		break;
+	case NODE_LINK_GET:
+		res = dp_node_link_get_32((struct dp_node_link *)node, flag);
+		break;
+	case NODE_LINK_EN_GET:
+		res = dp_node_link_en_get_32((struct dp_node_link_enable *)node,
+					     flag);
+		break;
+	case NODE_LINK_EN_SET:
+		res = dp_node_link_en_set_32((struct dp_node_link_enable *)node,
+					     flag);
+		break;
+	case NODE_UNLINK:
+		res = dp_node_unlink_32((struct dp_node_link *)node, flag);
+		break;
+	case LINK_ADD:
+		res = dp_link_add_32((struct dp_qos_link *)node, flag);
+		break;
+	case LINK_GET:
+		res = dp_link_get_32((struct dp_qos_link *)node, flag);
+		break;
+	case LINK_PRIO_SET:
+		res = dp_qos_link_prio_set_32((struct dp_node_prio *)node,
+					      flag);
+		break;
+	case LINK_PRIO_GET:
+		res = dp_qos_link_prio_get_32((struct dp_node_prio *)node,
+					      flag);
+		break;
+	case QUEUE_CFG_SET:
+		res = dp_queue_conf_set_32((struct dp_queue_conf *)node, flag);
+		break;
+	case QUEUE_CFG_GET:
+		res = dp_queue_conf_get_32((struct dp_queue_conf *)node, flag);
+		break;
+	case SHAPER_SET:
+		res = dp_shaper_conf_set_32((struct dp_shaper_conf *)node,
+					    flag);
+		break;
+	case SHAPER_GET:
+		res = dp_shaper_conf_get_32((struct dp_shaper_conf *)node,
+					    flag);
+		break;
+	case NODE_ALLOC:
+		res = dp_node_alloc_32((struct dp_node_alloc *)node, flag);
+		break;
+	case NODE_FREE:
+		res = dp_node_free_32((struct dp_node_alloc *)node, flag);
+		break;
+	case NODE_CHILDREN_FREE:
+		res =
+		dp_free_children_via_parent_32((struct dp_node_alloc *)node,
+					       flag);
+		break;
+	case DEQ_PORT_RES_GET:
+		res = dp_deq_port_res_get_32((struct dp_dequeue_res *)node,
+					     flag);
+		break;
+	case COUNTER_MODE_SET:
+		res = dp_counter_mode_set_32((struct dp_counter_conf *)node,
+					     flag);
+		break;
+	case COUNTER_MODE_GET:
+		res = dp_counter_mode_set_32((struct dp_counter_conf *)node,
+					     flag);
+		break;
+	case QUEUE_MAP_GET:
+		res = dp_queue_map_get_32((struct dp_queue_map_get *)node,
+					  flag);
+		break;
+	case QUEUE_MAP_SET:
+		res = dp_queue_map_set_32((struct dp_queue_map_set *)node,
+					  flag);
+		break;
+	case NODE_CHILDREN_GET:
+		res = dp_children_get_32((struct dp_node_child *)node, flag);
+		break;
+	case QOS_LEVEL_GET:
+		res = dp_qos_level_get_32((struct dp_qos_level *)node, flag);
+		break;
+	case QOS_GLOBAL_CFG_GET:
+		res = dp_qos_global_info_get_32((struct dp_qos_cfg_info *)node, flag);
+	case QOS_Q_LOGIC:
+		res = dp_get_queue_logic_32((struct dp_qos_q_logic*)node, flag);
+		break;
+		
+	default:
+		PR_ERR("no support yet cmd_id %d\n", cmd_id);
+		break;
+	}
+	return res;
+}
+
+#define MBPS_2_KBPS 1000
+/* convert pp shaper limit to dp shaper limit */
+static int limit_pp2dp(u32 pp_limit, u32 *dp_limit)
+{
+	int i;
+
+	if (!dp_limit) {
+		PR_ERR("dp_limit is NULL!\n");
+		return DP_FAILURE;
+	}
+
+	if (pp_limit > QOS_MAX_BANDWIDTH_LIMIT) {
+		PR_ERR("Wrong pp shaper limit: %u\n", pp_limit);
+		return DP_FAILURE;
+	}
+
+	for (i = 0; i < ARRAY_SIZE(limit_maps); i++) {
+		if (limit_maps[i].pp_limit == pp_limit) {
+			*dp_limit = limit_maps[i].dp_limit;
+			return DP_SUCCESS;
+		}
+	}
+	*dp_limit = pp_limit * MBPS_2_KBPS;/* mbps to kbps */
+
+	if ((*dp_limit <= 0) || (*dp_limit > DP_MAX_SHAPER_LIMIT)) {
+		PR_ERR("Wrong dp shaper limit: %u\n", *dp_limit);
+		return DP_FAILURE;
+	}
+	return DP_SUCCESS;
+}
+
+/* convert dp shaper limit to pp shaper limit */
+static int limit_dp2pp(u32 dp_limit, u32 *pp_limit)
+{
+	int i;
+
+	if (!pp_limit) {
+		PR_ERR("pp_limit is NULL!\n");
+		return DP_FAILURE;
+	}
+
+	if ((dp_limit > DP_MAX_SHAPER_LIMIT) || (dp_limit == 0)) {
+		PR_ERR("Wrong dp shaper limit: %u\n", dp_limit);
+		return DP_FAILURE;
+	}
+
+	for (i = 0; i < ARRAY_SIZE(limit_maps); i++) {
+		if (limit_maps[i].dp_limit == dp_limit) {
+			*pp_limit = limit_maps[i].pp_limit;
+			return DP_SUCCESS;
+		}
+	}
+
+	if (dp_limit % MBPS_2_KBPS)
+		*pp_limit = dp_limit / MBPS_2_KBPS + 1;/* kbps to mbps */
+	else
+		*pp_limit = dp_limit / MBPS_2_KBPS;
+
+	if (*pp_limit > QOS_MAX_BANDWIDTH_LIMIT) {
+		PR_ERR("Wrong dp shaper limit: %u\n", *pp_limit);
+		return DP_FAILURE;
+	}
+	return DP_SUCCESS;
+}
+
+/* convert PP arbitrate to DP arbitrate */
+int arbi_pp2dp_32(int pp_arbi)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(arbi_maps); i++) {
+		if (arbi_maps[i].pp_arbi == pp_arbi)
+			return arbi_maps[i].dp_arbi;
+	}
+	return DP_FAILURE;
+}
+
+/* convert DP arbitrate to PP arbitrate */
+int arbi_dp2pp_32(int dp_arbi)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(arbi_maps); i++) {
+		if (arbi_maps[i].dp_arbi == dp_arbi)
+			return arbi_maps[i].pp_arbi;
+	}
+	PR_ERR("Wrong dp_arbitrate: %d\n", dp_arbi);
+	return DP_FAILURE;
+}
+
+/* get_qid_by_node API
+ * checks for queue node id
+ * upon Success
+ *    return physical id of queue
+ *    else return DP_FAILURE
+ */
+static int get_qid_by_node(int inst, int node_id, int flag)
+{
+	int i;
+	struct hal_priv *priv = HAL(inst);
+
+	for (i = 0; i < MAX_QUEUE; i++) {
+		if (node_id == priv->qos_queue_stat[i].node_id)
+			return i;
+	}
+	return DP_FAILURE;
+}
+
+/* get_cqm_deq_port_by_node API
+ * checks for qos deque port
+ * upon Success
+ *    return physical cqm_deq_port id
+ *    else return DP_FAILURE
+ */
+static int get_cqm_deq_port_by_node(int inst, int node_id, int flag)
+{
+	int i;
+	struct hal_priv *priv = HAL(inst);
+
+	for (i = 0; i < MAX_CQM_DEQ; i++) {
+		if (node_id == priv->deq_port_stat[i].node_id)
+			return i;
+	}
+	return DP_FAILURE;
+}
+
+#ifndef DP_FLUSH_VIA_AUTO
+static int cqm_queue_flush_32(int cqm_inst, int cqm_drop_port, int qid)
+{
+/* Before call this API, the queue is already unmapped in lookup table,
+ * For the queue itself, it is blocked and resume.
+ * Also attached to drop port
+ */
+	/* need call low level CQM API */
+
+	cqm_qos_queue_flush(cqm_inst, cqm_drop_port, qid);
+
+	DP_DEBUG(DP_DBG_FLAG_QOS, "cqm_queue_flush_32 done\n");
+	return DP_SUCCESS;
+}
+#endif
+
+/* Note: When this API is returned,make sure the queue is in suspend/block
+ *      since the queue may need to move to other scheduler/port after flush.
+ *      node_id is logical node it here
+ */
+static int queue_flush_32(int inst, int node_id, int flag)
+{
+	struct hal_priv *priv = HAL(inst);
+	struct pp_qos_queue_conf queue_cfg = {0};
+#ifdef DP_FLUSH_VIA_AUTO
+	u32 qocc;
+	int times = 10000;
+#else
+	struct pp_qos_queue_conf tmp_q_cfg = {0};
+#endif
+	int qid = get_qid_by_node(inst, node_id, 0);
+	int res = DP_SUCCESS;
+	struct dp_lookup_entry *lookup = NULL;
+	struct cbm_lookup info = {0};
+
+	if (qid < 0) {
+		PR_ERR("no physical qid for q_node=%d\n", node_id);
+		res = DP_FAILURE;
+		goto EXIT;
+	}
+	if (qos_queue_conf_get_32(priv->qdev, node_id, &queue_cfg)) {
+		PR_ERR("qos_queue_conf_get_32 fail: q[%d/%d]\n",
+		       qid, node_id);
+		res = DP_FAILURE;
+		goto EXIT;
+	}
+	lookup = kzalloc(sizeof(*lookup), GFP_ATOMIC);
+	if (!lookup) {
+		res = DP_FAILURE;
+		PR_ERR("kmalloc fail to alloc %zd bytes\n", sizeof(*lookup));
+		goto EXIT;
+	}
+	/* map to drop queue and save the changed lookup entries for recover */
+	if (dp_map_to_drop_q_32(inst, qid, lookup)) {
+		PR_ERR("failed to dp_map_to_drop_q_32 for Q:%d\n", qid);
+		res = DP_FAILURE;
+		goto EXIT;
+	}
+	/* block/disable: ensure to drop all coming enqueue packet */
+	if (queue_cfg.blocked == 0) { /* to block */
+		if (pp_qos_queue_block(priv->qdev, node_id)) {
+			PR_ERR("pp_qos_queue_block fail: q[%d/%d]\n",
+			       qid, node_id);
+			res = DP_FAILURE;
+			goto EXIT;
+		}
+	}
+#ifdef DP_FLUSH_VIA_AUTO
+	while (times--) {
+		get_q_qocc_32(inst, qid, &qocc);
+		if (qocc == 0) /* no packet in the queue */
+			break;
+		udelay(10);
+	};
+	if (qocc)
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Auto Q[%d] Flushing failed:qocc=%u ???\n",
+			 qid, qocc);
+	else
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Auto Q[%d] Flushing ok:qocc=%u\n", qid, qocc);
+#else /* flush queue via CQM API */
+	if (queue_cfg.queue_child_prop.parent == priv->ppv4_drop_p) {
+		/* already attached to drop queue and can directly flush */
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Flush:Q[%d] already under drop port[/%d]\n",
+			 qid, priv->ppv4_drop_p);
+
+		cqm_queue_flush_32(dp_port_prop[inst].cbm_inst,
+				   priv->cqm_drop_p, qid);
+	} else {
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Queue movement before flush");
+
+		/*move to drop port and set block and resume the queue */
+		qos_queue_conf_set_default_32(&tmp_q_cfg); /*use new variable */
+		dp_wred_def(&tmp_q_cfg);
+		tmp_q_cfg.queue_child_prop.parent = priv->ppv4_drop_p;
+		if (qos_queue_set_32(priv->qdev, node_id, &tmp_q_cfg)) {
+			PR_ERR("qos_queue_set_32 fail for queue=%d to parent=%d\n",
+			       qid, tmp_q_cfg.queue_child_prop.parent);
+			goto EXIT;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Flush:Move Q[%d] to drop port[/%d]\n",
+			 qid,
+			 tmp_q_cfg.queue_child_prop.parent);
+
+		cqm_queue_flush_32(dp_port_prop[inst].cbm_inst,
+				   priv->cqm_drop_p, qid);
+#ifdef DP_FLUSH_SUSPEND_Q
+		/* set to suspend again before move back to original parent */
+		if (pp_qos_queue_suspend(priv->qdev, node_id)) {
+			PR_ERR("pp_qos_queue_suspend fail q[%d] to parent=%d\n",
+			       qid, tmp_q_cfg.queue_child_prop.parent);
+			goto EXIT;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "suspend queue[%d/%d]\n", qid, node_id);
+#endif
+		/* move back the queue to original parent
+		 * with orignal variable queue_cfg
+		 */
+		if (qos_queue_set_32(priv->qdev, node_id, &queue_cfg)) {
+			PR_ERR("qos_queue_conf_get_32 fail: q[%d/%d]\n",
+			       qid, node_id);
+			res = DP_FAILURE;
+			goto EXIT;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Flush:Move Q[%d] back to port[/%d]\n",
+			 qid, queue_cfg.queue_child_prop.parent);
+	}
+#endif /* end of DP_FLUSH_VIA_AUTO */
+
+#ifdef DP_FLUSH_SUSPEND_Q
+	if (pp_qos_queue_suspend(priv->qdev, node_id)) {
+		PR_ERR("qos_queue_set_32 fail\n");
+		goto EXIT;
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "suspend queue[%d/%d]\n", qid, node_id);
+#endif
+	/* restore lookup entry mapping for this qid if needed */
+	if (flag & FLUSH_RESTORE_LOOKUP) {
+		int i;
+
+		if (lookup->num) {
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "Try to restore qid[%d] lookup entry: %d\n",
+				 qid, lookup->num);
+			for (i = 0; i < lookup->num; i++) {
+				info.index = lookup->entry[i];
+				info.qid = qid;
+				dp_set_lookup_qid_via_index(&info);
+			}
+		}
+	}
+EXIT:
+	kfree(lookup);
+	lookup = NULL;
+	return res;
+}
+
+/* get_node_type_by_node_id API
+ * get node_type node_id in sch global table
+ * upon Success
+ *    return node_type of node_id
+ */
+static int get_node_type_by_node_id(int inst, int node_id, int flag)
+{
+	struct hal_priv *priv = HAL(inst);
+
+	return priv->qos_sch_stat[node_id].type;
+}
+
+/* get_free_child_idx API
+ * check free flag for child in parent's table and return index
+ * else return DP_FAILURE
+ */
+static int get_free_child_idx(int inst, int node_id, int flag)
+{
+	int i;
+	struct hal_priv *priv = HAL(inst);
+
+	for (i = 0; i < DP_MAX_CHILD_PER_NODE; i++) {
+		if (priv->qos_sch_stat[node_id].child[i].flag == PP_NODE_FREE)
+			return i;
+	}
+	return DP_FAILURE;
+}
+
+/* get_parent_node API
+ * check parent flag in node global table if active retrun parent id
+ * else return DP_FAILURE
+ */
+static int get_parent_node(int inst, int node_id, int flag)
+{
+	struct hal_priv *priv = HAL(inst);
+	int type = get_node_type_by_node_id(inst, node_id, 0);
+
+	if ((priv->qos_sch_stat[node_id].parent.flag) &&
+	    (type != DP_NODE_PORT))
+		return priv->qos_sch_stat[node_id].parent.node_id;
+	return DP_FAILURE;
+}
+
+/* get_child_idx_node_id API
+ * check free flag in parent's global table and return index
+ * else return DP_FAILURE
+ */
+static int get_child_idx_node_id(int inst, int node_id, int flag)
+{
+	struct hal_priv *priv = HAL(inst);
+	int i, p_id;
+
+	p_id = priv->qos_sch_stat[node_id].parent.node_id;
+
+	for (i = 0; i < DP_MAX_CHILD_PER_NODE; i++) {
+		if (node_id == priv->qos_sch_stat[p_id].child[i].node_id)
+			return i;
+	}
+	return DP_FAILURE;
+}
+
+/* node_queue_dec API
+ * for queue id = node_id, flag = DP_NODE_DEC
+ * Set Queue flag from PP_NODE_ACTIVE to PP_NODE_ALLOC
+ * else return DP_FAILURE
+ */
+static int node_queue_dec(int inst, int node_id, int flag)
+{
+	struct hal_priv *priv;
+	int phy_id, pid, idx;
+
+	priv = HAL(inst);
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	phy_id = get_qid_by_node(inst, node_id, flag);
+	if (phy_id == DP_FAILURE) {
+		PR_ERR("get_qid_by_node failed\n");
+		return DP_FAILURE;
+	}
+
+	pid = get_parent_node(inst, node_id, flag);
+	if (pid == DP_FAILURE) {
+		PR_ERR("get_parent_node failed for Q:%d\n", phy_id);
+		return DP_FAILURE;
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS, "parent:%d of Q:%d\n", pid, phy_id);
+
+	idx = get_child_idx_node_id(inst, node_id, 0);
+	if (idx == DP_FAILURE) {
+		PR_ERR("get_child_idx_node_id failed for Q:%d\n", phy_id);
+		return DP_FAILURE;
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "node_queue_dec: parent:%d of Q:[%d/%d] child idx:%d\n",
+		 pid, phy_id, node_id, idx);
+
+	if (!(priv->qos_queue_stat[phy_id].flag & PP_NODE_ACTIVE)) {
+		PR_ERR("Wrong Q[%d] Stat(%d):Expect ACTIVE\n", phy_id,
+		       priv->qos_queue_stat[phy_id].flag);
+		return DP_FAILURE;
+	}
+	if (!priv->qos_sch_stat[node_id].parent.flag) {
+		PR_ERR("Wrong Q[%d]'s Parent Stat(%d):Expect ACTIVE\n",
+		       node_id, priv->qos_sch_stat[node_id].parent.flag);
+		return DP_FAILURE;
+	}
+	if (pid == priv->qos_sch_stat[node_id].parent.node_id) {
+		priv->qos_sch_stat[pid].child[idx].flag = PP_NODE_FREE;
+		priv->qos_sch_stat[pid].child[idx].node_id = 0;
+		priv->qos_sch_stat[pid].child[idx].type = 0;
+	}
+	priv->qos_sch_stat[node_id].parent.node_id = 0;
+	priv->qos_sch_stat[node_id].parent.type = 0;
+	priv->qos_sch_stat[node_id].parent.flag = 0;
+	priv->qos_queue_stat[phy_id].flag |= PP_NODE_ALLOC;
+	priv->qos_sch_stat[node_id].p_flag |= PP_NODE_ALLOC;
+	return DP_SUCCESS;
+}
+
+/* node_queue_inc API
+ * for queue id = node_id, flag = DP_NODE_INC
+ * Set Queue flag from PP_NODE_ALLOC to PP_NODE_ACTIVE
+ * else return DP_FAILURE
+ */
+static int node_queue_inc(int inst, int node_id, int flag)
+{
+	struct hal_priv *priv;
+	int phy_id, pid, idx = 0;
+
+	priv = HAL(inst);
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	phy_id = get_qid_by_node(inst, node_id, flag);
+	if (phy_id == DP_FAILURE) {
+		PR_ERR("get_qid_by_node failed\n");
+		return DP_FAILURE;
+	}
+
+	pid = get_parent_node(inst, node_id, flag);
+	if (pid == DP_FAILURE) {
+		PR_ERR("get_parent_node failed for Q:%d\n", phy_id);
+		return DP_FAILURE;
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS, "parent:%d of Q:%d\n", pid, phy_id);
+
+	idx = get_free_child_idx(inst, pid, 0);
+	if (idx == DP_FAILURE) {
+		PR_ERR("get_free_child_idx failed for Q:%d\n", phy_id);
+		return DP_FAILURE;
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "node_queue_inc: parent:%d of Q:[%d/%d] child idx:%d\n",
+		 pid, phy_id, node_id, idx);
+
+	if (!(priv->qos_queue_stat[phy_id].flag & PP_NODE_ALLOC)) {
+		PR_ERR("Wrong Q[%d] Stat(%d):Expect ALLOC\n", phy_id,
+		       priv->qos_queue_stat[phy_id].flag);
+		return DP_FAILURE;
+	}
+	if (pid == priv->qos_sch_stat[node_id].parent.node_id) {
+		priv->qos_sch_stat[pid].child[idx].flag = PP_NODE_ACTIVE;
+		priv->qos_sch_stat[pid].child[idx].node_id = node_id;
+		priv->qos_sch_stat[pid].child[idx].type = DP_NODE_QUEUE;
+	}
+	priv->qos_queue_stat[phy_id].flag |= PP_NODE_ACTIVE;
+	priv->qos_sch_stat[node_id].p_flag |= PP_NODE_ACTIVE;
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "Q:[%d] type:%d idx:%d attach to parent:%d\n",
+		 CHILD(pid, idx).node_id,
+		 CHILD(pid, idx).type, idx, pid);
+	return DP_SUCCESS;
+}
+
+/* node_queue_rst API
+ * for queue id = node_id, flag = DP_NODE_RST
+ * Set Queue flag from PP_NODE_ALLOC to PP_NODE_FREE
+ * Set allocated memory free
+ * else return DP_FAILURE
+ */
+static int node_queue_rst(int inst, int node_id, int flag)
+{
+	struct hal_priv *priv;
+	int phy_id = get_qid_by_node(inst, node_id, flag);
+	int dp_port, resv_idx;
+
+	priv = HAL(inst);
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (phy_id == DP_FAILURE) {
+		PR_ERR("get_qid_by_node failed\n");
+		return DP_FAILURE;
+	}
+	dp_port = priv->qos_queue_stat[phy_id].dp_port;
+	resv_idx = priv->qos_queue_stat[phy_id].resv_idx;
+
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "node_queue_rst: Q:[%d/%d] resv_idx:%d\n",
+		 phy_id, node_id, resv_idx);
+
+	if (!(priv->qos_queue_stat[phy_id].flag & PP_NODE_ALLOC)) {
+		PR_ERR("Wrong Q[%d] Stat(%d):Expect ALLOC\n", phy_id,
+		       priv->qos_queue_stat[phy_id].flag);
+		return DP_FAILURE;
+	}
+
+	/* Check for Reserve resource */
+	if (priv->qos_queue_stat[phy_id].flag & PP_NODE_RESERVE) {
+		priv->resv[dp_port].resv_q[resv_idx].flag = PP_NODE_FREE;
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "node_queue_rst:Q:[%d/%d] resv_idx:%d of EP:%d free\n",
+			 phy_id, node_id, resv_idx, dp_port);
+	}
+	/* Remove resource from global table */
+	memset(&priv->qos_queue_stat[phy_id], 0,
+	       sizeof(priv->qos_queue_stat[phy_id]));
+	memset(&priv->qos_sch_stat[node_id], 0,
+	       sizeof(priv->qos_sch_stat[node_id]));
+	priv->qos_queue_stat[phy_id].resv_idx = INV_RESV_IDX;
+	priv->qos_sch_stat[node_id].resv_idx = INV_RESV_IDX;
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "node_queue_rst:%s Q:[%d/%d] resv_idx:%d dp_port=%d\n",
+		 "After mem free", phy_id, node_id, resv_idx, dp_port);
+	return DP_SUCCESS;
+}
+
+/* node_sched_dec API
+ * for scheduler id = node_id, flag = DP_NODE_DEC
+ * Set Sched flag from PP_NODE_ACTIVE to PP_NODE_ALLOC
+ * else return DP_FAILURE
+ */
+static int node_sched_dec(int inst, int node_id, int flag)
+{
+	struct hal_priv *priv;
+	int pid, idx;
+
+	priv = HAL(inst);
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (flag & C_FLAG) {
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "node_sched_dec: parentSCH:[%d]\n", node_id);
+
+		if (!priv->qos_sch_stat[node_id].child_num ||
+		    !(priv->qos_sch_stat[node_id].c_flag & PP_NODE_ACTIVE)) {
+			PR_ERR("Wrong Sch[%d] Stat(%d)/child_num(%d):%s\n",
+			       node_id, priv->qos_sch_stat[node_id].c_flag,
+			       priv->qos_sch_stat[node_id].child_num,
+			       "Expect ACTIVE Or non-zero child_num");
+			return DP_FAILURE;
+		}
+		priv->qos_sch_stat[node_id].child_num--;
+		if (!priv->qos_sch_stat[node_id].child_num)
+			priv->qos_sch_stat[node_id].c_flag |= PP_NODE_ALLOC;
+		return DP_SUCCESS;
+	} else if (flag & P_FLAG) {
+		pid = get_parent_node(inst, node_id, flag);
+		if (pid == DP_FAILURE) {
+			PR_ERR("get_parent_node failed for sched:%d\n",
+			       node_id);
+			return DP_FAILURE;
+		}
+
+		idx = get_child_idx_node_id(inst, node_id, flag);
+		if (idx == DP_FAILURE) {
+			PR_ERR("get_child_idx_node_id failed for sched:%d\n",
+			       node_id);
+			return DP_FAILURE;
+		}
+
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "node_sched_dec: parent:%d of SCH:[%d] child idx:%d\n",
+			 pid, node_id, idx);
+
+		if (!(priv->qos_sch_stat[node_id].p_flag & PP_NODE_ACTIVE)) {
+			PR_ERR("Wrong Sch[%d] Stat(%d):Expect ACTIVE\n",
+			       node_id, priv->qos_sch_stat[node_id].p_flag);
+			return DP_FAILURE;
+		}
+		if (!priv->qos_sch_stat[node_id].parent.flag) {
+			PR_ERR("Wrong SCH[%d] Parent Stat(%d):Expect ACTIV\n",
+			       node_id,
+			       priv->qos_sch_stat[node_id].parent.flag);
+			return DP_FAILURE;
+		}
+		if (pid == priv->qos_sch_stat[node_id].parent.node_id) {
+			priv->qos_sch_stat[pid].child[idx].flag = PP_NODE_FREE;
+			priv->qos_sch_stat[pid].child[idx].node_id = 0;
+			priv->qos_sch_stat[pid].child[idx].type = 0;
+		}
+		priv->qos_sch_stat[node_id].parent.node_id = 0;
+		priv->qos_sch_stat[node_id].parent.type = 0;
+		priv->qos_sch_stat[node_id].parent.flag = 0;
+		priv->qos_sch_stat[node_id].p_flag |= PP_NODE_ALLOC;
+		return DP_SUCCESS;
+	}
+	return DP_FAILURE;
+}
+
+/* node_sched_inc API
+ * for scheduler id = node_id, flag = DP_NODE_INC
+ * Set Sched flag from PP_NODE_ALLOC to PP_NODE_ACTIVE
+ * else return DP_FAILURE
+ */
+static int node_sched_inc(int inst, int node_id, int flag)
+{
+	struct hal_priv *priv;
+	int pid, idx;
+
+	priv = HAL(inst);
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (flag & C_FLAG) {
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "node_sched_inc: parent SCH:[%d]\n", node_id);
+
+		if (priv->qos_sch_stat[node_id].child_num &&
+		    !(priv->qos_sch_stat[node_id].c_flag & PP_NODE_ACTIVE)) {
+			PR_ERR("Wrong Sch[%d] Stat(%d)/child_num(%d):%s\n",
+			       node_id, priv->qos_sch_stat[node_id].c_flag,
+			       priv->qos_sch_stat[node_id].child_num,
+			       "Expect ACTIVE And Non-Zero child_num");
+			return DP_FAILURE;
+		}
+		if (!priv->qos_sch_stat[node_id].child_num &&
+		    !(priv->qos_sch_stat[node_id].c_flag & PP_NODE_ALLOC)) {
+			PR_ERR("Wrong Sch[%d] Stat(%d)/child_num(%d):%s\n",
+			       node_id, priv->qos_sch_stat[node_id].c_flag,
+			       priv->qos_sch_stat[node_id].child_num,
+			       "Expect ALLOC And zero child_num");
+			return DP_FAILURE;
+		}
+		priv->qos_sch_stat[node_id].child_num++;
+		priv->qos_sch_stat[node_id].c_flag |= PP_NODE_ACTIVE;
+		return DP_SUCCESS;
+	} else if (flag & P_FLAG) {
+		pid = get_parent_node(inst, node_id, flag);
+		if (pid == DP_FAILURE) {
+			PR_ERR("get_parent_node failed for sched:%d\n",
+			       node_id);
+			return DP_FAILURE;
+		}
+
+		idx = get_free_child_idx(inst, pid, 0);
+		if (idx == DP_FAILURE) {
+			PR_ERR("get_free_child_idx failed for sched:%d\n",
+			       node_id);
+			return DP_FAILURE;
+		}
+
+		if (!(priv->qos_sch_stat[node_id].p_flag & PP_NODE_ALLOC)) {
+			PR_ERR("Wrong Sch[%d] Stat(%d):Expect ALLOC\n",
+			       node_id, priv->qos_sch_stat[node_id].p_flag);
+			return DP_FAILURE;
+		}
+
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "node_sched_inc: parent:%d of SCH:[%d] child idx:%d\n",
+			 pid, node_id, idx);
+
+		if (pid == priv->qos_sch_stat[node_id].parent.node_id) {
+			CHILD(pid, idx).flag = PP_NODE_ACTIVE;
+			CHILD(pid, idx).node_id = node_id;
+			CHILD(pid, idx).type = DP_NODE_SCH;
+		}
+		priv->qos_sch_stat[node_id].p_flag |= PP_NODE_ACTIVE;
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "SCH:[%d] type:%d idx:%d attach to parent:%d\n",
+			 CHILD(pid, idx).node_id,
+			 CHILD(pid, idx).type, idx, pid);
+		return DP_SUCCESS;
+	}
+	return DP_FAILURE;
+}
+
+/* node_sched_rst API
+ * for scheduler id = node_id, flag = DP_NODE_RST
+ * sanity check for child_num and both c and p_flag in alloc state
+ * then reset whole sched
+ * Set Sched flag from PP_NODE_ALLOC to PP_NODE_FREE
+ * else return DP_FAILURE
+ */
+static int node_sched_rst(int inst, int node_id, int flag)
+{
+	struct hal_priv *priv;
+	int dp_port, resv_idx;
+
+	priv = HAL(inst);
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	dp_port = priv->qos_sch_stat[node_id].dp_port;
+	resv_idx = priv->qos_sch_stat[node_id].resv_idx;
+
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "node_sched_rst:dp_port=%d SCH:%d resv_idx:%d\n",
+		 dp_port, node_id, resv_idx);
+	/* sanity check for child_num and both c and p_flag in alloc state
+	 * then reset whole sched
+	 */
+	if (priv->qos_sch_stat[node_id].child_num ||
+	    !(priv->qos_sch_stat[node_id].c_flag & PP_NODE_ALLOC) ||
+	    !(priv->qos_sch_stat[node_id].p_flag & PP_NODE_ALLOC)) {
+		PR_ERR("Wrong Sch[%d] c_flag(%d)/p_flag(%d)/child_num(%d):%s\n",
+		       node_id, priv->qos_sch_stat[node_id].c_flag,
+		       priv->qos_sch_stat[node_id].p_flag,
+		       priv->qos_sch_stat[node_id].child_num,
+		       "Expect c_flag OR p_flag ALLOC OR Non-zero child_num");
+		return DP_FAILURE;
+	}
+	/* Free Reserved Resource */
+	if (priv->qos_sch_stat[node_id].p_flag & PP_NODE_RESERVE) {
+		priv->resv[dp_port].resv_sched[resv_idx].flag = PP_NODE_FREE;
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "node_sch_rst:Sch:[%d] resv_idx:%d of EP:%d free\n",
+			 node_id, resv_idx, dp_port);
+	}
+	/* Free Global Resource */
+	memset(&priv->qos_sch_stat[node_id], 0,
+	       sizeof(priv->qos_sch_stat[node_id]));
+	priv->qos_sch_stat[node_id].resv_idx = INV_RESV_IDX;
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "node_queue_rst:%s SCH:[/%d] resv_idx:%d dp_port=%d\n",
+		 "After mem free", node_id, resv_idx, dp_port);
+	return DP_SUCCESS;
+}
+
+/* node_port_dec API
+ * Check for child_num and active flag
+ * for port logical node_id, flag = DP_NODE_DEC
+ * Set Port flag from PP_NODE_ACTIVE to PP_NODE_ALLOC
+ * else return DP_FAILURE
+ */
+static int node_port_dec(int inst, int node_id, int flag)
+{
+	struct hal_priv *priv;
+	int phy_id;
+
+	priv = HAL(inst);
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	phy_id = get_cqm_deq_port_by_node(inst, node_id, flag);
+	if (phy_id == DP_FAILURE) {
+		PR_ERR("get_cqm_deq_port_by_node failed\n");
+		return DP_FAILURE;
+	}
+
+	if (!priv->deq_port_stat[phy_id].child_num ||
+	    !(priv->deq_port_stat[phy_id].flag & PP_NODE_ACTIVE)) {
+		PR_ERR("Wrong P[%d] Stat(%d)/child_num(%d):%s\n",
+		       phy_id, priv->deq_port_stat[phy_id].flag,
+		       priv->deq_port_stat[phy_id].child_num,
+		       "Expect ACTIVE Or non-zero child_num");
+		return DP_FAILURE;
+	}
+	priv->qos_sch_stat[node_id].child_num--;
+	priv->deq_port_stat[phy_id].child_num--;
+	if (!priv->deq_port_stat[phy_id].child_num)
+		priv->deq_port_stat[phy_id].flag = PP_NODE_ALLOC;
+	return DP_SUCCESS;
+}
+
+/* node_port_inc API
+ * for port logical node_id, flag = DP_NODE_INC
+ * Set Port flag from PP_NODE_ALLOC to PP_NODE_ACTIVE
+ * else return DP_FAILURE
+ */
+static int node_port_inc(int inst, int node_id, int flag)
+{
+	struct hal_priv *priv;
+	int phy_id;
+
+	priv = HAL(inst);
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	phy_id = get_cqm_deq_port_by_node(inst, node_id, flag);
+	if (phy_id == DP_FAILURE) {
+		PR_ERR("get_cqm_deq_port_by_node failed\n");
+		return DP_FAILURE;
+	}
+
+	if (priv->deq_port_stat[phy_id].child_num &&
+	    !(priv->deq_port_stat[phy_id].flag & PP_NODE_ACTIVE)) {
+		PR_ERR("Wrong P[%d] Stat(%d)/child_num(%d):%s\n", phy_id,
+		       priv->deq_port_stat[phy_id].flag,
+		       priv->deq_port_stat[phy_id].child_num,
+		       "Expect ACTIVE And Non-Zero child_num");
+		return DP_FAILURE;
+	}
+	if (!priv->deq_port_stat[phy_id].child_num &&
+	    !(priv->deq_port_stat[phy_id].flag & PP_NODE_ALLOC)) {
+		PR_ERR("Wrong P[%d] Stat(%d)/child_num(%d):%s\n", phy_id,
+		       priv->deq_port_stat[phy_id].flag,
+		       priv->deq_port_stat[phy_id].child_num,
+		       "Expect ALLOC And Zero child_num");
+		return DP_FAILURE;
+	}
+	priv->qos_sch_stat[node_id].child_num++;
+	priv->deq_port_stat[phy_id].child_num++;
+	priv->deq_port_stat[phy_id].flag = PP_NODE_ACTIVE;
+	return DP_SUCCESS;
+}
+
+/* node_port_rst API
+ * Check for child_num and alloc flag
+ * for port logical node_id, flag = DP_NODE_RST
+ * Set Port flag from PP_NODE_ALLOC to PP_NODE_FREE
+ * else return DP_FAILURE
+ */
+static int node_port_rst(int inst, int node_id, int flag)
+{
+	struct hal_priv *priv;
+	int phy_id;
+
+	priv = HAL(inst);
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	phy_id = get_cqm_deq_port_by_node(inst, node_id, flag);
+	if (phy_id == DP_FAILURE) {
+		PR_ERR("get_cqm_deq_port_by_node failed\n");
+		return DP_FAILURE;
+	}
+
+	if (priv->deq_port_stat[phy_id].child_num ||
+	    !(priv->deq_port_stat[phy_id].flag & PP_NODE_ALLOC)) {
+		PR_ERR("Wrong P[%d] Stat(%d)/child_num(%d):%s\n", phy_id,
+		       priv->deq_port_stat[phy_id].flag,
+		       priv->deq_port_stat[phy_id].child_num,
+		       "Expect ALLOC Or non-zero child_num");
+		return DP_FAILURE;
+	}
+	memset(&priv->deq_port_stat[phy_id], 0,
+	       sizeof(priv->deq_port_stat[phy_id]));
+	memset(&priv->qos_sch_stat[node_id], 0,
+	       sizeof(priv->qos_sch_stat[node_id]));
+	return DP_SUCCESS;
+}
+
+/* node_stat_update API
+ * node_id is logical node id
+ * if flag = DP_NODE_DEC
+ *           update flag PP_NODE_ACTIVE to PP_NODE_ALLOC if needed
+ *           update child info
+ * else if flag = DP_NODE_INC
+ *           update flag PP_NODE_ALLOC to PP_NODE_ACTIVE
+ * else if flag = DP_NODE_RST
+ *           update flag PP_NODE_ALLOC to PP_NODE_FREE
+ *           reset table info
+ * else return DP_FAILURE
+ */
+static int node_stat_update(int inst, int node_id, int flag)
+{
+	int node_type = get_node_type_by_node_id(inst, node_id, 0);
+
+	if (flag & DP_NODE_DEC) {
+		if (node_type == DP_NODE_QUEUE)
+			return node_queue_dec(inst, node_id, flag);
+		else if (node_type == DP_NODE_SCH)
+			return node_sched_dec(inst, node_id, flag);
+		else if (node_type == DP_NODE_PORT)
+			return node_port_dec(inst, node_id, flag);
+		return DP_FAILURE;
+	} else if (flag & DP_NODE_INC) {
+		if (node_type == DP_NODE_QUEUE)
+			return node_queue_inc(inst, node_id, flag);
+		else if (node_type == DP_NODE_SCH)
+			return node_sched_inc(inst, node_id, flag);
+		else if (node_type == DP_NODE_PORT)
+			return node_port_inc(inst, node_id, flag);
+		return DP_FAILURE;
+	} else if (flag & DP_NODE_RST) {
+		if (node_type == DP_NODE_QUEUE)
+			return node_queue_rst(inst, node_id, flag);
+		else if (node_type == DP_NODE_SCH)
+			return node_sched_rst(inst, node_id, flag);
+		else if (node_type == DP_NODE_PORT)
+			return node_port_rst(inst, node_id, flag);
+		return DP_FAILURE;
+	}
+	return DP_FAILURE;
+}
+
+#ifdef DP_TMP_DEL
+/* dp_link_unset API
+ * call node_stat_update with DP_NODE_DEC flag
+ * upon success unlinks node to parent and returns DP_SUCCESS
+ * else return DP_FAILURE
+ */
+static int dp_link_unset(struct dp_node_link *info, int flag)
+{
+	int node_id, p_id;
+	struct pp_qos_queue_conf queue_cfg = {0};
+	struct pp_qos_sched_conf sched_cfg = {0};
+	struct hal_priv *priv;
+
+	if (!info) {
+		PR_ERR("info cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(info->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (info->node_type == DP_NODE_QUEUE) {
+		node_id = priv->qos_queue_stat[info->node_id.q_id].node_id;
+		if (qos_queue_conf_get_32(priv->qdev, node_id, &queue_cfg)) {
+			PR_ERR("failed to qos_queue_conf_get_32\n");
+			return DP_FAILURE;
+		}
+		if (queue_cfg.queue_child_prop.parent ==
+						*(int *)&info->p_node_id) {
+			if (qos_queue_remove_32(priv->qdev, node_id)) {
+				PR_ERR("failed to qos_queue_remove_32\n");
+				return DP_FAILURE;
+			}
+		}
+		goto EXIT;
+	} else if (info->node_type == DP_NODE_SCH) {
+		if (qos_sched_conf_get_32(priv->qdev, info->node_id.sch_id,
+				       &sched_cfg)) {
+			PR_ERR("failed to qos_queue_conf_get_32\n");
+			return DP_FAILURE;
+		}
+		if (sched_cfg.sched_child_prop.parent ==
+						*(int *)&info->p_node_id) {
+			if (qos_sched_remove_32(priv->qdev,
+					     info->node_id.sch_id)) {
+				PR_ERR("failed to qos_sched_remove_32\n");
+				return DP_FAILURE;
+			}
+		}
+		goto EXIT;
+	}
+	return DP_FAILURE;
+
+EXIT:
+	/* Child flag update after unlink */
+	node_stat_update(info->inst, *(int *)&info->node_id,
+			 DP_NODE_DEC | P_FLAG);
+	/* reduce child_num in parent's global table */
+	if (info->p_node_type == DP_NODE_SCH)
+		node_stat_update(info->inst, PARENT(queue_cfg),
+				 DP_NODE_DEC | C_FLAG);
+	else /* convert parent to cqm_deq_port */
+		p_id = get_cqm_deq_port_by_node(info->inst,
+						*(int *)&info->p_node_id,
+						flag);
+		node_stat_update(info->inst, p_id, DP_NODE_DEC);
+	return DP_SUCCESS;
+}
+#endif
+
+/* dp_node_alloc_resv_pool API
+ * Checks for flag and input node
+ * upon success allocate resource from reserve table
+ * otherwise return failure
+ */
+static int dp_node_alloc_resv_pool(struct dp_node_alloc *node, int flag)
+{
+	int i, cnt, phy_id, node_id;
+	struct hal_priv *priv;
+	struct resv_q *resv_q;
+
+	if (!node) {
+		PR_ERR("node is  NULL\n");
+		return DP_FAILURE;
+	}
+	priv = node ? HAL(node->inst) : NULL;
+	if (!priv) {
+		PR_ERR("priv is NULL\n");
+		return DP_FAILURE;
+	}
+	resv_q = priv->resv[node->dp_port].resv_q;
+
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "inst=%d dp_port=%d num_resv_q=%d num_resv_sched=%d\n",
+		 node->inst,
+		 node->dp_port,
+		 priv->resv[node->dp_port].num_resv_q,
+		 priv->resv[node->dp_port].num_resv_sched);
+
+	if (node->type == DP_NODE_QUEUE) {
+		cnt = priv->resv[node->dp_port].num_resv_q;
+		if (!cnt)
+			return DP_FAILURE;
+		DP_DEBUG(DP_DBG_FLAG_QOS, "try to look for resv queue...\n");
+		for (i = 0; i < cnt; i++) {
+			if (resv_q[i].flag != PP_NODE_FREE)
+				continue;
+			phy_id = resv_q[i].physical_id;
+			node_id = resv_q[i].id;
+			resv_q[i].flag = PP_NODE_ALLOC;
+			priv->qos_queue_stat[phy_id].flag = PP_NODE_RESERVE |
+								PP_NODE_ALLOC;
+			priv->qos_queue_stat[phy_id].node_id = node_id;
+			priv->qos_queue_stat[phy_id].resv_idx = i;
+			priv->qos_queue_stat[phy_id].dp_port = node->dp_port;
+			priv->qos_sch_stat[node_id].dp_port = node->dp_port;
+			priv->qos_sch_stat[node_id].resv_idx = i;
+			priv->qos_sch_stat[node_id].type = DP_NODE_QUEUE;
+			priv->qos_sch_stat[node_id].p_flag = PP_NODE_RESERVE |
+								PP_NODE_ALLOC;
+			node->id.q_id = phy_id;
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "queue[%d/%d]:Resv_idx=%d\n",
+				 phy_id, node_id,
+				 priv->qos_queue_stat[phy_id].resv_idx);
+			return DP_SUCCESS;
+		}
+	} else if (node->type == DP_NODE_SCH) {
+		struct resv_sch *resv_sched;
+
+		cnt = priv->resv[node->dp_port].num_resv_sched;
+		if (!cnt)
+			return DP_FAILURE;
+		resv_sched = priv->resv[node->dp_port].resv_sched;
+		for (i = 0; i < cnt; i++) {
+			if (resv_sched[i].flag != PP_NODE_FREE)
+				continue;
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "try to look for resv sche...\n");
+			node_id = resv_sched[i].id;
+			resv_sched[i].flag = PP_NODE_ALLOC;
+			priv->qos_sch_stat[node_id].c_flag = PP_NODE_RESERVE |
+								PP_NODE_ALLOC;
+			priv->qos_sch_stat[node_id].p_flag = PP_NODE_RESERVE |
+								PP_NODE_ALLOC;
+			priv->qos_sch_stat[node_id].resv_idx = i;
+			priv->qos_sch_stat[node_id].child_num = 0;
+			priv->qos_sch_stat[node_id].dp_port = node->dp_port;
+			priv->qos_sch_stat[node_id].type = DP_NODE_SCH;
+			node->id.sch_id = node_id;
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "Sched[/%d]: Resv_idx=%d\n",
+				 resv_sched[i].id,
+				 priv->qos_sch_stat[node_id].resv_idx);
+			return DP_SUCCESS;
+		}
+	}
+	return DP_FAILURE;
+}
+
+/* dp_node_alloc_global_pool API
+ * Checks for flag and input node
+ * upon success allocate resource from global table
+ * otherwise return failure
+ */
+static int dp_node_alloc_global_pool(struct dp_node_alloc *node, int flag)
+{
+	int id, phy_id;
+	struct pp_qos_queue_info *qos_queue_info = NULL;
+	struct pp_qos_queue_conf *q_conf = NULL;
+	struct hal_priv *priv;
+	int res = DP_FAILURE;
+
+	if (!node) {
+		PR_ERR("node is  NULL\n");
+		goto EXIT;
+	}
+
+	priv = HAL(node->inst);
+	if (!priv) {
+		PR_ERR("priv is NULL\n");
+		goto EXIT;
+	}
+
+	q_conf = kzalloc(sizeof(*q_conf), GFP_KERNEL);
+	if (!q_conf) {
+		PR_ERR("fail to alloc %zd bytes\n", sizeof(*q_conf));
+		goto EXIT;
+	}
+
+	qos_queue_info = kzalloc(sizeof(*qos_queue_info), GFP_KERNEL);
+	if (!qos_queue_info) {
+		PR_ERR("fail to alloc %zd bytes\n", sizeof(*qos_queue_info));
+		goto EXIT;
+	}
+
+	if (node->type == DP_NODE_QUEUE) {
+		if (qos_queue_allocate_32(priv->qdev, &id)) {
+			PR_ERR("qos_queue_allocate_32 failed\n");
+			goto EXIT;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS, "qos_queue_allocate_32: %d\n", id);
+#define WORKAROUND_PORT_SET
+#define WORKAROUND_DROP_PORT
+#ifdef WORKAROUND_PORT_SET /* workaround to get physcal queue id */
+		/* workarpound: in order to get queue's physical queue id,
+		 * it is a must call pp_qos_queue_set first,
+		 * then call qos_queue_info_get_32 to get physical queue id
+		 */
+		qos_queue_conf_set_default_32(q_conf);
+		dp_wred_def(q_conf);
+#ifdef WORKAROUND_DROP_PORT /* use drop port */
+		q_conf->queue_child_prop.parent = priv->ppv4_drop_p;
+#else
+		q_conf->queue_child_prop.parent = priv->ppv4_tmp_p;
+#endif /* WORKAROUND_DROP_PORT */
+		if (qos_queue_set_32(priv->qdev, id, q_conf)) {
+			PR_ERR("qos_queue_set_32 fail for queue=%d to parent=%d\n",
+			       id, q_conf->queue_child_prop.parent);
+			goto EXIT;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Workaround queue(/%d)-> tmp parent(/%d)\n",
+			 id, q_conf->queue_child_prop.parent);
+		/* workarpound end ---- */
+#endif /* WORKAROUND_PORT_SET */
+		if (qos_queue_info_get_32(priv->qdev, id, qos_queue_info)) {
+			qos_queue_remove_32(priv->qdev, id);
+			PR_ERR("qos_queue_info_get_32: %d\n", id);
+			goto EXIT;
+		}
+
+		phy_id = qos_queue_info->physical_id;
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "queue info: %d/%d to parent(/%d) dp_port=%d\n",
+			 phy_id, id,
+			 q_conf->queue_child_prop.parent, node->dp_port);
+		priv->qos_queue_stat[phy_id].flag = PP_NODE_ALLOC;
+		priv->qos_queue_stat[phy_id].node_id = id;
+		priv->qos_queue_stat[phy_id].resv_idx = INV_RESV_IDX;
+		priv->qos_queue_stat[phy_id].dp_port = node->dp_port;
+		priv->qos_sch_stat[id].dp_port = node->dp_port;
+		priv->qos_sch_stat[id].resv_idx = INV_RESV_IDX;
+		priv->qos_sch_stat[id].type = DP_NODE_QUEUE;
+		priv->qos_sch_stat[id].p_flag = PP_NODE_ALLOC;
+		node->id.q_id = phy_id;
+		res = DP_SUCCESS;
+		goto EXIT;
+	} else if (node->type == DP_NODE_SCH) {
+		if (qos_sched_allocate_32(priv->qdev, &id)) {
+			PR_ERR("failed to qos_sched_allocate_32\n");
+			qos_sched_remove_32(priv->qdev, id);
+			goto EXIT;
+		}
+		priv->qos_sch_stat[id].c_flag = PP_NODE_ALLOC;
+		priv->qos_sch_stat[id].p_flag = PP_NODE_ALLOC;
+		priv->qos_sch_stat[id].resv_idx = INV_RESV_IDX;
+		priv->qos_sch_stat[id].dp_port = node->dp_port;
+		priv->qos_sch_stat[id].child_num = 0;
+		priv->qos_sch_stat[id].type = DP_NODE_SCH;
+		node->id.sch_id = id;
+		res = DP_SUCCESS;
+		goto EXIT;
+	} else {
+		PR_ERR("Unknown node type %d\n", node->type);
+	}
+
+EXIT:
+	kfree(q_conf);
+	kfree(qos_queue_info);
+	return res;
+}
+
+/* dp_alloc_qos_port API
+ * upon success returns qos_deq_port
+ * otherwise return failure
+ */
+static int dp_alloc_qos_port(struct dp_node_alloc *node, int flag)
+{
+	unsigned int qos_port;
+	int cqm_deq_port;
+	int inst;
+	struct pp_qos_port_conf port_cfg;
+	struct hal_priv *priv;
+
+	if (!node) {
+		PR_ERR("node NULL\n");
+		goto EXIT;
+	}
+	inst = node->inst;
+	priv = HAL(node->inst);
+	if (!priv) {
+		PR_ERR("priv NULL\n");
+		goto EXIT;
+	}
+	cqm_deq_port = node->id.cqm_deq_port;
+	DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+		 "inst=%d dp_port=%d cqm_deq_port=%d\n",
+		 node->inst, node->dp_port, cqm_deq_port);
+	if (cqm_deq_port == DP_NODE_AUTO_ID) {
+		PR_ERR("cqm_deq_port wrong: %d\n", cqm_deq_port);
+		goto EXIT;
+	}
+	if (priv->deq_port_stat[cqm_deq_port].flag != PP_NODE_FREE) {
+		DP_DEBUG(DP_DBG_FLAG_QOS, "cqm_deq_port[%d] already init:\n",
+			 cqm_deq_port);
+		return priv->deq_port_stat[cqm_deq_port].node_id;
+	}
+	if (qos_port_allocate_32(priv->qdev, cqm_deq_port, &qos_port)) {
+		PR_ERR("failed to qos_port_allocate_32:%d\n", cqm_deq_port);
+		goto EXIT;
+	}
+	/* PR_INFO("qos_port_alloc succeed: %d/%d\n",
+	 *	   cqm_deq_port, qos_port);
+	 */
+	/* Configure QOS dequeue port */
+	qos_port_conf_set_default_32(&port_cfg);
+	port_cfg.ring_address =
+		(unsigned long)dp_deq_port_tbl[inst][cqm_deq_port].
+							txpush_addr_qos;
+	port_cfg.ring_size = dp_deq_port_tbl[inst][cqm_deq_port].tx_ring_size;
+	port_cfg.credit = dp_deq_port_tbl[inst][cqm_deq_port].tx_pkt_credit;
+	if (port_cfg.credit)
+		port_cfg.packet_credit_enable = 1;
+	port_cfg.port_parent_prop.arbitration = PP_QOS_ARBITRATION_WRR;
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_DBG)
+	if (unlikely(dp_dbg_flag & DP_DBG_FLAG_QOS)) {
+		DP_DEBUG(DP_DBG_FLAG_QOS, "qos_port_set_32 parameter: %d/%d\n",
+			 cqm_deq_port, qos_port);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  ring_address_push=0x%lx\n",
+			 port_cfg.ring_address);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  ring_size=%d\n",
+			 port_cfg.ring_size);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  credit=%d\n",
+			 port_cfg.credit);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  packet_credit_enable=%d\n",
+			 port_cfg.packet_credit_enable);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  Arbitration=%d\n",
+			 port_cfg.port_parent_prop.arbitration);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "priv->qdev=0x%px\n",
+			 priv->qdev);
+	}
+#endif
+	if (qos_port_set_32(priv->qdev, qos_port, &port_cfg)) {
+		PR_ERR("qos_port_set_32 fail for port %d/%d\n",
+		       cqm_deq_port, qos_port);
+		qos_port_remove_32(priv->qdev, qos_port);
+		goto EXIT;
+	}
+	priv->deq_port_stat[cqm_deq_port].flag = PP_NODE_ALLOC;
+	priv->qos_sch_stat[qos_port].type = DP_NODE_PORT;
+	priv->qos_sch_stat[qos_port].child_num = 0;
+	priv->deq_port_stat[cqm_deq_port].node_id = qos_port;
+	DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+		 "dp_alloc_qos_port ok: port=%d/%d for dp_port=%d\n",
+		 cqm_deq_port, qos_port, node->dp_port);
+	return qos_port;
+EXIT:
+	return DP_FAILURE;
+}
+
+/* dp_node_alloc_32 API
+ * Checks for flag and input node type
+ * upon success allocate node from global/reserve resource
+ * otherwise return failure
+ */
+int dp_node_alloc_32(struct dp_node_alloc *node, int flag)
+{
+	struct hal_priv *priv;
+
+	if (!node) {
+		PR_ERR("node NULL\n");
+		return DP_FAILURE;
+	}
+
+	priv = HAL(node->inst);
+	if (!priv) {
+		PR_ERR("priv is NULL cannot proceed!!\n");
+		return DP_FAILURE;
+	}
+
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "allocate flag %d\n", flag);
+	if (flag & DP_ALLOC_RESV_ONLY) {
+	/* if can get from its reserved resource and return DP_SUCCESS.
+	 *	Otherwise return DP_FAIL;
+	 */
+		return dp_node_alloc_resv_pool(node, flag);
+	}
+	if (flag & DP_ALLOC_GLOBAL_ONLY) {
+	/* if can get from global pool and return DP_SUCCESS.
+	 *	Otherwise return DP_FAILURE;
+	 */
+		return dp_node_alloc_global_pool(node, flag);
+	}
+	if (flag & DP_ALLOC_GLOBAL_FIRST) {
+	/* if can get from the global pool, return DP_SUCCESS;
+	 * if can get from its reserved resource and return DP_SUCCESS;
+	 * return DP_FAILURE;
+	 */
+		if (dp_node_alloc_global_pool(node, flag) == DP_SUCCESS)
+			return DP_SUCCESS;
+		return dp_node_alloc_resv_pool(node, flag);
+	}
+	/* default order: reserved pool, */
+	/* if can get from its reserved resource and return DP_SUCCESS
+	 * if can get from the global pool, return DP_SUCCESS
+	 * return DP_FAILURE
+	 */
+	if (dp_node_alloc_resv_pool(node, flag) == DP_SUCCESS)
+		return DP_SUCCESS;
+	return dp_node_alloc_global_pool(node, flag);
+}
+
+/* dp_map_to_drop_q_32 API
+ * check index in lookup table for q_id
+ * param q_buf: to save the lookup entries which mapped to this q_id
+ * param num: q_buf size
+ * set drop_q and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_map_to_drop_q_32(int inst, int q_id, struct dp_lookup_entry *lookup)
+{
+	u32 i, k = 0;
+	struct hal_priv *priv = HAL(inst);
+	struct cbm_lookup cbm_lookup = {0};
+
+	for (i = 0; i < MAX_LOOKUP_TBL_SIZE; i++) {
+		cbm_lookup.index = i;
+		cbm_lookup.egflag = 0;
+		if (q_id == dp_get_lookup_qid_via_index(&cbm_lookup)) {
+			if (lookup) {
+				lookup->entry[k] = i;
+				k++;
+			}
+			cbm_lookup.index = i;
+			cbm_lookup.qid = priv->ppv4_drop_q;
+			dp_set_lookup_qid_via_index(&cbm_lookup);
+		}
+	}
+	if (lookup)
+		lookup->num = k;
+	return DP_SUCCESS;
+}
+
+/* dp_smart_free_from_child_32 API
+ * flush and unlink queue from its parent
+ * check parent's child list if empty free parent recursively
+ * else return DP_FAILURE
+ */
+static int dp_smart_free_from_child_32(struct dp_node_alloc *node, int flag)
+{
+	int id, res, f_free;
+	struct dp_node_link info = {0};
+	struct dp_node_alloc temp = {0};
+	struct hal_priv *priv;
+
+	if (!node) {
+		PR_ERR("node is NULL cannot proceed!!\n");
+		return DP_FAILURE;
+	}
+
+	priv = HAL(node->inst);
+	if (!priv) {
+		PR_ERR("priv is NULL cannot proceed!!\n");
+		return DP_FAILURE;
+	}
+
+	DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+		 "dp_smart_free_from_child_32:type=%d q_id=%d\n",
+		 node->type,
+		 node->id.q_id);
+	if (node->type == DP_NODE_QUEUE) {
+		if ((node->id.q_id < 0) || (node->id.q_id >= MAX_QUEUE)) {
+			PR_ERR("Wrong Parameter: QID[%d]Out Of Range\n",
+			       node->id.q_id);
+			return DP_FAILURE;
+		}
+
+		info.node_id.q_id = node->id.q_id;
+		info.node_type = node->type;
+		id = priv->qos_queue_stat[node->id.q_id].node_id;
+
+		info.p_node_id.q_id = priv->qos_sch_stat[id].parent.node_id;
+		info.p_node_type = priv->qos_sch_stat[id].parent.type;
+
+		if (!info.p_node_id.q_id) {
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "current node doesnot have parent\n");
+		} else if (dp_node_unlink_32(&info, 0)) {
+			PR_ERR("dp_node_unlink_32 failed\n");
+			return DP_FAILURE;
+		}
+
+		if (dp_node_free_32(node, 0)) {
+			PR_ERR("failed to free Queue:[%d]\n", node->id.q_id);
+			return DP_FAILURE;
+		}
+	} else if (node->type == DP_NODE_SCH) {
+		if ((node->id.sch_id < 0) ||
+		    (node->id.sch_id >= QOS_MAX_NODES)) {
+			PR_ERR("Wrong Parameter: Sched[%d]Out Of Range\n",
+			       node->id.sch_id);
+			return DP_FAILURE;
+		}
+
+		info.node_id.sch_id = node->id.sch_id;
+		info.node_type = node->type;
+		id = node->id.sch_id;
+		info.p_node_id.q_id = priv->qos_sch_stat[id].parent.node_id;
+		info.p_node_type = priv->qos_sch_stat[id].parent.type;
+
+		if (!info.p_node_id.sch_id) {
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "current node doesnot have parent\n");
+			return DP_FAILURE;
+		}
+
+		if (dp_node_free_32(node, 0)) {
+			PR_ERR("failed to free Sched:[%d]\n", node->id.sch_id);
+			return DP_FAILURE;
+		}
+	} else if (node->type == DP_NODE_PORT) {
+		if ((node->id.cqm_deq_port < 0) ||
+		    (node->id.cqm_deq_port >= MAX_CQM_DEQ)) {
+			PR_ERR("Wrong Parameter: Port[%d]Out Of Range\n",
+			       node->id.cqm_deq_port);
+			return DP_FAILURE;
+		}
+
+		if (dp_node_free_32(node, 0)) {
+			PR_ERR("failed to free Port:[%d]\n",
+			       node->id.cqm_deq_port);
+			return DP_FAILURE;
+		}
+		return DP_SUCCESS;
+	}
+
+	while (1) {
+		info.node_id = info.p_node_id;
+		info.node_type = info.p_node_type;
+		temp.id = info.p_node_id;
+		temp.type = info.p_node_type;
+
+		if (temp.type == DP_NODE_PORT) {
+			temp.id.cqm_deq_port =
+				get_cqm_deq_port_by_node(node->inst,
+							 temp.id.cqm_deq_port,
+							 flag);
+			id = temp.id.cqm_deq_port;
+			if ((id < 0) || (id >= MAX_CQM_DEQ)) {
+				PR_ERR("Wrong Parameter: Port[%d]%s\n",
+				       id, "Out Of Range");
+				return DP_FAILURE;
+			}
+		} else {
+			id = temp.id.sch_id;
+			if ((id < 0) || (id >= QOS_MAX_NODES)) {
+				PR_ERR("Wrong Parameter: Sched[%d]%s\n",
+				       id, "Out Of Range");
+				return DP_FAILURE;
+			}
+		}
+
+		DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+			 "dp_smart_free_from_child_32:type=%d q_id=%d\n",
+			 temp.type,
+			 id);
+		if ((temp.type == DP_NODE_SCH &&
+		     priv->qos_sch_stat[id].child_num) ||
+		    (temp.type == DP_NODE_PORT &&
+		     priv->deq_port_stat[id].child_num)) {
+			DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+				 "parent node(%d) still have child\n", id);
+			break;
+		}
+		f_free = (dp_node_link_get_32(&info, 0));
+		res = dp_node_free_32(&temp, 0);
+		if (res) {
+			PR_ERR("failed to free node:%d res %d\n",
+			       temp.id.q_id, res);
+			return DP_FAILURE;
+		}
+		if (f_free) {
+			DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+				 "current node doesnot have parent\n");
+			break;
+		}
+	}
+	return DP_SUCCESS;
+}
+
+/* dp_free_children_via_parent_32 API
+ * reset parent to free state
+ * check parent's child list and free all resources recursively
+ * else return DP_FAILURE
+ */
+int dp_free_children_via_parent_32(struct dp_node_alloc *node, int flag)
+{
+	int idx, id, pid, child_id;
+	struct dp_node_alloc temp = {0};
+	struct dp_node_link info = {0};
+	struct hal_priv *priv;
+
+	if (!node) {
+		PR_ERR("node is NULL cannot proceed!!\n");
+		return DP_FAILURE;
+	}
+
+	priv = HAL(node->inst);
+	if (!priv) {
+		PR_ERR("priv is NULL cannot proceed!!\n");
+		return DP_FAILURE;
+	}
+
+	if (node->type == DP_NODE_PORT) {
+		if ((node->id.cqm_deq_port < 0) ||
+		    (node->id.cqm_deq_port >= MAX_CQM_DEQ)) {
+			PR_ERR("Wrong Parameter: Port[%d]Out Of Range\n",
+			       node->id.cqm_deq_port);
+			return DP_FAILURE;
+		}
+		id = priv->deq_port_stat[node->id.cqm_deq_port].node_id;
+
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "parent Port(%d) have child num:%d\n",
+			 node->id.cqm_deq_port,
+			 priv->qos_sch_stat[id].child_num);
+
+		for (idx = 0; idx < DP_MAX_CHILD_PER_NODE; idx++) {
+			if (priv->qos_sch_stat[id].child[idx].flag &
+			    PP_NODE_ACTIVE) {
+				temp.type = CHILD(id, idx).type;
+				child_id =
+					get_qid_by_node(node->inst,
+							CHILD(id, idx).node_id,
+							0);
+				if (CHILD(id, idx).type == DP_NODE_SCH)
+					temp.id.q_id = CHILD(id, idx).node_id;
+				else
+					temp.id.q_id = child_id;
+				if (dp_free_children_via_parent_32(&temp, 0)) {
+					PR_ERR("fail %s=%d child:%d type=%d\n",
+					       "to free Port's",
+					       node->id.cqm_deq_port,
+					       CHILD(id, idx).node_id,
+					       CHILD(id, idx).type);
+					return DP_FAILURE;
+				}
+			}
+		}
+
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Port(%d)'s all children:%d freed!\n",
+			 node->id.cqm_deq_port,
+			 priv->qos_sch_stat[id].child_num);
+
+		if (!priv->qos_sch_stat[id].child_num) {
+			if (dp_node_free_32(node, 0)) {
+				PR_ERR("failed to free Port:[%d]\n",
+				       node->id.cqm_deq_port);
+				return DP_FAILURE;
+			}
+		}
+	} else if (node->type == DP_NODE_SCH) {
+		if ((node->id.sch_id < 0) ||
+		    (node->id.sch_id >= QOS_MAX_NODES)) {
+			PR_ERR("Wrong Parameter: Sched[%d]Out Of Range\n",
+			       node->id.sch_id);
+			return DP_FAILURE;
+		}
+		id = node->id.sch_id;
+
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "parent SCH(%d) have child num:%d\n",
+			 node->id.sch_id,
+			 priv->qos_sch_stat[id].child_num);
+
+		for (idx = 0; idx < DP_MAX_CHILD_PER_NODE; idx++) {
+			if (priv->qos_sch_stat[id].child[idx].flag &
+			    PP_NODE_ACTIVE) {
+				temp.type = CHILD(id, idx).type;
+				child_id =
+					get_qid_by_node(node->inst,
+							CHILD(id, idx).node_id,
+							0);
+				if (CHILD(id, idx).type == DP_NODE_SCH)
+					temp.id.q_id = CHILD(id, idx).node_id;
+				else
+					temp.id.q_id = child_id;
+				if (dp_free_children_via_parent_32(&temp, 0)) {
+					PR_ERR("fail %s=%d child:%d type=%d\n",
+					       "to free Sched's",
+					       node->id.sch_id,
+					       CHILD(id, idx).node_id,
+					       CHILD(id, idx).type);
+					return DP_FAILURE;
+				}
+				DP_DEBUG(DP_DBG_FLAG_QOS,
+					 "Free SCH(%d)'s child:%d done!\n",
+					 node->id.sch_id,
+					 CHILD(id, idx).node_id);
+			}
+		}
+
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "SCH(%d)'s all children:%d freed!\n",
+			 node->id.sch_id,
+			 priv->qos_sch_stat[id].child_num);
+
+		if (!priv->qos_sch_stat[id].child_num) {
+			if (dp_node_free_32(node, 0)) {
+				PR_ERR("failed to free Sched:[%d]\n",
+				       node->id.sch_id);
+				return DP_FAILURE;
+			}
+		}
+	} else if (node->type == DP_NODE_QUEUE) {
+		if ((node->id.q_id < 0) || (node->id.q_id >= MAX_QUEUE)) {
+			PR_ERR("Wrong Parameter: QID[%d]Out Of Range\n",
+			       node->id.q_id);
+			return DP_FAILURE;
+		}
+
+		/* get parent node from global table and fill info */
+		info.node_id.q_id = node->id.q_id;
+		info.node_type = node->type;
+		pid = priv->qos_queue_stat[node->id.q_id].node_id;
+		info.p_node_id.q_id = priv->qos_sch_stat[pid].parent.node_id;
+		info.p_node_type = priv->qos_sch_stat[pid].parent.type;
+
+		if (priv->qos_queue_stat[node->id.q_id].flag != PP_NODE_FREE) {
+			if (dp_node_unlink_32(&info, 0)) {
+				PR_ERR("dp_node_unlink_32 failed\n");
+				return DP_FAILURE;
+			}
+
+			if (dp_node_free_32(node, 0)) {
+				PR_ERR("failed to free Queue:[%d]\n",
+				       node->id.q_id);
+				return DP_FAILURE;
+			}
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "%s:Q[%d] Parent:%d type:%d\n",
+				 "free_children_via_parent",
+				 node->id.q_id,
+				 info.p_node_id.q_id,
+				 info.p_node_type);
+		}
+	} else {
+		PR_ERR("Incorrect Parameter:%d\n", node->type);
+		return DP_FAILURE;
+	}
+	return DP_SUCCESS;
+}
+
+struct link_free_var {
+	struct pp_qos_queue_conf queue_cfg;
+	struct pp_qos_queue_conf tmp_q;
+	struct pp_qos_sched_conf sched_cfg;
+	struct pp_qos_sched_conf tmp_sch;
+	struct dp_node_link info;
+	int sch_id, phy_id, node_id, parent_id, p_type;
+	int dp_port, resv_flag;
+};
+
+/* dp_node_free_32 API
+ * if (this node is not unlinked yet)
+ * unlink it
+ * If (this node is a reserved node)
+ * return this node to this device's reserved node table
+ * mark this node as Free in this device's reserved node table
+ * else
+ * return this node to the system global table
+ * mark this node free in system global table
+ */
+int dp_node_free_32(struct dp_node_alloc *node, int flag)
+{
+	struct hal_priv *priv;
+	struct link_free_var *t = NULL;
+	int res = DP_FAILURE;
+
+	if (!node) {
+		PR_ERR("node is NULL cannot proceed!!\n");
+		return DP_FAILURE;
+	}
+
+	priv = HAL(node->inst);
+	if (!priv) {
+		PR_ERR("priv is NULL cannot proceed!!\n");
+		return DP_FAILURE;
+	}
+	t = kzalloc(sizeof(*t), GFP_KERNEL);
+	if (!t) {
+		PR_ERR("fail to alloc %zd bytes\n", sizeof(*t));
+		return DP_FAILURE;
+	}
+
+	if (flag == DP_NODE_SMART_FREE) {/* dont pass flag */
+		res = dp_smart_free_from_child_32(node, 0);
+		if (res == DP_FAILURE) {
+			PR_ERR("dp_smart_free_from_child_32 failed\n");
+			goto EXIT;
+		}
+	}
+
+	if (node->type == DP_NODE_QUEUE) {
+		t->node_id = priv->qos_queue_stat[node->id.q_id].node_id;
+		t->dp_port = priv->qos_queue_stat[node->id.q_id].dp_port;
+		t->resv_flag = priv->qos_queue_stat[node->id.q_id].flag;
+
+		if (priv->qos_queue_stat[node->id.q_id].flag == PP_NODE_FREE) {
+			PR_ERR("Node Q[%d] is already Free Stat\n",
+			       node->id.q_id);
+			goto EXIT;
+		}
+
+		if (qos_queue_conf_get_32(priv->qdev, t->node_id, &t->queue_cfg)) {
+			res = node_stat_update(node->inst, t->node_id,
+					       DP_NODE_RST);
+			if (res == DP_FAILURE) {
+				PR_ERR("node_stat_update failed\n");
+				goto EXIT;
+			}
+			if (qos_queue_remove_32(priv->qdev, t->node_id)) {
+				PR_ERR("failed to qos_queue_remove_32\n");
+				goto EXIT;
+			}
+			res = DP_SUCCESS;
+			goto EXIT;
+		}
+		/* call node unlink api and set drop queue */
+		t->info.inst = node->inst;
+		t->info.node_id = node->id;
+		t->info.node_type = node->type;
+		if (dp_node_unlink_32(&t->info, 0)) {
+			PR_ERR("failed to dp_node_unlink_32 for Q:%d\n",
+			       node->id.q_id);
+			goto EXIT;
+		}
+
+		t->parent_id = t->queue_cfg.queue_child_prop.parent;
+		t->p_type = get_node_type_by_node_id(node->inst,
+						     t->parent_id, flag);
+		/* Remove Queue link only for global resource */
+		if (!(t->resv_flag & PP_NODE_RESERVE)) {
+			if (qos_queue_remove_32(priv->qdev, t->node_id)) {
+				PR_ERR("failed to qos_queue_remove_32\n");
+				goto EXIT;
+			}
+		}
+		if (node_stat_update(node->inst, t->node_id, DP_NODE_DEC)) {
+			PR_ERR("node_stat_update failed\n");
+			goto EXIT;
+		}
+		/* call node_stat_update to update parent status */
+		if (node_stat_update(node->inst, t->parent_id,
+				     DP_NODE_DEC | C_FLAG)) {
+			PR_ERR("stat update fail Q:[%d/%d]'s parent:%d\n",
+			       node->id.q_id, t->node_id, t->parent_id);
+			goto EXIT;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Q[%d] removed parent:[%d] stat updated\n",
+			 node->id.q_id, t->parent_id);
+		/* call node_Stat_update to free the node */
+		if (node_stat_update(node->inst, t->node_id,
+				     DP_NODE_RST)) {
+			PR_ERR("node_stat_update failed\n");
+			goto EXIT;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Queue[%d/%d] removed and stat updated\n",
+			 node->id.q_id, t->node_id);
+		/* Reserve Q temp attach to drop port */
+		if (t->resv_flag & PP_NODE_RESERVE) {
+			qos_queue_conf_set_default_32(&t->tmp_q);
+			t->tmp_q.queue_child_prop.parent = priv->ppv4_drop_p;
+			if (qos_queue_set_32(priv->qdev, t->node_id, &t->tmp_q)) {
+				PR_ERR("qos_queue_set_32 %s=%d to parent=%d\n",
+				       "fail to reserve queue", t->node_id,
+				       t->tmp_q.queue_child_prop.parent);
+				goto EXIT;
+			}
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "Q:%d/%d attached temp to Drop_port:%d\n",
+				 node->id.q_id,
+				 t->node_id, t->tmp_q.queue_child_prop.parent);
+		}
+		res = DP_SUCCESS;
+		goto EXIT;
+	} else if (node->type == DP_NODE_SCH) {
+		t->sch_id = node->id.sch_id;
+		t->dp_port = priv->qos_sch_stat[t->sch_id].dp_port;
+		t->resv_flag = priv->qos_sch_stat[t->sch_id].p_flag;
+
+		if (priv->qos_sch_stat[t->sch_id].child_num) {
+			PR_ERR("Node Sch[%d] still have child num %d\n",
+			       t->sch_id,
+			       priv->qos_sch_stat[t->sch_id].child_num);
+			goto EXIT;
+		}
+
+		if (priv->qos_sch_stat[t->sch_id].p_flag == PP_NODE_FREE) {
+			PR_ERR("Node Sch[%d] is already Free Stat\n",
+			       t->sch_id);
+			goto EXIT;
+		}
+
+		if (qos_sched_conf_get_32(priv->qdev, t->sch_id, &t->sched_cfg)) {
+			if (node_stat_update(node->inst, t->sch_id,
+					     DP_NODE_RST | P_FLAG)) {
+				PR_ERR("node_stat_update failed\n");
+				goto EXIT;
+			}
+			if (qos_sched_remove_32(priv->qdev, t->sch_id)) {
+				PR_ERR("failed to qos_sched_remove_32\n");
+				goto EXIT;
+			}
+			res = DP_SUCCESS;
+			goto EXIT;
+		}
+		t->parent_id = t->sched_cfg.sched_child_prop.parent;
+		t->p_type = get_node_type_by_node_id(node->inst,
+						     t->parent_id, flag);
+		/* Remove Sched link only if global resource */
+		if (!(t->resv_flag & PP_NODE_RESERVE)) {
+			if (qos_sched_remove_32(priv->qdev, t->sch_id)) {
+				PR_ERR("failed to qos_sched_remove_32\n");
+				goto EXIT;
+			}
+		}
+		if (node_stat_update(node->inst, t->sch_id,
+				     DP_NODE_DEC | P_FLAG)) {
+			PR_ERR("node_stat_update failed\n");
+			goto EXIT;
+		}
+		/* call node_stat_update to update parent status */
+		if (node_stat_update(node->inst, t->parent_id,
+				     DP_NODE_DEC | C_FLAG)) {
+			PR_ERR("stat update fail Sch:[%d]'s parent:%d\n",
+			       node->id.sch_id, t->parent_id);
+			goto EXIT;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "SCH[%d] removed and parent:[%d] stat updated\n",
+			node->id.sch_id, t->parent_id);
+		/* call node_stat_update to free the node */
+		if (node_stat_update(node->inst, t->sch_id,
+				     DP_NODE_RST | P_FLAG)) {
+			PR_ERR("Node Reset failed Sched[/%d]\n",
+			       node->id.sch_id);
+			goto EXIT;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Sched[%d] removed and stat updated\n",
+			 node->id.sch_id);
+		/* Reserve Sched temp attach to drop port */
+		if (t->resv_flag & PP_NODE_RESERVE) {
+			qos_sched_conf_set_default_32(&t->tmp_sch);
+			t->tmp_sch.sched_child_prop.parent = priv->ppv4_drop_p;
+			if (qos_sched_set_32(priv->qdev, t->sch_id, &t->tmp_sch)) {
+				PR_ERR("qos_sched_set_32 %s=%d to parent=%d\n",
+				       "fail to reserve SCH", t->sch_id,
+				       t->tmp_sch.sched_child_prop.parent);
+				goto EXIT;
+			}
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "SCH:%d attached temp to Drop_port:%d\n",
+				 t->sch_id,
+				 t->tmp_sch.sched_child_prop.parent);
+		}
+		res = DP_SUCCESS;
+		goto EXIT;
+	} else if (node->type == DP_NODE_PORT) {
+		t->phy_id = node->id.cqm_deq_port;
+		t->node_id = priv->deq_port_stat[t->phy_id].node_id;
+
+		if (priv->deq_port_stat[t->phy_id].child_num) {
+			PR_ERR("Node port[%d] still have child num %d\n",
+			       t->phy_id,
+			       priv->deq_port_stat[t->phy_id].child_num);
+			goto EXIT;
+		}
+		if (priv->deq_port_stat[t->phy_id].flag & PP_NODE_ACTIVE) {
+			res = node_stat_update(node->inst, t->node_id,
+					       DP_NODE_DEC);
+			if (res == DP_FAILURE) {
+				PR_ERR("Wrong Port %d flag:0x%x\n", t->phy_id,
+				       priv->deq_port_stat[t->phy_id].flag);
+				goto EXIT;
+			}
+			goto EXIT;
+		}
+		/* No reset API call for Port should freed by child's call */
+		if (priv->deq_port_stat[t->phy_id].flag & PP_NODE_ALLOC) {
+			res = DP_SUCCESS;
+			goto EXIT;
+		}
+		PR_ERR("Unexpect port %d flag %d\n",
+		       t->phy_id, priv->deq_port_stat[t->phy_id].flag);
+		goto EXIT;
+	}
+	PR_ERR("Unexpect node->type %d\n", node->type);
+
+EXIT:
+	if (res == DP_FAILURE)
+		PR_ERR("failed to free node:%d res %d\n",
+		       node->id.q_id, res);
+	kfree(t);
+	t = NULL;
+	return res;
+}
+
+/* dp_qos_parent_chk API
+ * checks for parent type
+ * upon Success
+ *    return parent node id
+ * else return DP_FAILURE
+ */
+static int dp_qos_parent_chk(struct dp_node_link *info, int flag)
+{
+	struct dp_node_alloc node;
+
+	if (info->p_node_type == DP_NODE_SCH) {
+		if (info->p_node_id.sch_id == DP_NODE_AUTO_ID) {
+			node.inst = info->inst;
+			node.type = info->p_node_type;
+			node.dp_port = info->dp_port;
+
+			if ((dp_node_alloc_32(&node, flag)) == DP_FAILURE) {
+				PR_ERR("dp_node_alloc_32 queue alloc fail\n");
+				return DP_FAILURE;
+			}
+			info->p_node_id = node.id;
+		}
+		return info->p_node_id.sch_id;
+	} else if (info->p_node_type == DP_NODE_PORT) {
+		node.inst = info->inst;
+		node.id = info->cqm_deq_port;
+		node.type = info->p_node_type;
+		node.dp_port = info->dp_port;
+		return dp_alloc_qos_port(&node, flag);
+	}
+	return DP_FAILURE;
+}
+
+/* dp_node_link_get_32 API
+ * upon success check node link info and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_node_link_get_32(struct dp_node_link *info, int flag)
+{
+	struct pp_qos_queue_conf queue_cfg = {0};
+	struct pp_qos_sched_conf sched_cfg = {0};
+	struct hal_priv *priv;
+	int node_id;
+
+	if (!info) {
+		PR_ERR("info cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(info->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (info->node_type == DP_NODE_QUEUE) {
+		node_id = priv->qos_queue_stat[info->node_id.q_id].node_id;
+		if (qos_queue_conf_get_32(priv->qdev, node_id, &queue_cfg)) {
+			PR_ERR("failed to qos_queue_conf_get_32\n");
+			return DP_FAILURE;
+		}
+		if (!queue_cfg.queue_child_prop.parent)
+			return DP_FAILURE;
+		if (!(priv->qos_queue_stat[info->node_id.q_id].flag &
+		      PP_NODE_ACTIVE)) {
+			return DP_FAILURE;
+		}
+		info->p_node_id.sch_id =
+			queue_cfg.queue_child_prop.parent;
+		info->p_node_type = get_node_type_by_node_id(info->inst,
+				queue_cfg.queue_child_prop.parent, flag);
+		info->prio_wfq = queue_cfg.queue_child_prop.priority;
+		info->arbi = 0;
+		info->leaf = 0;
+		return DP_SUCCESS;
+	} else if (info->node_type == DP_NODE_SCH) {
+		if (qos_sched_conf_get_32(priv->qdev, info->node_id.sch_id,
+				       &sched_cfg)) {
+			PR_ERR("failed to qos_sched_conf_get_32\n");
+			return DP_FAILURE;
+		}
+		if (!sched_cfg.sched_child_prop.parent) {
+			PR_ERR("sched child do not have parent\n");
+			return DP_FAILURE;
+		}
+		if (!(priv->qos_sch_stat[info->node_id.sch_id].p_flag &
+		      PP_NODE_ACTIVE)) {
+			PR_ERR("sched id %d flag not active, flag %d\n",
+			       info->node_id.sch_id,
+			       priv->qos_sch_stat[info->node_id.sch_id].p_flag);
+			return DP_FAILURE;
+		}
+		info->p_node_id.sch_id = sched_cfg.sched_child_prop.parent;
+		info->p_node_type = get_node_type_by_node_id(info->inst,
+				sched_cfg.sched_child_prop.parent, flag);
+		info->prio_wfq = sched_cfg.sched_child_prop.priority;
+		info->arbi =
+			arbi_pp2dp_32(sched_cfg.sched_parent_prop.arbitration);
+		info->leaf = 0;
+		return DP_SUCCESS;
+	}
+	return DP_FAILURE;
+}
+
+/* dp_link_set API
+ * upon success links node to parent and returns DP_SUCCESS
+ * else return DP_FAILURE
+ */
+static int dp_link_set(struct dp_node_link *info, int parent_node, int flag)
+{
+	int node_id;
+	int res = DP_FAILURE;
+	int node_flag = DP_NODE_INC;
+	struct hal_priv *priv = HAL(info->inst);
+	struct pp_qos_queue_conf *queue_cfg;
+	struct pp_qos_sched_conf *sched_cfg;
+
+	queue_cfg = kzalloc(sizeof(*queue_cfg), GFP_KERNEL);
+	sched_cfg = kzalloc(sizeof(*sched_cfg), GFP_KERNEL);
+	if (!queue_cfg || !sched_cfg)
+		goto ERROR_EXIT;
+
+	if (info->node_type == DP_NODE_QUEUE) {
+		qos_queue_conf_set_default_32(queue_cfg);
+		queue_cfg->queue_child_prop.parent = parent_node;
+		dp_wred_def(queue_cfg);
+		queue_cfg->queue_child_prop.priority = info->prio_wfq;
+		/* convert q_id to logical node id and pass it to
+		 * low level api
+		 */
+		node_id = priv->qos_queue_stat[info->node_id.q_id].node_id;
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Try to link Q[%d/%d] to parent[%d/%d] port[%d]\n",
+			 info->node_id.q_id,
+			 node_id,
+			 info->p_node_id.cqm_deq_port,
+			 queue_cfg->queue_child_prop.parent,
+			 info->cqm_deq_port.cqm_deq_port);
+		if (qos_queue_set_32(priv->qdev, node_id, queue_cfg)) {
+			PR_ERR("failed to qos_queue_set_32\n");
+			qos_queue_remove_32(priv->qdev, node_id);
+			goto ERROR_EXIT;
+		}
+		goto EXIT;
+	} else if (info->node_type == DP_NODE_SCH) {
+		qos_sched_conf_set_default_32(sched_cfg);
+		sched_cfg->sched_child_prop.parent = parent_node;
+		sched_cfg->sched_child_prop.priority = info->prio_wfq;
+		sched_cfg->sched_parent_prop.arbitration = arbi_dp2pp_32(info->arbi);
+		node_id = info->node_id.sch_id;
+
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Try to link SCH[/%d] to parent[%d/%d] port[%d]\n",
+			 node_id,
+			 info->p_node_id.cqm_deq_port,
+			 sched_cfg->sched_child_prop.parent,
+			 info->cqm_deq_port.cqm_deq_port);
+
+		if (qos_sched_set_32(priv->qdev, node_id, sched_cfg)) {
+			PR_ERR("failed to %s %d parent_node %d\n",
+			       "qos_sched_set_32 node_id",
+			       node_id, parent_node);
+			qos_sched_remove_32(priv->qdev, node_id);
+			goto ERROR_EXIT;
+		}
+		node_flag |= P_FLAG;
+		goto EXIT;
+	}
+	goto ERROR_EXIT;
+EXIT:
+	res = DP_SUCCESS;
+	/* fill parent info in child's global table */
+	priv->qos_sch_stat[node_id].parent.node_id = parent_node;
+	priv->qos_sch_stat[node_id].parent.type = info->p_node_type;
+	priv->qos_sch_stat[node_id].parent.flag = PP_NODE_ACTIVE;
+	/* increase child_num in parent's global table and status */
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "node_stat_update after dp_link_set start\n");
+	node_stat_update(info->inst, node_id, node_flag);
+	node_stat_update(info->inst, parent_node, DP_NODE_INC | C_FLAG);
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "node_stat_update after dp_link_set end\n");
+ERROR_EXIT:
+
+	kfree(queue_cfg);
+
+	kfree(sched_cfg);
+	return res;
+}
+
+/* get_parent_arbi API
+ * return parent's arbi of given node
+ * else return DP_FAILURE
+ */
+static int get_parent_arbi(int inst, int node_id, int flag)
+{
+	int pid, arbi;
+	struct hal_priv *priv = HAL(inst);
+	struct pp_qos_sched_conf sched_cfg = {0};
+	struct pp_qos_port_conf port_cfg = {0};
+
+	if (priv->qos_sch_stat[node_id].parent.flag == PP_NODE_FREE) {
+		PR_ERR("Parent is not set for node\n");
+		return DP_FAILURE;
+	}
+	pid = priv->qos_sch_stat[node_id].parent.node_id;
+
+	if (priv->qos_sch_stat[node_id].parent.type == DP_NODE_SCH) {
+		if (qos_sched_conf_get_32(priv->qdev, pid, &sched_cfg)) {
+			PR_ERR("fail to get sched config\n");
+			return DP_FAILURE;
+		}
+		arbi = arbi_pp2dp_32(sched_cfg.sched_parent_prop.arbitration);
+		if (arbi == DP_FAILURE)
+			PR_ERR("Wrong pp_arbitrate: %d for %s:%d\n",
+			       port_cfg.port_parent_prop.arbitration,
+			       (priv->qos_sch_stat[node_id].type == DP_NODE_SCH)
+			       ? "sched" : "Q",
+			       node_id);
+	} else if (priv->qos_sch_stat[node_id].parent.type == DP_NODE_PORT) {
+		if (qos_port_conf_get_32(priv->qdev, pid, &port_cfg)) {
+			PR_ERR("fail to get port config\n");
+			return DP_FAILURE;
+		}
+		arbi = arbi_pp2dp_32(port_cfg.port_parent_prop.arbitration);
+		if (arbi == DP_FAILURE)
+			PR_ERR("Wrong pp_arbitrate: %d for %s:%d\n",
+			       port_cfg.port_parent_prop.arbitration,
+			       (priv->qos_sch_stat[node_id].type == DP_NODE_SCH)
+			       ? "sched" : "Q",
+			       node_id);
+	} else {
+		PR_ERR("incorrect parent type:0x%x for node:%d.\n",
+		       priv->qos_sch_stat[node_id].parent.type,
+		       node_id);
+		return DP_FAILURE;
+	}
+	return arbi;
+}
+
+/* set_parent_arbi API
+ * set arbitration of node_id's all children and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+static int set_parent_arbi(int inst, int node_id, int arbi, int flag)
+{
+	int pid;
+	struct hal_priv *priv = HAL(inst);
+	struct pp_qos_sched_conf sched_cfg = {0};
+	struct pp_qos_port_conf port_cfg = {0};
+
+	if (priv->qos_sch_stat[node_id].parent.flag == PP_NODE_FREE) {
+		PR_ERR("Parent is not set for node\n");
+		return DP_FAILURE;
+	}
+	pid = priv->qos_sch_stat[node_id].parent.node_id;
+
+	arbi = arbi_dp2pp_32(arbi);
+
+	if (arbi == DP_FAILURE) {
+		PR_ERR("Incorrect arbitration value provided:%d!\n", arbi);
+		return DP_FAILURE;
+	}
+
+	if (priv->qos_sch_stat[node_id].parent.type == DP_NODE_SCH) {
+		if (qos_sched_conf_get_32(priv->qdev, pid, &sched_cfg)) {
+			PR_ERR("fail to get sched config\n");
+			return DP_FAILURE;
+		}
+		sched_cfg.sched_parent_prop.arbitration = arbi;
+		if (qos_sched_set_32(priv->qdev, pid, &sched_cfg)) {
+			PR_ERR("fail to set arbi sched:%d parent of node:%d\n",
+			       pid, node_id);
+			return DP_FAILURE;
+		}
+	} else if (priv->qos_sch_stat[node_id].parent.type == DP_NODE_PORT) {
+		if (qos_port_conf_get_32(priv->qdev, pid, &port_cfg)) {
+			PR_ERR("fail to get port config\n");
+			return DP_FAILURE;
+		}
+		port_cfg.port_parent_prop.arbitration = arbi;
+		if (qos_port_set_32(priv->qdev, pid, &port_cfg)) {
+			PR_ERR("fail to set arbi port:%d parent of node:%d\n",
+			       pid, node_id);
+			return DP_FAILURE;
+		}
+	} else {
+		PR_ERR("incorrect parent type:0x%x for node:%d.\n",
+		       priv->qos_sch_stat[node_id].parent.type,
+		       node_id);
+		return DP_FAILURE;
+	}
+	return DP_SUCCESS;
+}
+
+/* dp_qos_link_prio_set_32 API
+ * set node_prio struct for link and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_qos_link_prio_set_32(struct dp_node_prio *info, int flag)
+{
+	struct pp_qos_queue_conf queue_cfg = {0};
+	struct pp_qos_sched_conf sched_cfg = {0};
+	struct hal_priv *priv;
+	int node_id;
+
+	if (!info) {
+		PR_ERR("info cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(info->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (info->type == DP_NODE_QUEUE) {
+		if ((info->id.q_id < 0) || (info->id.q_id >= MAX_QUEUE)) {
+			PR_ERR("Invalid Queue ID:%d\n", info->id.q_id);
+			return DP_FAILURE;
+		}
+		if (priv->qos_queue_stat[info->id.q_id].flag == PP_NODE_FREE) {
+			PR_ERR("Invalid Queue flag:0x%x\n",
+			       priv->qos_queue_stat[info->id.q_id].flag);
+			return DP_FAILURE;
+		}
+		node_id = priv->qos_queue_stat[info->id.q_id].node_id;
+		if (qos_queue_conf_get_32(priv->qdev, node_id, &queue_cfg)) {
+			PR_ERR("fail to get queue prio and parent\n");
+			return DP_FAILURE;
+		}
+		queue_cfg.queue_child_prop.priority = info->prio_wfq;
+		DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+			 "Prio:%d paased to low level for queue[%d]\n",
+			 info->prio_wfq, info->id.q_id);
+		if (qos_queue_set_32(priv->qdev, node_id, &queue_cfg)) {
+			PR_ERR("failed to qos_queue_set_32\n");
+			return DP_FAILURE;
+		}
+		/* get parent conf and set arbi in parent */
+		if (set_parent_arbi(info->inst, node_id, info->arbi, flag)) {
+			PR_ERR("fail to set arbi:%d in Parent of Q:%d\n",
+			       info->arbi, node_id);
+			return DP_FAILURE;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+			 "dp_qos_link_prio_set_32:Q=%d arbi=%d prio=%d\n",
+			 info->id.q_id, info->arbi, info->prio_wfq);
+		return DP_SUCCESS;
+	} else if (info->type == DP_NODE_SCH) {
+		if ((info->id.sch_id < 0) ||
+		    (info->id.sch_id >= QOS_MAX_NODES)) {
+			PR_ERR("Invalid Sched ID:%d\n", info->id.sch_id);
+			return DP_FAILURE;
+		}
+		if (priv->qos_sch_stat[info->id.sch_id].p_flag ==
+		    PP_NODE_FREE) {
+			PR_ERR("Invalid Sched flag:0x%x\n",
+			       priv->qos_sch_stat[info->id.sch_id].p_flag);
+			return DP_FAILURE;
+		}
+		if (qos_sched_conf_get_32(priv->qdev, info->id.sch_id,
+				       &sched_cfg)) {
+			PR_ERR("fail to get sched prio and parent\n");
+			return DP_FAILURE;
+		}
+		sched_cfg.sched_child_prop.priority = info->prio_wfq;
+		DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+			 "Prio:%d paased to low level for Sched[%d]\n",
+			 info->prio_wfq, info->id.sch_id);
+		if (qos_sched_set_32(priv->qdev, info->id.sch_id, &sched_cfg)) {
+			PR_ERR("failed to qos_sched_set_32\n");
+			return DP_FAILURE;
+		}
+		/* get parent conf and set arbi in parent */
+		if (set_parent_arbi(info->inst, info->id.sch_id,
+				    info->arbi, 0)) {
+			PR_ERR("fail to set arbi:%d Parent of Sched:%d\n",
+			       info->arbi, info->id.sch_id);
+			return DP_FAILURE;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+			 "dp_qos_link_prio_set_32:Sched=%d arbi=%d prio=%d\n",
+			 info->id.sch_id, info->arbi, info->prio_wfq);
+		return DP_SUCCESS;
+	}
+	PR_ERR("incorrect info type provided:0x%x\n", info->type);
+	return DP_FAILURE;
+}
+
+/* dp_qos_link_prio_get_32 API
+ * get node_prio struct for link and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_qos_link_prio_get_32(struct dp_node_prio *info, int flag)
+{
+	struct pp_qos_queue_conf queue_cfg = {0};
+	struct pp_qos_sched_conf sched_cfg = {0};
+	struct hal_priv *priv;
+	int node_id, arbi;
+
+	if (!info) {
+		PR_ERR("info cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(info->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (info->type == DP_NODE_QUEUE) {
+		if ((info->id.q_id < 0) || (info->id.q_id >= MAX_QUEUE)) {
+			PR_ERR("Invalid Queue ID:%d\n", info->id.q_id);
+			return DP_FAILURE;
+		}
+		if (priv->qos_queue_stat[info->id.q_id].flag == PP_NODE_FREE) {
+			PR_ERR("Invalid Queue flag:%d\n",
+			       priv->qos_queue_stat[info->id.q_id].flag);
+			return DP_FAILURE;
+		}
+		node_id = priv->qos_queue_stat[info->id.q_id].node_id;
+		if (qos_queue_conf_get_32(priv->qdev, node_id, &queue_cfg)) {
+			PR_ERR("fail to get queue prio_wfq value!\n");
+			return DP_FAILURE;
+		}
+
+		arbi = get_parent_arbi(info->inst, node_id, flag);
+
+		if (arbi == DP_FAILURE) {
+			return DP_FAILURE;
+		}
+		info->arbi = arbi;
+		info->prio_wfq = queue_cfg.queue_child_prop.priority;
+		return DP_SUCCESS;
+	} else if (info->type == DP_NODE_SCH) {
+		if ((info->id.sch_id < 0) ||
+		    (info->id.sch_id >= QOS_MAX_NODES)) {
+			PR_ERR("Invalid Sched ID:%d\n", info->id.sch_id);
+			return DP_FAILURE;
+		}
+		if (priv->qos_sch_stat[info->id.sch_id].p_flag ==
+		    PP_NODE_FREE) {
+			PR_ERR("Invalid Sched flag:%d\n",
+			       priv->qos_sch_stat[info->id.sch_id].p_flag);
+			return DP_FAILURE;
+		}
+		if (qos_sched_conf_get_32(priv->qdev, info->id.sch_id,
+				       &sched_cfg)) {
+			PR_ERR("fail to get sched arbi and prio values!\n");
+			return DP_FAILURE;
+		}
+
+		arbi = get_parent_arbi(info->inst, info->id.sch_id, flag);
+
+		if (arbi == DP_FAILURE) {
+			return DP_FAILURE;
+		}
+		info->arbi = arbi;
+		info->prio_wfq = sched_cfg.sched_child_prop.priority;
+		return DP_SUCCESS;
+	}
+	PR_ERR("incorrect info type provided:0x%x\n", info->type);
+	return DP_FAILURE;
+}
+
+/* dp_deq_port_res_get_32 API
+ * Remove link of attached nodes and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+union local_t {
+	struct pp_qos_port_info p_info;
+	struct pp_qos_queue_info q_info;
+	struct pp_qos_queue_conf q_conf;
+	struct pp_qos_sched_conf sched_conf;
+};
+
+static union local_t t;
+
+int dp_deq_port_res_get_32(struct dp_dequeue_res *res, int flag)
+{
+	struct hal_priv *priv;
+	u16 q_ids[MAX_Q_PER_PORT] = {0};
+	unsigned int q_num;
+	unsigned int q_size = MAX_Q_PER_PORT;
+	int i, j, k;
+	int port_num = 1;
+	int p_id, idx;
+	struct pmac_port_info *port_info;
+
+	if (!res) {
+		PR_ERR("res cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(res->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	port_info = get_dp_port_info(res->inst, res->dp_port);
+	if (!port_info->deq_port_num)
+		return DP_FAILURE;
+	if (res->cqm_deq_idx == DEQ_PORT_OFFSET_ALL) {
+		port_num = port_info->deq_port_num;
+		res->cqm_deq_port = port_info->deq_port_base;
+		res->num_deq_ports = port_info->deq_port_num;
+	} else {
+		res->cqm_deq_port = port_info->deq_port_base + res->cqm_deq_idx;
+		res->num_deq_ports = 1;
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+		 "dp_deq_port_res_get_32:dp_port=%d cqm_deq_port=(%d ~%d) %d\n",
+		 res->dp_port,
+		 res->cqm_deq_port, res->cqm_deq_port + port_num - 1,
+		 port_info->deq_port_num);
+	res->num_q = 0;
+	idx = 0;
+	for (k = res->cqm_deq_port;
+	     k < (res->cqm_deq_port + port_num);
+	     k++) {
+		if (priv->deq_port_stat[k].flag == PP_NODE_FREE)
+			continue;
+		q_num = 0;
+		if (qos_port_get_queues_32(priv->qdev,
+					priv->deq_port_stat[k].node_id,
+					q_ids, q_size, &q_num)) {
+			PR_ERR("qos_port_get_queues_32: port[%d/%d]\n",
+			       k,
+			       priv->deq_port_stat[k].node_id);
+			return DP_FAILURE;
+		}
+		res->num_q += q_num;
+		if (!res->q_res)
+			continue;
+		DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL, "port[%d/%d] queue list\n",
+			 k, priv->deq_port_stat[k].node_id);
+		for (i = 0; i < q_num; i++)
+			DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+				 "  q[/%d]\n", q_ids[i]);
+		for (i = 0; (i < q_num) && (idx < res->q_res_size); i++) {
+			memset(&t.q_info, 0, sizeof(t.q_info));
+			if (qos_queue_info_get_32(priv->qdev,
+					       q_ids[i], &t.q_info)) {
+				PR_ERR("qos_port_info_get_32 fail:q[/%d]\n",
+				       q_ids[i]);
+				continue;
+			}
+			res->q_res[idx].q_id = t.q_info.physical_id;
+			res->q_res[idx].q_node = q_ids[i];
+			DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL, "q[%d/%d]\n",
+				 t.q_info.physical_id, q_ids[i]);
+			res->q_res[idx].sch_lvl = 0;
+			memset(&t.q_conf, 0, sizeof(t.q_conf));
+			if (qos_queue_conf_get_32(priv->qdev,
+					       q_ids[i], &t.q_conf)) {
+				PR_ERR("qos_port_conf_get_32 fail:q[/%d]\n",
+				       q_ids[i]);
+				continue;
+			}
+			p_id = t.q_conf.queue_child_prop.parent;
+			j = 0;
+			do {
+				if (priv->qos_sch_stat[p_id].type ==
+				    DP_NODE_PORT) {/* port */
+					res->q_res[idx].qos_deq_port = p_id;
+					res->q_res[idx].cqm_deq_port = k;
+					break;
+				} else if (priv->qos_sch_stat[p_id].type !=
+					   DP_NODE_SCH) {
+					PR_ERR("wrong p[/%d] type:%d\n",
+					       p_id,
+					       priv->qos_sch_stat[p_id].type);
+					break;
+				}
+				/* for sched as parent */
+				res->q_res[idx].sch_id[j] = p_id;
+				j++;
+				res->q_res[idx].sch_lvl = j;
+				/* get next parent */
+				if (qos_sched_conf_get_32(priv->qdev,
+						       p_id, &t.sched_conf)) {
+					PR_ERR("qos_sched_conf_get_32 %s[/%d]\n",
+					       "fail:sch", p_id);
+					break;
+				}
+				p_id = t.sched_conf.sched_child_prop.parent;
+			} while (1);
+			idx++;
+		}
+		return DP_SUCCESS;
+	}
+	return DP_SUCCESS;
+}
+
+/* dp_node_unlink_32 API
+ * check child node keep queue in blocked state
+ * flush queues and return DP_SUCCESS
+ * Else return DP_FAILURE
+ */
+int dp_node_unlink_32(struct dp_node_link *info, int flag)
+{
+	struct pp_qos_queue_conf queue_cfg = {0};
+	struct pp_qos_sched_conf sched_cfg = {0};
+	struct hal_priv *priv;
+	u16 queue_buf[MAX_Q_PER_PORT] = {0};
+	int queue_size = MAX_Q_PER_PORT;
+	int queue_num = 0;
+	int i, node_id;
+
+	if (!info) {
+		PR_ERR("info cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(info->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (info->node_type == DP_NODE_QUEUE) {
+		node_id = priv->qos_queue_stat[info->node_id.q_id].node_id;
+		/* Need to check ACTIVE Flag */
+		if (!(priv->qos_queue_stat[info->node_id.q_id].flag &
+		    PP_NODE_ACTIVE)) {
+			PR_ERR("Wrong Queue[%d] Stat(%d):Expect ACTIVE\n",
+				info->node_id.q_id,
+				priv->qos_queue_stat[info->node_id.q_id].flag);
+		}
+		if (qos_queue_conf_get_32(priv->qdev, node_id, &queue_cfg) == 0)
+			queue_flush_32(info->inst, node_id, 0);
+	} else if (info->node_type == DP_NODE_SCH) {
+		if (!(priv->qos_sch_stat[info->node_id.sch_id].c_flag &
+								PP_NODE_ACTIVE))
+			PR_ERR("Wrong Sched FLAG Expect ACTIVE\n");
+		if (qos_sched_conf_get_32(priv->qdev, info->node_id.sch_id,
+				       &sched_cfg))
+			return DP_FAILURE;
+		if (qos_sched_get_queues_32(priv->qdev, info->node_id.sch_id,
+					 queue_buf, queue_size, &queue_num))
+			return DP_FAILURE;
+		for (i = 0; i < queue_num; i++) {
+			if (qos_queue_conf_get_32(priv->qdev, queue_buf[i],
+					       &queue_cfg))
+				continue;
+			queue_flush_32(info->inst, queue_buf[i], 0);
+		}
+	}
+	return DP_SUCCESS;
+}
+
+struct link_add_var {
+	struct pp_qos_queue_conf queue_cfg;
+	struct pp_qos_sched_conf sched_cfg;
+	struct dp_node_alloc node;
+	u16 queue_buf[MAX_Q_PER_PORT];
+	int q_orig_block[MAX_Q_PER_PORT];
+	int q_orig_suspend[MAX_Q_PER_PORT];
+	int queue_size;
+	int queue_num;
+	int node_id;
+	struct pp_node parent;
+	int f_child_free;
+	int f_parent_free;
+	int f_sch_auto_id;
+	int f_restore;
+};
+
+/* dp_node_link_add_32 API
+ * check for parent type and allocate parent node
+ * then check for child type and allocate child node
+ * then call dp_link_set api to link child to parent
+ * upon success links node to given parent and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_node_link_add_32(struct dp_node_link *info, int flag)
+{
+	#define DP_SUSPEND(t) ((t)->queue_cfg.common_prop.suspended)
+	int i;
+	int res = DP_SUCCESS;
+	struct hal_priv *priv;
+	struct link_add_var *t = NULL;
+
+	if (!info) {
+		PR_ERR("info cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(info->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if ((!info->dp_port) && (info->dp_port != DP_PORT(info).dp_port)) {
+		PR_ERR("Fix wrong dp_port from %d to %d\n",
+			info->dp_port, DP_PORT(info).dp_port);
+		info->dp_port = DP_PORT(info).dp_port;
+	}
+	t = kzalloc(sizeof(*t), GFP_KERNEL);
+	if (!t) {
+		PR_ERR("fail to alloc %zd bytes\n", sizeof(*t));
+		return DP_FAILURE;
+	}
+	for (i = 0; i < ARRAY_SIZE(t->q_orig_block); i++) {
+		t->q_orig_block[i] = -1;
+		t->q_orig_suspend[i] = -1;
+	}
+	t->queue_size = MAX_Q_PER_PORT;
+
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "inst=%d dp_port=%d\n",
+		 info->inst, info->dp_port);
+	/* Get Parent node_id after sanity check */
+	if (info->p_node_type == DP_NODE_SCH &&
+	    info->p_node_id.sch_id == DP_NODE_AUTO_ID)
+		t->f_sch_auto_id = 1;
+	i = dp_qos_parent_chk(info, flag);
+	if (i == DP_FAILURE) {
+		PR_ERR("dp_qos_parent_chk fail\n");
+		goto EXIT_ERR;
+	}
+	t->parent.node_id = i;
+	t->parent.type = info->p_node_type;
+	t->parent.flag = 1;
+
+	/* Check parent's children limit not exceeded */
+	if (priv->qos_sch_stat[t->parent.node_id].child_num >=
+	    DP_MAX_CHILD_PER_NODE) {
+		PR_ERR("Child Num:%d is exceeding limit for Node:[%d]\n",
+		       priv->qos_sch_stat[t->parent.node_id].child_num,
+		       t->parent.node_id);
+		goto EXIT_ERR;
+	}
+
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "dp_qos_parent_chk succeed: parent node %d\n",
+		 t->parent.node_id);
+	/* workaround to pass parrent to queue allcoate api */
+	priv->ppv4_tmp_p = t->parent.node_id;
+	if (t->f_sch_auto_id)
+		t->f_parent_free = 1;
+
+	/* Get Child node after sanity check */
+	if (info->node_type == DP_NODE_QUEUE) {
+		if (info->node_id.q_id == DP_NODE_AUTO_ID) {
+			t->node.inst = info->inst;
+			t->node.dp_port = info->dp_port;
+			t->node.type = info->node_type;
+			if ((dp_node_alloc_32(&t->node, flag)) == DP_FAILURE) {
+				PR_ERR("dp_node_alloc_32 queue alloc fail\n");
+				goto EXIT_ERR;
+			}
+			info->node_id = t->node.id;
+			t->f_child_free = 1;
+		}
+		/* add check for free flag and error */
+		if (priv->qos_queue_stat[info->node_id.q_id].flag ==
+		    PP_NODE_FREE) {
+			PR_ERR("Queue ID:%d is in Free state:0x%x\n",
+			       info->node_id.q_id,
+			       priv->qos_queue_stat[info->node_id.q_id].flag);
+			goto EXIT_ERR;
+		}
+		/* convert q_id to logical node id and pass it to
+		 * low level api
+		 */
+
+		t->node_id = priv->qos_queue_stat[info->node_id.q_id].node_id;
+		if (qos_queue_conf_get_32(priv->qdev, t->node_id,
+				       &t->queue_cfg) == 0) {
+			t->queue_num = 1;
+			t->queue_buf[0] = t->node_id;
+			/* save original block/suspend status */
+			if (t->queue_cfg.blocked == 0)
+				t->q_orig_block[0] = t->queue_cfg.blocked;
+
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "qos_queue_flush_32 queue[%d]\n", t->node_id);
+			queue_flush_32(info->inst, t->node_id,
+				       FLUSH_RESTORE_LOOKUP);
+			if (t->queue_cfg.queue_child_prop.parent !=
+					 priv->ppv4_drop_p) {/* decrease stat */
+				/* Child flag update before link */
+				DP_DEBUG(DP_DBG_FLAG_QOS,
+					 "node_stat_update for queue[%d]\n",
+					 t->node_id);
+
+				if (node_stat_update(info->inst, t->node_id,
+						     DP_NODE_DEC)) {
+					PR_ERR("node_stat_update fail\n");
+					goto EXIT_ERR;
+				}
+				/* reduce child_num in parent's global table */
+				DP_DEBUG(DP_DBG_FLAG_QOS,
+					 "node_stat_update parent %d for q[%d]\n",
+					 PARENT(t->queue_cfg), t->node_id);
+				if (node_stat_update(info->inst,
+						     PARENT(t->queue_cfg),
+						     DP_NODE_DEC | C_FLAG)) {
+					PR_ERR("node_stat_update fail\n");
+					goto EXIT_ERR;
+				}
+			}
+		}
+		/* link set */
+		/* if parent is same, but need to fill in other parameters for
+		 * parents hence commenting below code
+		 */
+		/* if (info->p_node_id.sch_id == parent.node_id ||
+		 *    info->p_node_id.cqm_deq_port == parent.node_id)
+		 *	goto EXIT_ERR;
+		 */
+		if (dp_link_set(info, t->parent.node_id, flag)) {
+			PR_ERR("dp_link_set fail to link to parent\n");
+			goto EXIT_ERR;
+		}
+	} else if (info->node_type == DP_NODE_SCH) {
+		if (info->node_id.sch_id == DP_NODE_AUTO_ID) {
+			t->node.inst = info->inst;
+			t->node.dp_port = info->dp_port;
+			t->node.type = info->node_type;
+
+			if ((dp_node_alloc_32(&t->node, flag)) == DP_FAILURE) {
+				PR_ERR("dp_node_alloc_32 sched alloc fail\n");
+				goto EXIT_ERR;
+			}
+			info->node_id = t->node.id;
+			t->f_child_free = 1;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+			 "inst=%d dp_port=%d type=%d info->node_id %d\n",
+			 t->node.inst, t->node.dp_port, t->node.type,
+			 info->node_id.q_id);
+		/* add check for free flag and error */
+		if (priv->qos_sch_stat[info->node_id.sch_id].p_flag ==
+		    PP_NODE_FREE) {
+			PR_ERR("Sched:%d is in Free state:0x%x\n",
+			       info->node_id.sch_id,
+			       priv->qos_sch_stat[info->node_id.sch_id].p_flag);
+			goto EXIT_ERR;
+		}
+		if ((t->f_child_free == 0) &&
+		    qos_sched_conf_get_32(priv->qdev, info->node_id.sch_id,
+				       &t->sched_cfg) == 0) {
+			DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+				 "info->node_id.sch_id %d\n",
+				 info->node_id.sch_id);
+			if (qos_sched_get_queues_32(priv->qdev,
+						 info->node_id.sch_id,
+						 t->queue_buf, t->queue_size,
+						 &t->queue_num)) {
+				PR_ERR("Can not get queues:%d\n",
+				       info->node_id.sch_id);
+				goto EXIT_ERR;
+			}
+			for (i = 0; i < t->queue_num; i++) {
+				if (qos_queue_conf_get_32(priv->qdev,
+						       t->queue_buf[i],
+						       &t->queue_cfg))
+					continue;
+				if (t->queue_cfg.blocked == 0)
+					t->q_orig_block[i] =
+						t->queue_cfg.blocked;
+
+				queue_flush_32(info->inst, t->queue_buf[i],
+					       FLUSH_RESTORE_LOOKUP);
+			}
+			/* update flag for sch node */
+			if (node_stat_update(info->inst, info->node_id.sch_id,
+					     DP_NODE_DEC | P_FLAG)) {
+				PR_ERR("node_stat_update fail\n");
+				goto EXIT_ERR;
+			}
+			/* reduce child_num in parent's global table */
+			if (node_stat_update(info->inst, PARENT_S(t->sched_cfg),
+					     DP_NODE_DEC | C_FLAG)) {
+				PR_ERR("node_stat_update fail\n");
+				goto EXIT_ERR;
+			}
+		}
+		/* if parent is same, but need to fill in other parameters for
+		 * parents hence commenting below code
+		 */
+		/* if (info->p_node_id.sch_id == parent.node_id ||
+		 *    info->p_node_id.cqm_deq_port == parent.node_id)
+		 *	goto EXIT_ERR;
+		 */
+		if (dp_link_set(info, t->parent.node_id, flag)) {
+			PR_ERR("dp_link_set failed to link to parent\n");
+			goto EXIT_ERR;
+		}
+	}
+
+	for (i = 0; i <= t->queue_num; i++) {
+		if ((t->q_orig_block[i] < 0) &&/* non-valid block stat */
+		    (t->q_orig_suspend[i] < 0))/* non-valid suspend stat */
+			continue;
+		if (qos_queue_conf_get_32(priv->qdev, t->queue_buf[i],
+				       &t->queue_cfg))
+			continue;
+		t->f_restore = 0;
+		if (t->q_orig_block[i] >= 0) {
+			t->f_restore = 1;
+			t->queue_cfg.blocked = t->q_orig_block[i];/* restore */
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "to unblock queue[%d/%d]:block=%d\n",
+				 get_qid_by_node(info->inst,
+						 t->queue_buf[i], 0),
+				 t->queue_buf[i],
+				 t->queue_cfg.blocked);
+		}
+
+		if (!t->f_restore)
+			continue;
+		if (qos_queue_set_32(priv->qdev, t->queue_buf[i], &t->queue_cfg)) {
+			PR_ERR("qos_queue_set_32 fail for q[/%d]\n",
+			       t->queue_buf[i]);
+			res = DP_FAILURE;
+		}
+	}
+	kfree(t);
+	t = NULL;
+	return res;
+
+EXIT_ERR:
+	res = DP_FAILURE;
+	if (t->f_child_free) {
+		if (t->node.type == DP_NODE_QUEUE) {
+			DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+				 "queue remove id %d\n", t->node_id);
+			qos_queue_remove_32(priv->qdev, t->node_id);
+		} else if (t->node.type == DP_NODE_SCH) {
+			DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+				 "sched remove id %d\n", t->node.id.sch_id);
+			qos_sched_remove_32(priv->qdev, t->node.id.sch_id);
+		} else {
+			PR_ERR("Unexpect node type %d\n", t->node.type);
+		}
+	}
+	if (t->f_parent_free) {
+		if (info->p_node_type == DP_NODE_PORT) {
+			DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+				 "port remove id %d\n", t->parent.node_id);
+			qos_port_remove_32(priv->qdev, t->parent.node_id);
+		} else if (info->p_node_type == DP_NODE_SCH) {
+			DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+				 "sched remove id %d\n", t->parent.node_id);
+			qos_sched_remove_32(priv->qdev, t->parent.node_id);
+		} else {
+			PR_ERR("Unexpect node type %d\n", t->node.type);
+		}
+	}
+
+	kfree(t);
+	t = NULL;
+	return res;
+}
+
+/* dp_queue_conf_set_32 API
+ * Set Current Queue config and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_queue_conf_set_32(struct dp_queue_conf *cfg, int flag)
+{
+	struct pp_qos_queue_conf *conf;
+	struct hal_priv *priv;
+	int node_id, res = DP_FAILURE;
+
+	if (!cfg) {
+		PR_ERR("cfg cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(cfg->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	conf = kzalloc(sizeof(*conf), GFP_KERNEL);
+	if (!conf) {
+		PR_ERR("fail to alloc %zd bytes\n", sizeof(*conf));
+		return DP_FAILURE;
+	}
+
+	if ((cfg->q_id < 0) || (cfg->q_id >= MAX_QUEUE)) {
+		PR_ERR("Invalid Queue ID:%d\n", cfg->q_id);
+		goto EXIT;
+	}
+	if (priv->qos_queue_stat[cfg->q_id].flag == PP_NODE_FREE) {
+		PR_ERR("Invalid Queue flag:%d\n",
+		       priv->qos_queue_stat[cfg->q_id].flag);
+		goto EXIT;
+	}
+	node_id = priv->qos_queue_stat[cfg->q_id].node_id;
+
+	if (qos_queue_conf_get_32(priv->qdev, node_id, conf)) {
+		PR_ERR("qos_queue_conf_get_32 fail:%d\n", cfg->q_id);
+		goto EXIT;
+	}
+	if (flag & (cfg->act & DP_NODE_DIS))
+		conf->blocked = 1;
+	else if (flag & (cfg->act & DP_NODE_EN))
+		conf->blocked = 0;
+
+	if (flag & (cfg->drop == DP_QUEUE_DROP_WRED)) {
+		conf->wred_enable = 1;
+		conf->wred_max_avg_green = cfg->max_size[0];
+		conf->wred_max_avg_yellow = cfg->max_size[1];
+		conf->wred_min_avg_green = cfg->min_size[0];
+		conf->wred_min_avg_yellow = cfg->min_size[1];
+		conf->wred_slope_green = cfg->wred_slope[0];
+		conf->wred_slope_yellow = cfg->wred_slope[1];
+		conf->wred_min_guaranteed = cfg->wred_min_guaranteed;
+		conf->wred_max_allowed = cfg->wred_max_allowed;
+	} else if (flag & (cfg->drop == DP_QUEUE_DROP_TAIL)) {
+		PR_ERR("Further check PPv4 Tail Drop Capability.\n");
+		conf->wred_enable = 0;
+		conf->wred_min_avg_green = cfg->min_size[0];
+		conf->wred_min_avg_yellow = cfg->min_size[1];
+		conf->wred_min_guaranteed = cfg->wred_min_guaranteed;
+		conf->wred_max_allowed = cfg->wred_max_allowed;
+	}
+	if (qos_queue_set_32(priv->qdev, node_id, conf)) {
+		PR_ERR("failed to qos_queue_set_32:%d\n", cfg->q_id);
+		goto EXIT;
+	}
+	res = DP_SUCCESS;
+
+EXIT:
+	kfree(conf);
+	conf = NULL;
+	return res;
+}
+
+/* dp_queue_conf_get_32 API
+ * Get Current Queue config and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_queue_conf_get_32(struct dp_queue_conf *cfg, int flag)
+{
+	int node_id, res = DP_FAILURE;
+	struct pp_qos_queue_conf *conf;
+	struct hal_priv *priv;
+
+	if (!cfg) {
+		PR_ERR("cfg cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(cfg->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	conf = kzalloc(sizeof(*conf), GFP_KERNEL);
+	if (!conf) {
+		PR_ERR("fail to alloc %zd bytes\n", sizeof(*conf));
+		return DP_FAILURE;
+	}
+
+	if ((cfg->q_id < 0) || (cfg->q_id >= MAX_QUEUE)) {
+		PR_ERR("Invalid Queue ID:%d\n", cfg->q_id);
+		goto EXIT;
+	}
+	if (priv->qos_queue_stat[cfg->q_id].flag == PP_NODE_FREE) {
+		PR_ERR("Invalid Queue flag:%d\n",
+		       priv->qos_queue_stat[cfg->q_id].flag);
+		goto EXIT;
+	}
+	node_id = priv->qos_queue_stat[cfg->q_id].node_id;
+
+	if (qos_queue_conf_get_32(priv->qdev, node_id, conf)) {
+		PR_ERR("qos_queue_conf_get_32 fail\n");
+		goto EXIT;
+	}
+
+	if (conf->blocked)
+		cfg->act = DP_NODE_DIS;
+	else
+		cfg->act = DP_NODE_EN;
+
+	if (conf->wred_enable) {
+		cfg->drop = DP_QUEUE_DROP_WRED;
+		cfg->wred_slope[0] = conf->wred_slope_green;
+		cfg->wred_slope[1] = conf->wred_slope_yellow;
+		cfg->wred_slope[2] = 0;
+		cfg->wred_max_allowed = conf->wred_max_allowed;
+		cfg->wred_min_guaranteed = conf->wred_min_guaranteed;
+		cfg->min_size[0] = conf->wred_min_avg_green;
+		cfg->min_size[1] = conf->wred_min_avg_yellow;
+		cfg->min_size[2] = 0;
+		cfg->max_size[0] = conf->wred_max_avg_green;
+		cfg->max_size[1] = conf->wred_max_avg_yellow;
+		cfg->max_size[2] = 0;
+		//cfg->unit = conf->max_burst;
+		res = DP_SUCCESS;
+		goto EXIT;
+	}
+	cfg->drop = DP_QUEUE_DROP_TAIL;
+	cfg->min_size[0] = conf->wred_min_avg_green;
+	cfg->min_size[1] = conf->wred_min_avg_yellow;
+	cfg->max_size[0] = conf->wred_max_avg_green;
+	cfg->max_size[1] = conf->wred_max_avg_yellow;
+	//cfg->unit = conf->max_burst;
+	res = DP_SUCCESS;
+
+EXIT:
+	kfree(conf);
+	conf = NULL;
+	return res;
+}
+
+/* dp_node_link_en_set_32 API
+ * Enable current link node and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_node_link_en_set_32(struct dp_node_link_enable *en, int flag)
+{
+	struct pp_qos_queue_conf queue_cfg = {0};
+	struct pp_qos_sched_conf sched_cfg = {0};
+	struct pp_qos_port_conf port_cfg = {0};
+	struct hal_priv *priv;
+	int node_id;
+
+	if (!en) {
+		PR_ERR("en info cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(en->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if ((en->en & DP_NODE_EN) && (en->en & DP_NODE_DIS)) {
+		PR_ERR("enable & disable cannot be set together!\n");
+		return DP_FAILURE;
+	}
+	if ((en->en & DP_NODE_SUSPEND) && (en->en & DP_NODE_RESUME)) {
+		PR_ERR("suspend & resume cannot be set together!\n");
+		return DP_FAILURE;
+	}
+
+	if (en->type == DP_NODE_QUEUE) {
+		if (!(en->en & (DP_NODE_EN | DP_NODE_DIS | DP_NODE_SUSPEND |
+				    DP_NODE_RESUME))) {
+			PR_ERR("Incorrect commands provided!\n");
+			return DP_FAILURE;
+		}
+		if ((en->id.q_id < 0) || (en->id.q_id >= MAX_QUEUE)) {
+			PR_ERR("Wrong Parameter: QID[%d]Out Of Range\n",
+			       en->id.q_id);
+			return DP_FAILURE;
+		}
+		node_id = priv->qos_queue_stat[en->id.q_id].node_id;
+
+		if (priv->qos_queue_stat[en->id.q_id].flag == PP_NODE_FREE) {
+			PR_ERR("Node Q[%d] is not allcoated\n", en->id.q_id);
+			return DP_FAILURE;
+		}
+		if (en->en & DP_NODE_EN) {
+			if (pp_qos_queue_unblock(priv->qdev, node_id)) {
+				PR_ERR("pp_qos_queue_unblock fail Queue[%d]\n",
+				       en->id.q_id);
+				return DP_FAILURE;
+			}
+		}
+		if (en->en & DP_NODE_DIS) {
+			if (pp_qos_queue_block(priv->qdev, node_id)) {
+				PR_ERR("pp_qos_queue_block fail Queue[%d]\n",
+				       en->id.q_id);
+				return DP_FAILURE;
+			}
+		}
+
+		if (qos_queue_conf_get_32(priv->qdev, node_id, &queue_cfg)) {
+			PR_ERR("qos_queue_conf_get_32 fail: q[%d]\n", en->id.q_id);
+			return DP_FAILURE;
+		}
+		if (en->en & DP_NODE_EN) {
+			if (queue_cfg.blocked) {
+				PR_ERR("Incorrect value set for Queue[%d]:%d\n",
+				       en->id.q_id,
+				       queue_cfg.blocked);
+				return DP_FAILURE;
+			}
+		}
+		if (en->en & DP_NODE_DIS) {
+			if (!queue_cfg.blocked) {
+				PR_ERR("Incorrect value set for Queue[%d]:%d\n",
+				       en->id.q_id,
+				       queue_cfg.blocked);
+				return DP_FAILURE;
+			}
+		}
+
+	} else if (en->type == DP_NODE_SCH) {
+		if (!(en->en & (DP_NODE_SUSPEND | DP_NODE_RESUME))) {
+			PR_ERR("Incorrect commands provided!\n");
+			return DP_FAILURE;
+		}
+		if ((en->id.sch_id < 0) || (en->id.sch_id >= QOS_MAX_NODES)) {
+			PR_ERR("Wrong Parameter: Sched[%d]Out Of Range\n",
+			       en->id.sch_id);
+			return DP_FAILURE;
+		}
+		if (priv->qos_sch_stat[en->id.sch_id].p_flag == PP_NODE_FREE) {
+			PR_ERR("Node Sched[%d] is not allcoated\n",
+			       en->id.sch_id);
+			return DP_FAILURE;
+		}
+
+		if (qos_sched_conf_get_32(priv->qdev, en->id.sch_id, &sched_cfg)) {
+			PR_ERR("qos_sched_conf_get_32 fail: sch[%d]\n",
+			       en->id.sch_id);
+			return DP_FAILURE;
+		}
+
+	} else if (en->type == DP_NODE_PORT) {
+		if (!(en->en & (DP_NODE_EN | DP_NODE_DIS | DP_NODE_SUSPEND |
+				    DP_NODE_RESUME))) {
+			PR_ERR("Incorrect commands provided!\n");
+			return DP_FAILURE;
+		}
+		if ((en->id.cqm_deq_port < 0) ||
+		    (en->id.cqm_deq_port >= MAX_CQM_DEQ)) {
+			PR_ERR("Wrong Parameter: Port[%d]Out Of Range\n",
+			       en->id.cqm_deq_port);
+			return DP_FAILURE;
+		}
+		if (priv->deq_port_stat[en->id.cqm_deq_port].flag ==
+		    PP_NODE_FREE) {
+			PR_ERR("Node Port[%d] is not allcoated\n",
+			       en->id.cqm_deq_port);
+			return DP_FAILURE;
+		}
+		node_id = priv->deq_port_stat[en->id.cqm_deq_port].node_id;
+		if (en->en & DP_NODE_EN) {
+			if (pp_qos_port_unblock(priv->qdev, node_id)) {
+				PR_ERR("pp_qos_port_unblock fail Port[%d]\n",
+				       en->id.cqm_deq_port);
+				return DP_FAILURE;
+			}
+		}
+		if (en->en & DP_NODE_DIS) {
+			if (pp_qos_port_block(priv->qdev, node_id)) {
+				PR_ERR("pp_qos_port_block fail Port[%d]\n",
+				       en->id.cqm_deq_port);
+				return DP_FAILURE;
+			}
+		}
+		if (en->en & DP_NODE_SUSPEND) {
+			if (pp_qos_port_disable(priv->qdev, node_id)) {
+				PR_ERR("pp_qos_port_disable fail Port[%d]\n",
+				       en->id.cqm_deq_port);
+				return DP_FAILURE;
+			}
+		}
+		if (en->en & DP_NODE_RESUME) {
+			if (pp_qos_port_enable(priv->qdev, node_id)) {
+				PR_ERR("pp_qos_port_enable fail Port[%d]\n",
+				       en->id.cqm_deq_port);
+				return DP_FAILURE;
+			}
+		}
+		if (qos_port_conf_get_32(priv->qdev, node_id, &port_cfg)) {
+			PR_ERR("qos_port_conf_get_32 fail: port[%d]\n",
+			       en->id.cqm_deq_port);
+			return DP_FAILURE;
+		}
+		if (en->en & DP_NODE_SUSPEND) {
+			if (!port_cfg.disable) {
+				PR_ERR("Incorrect value set for Port[%d]:%d\n",
+				       en->id.cqm_deq_port,
+				       port_cfg.disable);
+				return DP_FAILURE;
+			}
+		}
+		if (en->en & DP_NODE_RESUME) {
+			if (port_cfg.disable) {
+				PR_ERR("Incorrect value set for Port[%d]:%d\n",
+				       en->id.cqm_deq_port,
+				       port_cfg.disable);
+				return DP_FAILURE;
+			}
+		}
+	}
+	return DP_SUCCESS;
+}
+
+/* dp_node_link_en_get_32 API
+ * Get status of link node and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_node_link_en_get_32(struct dp_node_link_enable *en, int flag)
+{
+	int node_id;
+	struct hal_priv *priv = HAL(en->inst);
+
+	if (!priv || !priv->qdev) {
+		PR_ERR("priv or priv->qdev NULL\n");
+		return DP_FAILURE;
+	}
+	if (!en) {
+		PR_ERR("en info NULL\n");
+		return DP_FAILURE;
+	}
+	if (en->type == DP_NODE_QUEUE) {
+		struct pp_qos_queue_conf q_conf = {0};
+
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "en->id.q_id=%d\n", en->id.q_id);
+		node_id = priv->qos_queue_stat[en->id.q_id].node_id;
+		if (qos_queue_conf_get_32(priv->qdev, node_id, &q_conf)) {
+			PR_ERR("qos_queue_conf_get_32 fail: q[%d]\n",
+			       en->id.q_id);
+			return DP_FAILURE;
+		}
+		if (q_conf.blocked)
+			en->en |= DP_NODE_DIS;
+		else
+			en->en |= DP_NODE_EN;
+	} else if (en->type == DP_NODE_SCH) {
+		struct pp_qos_sched_conf sched_conf = {0};
+
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "en->id.sch_id=%d\n", en->id.sch_id);
+		if (qos_sched_conf_get_32(priv->qdev, en->id.sch_id,
+				       &sched_conf)) {
+			PR_ERR("qos_sched_conf_get_32 fail: sched[/%d]\n",
+			       en->id.sch_id);
+			return DP_FAILURE;
+		}
+		en->en |= DP_NODE_EN;
+	} else if (en->type == DP_NODE_PORT) {
+		struct pp_qos_port_conf p_conf = {0};
+
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "en->id.cqm_deq_port=%d\n", en->id.cqm_deq_port);
+		node_id = priv->deq_port_stat[en->id.cqm_deq_port].node_id;
+		if (qos_port_conf_get_32(priv->qdev, node_id, &p_conf)) {
+			PR_ERR("qos_queue_conf_get_32 fail: port[%d]\n",
+			       en->id.cqm_deq_port);
+			return DP_FAILURE;
+		}
+		if (p_conf.disable)
+			en->en |= DP_NODE_DIS;
+		else
+			en->en |= DP_NODE_EN;
+	}
+	return DP_SUCCESS;
+}
+
+/* dp_link_get_32 API
+ * get full link based on queue and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_link_get_32(struct dp_qos_link *cfg, int flag)
+{
+	struct pp_qos_queue_conf queue_cfg = {0};
+	struct pp_qos_sched_conf sched_cfg = {0};
+	struct hal_priv *priv;
+	int i, node_id;
+
+	if (!cfg) {
+		PR_ERR("cfg cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(cfg->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	node_id = priv->qos_queue_stat[cfg->q_id].node_id;
+
+	if (!(priv->qos_queue_stat[cfg->q_id].flag & PP_NODE_ACTIVE)) {
+		PR_ERR("Incorrect queue:%d state:expect ACTIV\n", cfg->q_id);
+		return DP_FAILURE;
+	}
+
+	if (qos_queue_conf_get_32(priv->qdev, node_id, &queue_cfg)) {
+		PR_ERR("failed to qos_queue_conf_get_32\n");
+		return DP_FAILURE;
+	}
+	cfg->q_arbi = get_parent_arbi(cfg->inst, node_id, 0);
+	cfg->q_leaf = 0;
+	cfg->n_sch_lvl = 0;
+	cfg->q_prio_wfq = queue_cfg.queue_child_prop.priority;
+
+	if (priv->qos_sch_stat[node_id].parent.type == DP_NODE_PORT) {
+		cfg->cqm_deq_port = priv->qos_sch_stat[node_id].parent.node_id;
+		return DP_SUCCESS;
+	} else if (priv->qos_sch_stat[node_id].parent.type == DP_NODE_SCH) {
+		for (i = 0; i < DP_MAX_SCH_LVL - 1; i++) {
+			cfg->sch[i].id =
+				priv->qos_sch_stat[node_id].parent.node_id;
+			node_id = cfg->sch[i].id;
+			cfg->n_sch_lvl = i + 1;
+			cfg->sch[i].leaf = 0;
+			cfg->sch[i].arbi = get_parent_arbi(cfg->inst,
+							   cfg->sch[i].id, 0);
+			cfg->sch[i + 1].id =
+				priv->qos_sch_stat[node_id].parent.node_id;
+			if (qos_sched_conf_get_32(priv->qdev, cfg->sch[i].id,
+					       &sched_cfg)) {
+				PR_ERR("dp_link_get:sched[/%d] conf get fail\n",
+				       cfg->sch[i].id);
+				return DP_FAILURE;
+			}
+			cfg->sch[i].prio_wfq =
+					sched_cfg.sched_child_prop.priority;
+			if (priv->qos_sch_stat[cfg->sch[i].id].parent.type ==
+			    DP_NODE_PORT)
+				break;
+		}
+		cfg->cqm_deq_port =
+			priv->qos_sch_stat[cfg->sch[i].id].parent.node_id;
+		return DP_SUCCESS;
+	}
+	return DP_FAILURE;
+}
+
+/* dp_link_add_32 API
+ * configure end to end link and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_link_add_32(struct dp_qos_link *cfg, int flag)
+{
+	struct dp_node_link info = {0};
+	int i;
+	int f_q_free = 0;
+	int f_q_auto = 0; /* flag if node is DP_NODE_AUTO_ID */
+	struct f {
+		u16 flag;
+		u16 sch_id;
+		u16 f_auto; /* flag if node is DP_NODE_AUTO_ID */
+	};
+	struct f f_sch_free[DP_MAX_SCH_LVL] = {0};
+
+	if (!cfg) {
+		PR_ERR("cfg cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (cfg->n_sch_lvl > DP_MAX_SCH_LVL) {
+		PR_ERR("Incorrect sched_lvl:%s(%d) > %s(%d)\n",
+		       "cfg->n_sch_lvl", cfg->n_sch_lvl,
+		       "DP_MAX_SCH_LVL", DP_MAX_SCH_LVL);
+		return DP_FAILURE;
+	}
+
+	info.inst = cfg->inst;
+	info.dp_port = cfg->dp_port;
+	info.node_id.q_id = cfg->q_id;
+	info.cqm_deq_port.cqm_deq_port = cfg->cqm_deq_port;
+
+	if (cfg->q_id == DP_NODE_AUTO_ID)
+		f_q_auto = 1;
+
+	DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+		 "inst=%d dp_port=%d q_id=%d cqm_deq_port=%d n_sch_lvl=%d\n",
+		 info.inst, info.dp_port, info.node_id.q_id,
+		 info.cqm_deq_port.cqm_deq_port, cfg->n_sch_lvl);
+	if (cfg->n_sch_lvl) {
+		info.node_id.sch_id = cfg->sch[cfg->n_sch_lvl - 1].id;
+		info.node_type = DP_NODE_SCH;
+		info.p_node_id.cqm_deq_port = cfg->cqm_deq_port;
+		info.p_node_type = DP_NODE_PORT;
+		info.arbi = cfg->sch[cfg->n_sch_lvl - 1].arbi;
+		info.leaf = cfg->sch[cfg->n_sch_lvl - 1].leaf;
+		info.prio_wfq = cfg->sch[cfg->n_sch_lvl - 1].prio_wfq;
+		f_sch_free[cfg->n_sch_lvl - 1].flag = 1;
+
+		if (dp_node_link_add_32(&info, flag)) {
+			PR_ERR("Failed to link Sch:%d to Port:%d\n",
+			       cfg->sch[cfg->n_sch_lvl - 1].id,
+			       cfg->cqm_deq_port);
+			goto EXIT;
+		}
+		/* link sched to port */
+		if (cfg->sch[cfg->n_sch_lvl - 1].id == DP_NODE_AUTO_ID)
+			f_sch_free[cfg->n_sch_lvl - 1].f_auto = 1;
+
+		f_sch_free[cfg->n_sch_lvl - 1].sch_id = info.node_id.sch_id;
+
+		/* link sched to sched */
+		for (i = (cfg->n_sch_lvl - 2); i >= 0; i--) {
+			info.node_id.sch_id = cfg->sch[i].id;
+			info.node_type = DP_NODE_SCH;
+			info.p_node_id.sch_id = f_sch_free[i + 1].sch_id;
+			info.p_node_type = DP_NODE_SCH;
+			info.arbi = cfg->sch[i].arbi;
+			info.leaf = cfg->sch[i].leaf;
+			info.prio_wfq = cfg->sch[i].prio_wfq;
+			f_sch_free[i].flag = 1;
+
+			if (dp_node_link_add_32(&info, flag)) {
+				PR_ERR("Failed to link Sch:%d to Sch:%d\n",
+				       cfg->sch[i].id, cfg->sch[i + 1].id);
+				goto EXIT;
+			}
+			if (cfg->sch[i].id == DP_NODE_AUTO_ID)
+				f_sch_free[i].f_auto = 1;
+
+			f_sch_free[i].sch_id = info.node_id.sch_id;
+		}
+		/* link Queue to sched */
+		info.node_type = DP_NODE_QUEUE;
+		info.node_id.q_id = cfg->q_id;
+		info.p_node_id.sch_id = f_sch_free[0].sch_id;
+		info.p_node_type = DP_NODE_SCH;
+		info.arbi = cfg->sch[0].arbi;
+		info.leaf = cfg->sch[0].leaf;
+		info.prio_wfq = cfg->sch[0].prio_wfq;
+
+		if (dp_node_link_add_32(&info, flag)) {
+			PR_ERR("Failed to link Q:%d to Sch:%d\n",
+			       cfg->q_id, cfg->sch[0].id);
+			f_q_free = 1;
+			goto EXIT;
+		}
+	} else {
+		/* link Queue to Port */
+		info.node_type = DP_NODE_QUEUE;
+		info.p_node_id.cqm_deq_port = cfg->cqm_deq_port;
+		info.p_node_type = DP_NODE_PORT;
+		info.arbi = cfg->q_arbi;
+		info.leaf = cfg->q_leaf;
+		info.prio_wfq = cfg->q_prio_wfq;
+
+		if (dp_node_link_add_32(&info, flag)) {
+			PR_ERR("Failed to link Q:%d to Port:%d\n",
+			       cfg->q_id, cfg->cqm_deq_port);
+			f_q_free = 1;
+			goto EXIT;
+		}
+	}
+	return DP_SUCCESS;
+EXIT:
+	for (i = (cfg->n_sch_lvl - 1); i >= 0; i--) {
+		if (!f_sch_free[i].flag)
+			continue;
+		/* sch is auto_alloc move it to FREE */
+		if (f_sch_free[i].f_auto) {
+			struct dp_node_alloc node = {0};
+
+			node.id.sch_id = f_sch_free[i].sch_id;
+			node.type = DP_NODE_SCH;
+			node.dp_port = cfg->dp_port;
+			node.inst = cfg->inst;
+			dp_node_free_32(&node,
+					flag);
+		}
+		/* sch provided by caller move it to ALLOC */
+		if (node_stat_update(info.inst, f_sch_free[i].sch_id,
+				     DP_NODE_DEC)) {
+			PR_ERR("Failed to %s sched:%d DP_NODE_DEC\n",
+			       "node_stat_update",
+			       f_sch_free[i].sch_id);
+			continue;
+		}
+	}
+	if (f_q_free) {
+		/* queue is auto_alloc move it to FREE */
+		if (f_q_auto) {
+			struct dp_node_alloc node = {0};
+
+			node.id.q_id = cfg->q_id;
+			node.type = DP_NODE_QUEUE;
+			node.dp_port = cfg->dp_port;
+			node.inst = cfg->inst;
+			dp_node_free_32(&node, flag);
+		}
+		/* queue provided by caller move it to ALLOC */
+		if (node_stat_update(info.inst, cfg->q_id, DP_NODE_DEC)) {
+			PR_ERR("Failed to update stat qid %d DP_NODE_DEC\n",
+			       cfg->q_id);
+			return DP_FAILURE;
+		}
+	}
+	return DP_FAILURE;
+}
+
+/* dp_shaper_conf_set_32 API
+ * DP_NO_SHAPER_LIMIT no limit for shaper
+ * DP_MAX_SHAPER_LIMIT max limit for shaper
+ * configure shaper limit for node and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_shaper_conf_set_32(struct dp_shaper_conf *cfg, int flag)
+{
+	struct pp_qos_queue_conf queue_cfg = {0};
+	struct pp_qos_sched_conf sched_cfg = {0};
+	struct pp_qos_port_conf port_cfg = {0};
+	struct hal_priv *priv;
+	int node_id, res;
+	u32 bw_limit;
+
+	if (!cfg) {
+		PR_ERR("cfg cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(cfg->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (cfg->type == DP_NODE_QUEUE) {
+		if ((cfg->id.q_id < 0) || (cfg->id.q_id >= MAX_QUEUE)) {
+			PR_ERR("Invalid Queue ID:%d\n", cfg->id.q_id);
+			return DP_FAILURE;
+		}
+		if (priv->qos_queue_stat[cfg->id.q_id].flag == PP_NODE_FREE) {
+			PR_ERR("Invalid Queue flag:%d\n",
+			       priv->qos_queue_stat[cfg->id.q_id].flag);
+			return DP_FAILURE;
+		}
+		node_id = priv->qos_queue_stat[cfg->id.q_id].node_id;
+
+		if (qos_queue_conf_get_32(priv->qdev, node_id, &queue_cfg)) {
+			PR_ERR("qos_queue_conf_get_32 fail:%d\n", cfg->id.q_id);
+			return DP_FAILURE;
+		}
+
+		if ((cfg->cmd == DP_SHAPER_CMD_ADD) ||
+		    (cfg->cmd == DP_SHAPER_CMD_ENABLE)) {
+			res = limit_dp2pp(cfg->cir, &bw_limit);
+
+			if (res == DP_FAILURE) {
+				PR_ERR("Wrong dp shaper limit:%u\n", cfg->cir);
+				return DP_FAILURE;
+			}
+			queue_cfg.common_prop.bandwidth_limit = bw_limit;
+		} else if ((cfg->cmd == DP_SHAPER_CMD_REMOVE) ||
+			   (cfg->cmd == DP_SHAPER_CMD_DISABLE)) {
+			queue_cfg.common_prop.bandwidth_limit = 0;
+		} else {
+			PR_ERR("Incorrect command provided:%d\n", cfg->cmd);
+			return DP_FAILURE;
+		}
+
+		if (qos_queue_set_32(priv->qdev, node_id, &queue_cfg)) {
+			PR_ERR("qos_queue_set_32 fail:%d\n", cfg->id.q_id);
+			return DP_FAILURE;
+		}
+		return DP_SUCCESS;
+	} else if (cfg->type == DP_NODE_SCH) {
+		if ((cfg->id.sch_id < 0) || (cfg->id.sch_id >= QOS_MAX_NODES)) {
+			PR_ERR("Invalid Sched ID:%d\n", cfg->id.sch_id);
+			return DP_FAILURE;
+		}
+		if (priv->qos_sch_stat[cfg->id.sch_id].p_flag ==
+		    PP_NODE_FREE) {
+			PR_ERR("Invalid Sched flag:%d\n",
+			       priv->qos_sch_stat[cfg->id.sch_id].p_flag);
+			return DP_FAILURE;
+		}
+
+		if (qos_sched_conf_get_32(priv->qdev, cfg->id.sch_id,
+				       &sched_cfg)) {
+			PR_ERR("qos_sched_conf_get_32 fail:%d\n", cfg->id.sch_id);
+			return DP_FAILURE;
+		}
+
+		if ((cfg->cmd == DP_SHAPER_CMD_ADD) ||
+		    (cfg->cmd == DP_SHAPER_CMD_ENABLE)) {
+			res = limit_dp2pp(cfg->cir, &bw_limit);
+
+			if (res == DP_FAILURE) {
+				PR_ERR("Wrong dp shaper limit:%u\n", cfg->cir);
+				return DP_FAILURE;
+			}
+			sched_cfg.common_prop.bandwidth_limit = bw_limit;
+		} else if ((cfg->cmd == DP_SHAPER_CMD_REMOVE) ||
+			   (cfg->cmd == DP_SHAPER_CMD_DISABLE)) {
+			sched_cfg.common_prop.bandwidth_limit = 0;
+		} else {
+			PR_ERR("Incorrect command provided:%d\n", cfg->cmd);
+			return DP_FAILURE;
+		}
+
+		if (qos_sched_set_32(priv->qdev, cfg->id.sch_id, &sched_cfg)) {
+			PR_ERR("qos_sched_set_32 fail:%d\n", cfg->id.sch_id);
+			return DP_FAILURE;
+		}
+		return DP_SUCCESS;
+	} else if (cfg->type == DP_NODE_PORT) {
+		if ((cfg->id.cqm_deq_port < 0) ||
+		    (cfg->id.cqm_deq_port >= MAX_CQM_DEQ)) {
+			PR_ERR("Invalid Port ID:%d\n", cfg->id.cqm_deq_port);
+			return DP_FAILURE;
+		}
+		if (priv->deq_port_stat[cfg->id.cqm_deq_port].flag ==
+		    PP_NODE_FREE) {
+			PR_ERR("Invalid Port flag:%d\n",
+			       priv->deq_port_stat[cfg->id.cqm_deq_port].flag);
+			return DP_FAILURE;
+		}
+		node_id = priv->deq_port_stat[cfg->id.cqm_deq_port].node_id;
+
+		if (qos_port_conf_get_32(priv->qdev, node_id, &port_cfg)) {
+			PR_ERR("qos_port_conf_get_32 fail:%d\n",
+			       cfg->id.cqm_deq_port);
+			return DP_FAILURE;
+		}
+
+		if ((cfg->cmd == DP_SHAPER_CMD_ADD) ||
+		    (cfg->cmd == DP_SHAPER_CMD_ENABLE)) {
+			res = limit_dp2pp(cfg->cir, &bw_limit);
+
+			if (res == DP_FAILURE) {
+				PR_ERR("Wrong dp shaper limit:%u\n", cfg->cir);
+				return DP_FAILURE;
+			}
+			port_cfg.common_prop.bandwidth_limit = bw_limit;
+		} else if ((cfg->cmd == DP_SHAPER_CMD_REMOVE) ||
+			   (cfg->cmd == DP_SHAPER_CMD_DISABLE)) {
+			port_cfg.common_prop.bandwidth_limit = 0;
+		} else {
+			PR_ERR("Incorrect command provided:%d\n", cfg->cmd);
+			return DP_FAILURE;
+		}
+
+		if (qos_port_set_32(priv->qdev, node_id, &port_cfg)) {
+			PR_ERR("qos_port_set_32 fail:%d\n", cfg->id.cqm_deq_port);
+			return DP_FAILURE;
+		}
+		return DP_SUCCESS;
+	}
+	PR_ERR("Unkonwn type provided:0x%x\n", cfg->type);
+	return DP_FAILURE;
+}
+
+/* dp_shaper_conf_get_32 API
+ * DP_NO_SHAPER_LIMIT no limit for shaper
+ * DP_MAX_SHAPER_LIMIT max limit for shaper
+ * get shaper limit for node fill struct and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_shaper_conf_get_32(struct dp_shaper_conf *cfg, int flag)
+{
+	struct pp_qos_queue_conf queue_cfg = {0};
+	struct pp_qos_sched_conf sched_cfg = {0};
+	struct pp_qos_port_conf port_cfg = {0};
+	struct hal_priv *priv;
+	int node_id, res;
+	u32 bw_limit;
+
+	if (!cfg) {
+		PR_ERR("cfg cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(cfg->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (cfg->type == DP_NODE_QUEUE) {
+		if ((cfg->id.q_id < 0) || (cfg->id.q_id >= MAX_QUEUE)) {
+			PR_ERR("Invalid Queue ID:%d\n", cfg->id.q_id);
+			return DP_FAILURE;
+		}
+		if (priv->qos_queue_stat[cfg->id.q_id].flag == PP_NODE_FREE) {
+			PR_ERR("Invalid Queue flag:%d\n",
+			       priv->qos_queue_stat[cfg->id.q_id].flag);
+			return DP_FAILURE;
+		}
+		node_id = priv->qos_queue_stat[cfg->id.q_id].node_id;
+
+		if (qos_queue_conf_get_32(priv->qdev, node_id, &queue_cfg)) {
+			PR_ERR("qos_queue_conf_get_32 fail:%d\n", cfg->id.q_id);
+			return DP_FAILURE;
+		}
+		res = limit_pp2dp(queue_cfg.common_prop.bandwidth_limit,
+				  &bw_limit);
+
+		if (res == DP_FAILURE) {
+			PR_ERR("Wrong pp shaper limit:%u\n",
+			       queue_cfg.common_prop.bandwidth_limit);
+			return DP_FAILURE;
+		}
+	} else if (cfg->type == DP_NODE_SCH) {
+		if ((cfg->id.sch_id < 0) || (cfg->id.sch_id >= QOS_MAX_NODES)) {
+			PR_ERR("Invalid Sched ID:%d\n", cfg->id.sch_id);
+			return DP_FAILURE;
+		}
+		if (priv->qos_sch_stat[cfg->id.sch_id].p_flag ==
+		    PP_NODE_FREE) {
+			PR_ERR("Invalid Sched flag:%d\n",
+			       priv->qos_sch_stat[cfg->id.sch_id].p_flag);
+			return DP_FAILURE;
+		}
+
+		if (qos_sched_conf_get_32(priv->qdev, cfg->id.sch_id,
+				       &sched_cfg)) {
+			PR_ERR("qos_sched_conf_get_32 fail:%d\n", cfg->id.sch_id);
+			return DP_FAILURE;
+		}
+
+		res = limit_pp2dp(sched_cfg.common_prop.bandwidth_limit,
+				  &bw_limit);
+
+		if (res == DP_FAILURE) {
+			PR_ERR("Wrong pp shaper limit:%u\n",
+			       sched_cfg.common_prop.bandwidth_limit);
+			return DP_FAILURE;
+		}
+	} else if (cfg->type == DP_NODE_PORT) {
+		if ((cfg->id.cqm_deq_port < 0) ||
+		    (cfg->id.cqm_deq_port >= MAX_CQM_DEQ)) {
+			PR_ERR("Invalid Port ID:%d\n", cfg->id.cqm_deq_port);
+			return DP_FAILURE;
+		}
+		if (priv->deq_port_stat[cfg->id.cqm_deq_port].flag ==
+		    PP_NODE_FREE) {
+			PR_ERR("Invalid Port flag:%d\n",
+			       priv->deq_port_stat[cfg->id.cqm_deq_port].flag);
+			return DP_FAILURE;
+		}
+		node_id = priv->deq_port_stat[cfg->id.cqm_deq_port].node_id;
+		if (qos_port_conf_get_32(priv->qdev, node_id, &port_cfg)) {
+			PR_ERR("qos_port_conf_get_32 fail:%d\n",
+			       cfg->id.cqm_deq_port);
+			return DP_FAILURE;
+		}
+		res = limit_pp2dp(port_cfg.common_prop.bandwidth_limit,
+				  &bw_limit);
+
+		if (res == DP_FAILURE) {
+			PR_ERR("Wrong pp shaper limit:%u\n",
+			       port_cfg.common_prop.bandwidth_limit);
+			return DP_FAILURE;
+		}
+	} else {
+		PR_ERR("Unkonwn type provided:0x%x\n", cfg->type);
+		return DP_FAILURE;
+	}
+
+	cfg->cir = bw_limit;
+	cfg->pir = 0;
+	cfg->cbs = 0;
+	cfg->pbs = 0;
+	return DP_SUCCESS;
+}
+
+int dp_queue_map_get_32(struct dp_queue_map_get *cfg, int flag)
+{
+	struct hal_priv *priv;
+	cbm_queue_map_entry_t *qmap_entry = NULL;
+	s32 num_entry;
+	int i;
+	int res = DP_SUCCESS;
+
+	if (!cfg) {
+		PR_ERR("cfg cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(cfg->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	if ((cfg->q_id < 0) || (cfg->q_id >= MAX_QUEUE)) {
+		PR_ERR("Invalid Queue ID:%d\n", cfg->q_id);
+		return DP_FAILURE;
+	}
+	if (priv->qos_queue_stat[cfg->q_id].flag == PP_NODE_FREE) {
+		PR_ERR("Invalid Queue flag:%d\n",
+		       priv->qos_queue_stat[cfg->q_id].flag);
+		return DP_FAILURE;
+	}
+
+	if (cbm_queue_map_get(cfg->inst, cfg->q_id, cfg->egflag, &num_entry,
+			      &qmap_entry, 0)) {
+		PR_ERR("cbm_queue_map_get fail: qid=%d egflag=%d\n",
+		       cfg->q_id, cfg->egflag);
+		return DP_FAILURE;
+	}
+
+	cfg->num_entry = num_entry;
+
+	if (!qmap_entry) {
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "queue map entry returned null value\n");
+		if (num_entry) {
+			PR_ERR("num_entry is not null:%d\n", num_entry);
+			res = DP_FAILURE;
+		}
+		goto EXIT;
+	}
+
+	if (!cfg->qmap_entry)
+		goto EXIT;
+
+	if (num_entry > cfg->qmap_size) {
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "num_entry is greater than qmap_size:%d\n",
+			 num_entry);
+		goto EXIT;
+	}
+
+	for (i = 0; i < num_entry; i++) {
+		cfg->qmap_entry[i].qmap.flowid = qmap_entry[i].flowid;
+		cfg->qmap_entry[i].qmap.dec = qmap_entry[i].dec;
+		cfg->qmap_entry[i].qmap.enc = qmap_entry[i].enc;
+		cfg->qmap_entry[i].qmap.mpe1 = qmap_entry[i].mpe1;
+		cfg->qmap_entry[i].qmap.mpe2 = qmap_entry[i].mpe2;
+		cfg->qmap_entry[i].qmap.dp_port = qmap_entry[i].ep;
+		cfg->qmap_entry[i].qmap.class = qmap_entry[i].tc;
+		cfg->qmap_entry[i].qmap.subif = qmap_entry[i].sub_if_id;
+	}
+
+EXIT:
+	if (!qmap_entry)
+		kfree(qmap_entry);
+	qmap_entry = NULL;
+	return res;
+}
+
+int dp_queue_map_set_32(struct dp_queue_map_set *cfg, int flag)
+{
+	struct hal_priv *priv;
+	cbm_queue_map_entry_t qmap_cfg = {0};
+	u32 cqm_flags = 0;
+
+	if (!cfg) {
+		PR_ERR("cfg cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(cfg->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	if ((cfg->q_id < 0) || (cfg->q_id >= MAX_QUEUE)) {
+		PR_ERR("Invalid Queue ID:%d\n", cfg->q_id);
+		return DP_FAILURE;
+	}
+	if (priv->qos_queue_stat[cfg->q_id].flag == PP_NODE_FREE) {
+		PR_ERR("Invalid Queue flag:%d\n",
+		       priv->qos_queue_stat[cfg->q_id].flag);
+		return DP_FAILURE;
+	}
+
+	qmap_cfg.mpe1 = cfg->map.mpe1;
+	qmap_cfg.mpe2 = cfg->map.mpe2;
+	qmap_cfg.ep = cfg->map.dp_port;
+	qmap_cfg.flowid = cfg->map.flowid;
+	qmap_cfg.dec = cfg->map.dec;
+	qmap_cfg.enc = cfg->map.enc;
+	qmap_cfg.tc = cfg->map.class;
+	qmap_cfg.sub_if_id = cfg->map.subif;
+	if (cfg->mask.mpe1)
+		cqm_flags |= CBM_QUEUE_MAP_F_MPE1_DONTCARE;
+	if (cfg->mask.mpe2)
+		cqm_flags |= CBM_QUEUE_MAP_F_MPE2_DONTCARE;
+	if (cfg->mask.dp_port)
+		cqm_flags |= CBM_QUEUE_MAP_F_EP_DONTCARE;
+	if (cfg->mask.flowid) {
+		cqm_flags |= CBM_QUEUE_MAP_F_FLOWID_L_DONTCARE;
+		cqm_flags |= CBM_QUEUE_MAP_F_FLOWID_H_DONTCARE;
+	}
+	if (cfg->mask.dec)
+		cqm_flags |= CBM_QUEUE_MAP_F_DE_DONTCARE;
+	if (cfg->mask.enc)
+		cqm_flags |= CBM_QUEUE_MAP_F_EN_DONTCARE;
+	if (cfg->mask.class)
+		cqm_flags |= CBM_QUEUE_MAP_F_TC_DONTCARE;
+	if (cfg->mask.dp_port)
+		cqm_flags |= CBM_QUEUE_MAP_F_EP_DONTCARE;
+	if (cfg->mask.subif) {
+		cqm_flags |= CBM_QUEUE_MAP_F_SUBIF_DONTCARE;
+		if (get_dp_port_info(cfg->inst, cfg->map.dp_port)->gsw_mode ==
+				GSW_LOGICAL_PORT_9BIT_WLAN)
+			cqm_flags |= CBM_QUEUE_MAP_F_SUBIF_LSB_DONTCARE;
+	}
+
+	if (cbm_queue_map_set(cfg->inst, cfg->q_id, &qmap_cfg, cqm_flags)) {
+		PR_ERR("cbm_queue_map_set fail for Q:%d\n", cfg->q_id);
+		return DP_FAILURE;
+	}
+	return DP_SUCCESS;
+}
+
+int dp_counter_mode_set_32(struct dp_counter_conf *cfg, int flag)
+{
+	return DP_FAILURE;
+}
+
+int dp_counter_mode_get_32(struct dp_counter_conf *cfg, int flag)
+{
+	return DP_FAILURE;
+}
+
+int get_sch_level_32(int inst, int pid, int flag)
+{
+	struct hal_priv *priv;
+	int level;
+
+	priv = HAL(inst);
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	for (level = 0; level < DP_MAX_SCH_LVL; level++) {
+		if (priv->qos_sch_stat[pid].parent.type == DP_NODE_PORT) {
+			level = level + 1;
+			break;
+		}
+		pid = priv->qos_sch_stat[pid].parent.node_id;
+	}
+	return level;
+}
+
+/* dp_qos_level_get_32 API
+ * get max scheduler level and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_qos_level_get_32(struct dp_qos_level *dp, int flag)
+{
+	struct hal_priv *priv;
+	u16 i, id, pid, lvl_x = 0;
+
+	if (!dp) {
+		PR_ERR("dp cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	dp->max_sch_lvl = 0;
+	priv = HAL(dp->inst);
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	for (i = 0; i < MAX_QUEUE; i++) {
+		if (priv->qos_queue_stat[i].flag & PP_NODE_FREE)
+			continue;
+		id = priv->qos_queue_stat[i].node_id;
+
+		if (priv->qos_sch_stat[id].parent.type == DP_NODE_PORT) {
+			continue;
+		} else if (priv->qos_sch_stat[id].parent.type == DP_NODE_SCH) {
+			pid = priv->qos_sch_stat[id].parent.node_id;
+			lvl_x = get_sch_level_32(dp->inst, pid, 0);
+		}
+		if (lvl_x > dp->max_sch_lvl)
+			dp->max_sch_lvl = lvl_x;
+	}
+	if (dp->max_sch_lvl >= 0)
+		return DP_SUCCESS;
+	else
+		return DP_FAILURE;
+}
+
+#ifdef PP_QOS_LINK_EXAMPLE
+/* Example: queue -> QOS/CQM dequeue port
+ * inst: dp instance
+ * dp_port: dp port id
+ * t_conf: for pon, it is used to get cqm dequeue port via t-cont index
+ *         for others, its value should be 0
+ * q_node: queueu node id
+ */
+
+int ppv4_queue_port_example(int inst, int dp_port, int t_cont, int q_node)
+{
+	struct pp_qos_port_conf  port_cfg;
+	struct pp_qos_queue_conf queue_cfg;
+	int qos_port_node;
+	int rc;
+	int cqm_deq_port;
+	struct pmac_port_info *port_info;
+
+	port_info = get_dp_port_info(inst, dp_port);
+	cqm_deq_port = port_info->deq_port_base + t_cont;
+
+	/* Allocate qos dequeue port's node id via cqm_deq_port */
+	rc = qos_port_allocate_32(priv->qdev, cqm_deq_port, &qos_port_node);
+	if (rc) {
+		PR_ERR("failed to qos_port_allocate_32\n");
+		return DP_FAILURE;
+	}
+
+	/* Configure QOS dequeue port */
+	qos_port_conf_set_default_32(&port_cfg);
+	port_cfg.ring_address = (void *)(port_info->tx_ring_addr_txpush +
+		port_info->tx_ring_offset * t_cont);
+	port_cfg.ring_size = port_info->tx_ring_size;
+	if (port_info->tx_pkt_credit) {
+		port_cfg.packet_credit_enable = 1;
+		port_cfg.credit = port_info->tx_pkt_credit;
+	}
+#ifdef TX_BYTE_CREDIT
+	if (port_info->tx_b_credit) {
+		port_cfg.byte_credit_enable = 1;
+		port_cfg.byte_credit = port_info->tx_pkt_credit;
+	}
+#endif
+#ifdef EXT_BW
+	port_cfg.parent.arbitration = ARBITRATION_WRR;
+	port_cfg.common.bandwidth_limit = 1000;
+#endif
+	rc = qos_port_set_32(priv->qdev, qos_port_node, &port_cfg);
+	if (rc) {
+		PR_ERR("failed to qos_port_set_32\n");
+		qos_port_remove_32(priv->qdev, qos_port_node);
+		return DP_FAILURE;
+	}
+
+	/* Attach queue to QoS port */
+	qos_queue_conf_set_default_32(&queue_cfg);
+	queue_cfg.queue_child_prop.parent = qos_port_node;
+#ifdef EXT_BW
+	queue_cfg.max_burst  = 64;
+	queue_cfg.child.bandwidth_share = 50;
+	queue_cfg.wred_min_guaranteed = 1;
+	queue_cfg.wred_max_allowed = 10;
+#endif
+	rc = qos_queue_set_32(priv->qdev, q_node, &queue_cfg);
+	if (rc) {
+		PR_ERR("failed to qos_queue_set_32\n");
+		qos_port_remove_32(priv->qdev, qos_port_node);
+		return DP_FAILURE;
+	}
+	return DP_SUCCESS;
+}
+
+/* Example: queue -> scheduler1- > scheduler 2 -> QOS/CQM dequeue port
+ * inst: dp instance
+ * dp_port: dp port id
+ * t_conf: for pon, it is used to get cqm dequeue port via t-cont index
+ *         for others, its value should be 0
+ * q_node: queueu node id
+ * sch_node1: schduler node which connected with queue
+ * sch_node2: schduler node which connected with sch_node1
+ */
+int ppv4_queue_scheduler(int inst, int dp_port, int t_cont, int q_node,
+			 int sch_node1, int sch_node2)
+{
+	struct pp_qos_port_conf  port_cfg;
+	struct pp_qos_queue_conf queue_cfg;
+	struct pp_qos_sched_conf sched_cfg;
+	int qos_port_node;
+	int rc;
+	int cqm_deq_port;
+	struct pmac_port_info *port_info;
+
+	port_info = get_dp_port_info(inst, dp_port);
+	cqm_deq_port = port_info->deq_port_base + t_cont;
+
+	/* Allocate qos dequeue port's node id via cqm_deq_port */
+	rc = qos_port_allocate_32(priv->qdev, cqm_deq_port, &qos_port_node);
+	if (rc) {
+		PR_ERR("failed to qos_port_allocate_32\n");
+		return DP_FAILURE;
+	}
+
+	/* Configure QOS dequeue port */
+	qos_port_conf_set_default_32(&port_cfg);
+	port_cfg.ring_address = (void *)(port_info->tx_ring_addr_txpush +
+		port_info->tx_ring_offset * t_cont);
+	port_cfg.ring_size = port_info->tx_ring_size;
+	if (port_info->tx_pkt_credit) {
+		port_cfg.packet_credit_enable = 1;
+		port_cfg.credit = port_info->tx_pkt_credit;
+	}
+	if (port_info->tx_pkt_credit) {
+		port_cfg.byte_credit_enable = 1;
+		port_cfg.byte_credit = port_info->tx_pkt_credit;
+	}
+#ifdef EXT_BW
+	port_cfg.parent.arbitration = ARBITRATION_WRR;
+	port_cfg.common.bandwidth_limit = 1000;
+#endif
+	rc = qos_port_set_32(priv->qdev, qos_port_node, &port_cfg);
+	if (rc) {
+		PR_ERR("failed to qos_port_set_32\n");
+		qos_port_remove_32(priv->qdev, qos_port_node);
+		return DP_FAILURE;
+	}
+
+	/* Attach queue to sch_node1 */
+	qos_queue_conf_set_default_32(&queue_cfg);
+	queue_cfg.queue_child_prop.parent = sch_node1;
+#ifdef EXT_BW
+	queue_cfg.max_burst  = 64;
+	queue_cfg.child.bandwidth_share = 50;
+	queue_cfg.wred_min_guaranteed = 1;
+	queue_cfg.wred_max_allowed = 10;
+#endif
+	rc = qos_queue_set_32(priv->qdev, q_node, &queue_cfg);
+	if (rc) {
+		PR_ERR("failed to qos_queue_set_32\n");
+		qos_port_remove_32(priv->qdev, qos_port_node);
+		return DP_FAILURE;
+	}
+
+	/* Attach sch_node1 to sch_node2 */
+	qos_sched_conf_set_default_32(&sched_cfg);
+	sched_cfg.sched_child_prop.parent = sch_node2;
+	rc = qos_sched_set_32(priv->qdev, sch_node1, &sched_cfg);
+	if (rc) {
+		PR_ERR("failed to qos_sched_set_32\n");
+		qos_port_remove_32(priv->qdev, qos_port_node);
+		return DP_FAILURE;
+	}
+
+	/* Attach sch_node2 to qos/cqm dequeue port */
+	qos_sched_conf_set_default_32(&sched_cfg);
+	sched_cfg.sched_child_prop.parent = qos_port_node;
+	rc = qos_sched_set_32(priv->qdev, sch_node2, &sched_cfg);
+	if (rc) {
+		PR_ERR("failed to qos_sched_set_32\n");
+		qos_port_remove_32(priv->qdev, qos_port_node);
+		return DP_FAILURE;
+	}
+
+	return DP_SUCCESS;
+}
+
+#endif /* PP_QOS_LINK_EXAMPLE */
+
+static int get_children_list(int inst, struct dp_node *child, int node_id)
+{
+	int idx, num = 0, child_id;
+	struct hal_priv *priv = HAL(inst);
+
+	for (idx = 0; idx < DP_MAX_CHILD_PER_NODE; idx++) {
+		child_id =
+			get_qid_by_node(inst, CHILD(node_id, idx).node_id, 0);
+		if (priv->qos_sch_stat[node_id].child[idx].flag &
+		    PP_NODE_ACTIVE) {
+			child[idx].type = CHILD(node_id, idx).type;
+			if (child[idx].type == DP_NODE_SCH)
+				child[idx].id.q_id =
+						CHILD(node_id, idx).node_id;
+			else
+				child[idx].id.q_id = child_id;
+			num++;
+		}
+	}
+	return num;
+}
+
+/* dp_children_get_32 API
+ * Get direct chldren and type of given node and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_children_get_32(struct dp_node_child *cfg, int flag)
+{
+	int node_id, res = 0;
+	struct hal_priv *priv;
+
+	if (!cfg) {
+		PR_ERR("cfg cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(cfg->inst);
+	cfg->num = 0;
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (cfg->type == DP_NODE_SCH) {
+		if ((cfg->id.sch_id < 0) || (cfg->id.sch_id >= QOS_MAX_NODES)) {
+			PR_ERR("Invalid Sched ID:%d\n", cfg->id.sch_id);
+			return DP_FAILURE;
+		}
+		if (priv->qos_sch_stat[cfg->id.sch_id].c_flag == PP_NODE_FREE) {
+			PR_ERR("Invalid Sched flag:0x%x\n",
+			       priv->qos_sch_stat[cfg->id.sch_id].c_flag);
+			return DP_FAILURE;
+		}
+		node_id = cfg->id.sch_id;
+
+	} else if (cfg->type == DP_NODE_PORT) {
+		if ((cfg->id.cqm_deq_port < 0) ||
+		    (cfg->id.cqm_deq_port >= MAX_CQM_DEQ)) {
+			PR_ERR("Invalid Port ID:%d\n", cfg->id.cqm_deq_port);
+			return DP_FAILURE;
+		}
+		if (priv->deq_port_stat[cfg->id.cqm_deq_port].flag ==
+		    PP_NODE_FREE) {
+			PR_ERR("Invalid Port flag:0x%x\n",
+			       priv->deq_port_stat[cfg->id.cqm_deq_port].flag);
+			return DP_FAILURE;
+		}
+		node_id = priv->deq_port_stat[cfg->id.cqm_deq_port].node_id;
+
+	} else {
+		PR_ERR("Unkonwn type provided:0x%x\n", cfg->type);
+		return DP_FAILURE;
+	}
+	if (!priv->qos_sch_stat[node_id].child_num)
+		return DP_SUCCESS;
+
+	cfg->num = priv->qos_sch_stat[node_id].child_num;
+	res = get_children_list(cfg->inst, cfg->child, node_id);
+
+	if (cfg->num == res)
+		return DP_SUCCESS;
+
+	PR_ERR("child_num:[%d] not matched to res:[%d] for Node:%d\n",
+	       cfg->num, res, cfg->id.sch_id);
+	return DP_FAILURE;
+}
+
+/* #define DP_SUPPORT_RES_RESERVE */
+int dp_node_reserve_32(int inst, int ep, struct dp_port_data *data, int flags)
+{
+	int i;
+	unsigned int id;
+	struct pp_qos_queue_info info;
+	int len;
+	struct resv_q *resv_q;
+#ifdef WORKAROUND_DROP_PORT
+	struct pp_qos_queue_conf q_conf;
+#endif
+	struct hal_priv *priv = HAL(inst);
+	int res = DP_SUCCESS;
+
+#ifndef DP_SUPPORT_RES_RESERVE /* immediately return */
+	return res;
+#endif
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	/* free resved queue/scheduler */
+	if (flags == DP_F_DEREGISTER)
+		goto FREE_EXIT;
+
+	/* Need reserve for queue/scheduler */
+/* #define DP_SUPPORT_RES_TEST */
+#ifdef DP_SUPPORT_RES_TEST
+	data->num_resv_q = 4;
+	data->num_resv_sched = 4;
+#endif
+	/* to reserve the queue */
+	if (data->num_resv_q <= 0)
+		goto RESERVE_SCHED;
+	len = sizeof(struct resv_q) * data->num_resv_q;
+	priv->resv[ep].resv_q = kzalloc(len, GFP_ATOMIC);
+	if (!priv->resv[ep].resv_q) {
+		res = DP_FAILURE;
+		goto FREE_EXIT;
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS, "queue size =%d for ep=%d\n", len, ep);
+	resv_q = priv->resv[ep].resv_q;
+	for (i = 0; i < data->num_resv_q; i++) {
+		if (qos_queue_allocate_32(priv->qdev, &id)) {
+			res = DP_FAILURE;
+			PR_ERR("qos_queue_allocate_32 failed\n");
+			goto FREE_EXIT;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "qos_queue_allocate_32: %d\n", id);
+#ifdef WORKAROUND_DROP_PORT /* use drop port */
+		qos_queue_conf_set_default_32(&q_conf);
+		q_conf.wred_enable = 0;
+		q_conf.wred_max_allowed = DEF_QRED_MAX_ALLOW;
+		q_conf.queue_child_prop.parent = priv->ppv4_drop_p;
+		if (qos_queue_set_32(priv->qdev, id, &q_conf)) {
+			res = DP_FAILURE;
+			PR_ERR("qos_queue_set_32 fail for queue=%d to parent=%d\n",
+			       id, q_conf.queue_child_prop.parent);
+			goto FREE_EXIT;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Workaround queue(/%d)-> tmp parent(/%d)\n",
+			 id, q_conf.queue_child_prop.parent);
+#endif
+		if (qos_queue_info_get_32(priv->qdev, id, &info)) {
+			qos_queue_remove_32(priv->qdev, id);
+			res = DP_FAILURE;
+			PR_ERR("qos_queue_info_get_32: %d\n", id);
+			goto FREE_EXIT;
+		}
+		resv_q[i].id = id;
+		resv_q[i].physical_id = info.physical_id;
+		priv->resv[ep].num_resv_q = i + 1;
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "reseve q[%d/%d]\n",
+			 resv_q[i].id, resv_q[i].physical_id);
+	}
+RESERVE_SCHED:
+	/* reserve scheduler */
+	if (data->num_resv_sched > 0) {
+		int len;
+		struct resv_sch *p_sch = priv->resv[ep].resv_sched;
+
+		len = sizeof(struct resv_sch) * data->num_resv_sched;
+		priv->resv[ep].resv_sched = kzalloc(len, GFP_ATOMIC);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "sched size =%d for ep=%d\n",
+			 len, ep);
+		if (!priv->resv[ep].resv_sched) {
+			res = DP_FAILURE;
+			goto FREE_EXIT;
+		}
+		p_sch = priv->resv[ep].resv_sched;
+		for (i = 0; i < data->num_resv_sched; i++) {
+			if (qos_sched_allocate_32(priv->qdev, &id)) {
+				res = DP_FAILURE;
+				PR_ERR("qos_queue_allocate_32 failed\n");
+				goto FREE_EXIT;
+			}
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "qos_sched_allocate_32: %d\n", id);
+			p_sch[i].id = id;
+			priv->resv[ep].num_resv_sched = i + 1;
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "reseve sched[/%d]\n", resv_q[i].id);
+		}
+	}
+	return res;
+
+FREE_EXIT:
+	if (priv->resv[ep].resv_q) {
+		struct resv_q  *resv_q = priv->resv[ep].resv_q;
+
+		for (i = 0; i < priv->resv[ep].num_resv_q; i++)
+			qos_queue_remove_32(priv->qdev, resv_q[i].id);
+		kfree(resv_q);
+		priv->resv[ep].resv_q = NULL;
+		priv->resv[ep].num_resv_q = 0;
+	}
+	if (priv->resv[ep].resv_sched) {
+		struct resv_sch *resv_sch = priv->resv[ep].resv_sched;
+
+		for (i = 0; i < priv->resv[ep].num_resv_sched; i++)
+			qos_sched_remove_32(priv->qdev, resv_sch[i].id);
+		kfree(resv_sch);
+		priv->resv[ep].resv_sched = NULL;
+		priv->resv[ep].num_resv_sched = 0;
+	}
+	return res;
+}
+
+int dp_get_q_logic_32(int inst, int qid)
+{
+	struct hal_priv *priv = HAL(inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return PP_QOS_INVALID_ID;
+	}
+	return pp_qos_queue_id_get(priv->qdev, qid);
+}
+
+
+int dp_get_queue_logic_32(struct dp_qos_q_logic *cfg, int flag)
+{
+	struct hal_priv *priv = HAL(cfg->inst);
+	int res = DP_SUCCESS;
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	cfg->q_logic_id = dp_get_q_logic_32(cfg->inst, cfg->q_id);
+	if (cfg->q_logic_id == PP_QOS_INVALID_ID)
+		res = DP_FAILURE;
+	return res;
+}
+
+/* dp_qos_global_info_get_32 API
+ * Get global qos config information return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_qos_global_info_get_32(struct dp_qos_cfg_info *info, int flag)
+{
+	struct hal_priv *priv;
+	unsigned int quanta = 0;
+	
+	if (!info) {
+		PR_ERR("info cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(info->inst);
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	/* TODO Need to add later once PP QOS has this prototype */
+	#if 0
+	if (pp_qos_get_quanta(priv->qdev, &quanta)) {
+		PR_ERR("failed pp_qos_get_quanta\n");
+		return DP_FAILURE;
+	}
+	#endif
+	info->quanta = quanta;
+	DP_DEBUG(DP_DBG_FLAG_QOS, "quanta=%d\n", quanta);
+
+	return DP_SUCCESS;
+}
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_ppv4_session.c b/drivers/net/datapath/dpm/gswip32/datapath_ppv4_session.c
new file mode 100644
index 000000000000..0d3ba5dd3f72
--- /dev/null
+++ b/drivers/net/datapath/dpm/gswip32/datapath_ppv4_session.c
@@ -0,0 +1,522 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+#include <net/datapath_api.h>
+#include "../datapath.h"
+#include "datapath_misc.h"
+#include "datapath_ppv4_session.h"
+
+#define LGM_DP_HARDCODE_TEST
+
+#if IS_ENABLED(CONFIG_PPV4)
+#define DP_PP_API_DUMMY 0
+#else
+#define DP_PP_API_DUMMY 1
+#endif
+
+#if DP_PP_API_DUMMY
+s32 pp_port_add(u16 port_id, struct pp_port_cfg *cfg)
+{
+	return 0;
+}
+s32 pp_session_create(struct pp_sess_create_args *args, u32 *sess_id,
+		      struct pp_request *req)
+{
+	static int local_sess_id = 0;
+	*sess_id = local_sess_id++;
+
+	return 0;
+}
+
+s32 pp_hostif_add(struct pp_hostif_cfg *hif)
+{
+	return 0;
+}
+
+s32 pp_hostif_dflt_set(struct pp_hif_datapath *dp)
+{
+	return 0;
+}
+
+int cbm_gpid_lpid_map(struct cbm_gpid_lpid  *map)
+{
+	return 0;
+}
+#endif
+
+void init_gpid_map_table(int inst)
+{
+	struct hal_priv *priv = HAL(inst);
+	u32 i = 0;
+
+	memset(priv->gp_dp_map, 0, (sizeof(priv->gp_dp_map) * MAX_GPID));
+
+	for (i = 0; i < MAX_GPID; i++) {
+		if (i >= 0 && i <= DP_DYN_GPID_START)
+			priv->gp_dp_map[i].dpid = i;
+
+		/* Rest all GPID > 16 map to DPID 15,
+		 * Later Caller will change this
+		 */
+		if (i >= DP_SPL_GPID_START)
+			priv->gp_dp_map[i].dpid = (12 + (i-DP_SPL_GPID_START));
+	}
+
+	return;
+}
+
+static void __mark_alloc_gpid(int inst, int base, int end, int dpid)
+{
+	int tmp;
+	struct hal_priv *priv = HAL(inst);
+
+	for (tmp = base; (tmp < end) && (tmp < MAX_GPID); tmp++) {
+		priv->gp_dp_map[tmp].alloc_flags = 1;
+		priv->gp_dp_map[tmp].dpid = dpid;
+		priv->gp_dp_map[tmp].subif = -1;
+		priv->gp_dp_map[tmp].ref_cnt = 0;
+	}
+}
+
+/* This API will be used during DP_alloc_port/ext only */
+int alloc_gpid(int inst, enum GPID_TYPE type, int gpid_num, int dpid)
+{
+	u32 base, match;
+	struct hal_priv *priv = HAL(inst);
+	int start = 0, end = 0;
+
+	if (type < DP_DYN_GPID) {
+		DP_DEBUG(DP_DBG_FLAG_DBG, "Can't alloc overlaps with LPID\n");
+		return DP_FAILURE;
+	}
+
+	if (type == DP_DYN_GPID) {
+		start = DP_DYN_GPID_START;
+		end = DP_DYN_GPID_END;
+	} else if (type == DP_SPL_GPID) {
+		start = SPL_GPID_VIA_DPID(dpid);
+		end = start + 1;
+	}
+	for (base = start; base < end; base++) {
+		for (match = 0;
+		     (match < gpid_num) && ((base + match) < end);
+		     match++) {
+			if (priv->gp_dp_map[base + match].alloc_flags)
+				break;
+		}
+
+		if (match == gpid_num) {
+			__mark_alloc_gpid(inst, base, (base + match), dpid);
+			return base;
+		}
+	}
+
+	return DP_FAILURE; /* Alloc GPID Failure */
+}
+
+int free_gpid(int inst, int base, int gpid_num)
+{
+	u32 tmp;
+	struct hal_priv *priv = HAL(inst);
+
+	/* Expecting base always greater than DP_DYN_GPID_START */
+	if (base < DP_DYN_GPID_START)
+		return DP_FAILURE;
+
+	for (tmp = base; (tmp < MAX_GPID) && gpid_num; tmp++) {
+		priv->gp_dp_map[tmp].alloc_flags = 0;
+		priv->gp_dp_map[tmp].dpid = 0;
+		priv->gp_dp_map[tmp].subif = 0;
+		/* TODO: Dpid and vap reset*/
+		gpid_num--;
+	}
+
+	return DP_SUCCESS;
+}
+
+int get_dpid_from_gpid(int inst, int gpid)
+{
+	struct hal_priv *priv = HAL(inst);
+
+	return priv->gp_dp_map[gpid].dpid;
+}
+
+bool is_mem_port(int alloc_flag)
+{
+	if (alloc_flag & DP_F_ACA)
+		return true;
+
+	return false;
+}
+
+bool is_stream_port(int alloc_flag)
+{
+	if ((alloc_flag & DP_F_FAST_ETH_LAN) ||
+	    (alloc_flag & DP_F_FAST_ETH_WAN) ||
+	    (alloc_flag & DP_F_GINT) ||
+	    (alloc_flag & DP_F_GPON) ||
+	    (alloc_flag & DP_F_EPON))
+		return true;
+
+	return false;
+}
+
+
+int get_subif_size(u32 vap_mask)
+{
+	u32 i;
+	int num = 0;
+
+	for (i = 0; i < sizeof(i); i++)
+		if (vap_mask & (1 << i))
+			num ++;
+
+	return num;
+}
+
+/* dp_add_pp_gpid: to configure normal GPID or special GPID
+ * Note: try to get all GPID related configuration via dpid/vap
+         if spl_gpid is 1, vap is not valid
+ * If success, return DP_SUCCESS.
+ * else return -1 /DP_FAILURE
+ */
+int dp_add_pp_gpid(int inst, int dpid, int vap, int gpid, int spl_gpid)
+{
+	struct pp_port_cfg cfg = {0};
+	struct ctp_assign *ctp_info;
+	struct pmac_port_info *port_info;
+	int i;
+	struct hal_priv *priv;
+	struct dp_subif_info *sif;
+
+	priv = HAL(inst);
+	port_info = get_dp_port_info(inst, dpid);
+	ctp_info = get_ctp_assign(port_info->alloc_flags);
+	if (!ctp_info) {
+		PR_ERR("get_ctp_assign fail:0x%x for dpid=%d\n",
+		       port_info->alloc_flags, dpid);
+		return DP_SUCCESS;
+	}
+	sif = get_dp_port_subif(port_info, vap);
+	sif->gpid = gpid;
+	priv->gp_dp_map[gpid].ref_cnt++;
+	if (priv->gp_dp_map[gpid].ref_cnt > 1) /* GPID already configued */
+		return DP_SUCCESS;
+	if (!is_stream_port(port_info->alloc_flags))
+		cfg.rx.mem_port_en = 1;
+	cfg.rx.flow_ctrl_en = 1;
+	if (spl_gpid) {
+		cfg.rx.parse_type = NO_PARSE;
+		cfg.rx.cls.n_flds = 2;
+		/* convert traffic class to PP classification ID */
+		cfg.rx.cls.n_flds = 2;
+		cfg.rx.cls.cp[0].stw_off = DP_CLaSS_OFFSET;
+		cfg.rx.cls.cp[0].copy_size = DP_CLASS_SIZE;
+		cfg.rx.cls.cp[1].stw_off = ctp_info->vap_offset;
+		cfg.rx.cls.cp[1].copy_size = get_subif_size(ctp_info->vap_mask);
+	} else {
+		cfg.rx.parse_type = L2_PARSE;
+		/* Note: for subif based GPID, no need to stw operation since
+		 * it will be based on packet information, not stw data
+		 */
+		//return DP_FAILURE;
+	}
+	/* As told my hezi/Soma for the rx.policies_map setting
+	 * streaming port: 0
+	   WIFI: 2K, ingress policy
+	   other mem port: all
+	   But I don't really understand how PPv4 internal is using ???
+	*/
+#ifdef LGM_DP_HARDCODE_TEST
+	/* In fact it is rx pool map, Not really policy map.
+	 * index: bit 0-pool size 0, bit 1-pool size 1 and so on
+	 * This hard-coded setting is just for Ethernet LAN/WAN CPU path purpose
+	 */
+	if (is_stream_port(port_info->alloc_flags)) /* Stream port all pool */
+		cfg.rx.policies_map = 0;
+	else /* CPU */
+		cfg.rx.policies_map = 0xF;
+#endif /* end of LGM_DP_HARDCODE_TEST */
+	cfg.tx.max_pkt_size = sif->max_pkt_size;
+	/* headroom_size: maybe multipe of 128, currently may 256 ? */
+	cfg.tx.headroom_size = sif->headroom_size;
+	/* for timestamp, IPSEC  */
+	cfg.tx.tailroom_size = sif->tailroom_size;
+	cfg.tx.min_pkt_len = sif->min_pkt_len;
+	cfg.tx.pkt_only_en = 0;
+	if (is_stream_port(port_info->alloc_flags))
+		cfg.tx.seg_en = 0;
+	if (spl_gpid) {
+		cfg.tx.base_policy = port_info->policy_base;
+		for (i = 0; i < port_info->policy_num; i++)
+			cfg.tx.policies_map |= BIT(i);
+	} else {
+		cfg.tx.base_policy = sif->policy_base;
+		for (i = 0; i < sif->policy_num; i++)
+			cfg.tx.policies_map |= BIT(i);
+	}
+
+#ifdef LGM_DP_HARDCODE_TEST
+/* packet information to stream port */
+#define STREAM_MAX_PKT_ZIE  (1518 + 16) /* PMAC 16 bytes */
+#define STREAM_HEADERROOM   32   /* maybe zero ok ? */
+#define STREAM_TAILROOM     4  /* No need */
+
+/* packet information to CPU */
+#define CPU_MAX_PKT_ZIE  (1518 + 16 + 32) /*Hope no time tag ?: pmac + preL2 */
+#define CPU_HEADERROOM   208
+#define CPU_TAILROOM     320 /*aligned skb shar_info */
+
+	if (is_stream_port(port_info->alloc_flags)) {
+		if (cfg.tx.seg_en) {
+			/* Headroom & tailroom must be 0 if seg_en is set */
+			cfg.tx.headroom_size = 0;
+			cfg.tx.tailroom_size = 0;
+		} else {
+			cfg.tx.headroom_size = STREAM_HEADERROOM;
+			cfg.tx.tailroom_size = STREAM_TAILROOM;
+		}
+		cfg.tx.max_pkt_size = STREAM_MAX_PKT_ZIE;
+		cfg.tx.min_pkt_len = PP_MIN_TX_PKT_LEN_NONE; //PP_MIN_TX_PKT_LEN_64B;
+	} else { /*to CPU */
+		cfg.tx.max_pkt_size = CPU_MAX_PKT_ZIE;
+		cfg.tx.headroom_size = CPU_HEADERROOM;
+		cfg.tx.tailroom_size = CPU_TAILROOM;
+		cfg.tx.min_pkt_len = PP_MIN_TX_PKT_LEN_NONE;
+		cfg.tx.prel2_en = true;
+	}
+#endif  /* end of LGM_DP_HARDCODE_TEST */
+	PR_INFO("add gpid=%d\n", gpid);
+	PR_INFO("cfg.rx.cls.n_flds=%d\n", cfg.rx.cls.n_flds);
+	PR_INFO("cfg.rx.mem_port_en=%d\n", cfg.rx.mem_port_en);
+	PR_INFO("cfg.rx.flow_ctrl_en=%d\n", cfg.rx.flow_ctrl_en);
+	PR_INFO("cfg.rx.policies_map=0x%X\n", cfg.rx.policies_map);
+	PR_INFO("cfg.rx.parse_type=%d\n", cfg.rx.parse_type);
+
+	PR_INFO("cfg.tx.max_pkt_size=%d\n", cfg.tx.max_pkt_size);
+	PR_INFO("cfg.tx.headroom_size=%d\n", cfg.tx.headroom_size);
+	PR_INFO("cfg.tx.tailroom_size=%d\n", cfg.tx.tailroom_size);
+	PR_INFO("cfg.tx.min_pkt_len=%d\n", cfg.tx.min_pkt_len);
+	PR_INFO("cfg.tx.base_policy=%d\n", cfg.tx.base_policy);
+	PR_INFO("cfg.tx.policy_map=0x%X\n", cfg.tx.policies_map);
+	PR_INFO("cfg.tx.pkt_only_en=%d\n", cfg.tx.pkt_only_en);
+	PR_INFO("cfg.tx.seg_en=%d\n", cfg.tx.seg_en);
+
+
+	if (pp_port_add(gpid, &cfg)) {
+		PR_ERR("failed to create gpid: %d\n", gpid);
+		return DP_FAILURE;
+	}
+	PR_INFO("GPID[%d] added ok\n", gpid);
+	return DP_SUCCESS;
+}
+
+int dp_del_pp_gpid(int inst, int dpid, int vap, int gpid, int spl_gpid)
+{
+	return DP_SUCCESS;
+}
+
+
+/* dp_add_default_egress_sess: Add default egress session based on
+ *                             special GPID, class/subif only
+ * This API will be used only for CPU TX path to memory port for
+ * policy/pool conversion
+ */
+int dp_add_default_egress_sess(struct dp_session *sess, int flag)
+{
+	int i;
+	struct pp_sess_create_args args = {0};
+	u32 sess_id = -1;
+	int ret;
+	struct pmac_port_info *port_info;
+
+	args.in_port = sess->in_port;
+	args.eg_port = sess->eg_port;
+	args.fsqm_prio = 0;
+	args.color = PP_COLOR_GREEN;
+	args.flags = 0;
+	args.dst_q = dp_get_q_logic_32(sess->inst, sess->qid);
+	for (i =0; i < ARRAY_SIZE(args.sgc); i++)
+		args.sgc[i] = PP_SGC_INVALID;
+	for (i =0; i < ARRAY_SIZE(args.tbm); i++)
+		args.tbm[i] = PP_TBM_INVALID;
+	args.ud_sz = 0;
+	args.tmp_ud_sz = 0; /* 1 means 1 template of UD,
+			     * ie, 16 bytes
+			     */
+	args.ps_off = PP_INVALID_PS_OFF;
+	args.cls.n_flds = 2;
+	args.cls.fld_data[0] = sess->class;
+	args.cls.fld_data[1] = sess->vap;
+	args.in_pkt = NULL;
+	args.eg_pkt = NULL;
+	args.nhdr = NULL;
+	args.hash.h1 = sess->h1;
+	args.hash.h2 = sess->h2;
+	args.hash.sig = sess->sig;
+
+	ret = pp_session_create(&args, &sess_id, NULL);
+	if (ret) {
+		/* IF failure, call PPA API to add this session... */
+		DP_ERR("session create failed. Call PPA to continue");
+		return DP_FAILURE;
+	}
+	port_info = get_dp_port_info(sess->inst, sess->in_port);
+	get_dp_port_subif(port_info, sess->vap)->dfl_sess[sess->class] =
+		sess_id;
+	DP_INFO("session id = %u\n", sess_id);
+	return DP_SUCCESS;
+}
+
+/* dp_add_hostif: create hostif for CPU RX path
+ * This API is for normal CPU RX path traffic per GPID
+ * If exception session table full, dp_add_hostif will fail to add.
+ * But this API itseld still can return success.
+ */
+int dp_add_hostif(int inst, int dpid, int vap)
+{
+	struct pp_hostif_cfg hif = {0};
+	int i, ret;
+	struct pmac_port_info *port_info;
+	/* cpu0: 0(high) 1(low)
+	 * cpu1: 2(high) 3(low)
+	 * cpu2: 4(high) 5(low)
+	 * cpu3: 6(high) 7(low)
+	 */
+	struct pmac_port_info *cpu_info;
+
+	port_info = get_dp_port_info(inst, dpid);
+	cpu_info = get_dp_port_info(inst, 0);
+
+	hif.cls.port = get_dp_port_subif(port_info, vap)->gpid;
+	hif.dp.color = PP_COLOR_GREEN;  //??? enough
+	for (i =0; i < ARRAY_SIZE(hif.dp.sgc); i++)
+		hif.dp.sgc[i] = PP_SGC_INVALID;
+	for (i =0; i < ARRAY_SIZE(hif.dp.tbm); i++)
+		hif.dp.tbm[i] = PP_TBM_INVALID;
+
+	/* low priority */
+	hif.cls.tc_bitmap = BIT(0) | BIT(1);  /*need check GSWIP implementation ?*/
+	/*collect all CPU low priority queue/port */
+	for (i =0; (i < ARRAY_SIZE(hif.dp.eg)) && (i < MAX_SUBIFS); i++) {
+		struct dp_subif_info *sif;
+
+		sif = get_dp_port_subif(cpu_info, 2 * i + 1);
+		if (sif->flags) { /* vaid VAP */
+			hif.dp.eg[i].qos_q = dp_get_q_logic_32(inst, sif->qid);
+			hif.dp.eg[i].pid = sif->gpid;
+		}
+		else {
+			hif.dp.eg[i].qos_q = PP_QOS_INVALID_ID;
+			hif.dp.eg[i].pid = PP_PORT_INVALID;
+		}
+	}
+	DP_INFO("dpid=%d vap=%u\n", dpid, vap);
+	DP_INFO("hif.cls.port=%u\n", hif.cls.port);
+	DP_INFO("hif.cls.tc_bitmap=0x%x\n", hif.cls.tc_bitmap);
+	DP_INFO("hif.dp.color=%d\n", hif.dp.color);
+	for (i = 0; i < ARRAY_SIZE(hif.dp.eg); i++) {
+		DP_INFO("hif.dp.eg[%d].pid=%u\n", i, hif.dp.eg[i].pid);
+		DP_INFO("hif.dp.eg[%d].qos_q=%u\n", i, hif.dp.eg[i].qos_q);
+	}
+	ret= pp_hostif_add(&hif);
+	if (ret)
+		DP_ERR("hostif_add fail:dpid/gpid=%u/%u vap/tc=%d/%u\n",
+		       dpid, hif.cls.port, vap, hif.cls.tc_bitmap);
+
+	/* high priority */
+	hif.cls.tc_bitmap = BIT(2) | BIT(3); /*need check GSWIP implementation ?*/
+	/*collect all CPU low priority queue/port */
+	for (i =0; (i < ARRAY_SIZE(hif.dp.eg)) && (i < MAX_SUBIFS); i++) {
+		struct dp_subif_info *sif;
+
+		sif = get_dp_port_subif(cpu_info, 2 * i);
+		if (sif->flags) { /* Valid VAP */
+			hif.dp.eg[i].qos_q = dp_get_q_logic_32(inst, sif->qid);
+			hif.dp.eg[i].pid = sif->gpid;
+		}
+		else {
+			hif.dp.eg[i].qos_q = PP_QOS_INVALID_ID;
+			hif.dp.eg[i].pid  = PP_PORT_INVALID;
+		}
+	}
+	DP_INFO("dpid=%d vap=%u\n", dpid, vap);
+	DP_INFO("hif.cls.port=%u\n", hif.cls.port);
+	DP_INFO("hif.cls.tc_bitmap=0x%x\n", hif.cls.tc_bitmap);
+	DP_INFO("hif.dp.color=%u\n", hif.dp.color);
+	for (i = 0; i < ARRAY_SIZE(hif.dp.eg); i++) {
+		DP_INFO("hif.dp.eg[%d].pid=%u\n", i, hif.dp.eg[i].pid);
+		DP_INFO("hif.dp.eg[%d].qos_q=%u\n", i, hif.dp.eg[i].qos_q);
+	}
+	ret= pp_hostif_add(&hif);
+	if (ret)
+		DP_ERR("hostif_add fail:dpid/gpid=%d/%d vap/tc=%d/%d\n",
+		       dpid, hif.cls.port, vap, hif.cls.tc_bitmap);
+
+	return DP_SUCCESS;
+}
+
+
+/* dp_add_dflt_hostif: create default hostif
+ * This API is for default setting in case not match any exception sessions
+ */
+int dp_add_dflt_hostif(struct dp_dflt_hostif *hostif, int flag)
+{
+	struct pp_hif_datapath dp = {0};
+	int i;
+
+	if (!hostif) {
+		DP_ERR("hostif NULL\n");
+		return DP_FAILURE;
+	}
+	/* only allowed one queue for pp_hostif_dflt_set */
+	dp.eg[0].qos_q = dp_get_q_logic_32(hostif->inst, hostif->qid);
+	dp.eg[0].pid = hostif->gpid;
+	dp.color = PP_COLOR_GREEN;
+
+	for (i =1; i < ARRAY_SIZE(dp.eg); i++) {
+		dp.eg[i].qos_q = PP_QOS_INVALID_ID;
+		dp.eg[i].pid = PP_PORT_INVALID;
+	}
+	for (i =0; i < ARRAY_SIZE(dp.sgc); i++)
+		dp.sgc[i] = PP_SGC_INVALID;
+	for (i =0; i < ARRAY_SIZE(dp.tbm); i++)
+		dp.tbm[i] = PP_TBM_INVALID;
+
+	return pp_hostif_dflt_set(&dp);
+}
+
+int dp_subif_pp_set(int inst, int portid, int vap,
+		    struct subif_platform_data *data, u32 flags)
+{
+	struct pmac_port_info *port_info;
+	int gpid, num = vap;
+
+	port_info = get_dp_port_info(inst, portid);
+	if ((port_info->alloc_flags & DP_F_GPON) ||
+	    port_info->alloc_flags & DP_F_GPON)
+		gpid = port_info->gpid_base;
+	else {
+		if (vap >= port_info->gpid_num)
+			num = port_info->gpid_num;
+		gpid = port_info->gpid_base + num;
+	}
+	PR_INFO("dp_subif_pp_set=%d\n", gpid);
+	if (dp_add_pp_gpid(inst, portid, vap, gpid, 0) == DP_FAILURE){
+		DP_ERR("dp_add_pp_gpid for dport/vap=%d/%d\n", portid, vap);
+		return -1;
+	}
+	get_dp_port_subif(port_info, vap)->gpid = gpid;
+
+	/* need increase GPID reference counter and store DPID
+	 * Also need update to pmac_port_info array
+	*/
+	dp_add_hostif(inst, portid, vap);
+	return 0;
+}
+
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_ppv4_session.h b/drivers/net/datapath/dpm/gswip32/datapath_ppv4_session.h
new file mode 100644
index 000000000000..e22d1fb03a43
--- /dev/null
+++ b/drivers/net/datapath/dpm/gswip32/datapath_ppv4_session.h
@@ -0,0 +1,73 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Joby Thampan <joby.thampan@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#ifndef DATAPATH_PPV4_SESSION_H_
+#define DATAPATH_PPV4_SESSION_H_
+
+#define DP_CLaSS_OFFSET  28
+#define DP_CLASS_SIZE    4
+
+enum GPID_TYPE {
+	DP_RES_GPID = 0,  /* 0 - 15,    Dont Use, as it overlaps with LPID */
+	DP_DYN_GPID, 	  /* 16 - 239,  Dynamically allocated by DP */
+	DP_SPL_GPID 	  /* 240 - 255, 16 Special GPID per DPID */
+};
+
+#define DP_DYN_GPID_START	16
+#define DP_DYN_GPID_END		239
+#define DP_SPL_GPID_START	240
+#define DP_SPL_GPID_END		255
+
+#define IS_SPECIAL_GPID(gpid)	\
+	((gpid >= DP_SPL_GPID_START) && (gpid <= DP_SPL_GPID_END))?1:0
+/* Get Special GPID via DPID with fixed algo:
+   DPID    special_gpid
+   15      255
+   14      254
+   13      253
+   ...
+   10 .... 250
+ */
+#define SPL_GPID_VIA_DPID(dpid) (DP_SPL_GPID_END - (DP_DYN_GPID_START - 1 - dpid))
+struct dp_dflt_hostif {
+	int inst;
+	int qid;
+	int gpid;
+	int color;
+};
+
+
+struct dp_session {
+	int inst; /* reserved for future */
+	int in_port; /* ingress GPID: Special GPID for this DC LPID.*/
+	int eg_port; /* eg/dst GPID:  Actual GPID (DC LPID + subif) */
+	int qid; /* physical qid */
+	int class; /* traffic class */
+	int vap;  /* same as subif_grp */
+	u32 h1;  /* hash1 function result */
+	u32 h2;  /* hash2 function result */
+	u32 sig; /* signature */
+};
+
+#define dp_min_tx_pkt_len pp_min_tx_pkt_len
+
+int alloc_gpid(int inst, enum GPID_TYPE type, int gpid_num, int dpid);
+int free_gpid(int inst, int base, int gpid_num);
+int get_dpid_from_gpid(int inst, int gpid);
+int dp_add_dflt_hostif(struct dp_dflt_hostif *hostif, int flag);
+int dp_add_pp_gpid(int inst, int dpid, int vap, int gpid, int spl_gpid);
+int dp_del_pp_gpid(int inst, int dpid, int vap, int gpid, int spl_gpid);
+bool is_stream_port(int alloc_flag);
+struct subif_platform_data;
+int dp_subif_pp_set(int inst, int portid, int vap,
+		    struct subif_platform_data *data, u32 flags);
+int dp_add_default_egress_sess(struct dp_session *sess, int flag);
+int dp_get_q_logic_32(int inst, int qid);
+
+#endif
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_proc.c b/drivers/net/datapath/dpm/gswip32/datapath_proc.c
new file mode 100644
index 000000000000..a14fd7bd5e64
--- /dev/null
+++ b/drivers/net/datapath/dpm/gswip32/datapath_proc.c
@@ -0,0 +1,397 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include <linux/module.h>
+#include <net/datapath_proc_api.h>	/*for proc api */
+#include <net/datapath_api.h>
+#include <linux/list.h>
+
+#include "../datapath.h"
+#include "datapath_misc.h"
+
+#include "../datapath_swdev.h"
+
+#define PROC_PARSER "parser"
+#define DP_PROC_CBMLOOKUP "lookup"
+
+struct list_head fdb_tbl_list_32;
+
+static void proc_parser_read(struct seq_file *s);
+static ssize_t proc_parser_write(struct file *, const char *, size_t,
+				 loff_t *);
+
+static void proc_parser_read(struct seq_file *s)
+{
+	s8 cpu, mpe1, mpe2, mpe3;
+
+	if (!capable(CAP_NET_ADMIN))
+		return;
+	dp_get_gsw_parser_32(&cpu, &mpe1, &mpe2, &mpe3);
+	seq_printf(s, "cpu : %s with parser size =%d bytes\n",
+		   parser_flag_str(cpu), parser_size_via_index(0));
+	seq_printf(s, "mpe1: %s with parser size =%d bytes\n",
+		   parser_flag_str(mpe1), parser_size_via_index(1));
+	seq_printf(s, "mpe2: %s with parser size =%d bytes\n",
+		   parser_flag_str(mpe2), parser_size_via_index(2));
+	seq_printf(s, "mpe3: %s with parser size =%d bytes\n",
+		   parser_flag_str(mpe3), parser_size_via_index(3));
+}
+
+ssize_t proc_parser_write(struct file *file, const char *buf,
+			  size_t count, loff_t *ppos)
+{
+	int len;
+	char str[64];
+	int num, i;
+	char *param_list[20];
+	s8 cpu = 0, mpe1 = 0, mpe2 = 0, mpe3 = 0, flag = 0;
+	int pce_rule_id;
+	static GSW_PCE_rule_t pce;
+	int inst = 0;
+	struct core_ops *gsw_handle;
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+	memset(&pce, 0, sizeof(pce));
+	gsw_handle = dp_port_prop[inst].ops[GSWIP_R];
+	len = (sizeof(str) > count) ? count : sizeof(str) - 1;
+	len -= copy_from_user(str, buf, len);
+	str[len] = 0;
+	num = dp_split_buffer(str, param_list, ARRAY_SIZE(param_list));
+
+	if (dp_strncmpi(param_list[0],
+			"enable",
+			strlen("enable")) == 0) {
+		for (i = 1; i < num; i++) {
+			if (dp_strncmpi(param_list[i],
+					"cpu",
+					strlen("cpu")) == 0) {
+				flag |= 0x1;
+				cpu = 2;
+			}
+
+			if (dp_strncmpi(param_list[i],
+					"mpe1",
+					strlen("mpe1")) == 0) {
+				flag |= 0x2;
+				mpe1 = 2;
+			}
+
+			if (dp_strncmpi(param_list[i],
+					"mpe2",
+					strlen("mpe2")) == 0) {
+				flag |= 0x4;
+				mpe2 = 2;
+			}
+
+			if (dp_strncmpi(param_list[i],
+					"mpe3",
+					strlen("mpe3")) == 0) {
+				flag |= 0x8;
+				mpe3 = 2;
+			}
+		}
+
+		if (!flag) {
+			flag = 0x1 | 0x2 | 0x4 | 0x8;
+			cpu = 2;
+			mpe1 = 2;
+			mpe2 = 2;
+			mpe3 = 2;
+		}
+
+		DP_DEBUG(DP_DBG_FLAG_DBG,
+			 "flag=0x%x mpe3/2/1/cpu=%d/%d/%d/%d\n", flag, mpe3,
+			 mpe2, mpe1, cpu);
+		dp_set_gsw_parser_32(flag, cpu, mpe1, mpe2, mpe3);
+	} else if (dp_strncmpi(param_list[0],
+				"disable",
+				strlen("disable")) == 0) {
+		for (i = 1; i < num; i++) {
+			if (dp_strncmpi(param_list[i],
+					"cpu",
+					strlen("cpu")) == 0) {
+				flag |= 0x1;
+				cpu = 0;
+			}
+
+			if (dp_strncmpi(param_list[i],
+					"mpe1",
+					strlen("mpe1")) == 0) {
+				flag |= 0x2;
+				mpe1 = 0;
+			}
+
+			if (dp_strncmpi(param_list[i],
+					"mpe2",
+					strlen("mpe2")) == 0) {
+				flag |= 0x4;
+				mpe2 = 0;
+			}
+
+			if (dp_strncmpi(param_list[i],
+					"mpe3",
+					strlen("mpe3")) == 0) {
+				flag |= 0x8;
+				mpe3 = 0;
+			}
+		}
+
+		if (!flag) {
+			flag = 0x1 | 0x2 | 0x4 | 0x8;
+			cpu = 0;
+			mpe1 = 0;
+			mpe2 = 0;
+			mpe3 = 0;
+		}
+
+		DP_DEBUG(DP_DBG_FLAG_DBG,
+			 "flag=0x%x mpe3/2/1/cpu=%d/%d/%d/%d\n", flag, mpe3,
+			 mpe2, mpe1, cpu);
+		dp_set_gsw_parser_32(flag, cpu, mpe1, mpe2, mpe3);
+	} else if (dp_strncmpi(param_list[0],
+				"refresh",
+				strlen("refresh")) == 0) {
+		dp_get_gsw_parser_32(NULL, NULL, NULL, NULL);
+		PR_INFO("value:cpu=%d mpe1=%d mpe2=%d mpe3=%d\n", pinfo[0].v,
+			pinfo[1].v, pinfo[2].v, pinfo[3].v);
+		PR_INFO("size :cpu=%d mpe1=%d mpe2=%d mpe3=%d\n",
+			pinfo[0].size, pinfo[1].size, pinfo[2].size,
+			pinfo[3].size);
+		return count;
+	} else if (dp_strncmpi(param_list[0], "mark", strlen("mark")) == 0) {
+		int flag = dp_atoi(param_list[1]);
+
+		pce_rule_id = dp_atoi(param_list[2]);
+
+		if (flag < 0)
+			flag = 0;
+		else if (flag > 3)
+			flag = 3;
+		PR_INFO("eProcessPath_Action set to %d\n", flag);
+		/*: All packets set to same mpe flag as specified */
+		memset(&pce, 0, sizeof(pce));
+		pce.pattern.nIndex = pce_rule_id;
+		pce.pattern.bEnable = 1;
+
+		pce.pattern.bParserFlagMSB_Enable = 1;
+		/* rule.pce.pattern.nParserFlagMSB = 0x0021; */
+		pce.pattern.nParserFlagMSB_Mask = 0xffff;
+		pce.pattern.bParserFlagLSB_Enable = 1;
+		/* rule.pce.pattern.nParserFlagLSB = 0x0000; */
+		pce.pattern.nParserFlagLSB_Mask = 0xffff;
+		/* rule.pce.pattern.eDstIP_Select = 2; */
+
+		pce.pattern.nDstIP_Mask = 0xffffffff;
+		pce.pattern.bDstIP_Exclude = 0;
+
+		pce.action.bRtDstPortMaskCmp_Action = 1;
+		pce.action.bRtSrcPortMaskCmp_Action = 1;
+		pce.action.bRtDstIpMaskCmp_Action = 1;
+		pce.action.bRtSrcIpMaskCmp_Action = 1;
+
+		pce.action.bRoutExtId_Action = 1;
+		pce.action.nRoutExtId = 0; /*RT_EXTID_UDP; */
+		pce.action.bRtAccelEna_Action = 1;
+		pce.action.bRtCtrlEna_Action = 1;
+		pce.action.eProcessPath_Action = flag;
+		pce.action.bRMON_Action = 1;
+		pce.action.nRMON_Id = 0;	/*RMON_UDP_CNTR; */
+
+		if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_tflow_ops
+				 .TFLOW_PceRuleWrite, gsw_handle, &pce)) {
+			PR_ERR("PCE rule add fail: GSW_PCE_RULE_WRITE\n");
+			return count;
+		}
+
+	} else if (dp_strncmpi(param_list[0], "unmark", 6) == 0) {
+		/*: All packets set to same mpe flag as specified */
+		memset(&pce, 0, sizeof(pce));
+		pce_rule_id = dp_atoi(param_list[1]);
+		pce.pattern.nIndex = pce_rule_id;
+		pce.pattern.bEnable = 0;
+		if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_tflow_ops
+				 .TFLOW_PceRuleWrite, gsw_handle, &pce)) {
+			PR_ERR("PCE rule add fail:GSW_PCE_RULE_WRITE\n");
+			return count;
+		}
+	} else {
+		PR_INFO("Usage: echo %s [cpu] [mpe1] [mpe2] [mpe3] > parser\n",
+			"<enable/disable>");
+		PR_INFO("Usage: echo <refresh> parser\n");
+
+		PR_INFO("Usage: echo %s > parser\n",
+			"mark eProcessPath_Action_value(0~3) pce_rule_id");
+		PR_INFO("Usage: echo unmark pce_rule_id > parser\n");
+		return count;
+	}
+
+	return count;
+}
+
+char *get_bp_member_string_32(int inst, u16 bp, char *buf)
+{
+	GSW_BRIDGE_portConfig_t bp_cfg;
+	int i, ret;
+	struct core_ops *gsw_handle;
+
+	gsw_handle = dp_port_prop[inst].ops[GSWIP_L];
+	if (!buf)
+		return NULL;
+	buf[0] = 0;
+	bp_cfg.nBridgePortId = bp;
+	bp_cfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP |
+		GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID;
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			   .BridgePort_ConfigGet, gsw_handle, &bp_cfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("Failed to get bridge port's member for bridgeport=%d\n",
+		       bp_cfg.nBridgePortId);
+		return buf;
+	}
+	for (i = 0; i < MAX_BP_NUM; i++)
+		if (GET_BP_MAP(bp_cfg.nBridgePortMap, i))
+			sprintf(buf + strlen(buf), "%d ", i);
+	sprintf(buf + strlen(buf), " Fid=%d ", bp_cfg.nBridgeId);
+	return buf;
+}
+
+/* proc_print_ctp_bp_info_32 is an callback API, not a standalone proc API */
+int proc_print_ctp_bp_info_32(struct seq_file *s, int inst,
+			   struct pmac_port_info *port,
+			   int subif_index, u32 flag)
+{
+	struct logic_dev *tmp;
+	struct dp_subif_info *sif = get_dp_port_subif(port, subif_index);
+	int bp = sif->bp;
+	unsigned char *buf = NULL;
+	char *p;
+
+	if (!(port->alloc_flags & DP_F_CPU)) {
+		buf = kmalloc(MAX_BP_NUM * 5 + 1, GFP_KERNEL);
+		p = get_bp_member_string_32(inst, bp, buf);
+		seq_printf(s, "          : bp=%d(member:%s)\n", bp,
+			   p ? p : "");
+		list_for_each_entry(tmp, &sif->logic_dev, list) {
+			seq_printf(s, "             %s: bp=%d(member:%s\n",
+				   tmp->dev->name, tmp->bp,
+				   get_bp_member_string_32(inst, tmp->bp, buf));
+		}
+		kfree(buf);
+	}
+	return 0;
+}
+
+static struct dp_proc_entry dp_proc_entries[] = {
+	/*name single_callback_t multi_callback_t/_start write_callback_t */
+	{PROC_PARSER, proc_parser_read, NULL, NULL, proc_parser_write},
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_CPUFREQ)
+	{PROC_COC, proc_coc_read, NULL, NULL, proc_coc_write},
+#endif
+	{DP_PROC_CBMLOOKUP, NULL, lookup_dump32, lookup_start32,
+		proc_get_qid_via_index32},
+
+	/*the last place holder */
+	{NULL, NULL, NULL, NULL, NULL}
+};
+
+int dp_sub_proc_install_32(void)
+{
+	int i;
+
+	if (!dp_proc_node) {
+		PR_ERR("dp_sub_proc_install failed\n");
+		return 0;
+	}
+
+	for (i = 0; i < ARRAY_SIZE(dp_proc_entries); i++)
+		dp_proc_entry_create(dp_proc_node, &dp_proc_entries[i]);
+	return 0;
+}
+
+#define INVALID_DMA_CH 255
+char *get_dma_flags_str32(u32 epn, char *buf, int buf_len)
+{
+	char tmp[30]; /*must be static */
+	u32 flags;
+	u32 tx_ch, k;
+	u8 f_found;
+	int i;
+	int inst = 0;
+
+	if (!buf || (buf_len < 1))
+		return NULL;
+	tx_ch = 0;
+	flags = 0;
+	tmp[0] = '\0';
+	f_found = 0;
+	for (i = 0; i < ARRAY_SIZE(dp_port_info); i++) {
+		if ((get_dp_port_info(inst, i)->flag_other &
+		    CBM_PORT_DMA_CHAN_SET) &&
+		    (get_dp_port_info(inst, i)->deq_port_base == epn)) {
+			tx_ch = get_dp_port_info(inst, i)->dma_chan;
+			break;
+		}
+	}
+	if (i >= ARRAY_SIZE(dp_port_info))
+		goto EXIT;
+	sprintf(tmp, "--");
+	if (tx_ch != INVALID_DMA_CH) {
+		if (!(flags & DP_F_FAST_DSL))/*DSLupstrem no DMA1/2 TX CHannel*/
+			sprintf(tmp + strlen(tmp), "CH%02d ", tx_ch);
+		else
+			sprintf(tmp + strlen(tmp), "CHXX ");
+	} else {
+		sprintf(tmp + strlen(tmp), "CHXX ");
+	}
+	if (flags == 0) {
+		if (epn < 4)
+			sprintf(tmp + strlen(tmp), "CPU");
+		else
+			sprintf(tmp + strlen(tmp), "Flag0");
+
+		goto EXIT;
+	}
+	for (k = 0; k < get_dp_port_type_str_size(); k++) {
+		if (flags & dp_port_flag[k]) {
+			sprintf(tmp + strlen(tmp), "%s ",
+				dp_port_type_str[k]);
+			f_found = 1;
+		}
+	}
+	if ((f_found == 1) &&
+	    (flags == DP_F_FAST_ETH_LAN)) { /*try to find its ep */
+		u32 i, num, j;
+		cbm_tmu_res_t *res;
+		struct pmac_port_info *port;
+
+		for (i = 3; i <= 4; i++) {	/*2 LAN port */
+			num = 0;
+			port = get_dp_port_info(inst, i);
+			if (!port)
+				continue;
+			if (cbm_dp_port_resources_get
+			    (&i, &num, &res, port->alloc_flags))
+				continue;
+			for (j = 0; j < num; j++) {
+				if (res[j].tmu_port != epn)/* not match */
+					continue;
+				sprintf(tmp + strlen(tmp), "%d", i);
+			}
+			kfree(res);
+		}
+	}
+	if (!f_found)
+		sprintf(tmp + strlen(tmp), "Unknown[0x%x]\n", flags);
+
+ EXIT:
+	strncpy(buf, tmp, buf_len);
+	return buf;
+}
+EXPORT_SYMBOL(get_dma_flags_str32);
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_proc.h b/drivers/net/datapath/dpm/gswip32/datapath_proc.h
new file mode 100644
index 000000000000..9a8f011b115a
--- /dev/null
+++ b/drivers/net/datapath/dpm/gswip32/datapath_proc.h
@@ -0,0 +1,9 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_rx.c b/drivers/net/datapath/dpm/gswip32/datapath_rx.c
new file mode 100644
index 000000000000..d556ef98f5e6
--- /dev/null
+++ b/drivers/net/datapath/dpm/gswip32/datapath_rx.c
@@ -0,0 +1,300 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/etherdevice.h>
+#include <net/datapath_api.h>
+#include "../datapath.h"
+#include "datapath_misc.h"
+#include "datapath_ppv4_session.h"
+
+void rx_dbg_32(u32 f, struct sk_buff *skb, struct dma_rx_desc_0 *desc0,
+	       struct dma_rx_desc_1 *desc1, struct dma_rx_desc_2 *desc2,
+	       struct dma_rx_desc_3 *desc3, struct pmac_rx_hdr *pmac,
+	       u32 *prel2, int prel2_len,
+	       int gpid, int dpid)
+{
+	int inst = 0;
+
+	DP_DEBUG(DP_DBG_FLAG_DUMP_RX, "\nDPID=%d GPID=%d\n",dpid, gpid);
+	DP_DEBUG(DP_DBG_FLAG_DUMP_RX,
+		 "\ndp_rx:skb->data=%px Loc=%x offset=%d skb->len=%d\n",
+		 skb->data, desc2->field.data_ptr,
+		 desc2->field.byte_offset, skb->len);
+
+
+	if ((f) & DP_DBG_FLAG_DUMP_RX_DESCRIPTOR)
+		dp_port_prop[inst].info.dump_rx_dma_desc(desc0, desc1,
+				desc2, desc3);
+
+	if (((f) & DP_DBG_FLAG_DUMP_RX_PMAC) && pmac)
+		dp_port_prop[inst].info.dump_rx_pmac(pmac);
+
+	if (((f) & DP_DBG_FLAG_DUMP_RX) && prel2)
+		dp_dump_raw_data((char *)prel2, prel2_len, "Pre L2 Header");
+
+	if ((f) & DP_DBG_FLAG_DUMP_RX_DATA)
+		dp_dump_raw_data((char *)skb->data,
+				 skb->len,
+				 "Original Data");
+
+}
+
+int32_t dp_rx_32(struct sk_buff *skb, u32 flags)
+{
+	int res = DP_SUCCESS;
+#if IS_ENABLED(CONFIG_INTEL_CBM_SKB)
+	struct dma_rx_desc_0 *desc_0 = (struct dma_rx_desc_0 *)&skb->DW0;
+	struct dma_rx_desc_1 *desc_1 = (struct dma_rx_desc_1 *)&skb->DW1;
+	struct dma_rx_desc_2 *desc_2 = (struct dma_rx_desc_2 *)&skb->DW2;
+	struct dma_rx_desc_3 *desc_3 = (struct dma_rx_desc_3 *)&skb->DW3;
+#else
+	#error "need add proper logic here"
+	struct dma_rx_desc_0 *desc_0 = NULL;
+	struct dma_rx_desc_1 *desc_1 = NULL;
+	struct dma_rx_desc_2 *desc_2 = NULL;
+	struct dma_rx_desc_3 *desc_3 = NULL;
+#endif
+	int vap; /*vap: 0-15 */
+	struct net_device *dev;
+	dp_rx_fn_t rx_fn;
+	char decryp = 0;
+	u8 inst = 0;
+	struct pmac_port_info *dp_port;
+	struct dp_subif_info *p_subif;
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_PTP1588)
+	struct mac_ops *ops;
+#endif
+	char *data_offset;
+	int data_len;
+	int gpid = desc_1->field.ep;
+	int dpid = 0;
+	struct cbm_tx_data cbm_data = {0};
+	struct pmac_rx_hdr *pmac = NULL;
+	int pmac_len = desc_1->field.pmac ? sizeof(struct pmac_rx_hdr) : 0;
+	int prel2_len = 0;
+	u32 *pre_l2 = NULL;
+	struct pp_desc *pp_desc;
+	struct dev_mib *mib;
+
+	dp_port = get_dp_port_info(inst, 0);
+
+	if (!skb || !skb->data) {
+		PR_ERR("skb NULL or skb->data is NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (pmac_len) {
+		pmac = (struct pmac_rx_hdr *)(skb->data);
+		if (desc_1->field.pre_l2 == 3) {
+			DP_DEBUG(DP_DBG_FLAG_DUMP_RX,
+				 "With Pmac Hdr 48 byte PreL2 not supported\n");
+			return DP_FAILURE;
+		}
+		prel2_len = pmac_len;
+	}
+
+	prel2_len += (desc_1->field.pre_l2 * 16);
+	if(prel2_len)
+		pre_l2 = (u32 *)(skb->data + pmac_len);
+
+	/* only for those packet from PPv4 should  call pp hook ? */
+	pp_rx_pkt_hook(skb);
+	pp_desc = pp_pkt_desc_get(skb);
+	if (!pp_desc) {
+		DP_ERR("pp_pkt_desc_get fail\n");
+		goto RX_DROP2;
+	}
+
+	if (desc_1->field.ep < (get_dp_port_info(inst, 0)->gpid_base + get_dp_port_info(inst, 0)->gpid_num))
+		gpid = pp_desc->ud.rx_port;
+
+	dpid = get_dpid_from_gpid(0, gpid);
+
+	if (unlikely(dp_dbg_flag))
+		rx_dbg_32(dp_dbg_flag, skb, desc_0, desc_1, desc_2,
+			  desc_3, pmac, pre_l2, prel2_len,
+			  gpid, dpid);
+
+	if (unlikely(!dpid)) { /*Normally shouldnot go to here */
+		DP_ERR("Impossible: DPID Invalid (0), Desc rx'd: D0: %08x D1: %08x D2: %08x D3: %08x\n",
+		       *(u32 *)desc_0, *(u32 *)desc_1, *(u32 *)desc_2, *(u32 *)desc_3);
+		DP_ERR("QoS Descriptor at buf_base %px Desc rx'd: D0: %08x D1: %08x D2: %08x D3: %08x\n",
+                skb->buf_base, *(skb->buf_base), *(skb->buf_base + sizeof(u32)), *(skb->buf_base + (2 * sizeof(u32))), *(skb->buf_base + (3 * sizeof(u32))));
+		goto RX_DROP;
+	}
+
+	vap = GET_VAP(desc_0->field.dest_sub_if_id,
+		      get_dp_port_info(inst, dpid)->vap_offset,
+		      get_dp_port_info(inst, dpid)->vap_mask);
+	dp_port = get_dp_port_info(inst, dpid);
+	p_subif = get_dp_port_subif(dp_port, vap);
+	mib = get_dp_port_subif_mib(p_subif);
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_PTP1588)
+
+	if (dp_port->f_ptp) {
+		ops = dp_port_prop[inst].mac_ops[dpid];
+
+		if (ops)
+			ops->do_rx_hwts(ops, skb);
+	}
+
+#endif
+
+	if (IS_SPECIAL_GPID(gpid)) {
+		u8 classid = desc_1->field.classid;
+
+		if (atomic_inc_and_test(&p_subif->f_dfl_sess[classid]) == 1) {
+			struct dp_session sess;
+			sess.inst = inst;
+			sess.in_port = gpid;
+			sess.eg_port = p_subif->gpid;
+			sess.qid = p_subif->qid;
+			sess.vap = vap;
+			sess.h1 = pp_desc->ud.hash_h1;
+			sess.h2 = pp_desc->ud.hash_h2;
+			sess.sig = pp_desc->ud.hash_sig;
+			if (dp_add_default_egress_sess(&sess, 0)) {
+				atomic_dec(&p_subif->f_dfl_sess[classid]);
+				PR_ERR("Fail to create default egress \n");
+				goto RX_DROP;
+			}
+		} else {
+			atomic_dec(&p_subif->f_dfl_sess[classid]);
+		}
+
+		cbm_data.dp_inst = inst;
+		cbm_data.f_byqos = 1;
+		res = cbm_cpu_pkt_tx(skb, &cbm_data, 0); /* no need to insert pmac
+						      * since it should be
+						      * already there
+						      */
+		UP_STATS(mib->tx_cbm_pkt);
+		return res;
+	}
+
+	/*PON traffic always have timestamp attached,removing Timestamp */
+	if (dp_port->alloc_flags & (DP_F_GPON | DP_F_EPON)) {
+		/* Stripping of last 10 bytes timestamp */
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_PTP1588)
+		if (!dp_port->f_ptp)
+			__pskb_trim(skb, skb->len - DP_TS_HDRLEN);
+
+#else
+		__pskb_trim(skb, skb->len - DP_TS_HDRLEN);
+#endif
+	}
+
+	rx_fn = dp_port->cb.rx_fn;
+
+	if (likely(rx_fn && dp_port->status)) {
+		/*Clear some fields as SWAS V3.7 required */
+		desc_3->all &= dma_rx_desc_mask3.all;
+		skb->priority = desc_1->field.classid;
+		skb->dev = p_subif->netif;
+		dev = p_subif->netif;
+
+		if (decryp) { /*workaround mark for bypass xfrm policy*/
+			desc_1->field.dec = 1;
+			desc_1->field.enc = 1;
+		}
+
+		if (!dev &&
+		    ((dp_port->alloc_flags & DP_F_FAST_DSL) == 0)) {
+			UP_STATS(mib->rx_fn_dropped);
+			goto RX_DROP;
+		}
+
+		if (unlikely(dp_dbg_flag)) {
+			DP_DEBUG(DP_DBG_FLAG_DUMP_RX, "DPID=%d GPID=%d vap=%d\n",
+				 dpid, gpid, vap);
+
+			if (dp_dbg_flag & DP_DBG_FLAG_DUMP_RX_DATA) {
+				if (pmac_len) {
+					data_len = skb->len - pmac_len;
+					dp_dump_raw_data(skb->data,
+							 pmac_len,
+							 "pmac to top drv");
+				} else {
+					data_offset = skb->data;
+					data_len = skb->len;
+					dp_dump_raw_data(data_offset,
+						 	data_len,
+						 	"Data to top drv");
+				}
+			}
+
+			if (dp_dbg_flag & DP_DBG_FLAG_DUMP_RX_DESCRIPTOR)
+				dp_port_prop[inst].info.dump_rx_dma_desc(
+					desc_0, desc_1,
+					desc_2, desc_3);
+		}
+
+
+		/*
+		 * If switch h/w acceleration is enabled,setting of this bit
+		 * avoid forwarding duplicate packets from linux
+		 */
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_SWITCHDEV)
+
+		if (p_subif->fid > 0)
+			skb->offload_fwd_mark = 1;
+
+#endif
+
+		/*Remove PMAC from SKB */
+        if (pmac_len)
+		    skb_pull(skb, pmac_len);
+
+		if((STATS_GET(p_subif->rx_flag) <= 0)) {
+			UP_STATS(mib->rx_fn_dropped);
+			goto RX_DROP2;
+		}
+		/* If Redirect Bit Set, Destination is Known */
+		if (desc_1->field.redir == 0) {
+			rx_fn(dev, NULL, skb, skb->len);
+			UP_STATS(mib->rx_fn_rxif_pkt);
+		} else {
+			rx_fn(NULL, dev, skb, skb->len);
+			UP_STATS(mib->rx_fn_txif_pkt);
+		}
+
+		return DP_SUCCESS;
+	}
+
+	if (unlikely(dpid >=
+		     dp_port_prop[inst].info.cap.max_num_dp_ports - 1)) {
+		PR_ERR("Drop for wrong ep or src port id=%u ??\n",
+		       dpid);
+		goto RX_DROP;
+	} else if (unlikely(dp_port->status == PORT_FREE)) {
+		DP_DEBUG(DP_DBG_FLAG_DUMP_RX, "Drop for port %u free\n",
+			 dpid);
+		goto RX_DROP;
+	} else if (unlikely(!rx_fn)) {
+		DP_DEBUG(DP_DBG_FLAG_DUMP_RX,
+			 "Drop for subif of port %u not registered yet\n",
+			 dpid);
+		UP_STATS(mib->rx_fn_dropped);
+		goto RX_DROP2;
+	} else {
+		pr_info("Unknown issue\n");
+	}
+
+RX_DROP:
+	UP_STATS(dp_port->rx_err_drop);
+RX_DROP2:
+
+	if (skb)
+		dev_kfree_skb_any(skb);
+
+	return res;
+}
+
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_switchdev.c b/drivers/net/datapath/dpm/gswip32/datapath_switchdev.c
new file mode 100644
index 000000000000..7d15c166aa8c
--- /dev/null
+++ b/drivers/net/datapath/dpm/gswip32/datapath_switchdev.c
@@ -0,0 +1,358 @@
+/*
+ * Copyright (C) Intel Corporation
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/etherdevice.h>
+#include <net/datapath_api.h>
+#include "../datapath_swdev.h"
+#include "../datapath.h"
+#include "datapath_misc.h"
+
+int dp_swdev_alloc_bridge_id_32(int inst)
+{
+	GSW_return_t ret;
+	GSW_BRIDGE_alloc_t br;
+	struct core_ops *gsw_handle;
+
+	gsw_handle = dp_port_prop[inst].ops[0];
+	memset(&br, 0, sizeof(br));
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdg_ops.Bridge_Alloc,
+			   gsw_handle, &br);
+	if ((ret != GSW_statusOk) ||
+	    (br.nBridgeId < 0)) {
+		PR_ERR("Failed to get a FID\n");
+		return -1;
+	}
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "FID=%d\n", br.nBridgeId);
+	return br.nBridgeId;
+}
+
+int dp_swdev_bridge_port_cfg_set(struct br_info *br_item,
+				 int inst, int bport)
+{
+	GSW_return_t ret;
+	struct bridge_member_port *bport_list = NULL;
+	GSW_BRIDGE_portConfig_t brportcfg;
+	struct core_ops *gsw_handle;
+
+	gsw_handle = dp_port_prop[inst].ops[0];
+	/*To set other members to the current bport*/
+	memset(&brportcfg, 0, sizeof(GSW_BRIDGE_portConfig_t));
+	brportcfg.nBridgePortId = bport;
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "Set current BP=%d inst:%d\n",
+		 brportcfg.nBridgePortId, inst);
+	brportcfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops.
+			   BridgePort_ConfigGet, gsw_handle, &brportcfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("fail in getting bridge port config\r\n");
+		return -1;
+	}
+	list_for_each_entry(bport_list, &br_item->bp_list, list) {
+		if (bport_list->portid != bport) {
+			SET_BP_MAP(brportcfg.nBridgePortMap,
+				   bport_list->portid);
+		}
+	}
+	brportcfg.nBridgeId = br_item->fid;
+	brportcfg.nBridgePortId = bport;
+	brportcfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID |
+		GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops.
+			   BridgePort_ConfigSet, gsw_handle, &brportcfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("Fail in allocating/configuring bridge port\n");
+		return -1;
+	}
+	/* To set other member portmap with current bridge port map */
+	list_for_each_entry(bport_list, &br_item->bp_list, list) {
+		if (bport_list->portid != bport) {
+			memset(&brportcfg, 0,
+			       sizeof(GSW_BRIDGE_portConfig_t));
+			brportcfg.nBridgePortId = bport_list->portid;
+			DP_DEBUG(DP_DBG_FLAG_SWDEV,
+				 "Set other BP=%d inst:%d\n",
+				 brportcfg.nBridgePortId, inst);
+			brportcfg.eMask =
+				GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
+			ret = gsw_core_api((dp_gsw_cb)gsw_handle
+					->gsw_brdgport_ops
+					.BridgePort_ConfigGet,
+					gsw_handle, &brportcfg);
+			if (ret != GSW_statusOk) {
+				PR_ERR
+					("fail in getting br port config\r\n");
+				return -1;
+			}
+			SET_BP_MAP(brportcfg.nBridgePortMap, bport);
+			brportcfg.nBridgeId = br_item->fid;
+			brportcfg.nBridgePortId = bport_list->portid;
+			brportcfg.eMask =
+				GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID |
+				GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
+			ret = gsw_core_api((dp_gsw_cb)gsw_handle
+					->gsw_brdgport_ops
+					.BridgePort_ConfigSet,
+					gsw_handle, &brportcfg);
+			if (ret != GSW_statusOk) {
+				PR_ERR("Fail alloc/cfg bridge port\n");
+				return -1;
+			}
+		}
+	}
+	return 0;
+}
+
+int dp_swdev_bridge_port_cfg_reset_32(struct br_info *br_item,
+				   int inst, int bport)
+{
+	GSW_BRIDGE_portConfig_t brportcfg;
+	struct bridge_member_port *bport_list = NULL;
+	int i, cnt = 0, bp = 0;
+	GSW_return_t ret;
+	struct core_ops *gsw_handle;
+
+	gsw_handle = dp_port_prop[inst].ops[0];
+	memset(&brportcfg, 0, sizeof(GSW_BRIDGE_portConfig_t));
+	brportcfg.nBridgePortId = bport;
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "Reset BP=%d inst:%d\n",
+		 brportcfg.nBridgePortId, inst);
+	brportcfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
+	/*Reset other members from current bport map*/
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			   .BridgePort_ConfigGet, gsw_handle, &brportcfg);
+	if (ret != GSW_statusOk) {
+		/* Note: here may fail if this device is not removed from
+		 * linux bridge via brctl delif but user try to un-regiser
+		 * from DP. The correct flow to unregister is like below:
+		 *  1) brctl delif xxx xxxx: remove this device from bridge
+		 *  2) dp_register_subif_ext: to un-register device from DP
+		 * Anyway, it will also work if not follow this propsal.
+		 * The only side effect is this API call fail since GSWIP
+		 * bridge port is already freed during subif_hw_reset before
+		 * this API call
+		 */
+		DP_DEBUG(DP_DBG_FLAG_SWDEV,
+			 "GSW_BRIDGE_portConfig_t fail:bp=%d\n", bport);
+		return -1;
+	}
+	for (i = 0; i < MAX_BP_NUM; i++) {
+		if (GET_BP_MAP(brportcfg.nBridgePortMap, i)) {
+			bp = i;
+			cnt++;
+		}
+	}
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "cnt:%d last bp:%d\n", cnt, bp);
+	list_for_each_entry(bport_list, &br_item->bp_list, list) {
+		if (bport_list->portid != bport) {
+			DP_DEBUG(DP_DBG_FLAG_SWDEV,
+				 "reset other from current BP=%d inst:%d\n",
+				 brportcfg.nBridgePortId, inst);
+			UNSET_BP_MAP(brportcfg.nBridgePortMap,
+				     bport_list->portid);
+		}
+	}
+	brportcfg.nBridgeId = CPU_FID; /*reset of FID*/
+	brportcfg.nBridgePortId = bport;
+	brportcfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID |
+			  GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			   .BridgePort_ConfigSet, gsw_handle, &brportcfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("Fail in configuring GSW_BRIDGE_portConfig_t in %s\r\n",
+		       __func__);
+		return -1;
+	}
+	/*Reset current bp from all other bridge port's port map*/
+	list_for_each_entry(bport_list, &br_item->bp_list, list) {
+		if (bport_list->portid != bport) {
+			memset(&brportcfg, 0,
+			       sizeof(GSW_BRIDGE_portConfig_t));
+			brportcfg.nBridgePortId = bport_list->portid;
+			DP_DEBUG(DP_DBG_FLAG_SWDEV,
+				 "reset current BP from other BP=%d inst:%d\n",
+				 brportcfg.nBridgePortId, inst);
+			brportcfg.eMask =
+				 GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP |
+				 GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID;
+			ret = gsw_core_api((dp_gsw_cb)gsw_handle
+					 ->gsw_brdgport_ops
+					 .BridgePort_ConfigGet,
+					 gsw_handle, &brportcfg);
+			if (ret != GSW_statusOk) {
+				PR_ERR("failed getting br port cfg\r\n");
+				return -1;
+			}
+			UNSET_BP_MAP(brportcfg.nBridgePortMap, bport);
+			brportcfg.nBridgePortId = bport_list->portid;
+			brportcfg.eMask =
+				 GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID |
+				 GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
+			ret = gsw_core_api((dp_gsw_cb)gsw_handle
+					 ->gsw_brdgport_ops
+					 .BridgePort_ConfigSet,
+					 gsw_handle, &brportcfg);
+			if (ret != GSW_statusOk) {
+				PR_ERR("Fail alloc/cfg br port\n");
+				return -1;
+			}
+		}
+	}
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "%s success\n", __func__);
+	/*Remove bridge entry if no member in port map of
+	 * current bport except CPU port
+	 */
+	if ((bp == 0 && cnt == 1) || (cnt == 0))
+		return DEL_BRENTRY;
+
+	return 0;
+}
+
+int dp_swdev_bridge_cfg_set_32(int inst, u16 fid)
+{
+	GSW_return_t ret;
+	GSW_BRIDGE_config_t brcfg;
+	GSW_BRIDGE_alloc_t br;
+	struct core_ops *gsw_handle;
+
+	gsw_handle = dp_port_prop[inst].ops[0];
+	memset(&br, 0, sizeof(br));
+	memset(&brcfg, 0, sizeof(brcfg));
+	brcfg.nBridgeId = fid;
+	brcfg.eMask = GSW_BRIDGE_CONFIG_MASK_FORWARDING_MODE;
+	brcfg.eForwardBroadcast = GSW_BRIDGE_FORWARD_FLOOD;
+	brcfg.eForwardUnknownMulticastNonIp = GSW_BRIDGE_FORWARD_FLOOD;
+	brcfg.eForwardUnknownUnicast = GSW_BRIDGE_FORWARD_FLOOD;
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdg_ops
+			   .Bridge_ConfigSet, gsw_handle, &brcfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("Failed to set bridge id(%d)\n", brcfg.nBridgeId);
+		br.nBridgeId = fid;
+		gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdg_ops.Bridge_Free,
+			     gsw_handle, &br);
+		return -1;
+	}
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "FID(%d) cfg success for inst %d\n",
+		 fid, inst);
+	return 0;
+}
+
+int dp_swdev_free_brcfg_32(int inst, u16 fid)
+{
+	GSW_return_t ret;
+	GSW_BRIDGE_alloc_t br;
+	struct core_ops *gsw_handle;
+
+	gsw_handle = dp_port_prop[inst].ops[0];
+	memset(&br, 0, sizeof(br));
+	br.nBridgeId = fid;
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdg_ops.Bridge_Free,
+			   gsw_handle, &br);
+	if (ret != GSW_statusOk) {
+		PR_ERR("Failed to free bridge id(%d)\n", br.nBridgeId);
+		return -1;
+	}
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "FID(%d) freed for inst:%d\n", fid, inst);
+	return 0;
+}
+
+int dp_gswip_ext_vlan_32(int inst, int vap, int ep)
+{
+	struct core_ops *gsw_handle;
+	struct ext_vlan_info *vlan;
+	struct vlan_prop vlan_prop = {0};
+	struct pmac_port_info *port;
+	struct logic_dev *tmp = NULL;
+	int flag = 0, ret, i = 0;
+	int v1 = 0, v2 = 0;
+	struct pmac_subif_info *sif;
+
+	gsw_handle = dp_port_prop[inst].ops[0];
+	port = get_dp_port_info(inst, ep);
+	vlan = kzalloc(sizeof(*vlan), GFP_KERNEL);
+	if (!vlan) {
+		PR_ERR("failed to alloc ext_vlan of %d bytes\n", sizeof(*vlan));
+		return 0;
+	}
+	vlan->vlan2_list = kzalloc(sizeof(*vlan->vlan2_list), GFP_KERNEL);
+	if (!vlan->vlan2_list) {
+		PR_ERR("failed to alloc ext_vlan of %d bytes\n",
+		       sizeof(*vlan->vlan2_list));
+		goto EXIT;
+	}
+	vlan->vlan1_list = kzalloc(sizeof(*vlan->vlan1_list), GFP_KERNEL);
+	if (!vlan->vlan1_list) {
+		PR_ERR("failed to alloc ext_vlan of %d bytes\n",
+		       sizeof(*vlan->vlan1_list));
+		goto EXIT;
+	}
+	sif = get_dp_port_subif(port, vap);
+	list_for_each_entry(tmp, &sif->logic_dev, list) {
+		DP_DEBUG(DP_DBG_FLAG_SWDEV, "tmp dev name:%s\n",
+			 tmp->dev ? tmp->dev->name : "NULL");
+		if (!tmp->dev) {
+			PR_ERR("tmp->dev is NULL\n");
+			goto EXIT;
+		}
+		ret = dp_swdev_chk_bport_in_br(tmp->dev, tmp->bp, inst);
+		if (ret == 0) {
+			get_vlan_via_dev(tmp->dev, &vlan_prop);
+			if (vlan_prop.num == 2) {
+				DP_DEBUG(DP_DBG_FLAG_SWDEV,
+					 "VLAN Inner proto=%x, vid=%d\n",
+					 vlan_prop.in_proto, vlan_prop.in_vid);
+				DP_DEBUG(DP_DBG_FLAG_SWDEV,
+					 "VLAN out proto=%x, vid=%d\n",
+					 vlan_prop.out_proto, vlan_prop.out_vid);
+				vlan->vlan2_list[v2].outer_vlan.vid = vlan_prop.out_vid;
+				vlan->vlan2_list[v2].outer_vlan.tpid =
+								vlan_prop.out_proto;
+				vlan->vlan2_list[v2].ether_type = 0;
+				vlan->vlan2_list[v2].inner_vlan.vid = vlan_prop.in_vid;
+				vlan->vlan2_list[v2].inner_vlan.tpid =
+								vlan_prop.in_proto;
+				vlan->vlan2_list[v2].bp = tmp->bp;
+				v2 += 1;
+			} else if (vlan_prop.num == 1) {
+				DP_DEBUG(DP_DBG_FLAG_SWDEV,
+					 "outer VLAN proto=%x, vid=%d\n",
+					 vlan_prop.out_proto, vlan_prop.out_vid);
+				vlan->vlan1_list[v1].outer_vlan.vid = vlan_prop.out_vid;
+				vlan->vlan1_list[v1].outer_vlan.tpid =
+								vlan_prop.out_proto;
+				vlan->vlan1_list[v1].bp = tmp->bp;
+				v1 += 1;
+			}
+			i += 1;
+		}
+	}
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "vlan1=%d vlan2=%d total vlan int=%d\n",
+		 v1, v2, i);
+	vlan->n_vlan1 = v1;
+	vlan->n_vlan2 = v2;
+	vlan->bp = sif->bp;
+	vlan->logic_port = port->port_id;
+	vlan->subif_grp = sif->subif;/*subif value*/
+
+	if (sif->swdev_priv)
+		vlan->priv = sif->swdev_priv;
+	else
+		vlan->priv = NULL;
+	ret = set_gswip_ext_vlan_32(gsw_handle, vlan, flag);
+	if (ret == 0)
+		sif->swdev_priv = vlan->priv;
+	else
+		PR_ERR("set gswip ext vlan return error\n");
+
+EXIT:
+	kfree(vlan->vlan2_list);
+	kfree(vlan->vlan1_list);
+	kfree(vlan);
+	return 0; /*return -EIO from GSWIP but later cannot fail swdev*/
+}
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_switchdev.h b/drivers/net/datapath/dpm/gswip32/datapath_switchdev.h
new file mode 100644
index 000000000000..e6b21b306504
--- /dev/null
+++ b/drivers/net/datapath/dpm/gswip32/datapath_switchdev.h
@@ -0,0 +1,20 @@
+/*
+ * Copyright (C) Intel Corporation
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#ifndef DATAPATH_SWITCHDEV_H_
+#define DATAPATH_SWITCHDEV_H_
+
+int dp_swdev_alloc_bridge_id_32(int inst);
+int dp_swdev_bridge_port_cfg_set_32(struct br_info *br_item,
+				 int inst, int bport);
+int dp_swdev_bridge_port_cfg_reset_32(struct br_info *br_item,
+				   int inst, int bport);
+int dp_swdev_bridge_cfg_set_32(int inst, u16 fid);
+int dp_swdev_free_brcfg_32(int inst, u16 fid);
+int dp_gswip_ext_vlan_32(int inst, int vap, int ep);
+#endif
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_tc_asym_vlan.c b/drivers/net/datapath/dpm/gswip32/datapath_tc_asym_vlan.c
new file mode 100644
index 000000000000..832d47acb371
--- /dev/null
+++ b/drivers/net/datapath/dpm/gswip32/datapath_tc_asym_vlan.c
@@ -0,0 +1,880 @@
+/*
+ * Copyright (C) Intel Corporation
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include <net/switch_api/gsw_flow_ops.h>
+#include <net/datapath_api.h>
+#include <net/datapath_api_vlan.h>
+#include "../datapath.h"
+/***************************
+ *	Code for TC VLAN
+ ***************************/
+
+static int update_bp(struct core_ops *ops,
+		     u32 bpid,
+		     int ingress,
+		     GSW_VLANFILTER_alloc_t *pfilter,
+		     GSW_EXTENDEDVLAN_alloc_t *pextvlan)
+{
+	int ret;
+	GSW_BRIDGE_portConfig_t bpcfg1, bpcfg2;
+
+	memset(&bpcfg1, 0, sizeof(bpcfg1));
+	memset(&bpcfg2, 0, sizeof(bpcfg2));
+	bpcfg1.nBridgePortId = bpid;
+	bpcfg2.nBridgePortId = bpid;
+	if (ingress) {
+		bpcfg1.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_INGRESS_VLAN_FILTER
+				| GSW_BRIDGE_PORT_CONFIG_MASK_INGRESS_VLAN;
+		if (!pfilter) {
+			bpcfg2.bIngressVlanFilterEnable = LTQ_FALSE;
+		} else {
+			bpcfg2.bIngressVlanFilterEnable = LTQ_TRUE;
+			bpcfg2.nIngressVlanFilterBlockId =
+				pfilter->nVlanFilterBlockId;
+			bpcfg2.nIngressVlanFilterBlockSize = 0;
+		}
+		if (!pextvlan) {
+			bpcfg2.bIngressExtendedVlanEnable = LTQ_FALSE;
+		} else {
+			bpcfg2.bIngressExtendedVlanEnable = LTQ_TRUE;
+			bpcfg2.nIngressExtendedVlanBlockId =
+				pextvlan->nExtendedVlanBlockId;
+			bpcfg2.nIngressExtendedVlanBlockSize = 0;
+		}
+	} else {
+		bpcfg1.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_VLAN_FILTER1
+				| GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_VLAN;
+		if (!pfilter) {
+			bpcfg2.bEgressVlanFilter1Enable = LTQ_FALSE;
+		} else {
+			bpcfg2.bEgressVlanFilter1Enable = LTQ_TRUE;
+			bpcfg2.nEgressVlanFilter1BlockId =
+				pfilter->nVlanFilterBlockId;
+			bpcfg2.nEgressVlanFilter1BlockSize = 0;
+		}
+		if (!pextvlan) {
+			bpcfg2.bEgressExtendedVlanEnable = LTQ_FALSE;
+		} else {
+			bpcfg2.bEgressExtendedVlanEnable = LTQ_TRUE;
+			bpcfg2.nEgressExtendedVlanBlockId =
+				pextvlan->nExtendedVlanBlockId;
+			bpcfg2.nEgressExtendedVlanBlockSize = 0;
+		}
+	}
+	bpcfg2.eMask = bpcfg1.eMask;
+
+	ret = ops->gsw_brdgport_ops.BridgePort_ConfigGet(ops, &bpcfg1);
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	ret = ops->gsw_brdgport_ops.BridgePort_ConfigSet(ops, &bpcfg2);
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	if (ingress) {
+		if (bpcfg1.bIngressVlanFilterEnable != LTQ_FALSE) {
+			GSW_VLANFILTER_alloc_t alloc = {0};
+
+			alloc.nVlanFilterBlockId =
+				bpcfg1.nIngressVlanFilterBlockId;
+			ops->gsw_vlanfilter_ops.VlanFilter_Free(ops, &alloc);
+		}
+		if (bpcfg1.bIngressExtendedVlanEnable != LTQ_FALSE) {
+			GSW_EXTENDEDVLAN_alloc_t alloc = {0};
+
+			alloc.nExtendedVlanBlockId =
+				bpcfg1.nIngressExtendedVlanBlockId;
+			ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+		}
+	} else {
+		if (bpcfg1.bEgressVlanFilter1Enable != LTQ_FALSE) {
+			GSW_VLANFILTER_alloc_t alloc = {0};
+
+			alloc.nVlanFilterBlockId =
+				bpcfg1.nEgressVlanFilter1BlockId;
+			ops->gsw_vlanfilter_ops.VlanFilter_Free(ops, &alloc);
+		}
+		if (bpcfg1.bEgressExtendedVlanEnable != LTQ_FALSE) {
+			GSW_EXTENDEDVLAN_alloc_t alloc = {0};
+
+			alloc.nExtendedVlanBlockId =
+				bpcfg1.nEgressExtendedVlanBlockId;
+			ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+		}
+	}
+
+	return 0;
+}
+
+static int update_ctp(struct core_ops *ops,
+		      u32 lpid,
+		      u32 subifidg,
+		      int ingress,
+		      int multicast,
+		      GSW_EXTENDEDVLAN_alloc_t *pextvlan)
+{
+	int ret;
+	GSW_CTP_portConfig_t ctpcfg1 = {0}, ctpcfg2 = {0};
+
+	ctpcfg1.nLogicalPortId = lpid;
+	ctpcfg2.nLogicalPortId = lpid;
+	ctpcfg1.nSubIfIdGroup = subifidg;
+	ctpcfg2.nSubIfIdGroup = subifidg;
+	if (ingress) {
+		if (multicast) {
+			ctpcfg1.eMask = GSW_CTP_PORT_CONFIG_MASK_INGRESS_VLAN_IGMP;
+			if (!pextvlan) {
+				ctpcfg2.bIngressExtendedVlanIgmpEnable = LTQ_FALSE;
+			} else {
+				ctpcfg2.bIngressExtendedVlanIgmpEnable = LTQ_TRUE;
+				ctpcfg2.nIngressExtendedVlanBlockIdIgmp =
+					pextvlan->nExtendedVlanBlockId;
+				ctpcfg2.nIngressExtendedVlanBlockSizeIgmp = 0;
+			}
+		} else {
+			ctpcfg1.eMask = GSW_CTP_PORT_CONFIG_MASK_INGRESS_VLAN;
+			if (!pextvlan) {
+				ctpcfg2.bIngressExtendedVlanEnable = LTQ_FALSE;
+			} else {
+				ctpcfg2.bIngressExtendedVlanEnable = LTQ_TRUE;
+				ctpcfg2.nIngressExtendedVlanBlockId =
+					pextvlan->nExtendedVlanBlockId;
+				ctpcfg2.nIngressExtendedVlanBlockSize = 0;
+			}
+		}
+	} else {
+		if (multicast) {
+			ctpcfg1.eMask = GSW_CTP_PORT_CONFIG_MASK_EGRESS_VLAN_IGMP;
+			if (!pextvlan) {
+				ctpcfg2.bEgressExtendedVlanIgmpEnable = LTQ_FALSE;
+			} else {
+				ctpcfg2.bEgressExtendedVlanIgmpEnable = LTQ_TRUE;
+				ctpcfg2.nEgressExtendedVlanBlockIdIgmp =
+					pextvlan->nExtendedVlanBlockId;
+				ctpcfg2.nEgressExtendedVlanBlockSizeIgmp = 0;
+			}
+		} else {
+			ctpcfg1.eMask = GSW_CTP_PORT_CONFIG_MASK_EGRESS_VLAN;
+			if (!pextvlan) {
+				ctpcfg2.bEgressExtendedVlanEnable = LTQ_FALSE;
+			} else {
+				ctpcfg2.bEgressExtendedVlanEnable = LTQ_TRUE;
+				ctpcfg2.nEgressExtendedVlanBlockId =
+					pextvlan->nExtendedVlanBlockId;
+				ctpcfg2.nEgressExtendedVlanBlockSize = 0;
+			}
+		}
+	}
+	ctpcfg2.eMask = ctpcfg1.eMask;
+
+	ret = ops->gsw_ctp_ops.CTP_PortConfigGet(ops, &ctpcfg1);
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	ret = ops->gsw_ctp_ops.CTP_PortConfigSet(ops, &ctpcfg2);
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	if (ingress) {
+		if (multicast) {
+			if (ctpcfg1.bIngressExtendedVlanIgmpEnable != LTQ_FALSE) {
+				GSW_EXTENDEDVLAN_alloc_t alloc = {0};
+
+				alloc.nExtendedVlanBlockId =
+					ctpcfg1.nIngressExtendedVlanBlockIdIgmp;
+				ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+			}
+		} else {
+			if (ctpcfg1.bIngressExtendedVlanEnable != LTQ_FALSE) {
+				GSW_EXTENDEDVLAN_alloc_t alloc = {0};
+
+				alloc.nExtendedVlanBlockId =
+					ctpcfg1.nIngressExtendedVlanBlockId;
+				ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+			}
+		}
+	} else {
+		if (multicast) {
+			if (ctpcfg1.bEgressExtendedVlanIgmpEnable != LTQ_FALSE) {
+				GSW_EXTENDEDVLAN_alloc_t alloc = {0};
+
+				alloc.nExtendedVlanBlockId =
+					ctpcfg1.nEgressExtendedVlanBlockIdIgmp;
+				ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+			}
+		} else {
+			if (ctpcfg1.bEgressExtendedVlanEnable != LTQ_FALSE) {
+				GSW_EXTENDEDVLAN_alloc_t alloc = {0};
+
+				alloc.nExtendedVlanBlockId =
+					ctpcfg1.nEgressExtendedVlanBlockId;
+				ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+			}
+		}
+	}
+
+	return 0;
+}
+
+static int vlan_filter_mode(struct dp_pattern_vlan *pout)
+{
+	int k;
+
+	k = 0;
+	if (pout->tpid != DP_VLAN_PATTERN_NOT_CARE)
+		k |= 1 << 4;
+	if (pout->proto != DP_VLAN_PATTERN_NOT_CARE)
+		k |= 1 << 3;
+	if (pout->dei != DP_VLAN_PATTERN_NOT_CARE)
+		k |= 1 << 2;
+	if (pout->prio != DP_VLAN_PATTERN_NOT_CARE)
+		k |= 1 << 1;
+	if (pout->vid != DP_VLAN_PATTERN_NOT_CARE)
+		k |= 1;
+
+	return k;
+}
+
+static int tc_vlan_filter(struct core_ops *ops,
+			  struct dp_tc_vlan *vlan,
+			  struct dp_tc_vlan_info *info)
+{
+	/* default return 1 to indicate Extended VLAN is required */
+	int ret = 1;
+	int total = vlan->n_vlan0 + vlan->n_vlan1 + vlan->n_vlan2;
+	int untagged = -1;
+	int tagged = -1;
+	int mode = -1;
+	GSW_VLANFILTER_alloc_t alloc;
+	GSW_VLANFILTER_config_t *pcfg;
+	int i, j, k;
+
+	if (total <= 0) {
+		/* Update bridge port */
+		ret = update_bp(ops,
+				(u32)info->bp,
+				vlan->dir == DP_DIR_INGRESS,
+				NULL,
+				NULL);
+		return ret;
+	}
+
+	/* bridge port */
+	pcfg = kmalloc_array(total, sizeof(*pcfg), GFP_KERNEL);
+	if (!pcfg)
+		return -ENOMEM;
+	memset(pcfg, 0, sizeof(*pcfg) * total);
+
+	j = 0;
+
+	/* untagged rule */
+	for (i = 0; i < vlan->n_vlan0; i++) {
+		if (!vlan->vlan0_list[i].def)
+		/* VLAN filter for untagged packet have default rule only */
+			goto EXIT;
+		if ((vlan->vlan0_list[i].act.act & DP_VLAN_ACT_FWD)) {
+		/* default value was set to drop */
+			if (untagged > 0)
+				goto EXIT;
+		/* default value is forward */
+			untagged = 0;
+		} else if ((vlan->vlan0_list[i].act.act & DP_VLAN_ACT_DROP)) {
+		/* default value was set to forward */
+			if (untagged == 0)
+				goto EXIT;
+		/* default value is drop */
+			untagged = 1;
+		} else
+		/* packet editing required */
+			goto EXIT;
+		continue;
+	}
+
+	/* 1-tag rule */
+	for (i = 0; i < vlan->n_vlan1; i++) {
+		if (vlan->vlan1_list[i].def) {
+			if ((vlan->vlan1_list[i].act.act & DP_VLAN_ACT_FWD)) {
+			/* default value was set to drop */
+				if (tagged > 0)
+					goto EXIT;
+				tagged = 0;
+			} else if ((vlan->vlan1_list[i].act.act &
+				    DP_VLAN_ACT_DROP)) {
+			/* default value was set to forward */
+				if (tagged == 0)
+					goto EXIT;
+				tagged = 1;
+			} else
+			/* packet editing required */
+				goto EXIT;
+			continue;
+		}
+		/* Action other than FWD/DROP is not accepted */
+		if ((vlan->vlan1_list[i].act.act & DP_VLAN_ACT_FWD))
+			pcfg[j].bDiscardMatched = LTQ_FALSE;
+		else if ((vlan->vlan1_list[i].act.act & DP_VLAN_ACT_DROP))
+			pcfg[j].bDiscardMatched = LTQ_TRUE;
+		else
+			goto EXIT;
+		/* VLAN filter only support VID, PCP, or TCI only */
+		k = vlan_filter_mode(&vlan->vlan1_list[i].outer);
+		switch (k) {
+		case 1:
+			pcfg[j].eVlanFilterMask = GSW_VLAN_FILTER_TCI_MASK_VID;
+			pcfg[j].nVal = (u32)vlan->vlan1_list[i].outer.vid;
+			break;
+		case 2:
+			pcfg[j].eVlanFilterMask = GSW_VLAN_FILTER_TCI_MASK_PCP;
+			pcfg[j].nVal = (u32)vlan->vlan1_list[i].outer.prio;
+			break;
+		case 7:
+			pcfg[j].eVlanFilterMask = GSW_VLAN_FILTER_TCI_MASK_TCI;
+			pcfg[j].nVal =
+				(u32)vlan->vlan1_list[i].outer.vid & 0xFFF;
+			pcfg[j].nVal |=
+				((u32)vlan->vlan1_list[i].outer.prio & 7) << 13;
+			pcfg[j].nVal |=
+				((u32)vlan->vlan1_list[i].outer.dei & 1) << 12;
+			break;
+		default:
+			goto EXIT;
+		}
+		if (mode < 0)
+			mode = k;
+		else if (mode != k)
+			goto EXIT;
+		j++;
+	}
+
+	/* 2-tag rule */
+	for (i = 0; i < vlan->n_vlan2; i++) {
+		if (vlan->vlan2_list[i].def) {
+			if ((vlan->vlan2_list[i].act.act & DP_VLAN_ACT_FWD)) {
+				if (tagged > 0)
+					goto EXIT;
+				tagged = 0;
+			} else if ((vlan->vlan2_list[i].act.act &
+				    DP_VLAN_ACT_DROP)) {
+				if (tagged == 0)
+					goto EXIT;
+				tagged = 1;
+			} else {
+				goto EXIT;
+			}
+			continue;
+		}
+		/* Action other than FWD/DROP is not accepted */
+		if ((vlan->vlan2_list[i].act.act & DP_VLAN_ACT_FWD))
+			pcfg[j].bDiscardMatched = LTQ_FALSE;
+		else if ((vlan->vlan2_list[i].act.act & DP_VLAN_ACT_DROP))
+			pcfg[j].bDiscardMatched = LTQ_TRUE;
+		else
+			goto EXIT;
+		/* VLAN filter only support VID, PCP, or TCI only */
+		k = vlan_filter_mode(&vlan->vlan2_list[i].outer);
+		switch (k) {
+		case 1:
+			pcfg[j].eVlanFilterMask = GSW_VLAN_FILTER_TCI_MASK_VID;
+			pcfg[j].nVal = (u32)vlan->vlan2_list[i].outer.vid;
+			break;
+		case 2:
+			pcfg[j].eVlanFilterMask = GSW_VLAN_FILTER_TCI_MASK_PCP;
+			pcfg[j].nVal = (u32)vlan->vlan2_list[i].outer.prio;
+			break;
+		case 7:
+			pcfg[j].eVlanFilterMask = GSW_VLAN_FILTER_TCI_MASK_TCI;
+			pcfg[j].nVal = (u32)vlan->vlan2_list[i].outer.vid &
+				       0xFFF;
+			pcfg[j].nVal |= ((u32)
+					 vlan->vlan2_list[i].outer.prio & 7)
+					 << 13;
+			pcfg[j].nVal |= ((u32)
+					 vlan->vlan2_list[i].outer.dei & 1)
+					 << 12;
+			break;
+		default:
+			goto EXIT;
+		}
+		if (mode < 0)
+			mode = k;
+		else if (mode != k)
+			goto EXIT;
+		/* Inner VLAN should be "don't care" for all fields */
+		k = vlan_filter_mode(&vlan->vlan2_list[i].inner);
+		if (k != 0)
+			goto EXIT;
+		j++;
+	}
+
+	/* Check if we have anything to configure */
+	if (j <= 0) {
+		/* Update bridge port */
+		ret = update_bp(ops,
+				(u32)info->bp,
+				vlan->dir == DP_DIR_INGRESS,
+				NULL,
+				NULL);
+		goto EXIT;
+	}
+
+	if (untagged < 0)
+		untagged = 0;
+	if (tagged < 0)
+		tagged = 0;
+
+	/* Allocate VLAN filter */
+	memset(&alloc, 0, sizeof(alloc));
+	alloc.nNumberOfEntries = (u32)j;
+	alloc.bDiscardUntagged = untagged ? LTQ_TRUE : LTQ_FALSE;
+	alloc.bDiscardUnmatchedTagged = tagged ? LTQ_TRUE : LTQ_FALSE;
+	ret = ops->gsw_vlanfilter_ops.VlanFilter_Alloc(ops, &alloc);
+	if (ret != GSW_statusOk) {
+		ret = -EIO;
+		goto EXIT;
+	}
+	DP_DEBUG(DP_DBG_FLAG_PAE, "VlanFilter_Alloc - %u[%u]\n",
+		 alloc.nVlanFilterBlockId, alloc.nNumberOfEntries);
+
+	/* Configure each VLAN filter */
+	for (i = 0; i < j; i++) {
+		pcfg[i].nVlanFilterBlockId = alloc.nVlanFilterBlockId;
+		pcfg[i].nEntryIndex = (u32)i;
+		ret = ops->gsw_vlanfilter_ops.VlanFilter_Set(ops, &pcfg[i]);
+		if (ret != GSW_statusOk) {
+			ops->gsw_vlanfilter_ops.VlanFilter_Free(ops, &alloc);
+			ret = -EIO;
+			goto EXIT;
+		}
+	}
+
+	/* Update bridge port */
+	ret = update_bp(ops,
+			(u32)info->bp,
+			vlan->dir == DP_DIR_INGRESS,
+			&alloc,
+			NULL);
+	if (ret != 0)
+		ops->gsw_vlanfilter_ops.VlanFilter_Free(ops, &alloc);
+
+EXIT:
+	kfree(pcfg);
+	return ret;
+}
+
+static int write_vtetype(struct core_ops *ops, u16 val)
+{
+	int ret;
+	GSW_register_t reg = {0};
+
+	reg.nRegAddr = 0xA42;
+	reg.nData = val;
+	ret = ops->gsw_common_ops.RegisterSet(ops, &reg);
+	if (ret == GSW_statusOk)
+		return 0;
+	else
+		return -EIO;
+}
+
+static int ext_vlan_filter_cfg(struct core_ops *ops,
+			       GSW_EXTENDEDVLAN_filterVLAN_t *pcfg,
+			       struct dp_pattern_vlan *pattern)
+{
+	int ret = 0;
+
+	if (pattern->prio == DP_VLAN_PATTERN_NOT_CARE) {
+		pcfg->bPriorityEnable = LTQ_FALSE;
+	} else {
+		pcfg->bPriorityEnable = LTQ_TRUE;
+		pcfg->nPriorityVal = pattern->prio;
+	}
+	if (pattern->vid == DP_VLAN_PATTERN_NOT_CARE) {
+		pcfg->bVidEnable = LTQ_FALSE;
+	} else {
+		pcfg->bVidEnable = LTQ_TRUE;
+		pcfg->nVidVal = pattern->vid;
+	}
+	if (pattern->tpid == DP_VLAN_PATTERN_NOT_CARE) {
+		pcfg->eTpid = GSW_EXTENDEDVLAN_FILTER_TPID_NO_FILTER;
+	} else if (pattern->tpid == ETH_P_8021Q) {
+		pcfg->eTpid = GSW_EXTENDEDVLAN_FILTER_TPID_8021Q;
+	} else {
+		pcfg->eTpid = GSW_EXTENDEDVLAN_FILTER_TPID_VTETYPE;
+		ret = write_vtetype(ops, (u16)pattern->tpid);
+	}
+	if (pattern->dei == DP_VLAN_PATTERN_NOT_CARE)
+		pcfg->eDei = GSW_EXTENDEDVLAN_FILTER_DEI_NO_FILTER;
+	else if (pattern->dei == 0)
+		pcfg->eDei = GSW_EXTENDEDVLAN_FILTER_DEI_0;
+	else
+		pcfg->eDei = GSW_EXTENDEDVLAN_FILTER_DEI_1;
+
+	return ret;
+}
+
+static int ext_vlan_insert(struct core_ops *ops,
+			   GSW_EXTENDEDVLAN_treatmentVlan_t *pcfg,
+			   struct dp_act_vlan *act,
+			   unsigned int idx)
+{
+	int ret;
+
+	DP_DEBUG(DP_DBG_FLAG_PAE, "act->tpid[%u]: 0x%04x\n",
+		 idx, act->tpid[idx]);
+	switch (act->tpid[idx]) {
+	case CP_FROM_INNER:
+		pcfg->eTpid = GSW_EXTENDEDVLAN_TREATMENT_INNER_TPID;
+		break;
+	case CP_FROM_OUTER:
+		pcfg->eTpid = GSW_EXTENDEDVLAN_TREATMENT_OUTER_TPID;
+		break;
+	case ETH_P_8021Q:
+		pcfg->eTpid = GSW_EXTENDEDVLAN_TREATMENT_8021Q;
+		break;
+	default:
+		pcfg->eTpid = GSW_EXTENDEDVLAN_TREATMENT_VTETYPE;
+		ret = write_vtetype(ops, (u16)act->tpid[idx]);
+		if (ret)
+			return ret;
+	}
+
+	switch (act->vid[idx]) {
+	case CP_FROM_INNER:
+		pcfg->eVidMode = GSW_EXTENDEDVLAN_TREATMENT_INNER_VID;
+		break;
+	case CP_FROM_OUTER:
+		pcfg->eVidMode = GSW_EXTENDEDVLAN_TREATMENT_OUTER_VID;
+		break;
+	default:
+		pcfg->eVidMode = GSW_EXTENDEDVLAN_TREATMENT_VID_VAL;
+		pcfg->eVidVal = (u32)act->vid[idx];
+	}
+
+	switch (act->dei[idx]) {
+	case CP_FROM_INNER:
+		pcfg->eDei = GSW_EXTENDEDVLAN_TREATMENT_INNER_DEI;
+		break;
+	case CP_FROM_OUTER:
+		pcfg->eDei = GSW_EXTENDEDVLAN_TREATMENT_OUTER_DEI;
+		break;
+	case 0:
+		pcfg->eDei = GSW_EXTENDEDVLAN_TREATMENT_DEI_0;
+		break;
+	case 1:
+		pcfg->eDei = GSW_EXTENDEDVLAN_TREATMENT_DEI_1;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	switch (act->prio[idx]) {
+	case CP_FROM_INNER:
+		pcfg->ePriorityMode = GSW_EXTENDEDVLAN_TREATMENT_INNER_PRORITY;
+		break;
+	case CP_FROM_OUTER:
+		pcfg->ePriorityMode = GSW_EXTENDEDVLAN_TREATMENT_OUTER_PRORITY;
+		break;
+	case DERIVE_FROM_DSCP:
+		pcfg->ePriorityMode = GSW_EXTENDEDVLAN_TREATMENT_DSCP;
+		break;
+	default:
+		pcfg->ePriorityMode = GSW_EXTENDEDVLAN_TREATMENT_PRIORITY_VAL;
+		pcfg->ePriorityVal = (u32)act->prio[idx];
+		break;
+	}
+	return 0;
+}
+
+static int ext_vlan_action_cfg(struct core_ops *ops,
+			       GSW_EXTENDEDVLAN_treatment_t *pcfg,
+			       struct dp_act_vlan *act)
+{
+	int ret;
+
+	DP_DEBUG(DP_DBG_FLAG_PAE, "act->act: 0x%02x\n", (unsigned int)act->act);
+
+	/* Copy DSCP table */
+	memcpy(pcfg->nDscp2PcpMap, act->dscp_pcp_map,
+	       sizeof(pcfg->nDscp2PcpMap));
+	/* forward without modification */
+	if ((act->act & DP_VLAN_ACT_FWD))
+		return 0;
+
+	/* drop the packet */
+	if ((act->act & DP_VLAN_ACT_DROP)) {
+		pcfg->eRemoveTag = GSW_EXTENDEDVLAN_TREATMENT_DISCARD_UPSTREAM;
+		return 0;
+	}
+
+	/* remove tag */
+	if ((act->act & DP_VLAN_ACT_POP)) {
+		DP_DEBUG(DP_DBG_FLAG_PAE, "act->pop_n: %d\n", act->pop_n);
+		switch (act->pop_n) {
+		case 1:
+			pcfg->eRemoveTag =
+				GSW_EXTENDEDVLAN_TREATMENT_REMOVE_1_TAG;
+			break;
+		case 2:
+			pcfg->eRemoveTag =
+				GSW_EXTENDEDVLAN_TREATMENT_REMOVE_2_TAG;
+			break;
+		default:
+			return -EINVAL;
+		}
+	}
+
+	if (!(act->act & DP_VLAN_ACT_PUSH))
+		return 0;
+
+	switch (act->push_n) {
+	case 2:
+		pcfg->bAddInnerVlan = LTQ_TRUE;
+		ret = ext_vlan_insert(ops, &pcfg->sInnerVlan, act, 1);
+		if (ret)
+			return ret;
+	case 1:
+		pcfg->bAddOuterVlan = LTQ_TRUE;
+		return ext_vlan_insert(ops, &pcfg->sOuterVlan, act, 0);
+	default:
+		return -EINVAL;
+	}
+}
+
+static int ext_vlan_cfg(struct core_ops *ops,
+			GSW_EXTENDEDVLAN_config_t *pcfg,
+			int def,
+			int ethertype,
+			struct dp_pattern_vlan *outer,
+			struct dp_pattern_vlan *inner,
+			struct dp_act_vlan *act)
+{
+	const int ethertype_map[] = {
+		DP_VLAN_PATTERN_NOT_CARE,
+		GSW_EXTENDEDVLAN_FILTER_ETHERTYPE_NO_FILTER,
+		DP_PROTO_IP4,
+		GSW_EXTENDEDVLAN_FILTER_ETHERTYPE_IPOE,
+		DP_PROTO_PPPOE,
+		GSW_EXTENDEDVLAN_FILTER_ETHERTYPE_PPPOE,
+		DP_PROTO_ARP,
+		GSW_EXTENDEDVLAN_FILTER_ETHERTYPE_ARP,
+		DP_PROTO_IP6,
+		GSW_EXTENDEDVLAN_FILTER_ETHERTYPE_IPV6IPOE,
+		DP_PROTO_EAPOL,
+		GSW_EXTENDEDVLAN_FILTER_ETHERTYPE_EAPOL,
+	};
+	int ret;
+	unsigned int i;
+
+	for (i = 0; i < ARRAY_SIZE(ethertype_map); i += 2) {
+		if (ethertype == ethertype_map[i])
+			break;
+	}
+	if (i >= ARRAY_SIZE(ethertype_map))
+		return -EINVAL;
+	pcfg->sFilter.eEtherType = (u32)ethertype_map[i + 1];
+
+	switch ((inner ? 2 : 0) | (outer ? 1 : 0)) {
+	case 0:
+		DP_DEBUG(DP_DBG_FLAG_PAE, "Untagged Rule\n");
+		/* untagged rule */
+		if (def)	/* default untagged rule is not supported */
+			return -EINVAL;
+		pcfg->sFilter.sInnerVlan.eType =
+			GSW_EXTENDEDVLAN_FILTER_TYPE_NO_TAG;
+		pcfg->sFilter.sOuterVlan.eType =
+			GSW_EXTENDEDVLAN_FILTER_TYPE_NO_TAG;
+		break;
+	case 1:
+		DP_DEBUG(DP_DBG_FLAG_PAE, "1-tag Rule\n");
+		/* 1-tag rule */
+		pcfg->sFilter.sInnerVlan.eType =
+			GSW_EXTENDEDVLAN_FILTER_TYPE_NO_TAG;
+		if (def) {
+			/* default 1-tag rule */
+			pcfg->sFilter.sOuterVlan.eType =
+				GSW_EXTENDEDVLAN_FILTER_TYPE_DEFAULT;
+			break;
+		}
+		pcfg->sFilter.sOuterVlan.eType =
+			GSW_EXTENDEDVLAN_FILTER_TYPE_NORMAL;
+		ret = ext_vlan_filter_cfg(ops,
+					  &pcfg->sFilter.sOuterVlan,
+					  outer);
+		if (ret)
+			return ret;
+		break;
+	case 3:
+		DP_DEBUG(DP_DBG_FLAG_PAE, "2-tag Rule\n");
+		/* 2-tag rule */
+		if (def) {
+			/* default 2-tag rule */
+			pcfg->sFilter.sOuterVlan.eType =
+				GSW_EXTENDEDVLAN_FILTER_TYPE_NO_FILTER;
+			pcfg->sFilter.sInnerVlan.eType =
+				GSW_EXTENDEDVLAN_FILTER_TYPE_DEFAULT;
+			break;
+		}
+		pcfg->sFilter.sOuterVlan.eType =
+			GSW_EXTENDEDVLAN_FILTER_TYPE_NORMAL;
+		pcfg->sFilter.sInnerVlan.eType =
+			GSW_EXTENDEDVLAN_FILTER_TYPE_NORMAL;
+		ret = ext_vlan_filter_cfg(ops,
+					  &pcfg->sFilter.sOuterVlan,
+					  outer);
+		if (ret)
+			return ret;
+		ret = ext_vlan_filter_cfg(ops,
+					  &pcfg->sFilter.sInnerVlan,
+					  inner);
+		if (ret)
+			return ret;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	ret = ext_vlan_action_cfg(ops, &pcfg->sTreatment, act);
+	if (ret)
+		return ret;
+
+	return ops->gsw_extvlan_ops.ExtendedVlan_Set(ops, pcfg);
+}
+
+static int tc_ext_vlan(struct core_ops *ops,
+		       struct dp_tc_vlan *vlan,
+		       struct dp_tc_vlan_info *info)
+{
+	int ret;
+	int total = vlan->n_vlan0 + vlan->n_vlan1 + vlan->n_vlan2;
+	GSW_EXTENDEDVLAN_alloc_t alloc = {0};
+	GSW_EXTENDEDVLAN_config_t cfg;
+	int i, j;
+
+	if (total > 0) {
+		alloc.nNumberOfEntries = (u32)total;
+		ret = ops->gsw_extvlan_ops.ExtendedVlan_Alloc(ops, &alloc);
+		if (ret != GSW_statusOk)
+			return -EIO;
+		DP_DEBUG(DP_DBG_FLAG_PAE, "ExtendedVlan_Alloc - %u[%u]\n",
+			 alloc.nExtendedVlanBlockId,
+			 alloc.nNumberOfEntries);
+
+		j = 0;
+
+		/* untagged rule */
+		for (i = 0; i < vlan->n_vlan0; i++) {
+			memset(&cfg, 0, sizeof(cfg));
+			cfg.nExtendedVlanBlockId = alloc.nExtendedVlanBlockId;
+			cfg.nEntryIndex = j;
+			j++;
+			ret = ext_vlan_cfg(ops,
+					   &cfg,
+					   vlan->vlan0_list[i].def,
+					   vlan->vlan0_list[i].outer.proto,
+					   NULL,
+					   NULL,
+					   &vlan->vlan0_list[i].act);
+			if (ret != 0)
+				goto ERROR;
+		}
+
+		/* 1-tag rule */
+		for (i = 0; i < vlan->n_vlan1; i++) {
+			memset(&cfg, 0, sizeof(cfg));
+			cfg.nExtendedVlanBlockId = alloc.nExtendedVlanBlockId;
+			cfg.nEntryIndex = j;
+			j++;
+			ret = ext_vlan_cfg(ops,
+					   &cfg,
+					   vlan->vlan1_list[i].def,
+					   vlan->vlan1_list[i].outer.proto,
+					   &vlan->vlan1_list[i].outer,
+					   NULL,
+					   &vlan->vlan1_list[i].act);
+			if (ret != 0)
+				goto ERROR;
+		}
+
+		/* 2-tag rule */
+		for (i = 0; i < vlan->n_vlan2; i++) {
+			memset(&cfg, 0, sizeof(cfg));
+			cfg.nExtendedVlanBlockId = alloc.nExtendedVlanBlockId;
+			cfg.nEntryIndex = j;
+			j++;
+			ret = ext_vlan_cfg(ops,
+					   &cfg,
+					   vlan->vlan2_list[i].def,
+					   vlan->vlan2_list[i].outer.proto,
+					   &vlan->vlan2_list[i].outer,
+					   &vlan->vlan2_list[i].inner,
+					   &vlan->vlan2_list[i].act);
+			if (ret != 0)
+				goto ERROR;
+		}
+	}
+
+	if ((info->dev_type & 0x01) == 0) {
+		/* Configure CTP */
+		ret = update_ctp(ops,
+				 (u32)info->dp_port,
+				 (u32)info->subix,
+				 vlan->dir == DP_DIR_INGRESS,
+				 (info->dev_type & 0x02) != 0,
+				 total > 0 ? &alloc : NULL);
+	} else {
+		/* Configure bridge port */
+		ret = update_bp(ops,
+				(u32)info->bp,
+				vlan->dir == DP_DIR_INGRESS,
+				NULL,
+				total > 0 ? &alloc : NULL);
+	}
+	if (ret == 0)
+		return 0;
+
+ERROR:
+	ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+	return ret;
+}
+
+/*tc_vlan_set_32:
+ *param[in] vlan: directly from DP core API's parameter
+ *param[in] info: traslated via vlan->dev.
+ *       if it is BP/pmapper dev, info->dev_type=DP_DEV_TYPE_BP_PMAPPER {
+ *                                info->bp is set
+ *            if vlan->def_apply == DP_VLAN_APPLY_CTP,
+ *                                info->subix is set also
+ *       } elif it is CTP/GEM dev, info->dev_type=DP_DEV_TYPE_CTP_GEM {
+ *                                info->subix is set
+ *       }
+ *flag: reserved for future
+ */
+int tc_vlan_set_32(struct core_ops *ops,
+		   struct dp_tc_vlan *vlan,
+		   struct dp_tc_vlan_info *info,
+		   int flag)
+{
+	/* If it's bridge port, try to configure VLAN filter. */
+	if ((info->dev_type & 0x01) != 0) {
+		int ret;
+
+		/* Multicast (IGMP Controlled) VLAN is not supported on Bridge Port */
+		if ((info->dev_type & 0x02) != 0)
+			return -EINVAL;
+
+		ret = tc_vlan_filter(ops, vlan, info);
+		/* Either managed to configure VLAN filter
+		 * or error happens in GSW API
+		 */
+		if (ret <= 0)
+			return ret;
+		/* VLAN filter can't cover the case and need extended VLAN */
+	}
+
+	/* Configure extended VLAN */
+	return tc_ext_vlan(ops, vlan, info);
+}
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_tx.c b/drivers/net/datapath/dpm/gswip32/datapath_tx.c
new file mode 100644
index 000000000000..33946aac0428
--- /dev/null
+++ b/drivers/net/datapath/dpm/gswip32/datapath_tx.c
@@ -0,0 +1,484 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/etherdevice.h>
+#include <net/datapath_api.h>
+#include "../datapath.h"
+#include "datapath_misc.h"
+#include "../datapath_instance.h"
+
+void dp_xmit_dbg(
+	char *title,
+	struct sk_buff *skb,
+	s32 ep,
+	s32 len,
+	u32 flags,
+	struct pmac_tx_hdr *pmac,
+	dp_subif_t *rx_subif,
+	int need_pmac,
+	int gso,
+	int checksum)
+{
+#if defined(DP_SKB_HACK)
+	DP_DEBUG(DP_DBG_FLAG_DUMP_TX,
+		 "%s: dp_xmit:skb->data/len=0x%px/%d data_ptr=%x from port=%d and subitf=%d\n",
+		 title,
+		 skb->data, len,
+		 ((struct dma_tx_desc_2 *)&skb->DW2)->field.data_ptr,
+		 ep, rx_subif->subif);
+#endif
+	if (dp_dbg_flag & DP_DBG_FLAG_DUMP_TX_DATA) {
+		if (pmac) {
+			dp_dump_raw_data((char *)pmac, PMAC_SIZE, "Tx Data");
+			dp_dump_raw_data(skb->data,
+					 skb->len,
+					 "Tx Data");
+		} else
+			dp_dump_raw_data(skb->data,
+					 skb->len,
+					 "Tx Data");
+	}
+
+	DP_DEBUG(DP_DBG_FLAG_DUMP_TX_SUM,
+		 "ip_summed=%s(%d) encapsulation=%s\n",
+		 dp_skb_csum_str(skb), skb->ip_summed,
+		 skb->encapsulation ? "Yes" : "No");
+
+	if (skb->encapsulation)
+		DP_DEBUG(DP_DBG_FLAG_DUMP_TX_SUM,
+			 "inner ip start=0x%lx(%d), transport=0x%lx(%d)\n",
+			 (unsigned long)skb_inner_network_header(skb),
+			 (int)(skb_inner_network_header(skb) -
+			       skb->data),
+			 (unsigned long)
+			 skb_inner_transport_header(skb),
+			 (int)(skb_inner_transport_header(skb) -
+			       skb_inner_network_header(skb)));
+	else
+		DP_DEBUG(DP_DBG_FLAG_DUMP_TX_SUM,
+			 "ip start=0x%lx(%d), transport=0x%lx(%d)\n",
+			 (unsigned long) skb_network_header(skb),
+			 (int)(skb_network_header(skb) - skb->data),
+			 (unsigned long)skb_transport_header(skb),
+			 (int)(skb_transport_header(skb) -
+			       skb_network_header(skb)));
+
+	if (dp_dbg_flag & DP_DBG_FLAG_DUMP_TX_DESCRIPTOR)
+#if defined(DP_SKB_HACK)
+		dp_port_prop[0].info.dump_tx_dma_desc(
+			(struct dma_tx_desc_0 *)&skb->DW0,
+			(struct dma_tx_desc_1 *)&skb->DW1,
+			(struct dma_tx_desc_2 *)&skb->DW2,
+			(struct dma_tx_desc_3 *)&skb->DW3);
+#else
+	;
+#endif
+
+	DP_DEBUG(DP_DBG_FLAG_DUMP_TX, "flags=0x%x skb->len=%d\n",
+		 flags, skb->len);
+	DP_DEBUG(DP_DBG_FLAG_DUMP_TX,
+		 "skb->data=0x%px with pmac hdr size=%zu\n", skb->data,
+		 sizeof(struct pmac_tx_hdr));
+
+	if (need_pmac) { /*insert one pmac header */
+		DP_DEBUG(DP_DBG_FLAG_DUMP_TX,
+			 "need pmac\n");
+
+		if (pmac && (dp_dbg_flag & DP_DBG_FLAG_DUMP_TX_DESCRIPTOR))
+			dp_port_prop[0].info.dump_tx_pmac(pmac);
+	} else {
+		DP_DEBUG(DP_DBG_FLAG_DUMP_TX, "no pmac\n");
+	}
+
+	if (gso)
+		DP_DEBUG(DP_DBG_FLAG_DUMP_TX, "GSO pkt\n");
+	else
+		DP_DEBUG(DP_DBG_FLAG_DUMP_TX, "Non-GSO pkt\n");
+
+	if (checksum)
+		DP_DEBUG(DP_DBG_FLAG_DUMP_TX, "Need checksum offload\n");
+	else
+		DP_DEBUG(DP_DBG_FLAG_DUMP_TX, "No need checksum offload pkt\n");
+
+	DP_DEBUG(DP_DBG_FLAG_DUMP_TX, "\n\n");
+}
+
+int32_t dp_xmit_32(struct net_device *rx_if, dp_subif_t *rx_subif,
+		   struct sk_buff *skb, int32_t len, uint32_t flags)
+{
+	struct dma_rx_desc_0 *desc_0;
+	struct dma_rx_desc_1 *desc_1;
+	struct dma_rx_desc_2 *desc_2;
+	struct dma_rx_desc_3 *desc_3;
+	struct pmac_port_info *dp_info;
+	struct pmac_tx_hdr pmac = {0};
+	u32 ip_offset, tcp_h_offset, tcp_type;
+	char tx_chksum_flag = 0; /*check csum cal can be supported or not */
+	char insert_pmac_f = 1;	/*flag to insert one pmac */
+	int res = DP_SUCCESS;
+	int ep, vap;
+	enum dp_xmit_errors err_ret = 0;
+	int inst = 0;
+	struct cbm_tx_data data;
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_PTP1588)
+	struct mac_ops *ops;
+	int rec_id = 0;
+#endif
+	struct dp_subif_info *sif;
+
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_EXTRA_DEBUG)
+	if (unlikely(!dp_init_ok)) {
+		err_ret = DP_XMIT_ERR_NOT_INIT;
+		goto lbl_err_ret;
+	}
+
+	if (unlikely(!rx_subif)) {
+		err_ret = DP_XMIT_ERR_NULL_SUBIF;
+		goto lbl_err_ret;
+	}
+
+	if (unlikely(!skb)) {
+		err_ret = DP_XMIT_ERR_NULL_SKB;
+		goto lbl_err_ret;
+	}
+
+#endif
+	ep = rx_subif->port_id;
+
+	if (unlikely(ep >= dp_port_prop[inst].info.cap.max_num_dp_ports)) {
+		err_ret = DP_XMIT_ERR_PORT_TOO_BIG;
+		goto lbl_err_ret;
+	}
+
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_EXTRA_DEBUG)
+	if (unlikely(in_irq())) {
+		err_ret = DP_XMIT_ERR_IN_IRQ;
+		goto lbl_err_ret;
+	}
+
+#endif
+	dp_info = get_dp_port_info(inst, ep);
+	vap = GET_VAP(rx_subif->subif, dp_info->vap_offset, dp_info->vap_mask);
+	sif = get_dp_port_subif(dp_info, vap);
+
+	if (unlikely(!rx_if && /*For atm pppoa case, rx_if is NULL now */
+		     !(dp_info->alloc_flags & DP_F_FAST_DSL))) {
+		err_ret = DP_XMIT_ERR_NULL_IF;
+		goto lbl_err_ret;
+	}
+
+	if (unlikely(dp_dbg_flag))
+		dp_xmit_dbg("\nOrig", skb, ep, len, flags,
+			    NULL, rx_subif, 0, 0, flags & DP_TX_CAL_CHKSUM);
+
+	/*No PMAC for WAVE500 and DSL by default except bonding case */
+	if (unlikely(NO_NEED_PMAC(dp_info->alloc_flags)))
+		insert_pmac_f = 0;
+
+	/**********************************************
+	 *Must put these 4 lines after INSERT_PMAC
+	 *since INSERT_PMAC will change skb if needed
+	 *********************************************/
+#if defined(DP_SKB_HACK)
+	desc_0 = (struct dma_tx_desc_0 *)&skb->DW0;
+	desc_1 = (struct dma_tx_desc_1 *)&skb->DW1;
+	desc_2 = (struct dma_tx_desc_2 *)&skb->DW2;
+	desc_3 = (struct dma_tx_desc_3 *)&skb->DW3;
+#endif
+	if (flags & DP_TX_CAL_CHKSUM) {
+		int ret_flg;
+
+		if (!dp_port_prop[inst].info.check_csum_cap()) {
+			err_ret = DP_XMIT_ERR_CSM_NO_SUPPORT;
+			goto lbl_err_ret;
+		}
+
+		ret_flg = get_offset_clear_chksum(skb, &ip_offset,
+						  &tcp_h_offset, &tcp_type);
+
+		if (likely(ret_flg == 0))
+			/*HW can support checksum offload*/
+			tx_chksum_flag = 1;
+
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_EXTRA_DEBUG)
+		else if (ret_flg == -1)
+			pr_info_once("packet can't do hw checksum\n");
+
+#endif
+	}
+
+        desc_3->all = (desc_3->all & dp_info->dma3_mask_template[TEMPL_NORMAL].all);
+
+	if (desc_3->field.dic) {
+		desc_3->field.dic = 1;
+	}
+
+	desc_1->field.classid = (skb->priority >= 15) ? 15 : skb->priority;
+	desc_2->all = ((uintptr_t)skb->data) & 0xFFFFFFFF;
+	desc_3->field.haddr = (((uintptr_t)skb->data) >> 8) & 0xF;
+
+	/*for ETH LAN/WAN */
+	if (dp_info->alloc_flags & (DP_F_FAST_ETH_LAN | DP_F_FAST_ETH_WAN |
+				    DP_F_GPON | DP_F_EPON)) {
+		/*always with pmac*/
+		if (likely(tx_chksum_flag)) {
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_CHECKSUM, &pmac,
+							desc_0, desc_1,
+							dp_info);
+			set_chksum(&pmac, tcp_type, ip_offset,
+				   ip_offset_hw_adjust, tcp_h_offset);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
+		} else {
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_NORMAL, &pmac,
+							desc_0, desc_1,
+							dp_info);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
+		}
+
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_PTP1588)
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_PTP1588_SW_WORKAROUND)
+
+		if (dp_info->f_ptp)
+#else
+		if (dp_info->f_ptp &&
+		    (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP))
+#endif
+		{
+			ops = dp_port_prop[inst].mac_ops[dp_info->port_id];
+
+			if (!ops) {
+				err_ret = DP_XMIT_PTP_ERR;
+				goto lbl_err_ret;
+			}
+
+			rec_id = ops->do_tx_hwts(ops, skb);
+
+			if (rec_id < 0) {
+				err_ret = DP_XMIT_PTP_ERR;
+				goto lbl_err_ret;
+			}
+
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_PTP, &pmac,
+							desc_0, desc_1,
+							dp_info);
+			pmac.record_id_msb = rec_id;
+		}
+
+#endif
+	} else if (dp_info->alloc_flags & DP_F_FAST_DSL) { /*some with pmac*/
+		if (unlikely(flags & DP_TX_CAL_CHKSUM)) { /* w/ pmac*/
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_CHECKSUM, &pmac,
+							desc_0, desc_1,
+							dp_info);
+			set_chksum(&pmac, tcp_type, ip_offset,
+				   ip_offset_hw_adjust, tcp_h_offset);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
+
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_ACA_CSUM_WORKAROUND)
+
+			if (aca_portid > 0)
+				desc_1->field.ep = aca_portid;
+
+#endif
+		} else if (flags & DP_TX_DSL_FCS) {/* after checksum check */
+			/* w/ pmac for FCS purpose*/
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_OTHERS, &pmac,
+							desc_0, desc_1,
+							dp_info);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
+			insert_pmac_f = 1;
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_ACA_CSUM_WORKAROUND)
+
+			if (aca_portid > 0)
+				desc_1->field.ep = aca_portid;
+
+#endif
+		} else { /*no pmac */
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_NORMAL, NULL,
+							desc_0, desc_1,
+							dp_info);
+		}
+	} else if (dp_info->alloc_flags & DP_F_FAST_WLAN) {/*some with pmac*/
+		/*normally no pmac. But if need checksum, need pmac*/
+		if (unlikely(tx_chksum_flag)) { /*with pmac*/
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_CHECKSUM, &pmac,
+							desc_0, desc_1,
+							dp_info);
+			set_chksum(&pmac, tcp_type, ip_offset,
+				   ip_offset_hw_adjust, tcp_h_offset);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_ACA_CSUM_WORKAROUND)
+
+			if (aca_portid > 0)
+				desc_1->field.ep = aca_portid;
+
+#endif
+		} else { /*no pmac*/
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_NORMAL, NULL,
+							desc_0, desc_1,
+							dp_info);
+		}
+	} else if (dp_info->alloc_flags & DP_F_DIRECTLINK) { /*always w/ pmac*/
+		if (unlikely(flags & DP_TX_CAL_CHKSUM)) { /* w/ pmac*/
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_CHECKSUM, &pmac,
+							desc_0, desc_1,
+							dp_info);
+			set_chksum(&pmac, tcp_type, ip_offset,
+				   ip_offset_hw_adjust, tcp_h_offset);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
+		} else if (flags & DP_TX_TO_DL_MPEFW) { /*w/ pmac*/
+			/*copy from checksum's pmac template setting,
+			 *but need to reset tcp_chksum in TCP header
+			 */
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_OTHERS, &pmac,
+							desc_0, desc_1,
+							dp_info);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
+		} else { /*do like normal directpath with pmac */
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_NORMAL, &pmac,
+							desc_0, desc_1,
+							dp_info);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
+		}
+	} else { /*normal directpath: always w/ pmac */
+		if (unlikely(tx_chksum_flag)) {
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_CHECKSUM,
+							&pmac,
+							desc_0,
+							desc_1,
+							dp_info);
+			set_chksum(&pmac, tcp_type, ip_offset,
+				   ip_offset_hw_adjust, tcp_h_offset);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
+		} else { /*w/ pmac */
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_NORMAL, &pmac,
+							desc_0, desc_1,
+							dp_info);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
+		}
+	}
+
+	desc_3->field.data_len = skb->len;
+	desc_1->field.ep = sif->gpid; /* use gpid */
+
+	if (unlikely(dp_dbg_flag)) {
+		if (insert_pmac_f)
+			dp_xmit_dbg("After", skb, ep, len, flags, &pmac,
+				    rx_subif, insert_pmac_f, skb_is_gso(skb),
+				    tx_chksum_flag);
+		else
+			dp_xmit_dbg("After", skb, ep, len, flags, NULL,
+				    rx_subif, insert_pmac_f, skb_is_gso(skb),
+				    tx_chksum_flag);
+	}
+
+	pp_tx_pkt_hook(skb, desc_1->field.ep);
+#if IS_ENABLED(CONFIG_LTQ_TOE_DRIVER)
+	if (skb_is_gso(skb)) {
+		res = ltq_tso_xmit(skb, &pmac, sizeof(pmac), 0);
+		UP_STATS(get_dp_port_subif_mib(sif)->tx_tso_pkt);
+		return res;
+	}
+#endif /* CONFIG_LTQ_TOE_DRIVER */
+
+#if IS_ENABLED(CONFIG_INTEL_DATAPATH_EXTRA_DEBUG)
+
+	if (unlikely(!desc_1->field.ep)) {
+		PR_INFO("Why after get_dma_pmac_templ ep is zero\n");
+		err_ret = DP_XMIT_ERR_EP_ZERO;
+		goto lbl_err_ret;
+	}
+#endif
+
+	if (insert_pmac_f) {
+		data.pmac = (u8 *)&pmac;
+		data.pmac_len = sizeof(pmac);
+		data.dp_inst = inst;
+		data.dp_inst = 0;
+		desc_1->field.pmac = 1;
+	} else {
+		data.pmac = NULL;
+		data.pmac_len = 0;
+		data.dp_inst = inst;
+		data.dp_inst = 0;
+		desc_1->field.pmac = 0;
+	}
+	if (is_stream_port(dp_info->alloc_flags))
+		data.f_byqos = 0;
+	else
+		data.f_byqos = 1;
+	res = cbm_cpu_pkt_tx(skb, &data, 0);
+	UP_STATS(get_dp_port_subif_mib(sif)->tx_cbm_pkt);
+	return res;
+
+lbl_err_ret:
+
+	switch (err_ret) {
+	case DP_XMIT_ERR_NOT_INIT:
+		PR_RATELIMITED("dp_xmit failed for dp no init yet\n");
+		break;
+
+	case DP_XMIT_ERR_IN_IRQ:
+		PR_RATELIMITED("dp_xmit not allowed in interrupt context\n");
+		break;
+
+	case DP_XMIT_ERR_NULL_SUBIF:
+		PR_RATELIMITED("dp_xmit failed for rx_subif null\n");
+		UP_STATS(get_dp_port_info(inst, 0)->tx_err_drop);
+		break;
+
+	case DP_XMIT_ERR_PORT_TOO_BIG:
+		UP_STATS(get_dp_port_info(inst, 0)->tx_err_drop);
+		PR_RATELIMITED("rx_subif->port_id >= max_ports");
+		break;
+
+	case DP_XMIT_ERR_NULL_SKB:
+		PR_RATELIMITED("skb NULL");
+		UP_STATS(get_dp_port_info(inst, rx_subif->port_id)->
+			 tx_err_drop);
+		break;
+
+	case DP_XMIT_ERR_NULL_IF:
+		UP_STATS(get_dp_port_subif_mib(sif)->tx_pkt_dropped);
+		PR_RATELIMITED("rx_if NULL");
+		break;
+
+	case DP_XMIT_ERR_REALLOC_SKB:
+		PR_INFO_ONCE("dp_create_new_skb failed\n");
+		break;
+
+	case DP_XMIT_ERR_EP_ZERO:
+		PR_ERR("Why ep zero in dp_xmit for %s\n",
+		       skb->dev ? skb->dev->name : "NULL");
+		break;
+
+	case DP_XMIT_ERR_GSO_NOHEADROOM:
+		PR_ERR("No enough skb headerroom(GSO). Need tune SKB buffer\n");
+		break;
+
+	case DP_XMIT_ERR_CSM_NO_SUPPORT:
+		PR_RATELIMITED("dp_xmit not support checksum\n");
+		break;
+
+	case DP_XMIT_PTP_ERR:
+		break;
+
+	default:
+		UP_STATS(get_dp_port_subif_mib(sif)->tx_pkt_dropped);
+		PR_INFO_ONCE("Why come to here:%x\n", dp_info->status);
+	}
+
+	if (skb)
+		dev_kfree_skb_any(skb);
+
+	return DP_FAILURE;
+}
+
diff --git a/drivers/net/ethernet/lantiq/Kconfig b/drivers/net/ethernet/lantiq/Kconfig
index 43001ef2561d..70fbb54fd875 100644
--- a/drivers/net/ethernet/lantiq/Kconfig
+++ b/drivers/net/ethernet/lantiq/Kconfig
@@ -96,7 +96,7 @@ config INTEL_XPCS
 	  This driver provides support for Xpcs for 10G.
 
 
-source "drivers/net/ethernet/lantiq/datapath/Kconfig"
+source "drivers/net/datapath/dpm/Kconfig"
 source "drivers/net/ethernet/lantiq/switch-api/Kconfig"
 source "drivers/net/ethernet/lantiq/cqm/Kconfig"
 source "drivers/net/ethernet/lantiq/tmu/Kconfig"
diff --git a/drivers/net/ethernet/lantiq/Makefile b/drivers/net/ethernet/lantiq/Makefile
index c5ff20cf5db4..5b71302e20d2 100644
--- a/drivers/net/ethernet/lantiq/Makefile
+++ b/drivers/net/ethernet/lantiq/Makefile
@@ -2,7 +2,7 @@ obj-$(CONFIG_LANTIQ_VRX318) += ltq_vrx318.o
 obj-$(CONFIG_LANTIQ_ETH_FRAMEWORK) += lantiq_eth_framework.o
 obj-$(CONFIG_INTEL_XPCS) += xpcs/
 obj-$(CONFIG_LTQ_ETHSW_API) += switch-api/
-obj-$(CONFIG_LTQ_DATAPATH) += datapath/
+obj-$(CONFIG_LTQ_DATAPATH) += ../../datapath/dpm/
 obj-$(CONFIG_LTQ_TMU) += tmu/
 obj-$(CONFIG_LTQ_PPV4) += ppv4/
 obj-$(CONFIG_LTQ_CBM) += cqm/
