From 450a4c5ae91f2012b219c586509a85ae67864c04 Mon Sep 17 00:00:00 2001
From: Gu Chao <gux.chao@intel.com>
Date: Thu, 4 Jul 2019 15:09:17 +0800
Subject: [PATCH] DRVLIB_SW-2541: Fix some DP coding style

- Fix lines that break at '.' or '->'
- Fix most of checkpatch warnings
---
 drivers/net/datapath/dpm/datapath.h                |  16 +-
 drivers/net/datapath/dpm/datapath_api.c            |  14 +-
 drivers/net/datapath/dpm/datapath_instance.c       |  10 +-
 drivers/net/datapath/dpm/datapath_instance.h       |   1 -
 drivers/net/datapath/dpm/datapath_ioctl.c          |   2 +-
 drivers/net/datapath/dpm/datapath_misc.c           |  29 +-
 drivers/net/datapath/dpm/datapath_proc.c           |  22 +-
 drivers/net/datapath/dpm/datapath_proc_api.c       |   1 +
 drivers/net/datapath/dpm/datapath_qos.c            | 104 ++--
 drivers/net/datapath/dpm/datapath_swdev.c          |  84 ++-
 drivers/net/datapath/dpm/gswip30/datapath_coc.c    |  34 +-
 drivers/net/datapath/dpm/gswip30/datapath_gswip.c  |  27 +-
 drivers/net/datapath/dpm/gswip30/datapath_mib.c    |  24 +-
 drivers/net/datapath/dpm/gswip30/datapath_misc.c   |  19 +-
 drivers/net/datapath/dpm/gswip30/datapath_proc.c   |  53 +-
 drivers/net/datapath/dpm/gswip30/datapath_rx.c     |  18 +-
 drivers/net/datapath/dpm/gswip30/datapath_tx.c     | 581 ++++++++++-----------
 .../net/datapath/dpm/gswip31/datapath_ext_vlan.c   |  16 +-
 drivers/net/datapath/dpm/gswip31/datapath_gswip.c  | 153 +++---
 drivers/net/datapath/dpm/gswip31/datapath_misc.c   |  90 ++--
 .../net/datapath/dpm/gswip31/datapath_ppv4_api.c   |  25 +-
 drivers/net/datapath/dpm/gswip31/datapath_proc.c   |  15 +-
 drivers/net/datapath/dpm/gswip31/datapath_rx.c     |  13 +-
 .../net/datapath/dpm/gswip31/datapath_switchdev.c  |  73 +--
 .../datapath/dpm/gswip31/datapath_tc_asym_vlan.c   |  48 +-
 drivers/net/datapath/dpm/gswip31/datapath_tx.c     |   6 +-
 .../net/datapath/dpm/gswip32/datapath_ext_vlan.c   |  18 +-
 drivers/net/datapath/dpm/gswip32/datapath_gswip.c  | 196 +++----
 .../datapath/dpm/gswip32/datapath_lookup_proc.c    |  15 +-
 drivers/net/datapath/dpm/gswip32/datapath_misc.c   | 233 +++++----
 drivers/net/datapath/dpm/gswip32/datapath_misc.h   |  32 +-
 drivers/net/datapath/dpm/gswip32/datapath_ppv4.c   |  48 +-
 drivers/net/datapath/dpm/gswip32/datapath_ppv4.h   |  40 +-
 .../net/datapath/dpm/gswip32/datapath_ppv4_api.c   | 130 +++--
 .../datapath/dpm/gswip32/datapath_ppv4_session.c   |  55 +-
 .../datapath/dpm/gswip32/datapath_ppv4_session.h   |  24 +-
 drivers/net/datapath/dpm/gswip32/datapath_proc.c   |  19 +-
 drivers/net/datapath/dpm/gswip32/datapath_rx.c     |  44 +-
 .../net/datapath/dpm/gswip32/datapath_switchdev.c  |  76 +--
 .../net/datapath/dpm/gswip32/datapath_switchdev.h  |   4 +-
 .../datapath/dpm/gswip32/datapath_tc_asym_vlan.c   |  40 +-
 drivers/net/datapath/dpm/gswip32/datapath_tx.c     |   4 +-
 include/net/datapath_api.h                         | 136 ++---
 include/net/datapath_api_qos.h                     |  17 +-
 44 files changed, 1352 insertions(+), 1257 deletions(-)

diff --git a/drivers/net/datapath/dpm/datapath.h b/drivers/net/datapath/dpm/datapath.h
index f6bf127cd782..6cc430e47d33 100644
--- a/drivers/net/datapath/dpm/datapath.h
+++ b/drivers/net/datapath/dpm/datapath.h
@@ -31,14 +31,14 @@
 #include <net/pp_qos_drv_slim.h>
 #include <linux/pp_qos_api.h>
 #else
-	#if LINUX_VERSION_CODE < KERNEL_VERSION(4,19,0)
+	#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 19, 0)
 		#include <net/pp_qos_drv.h>
 	#else
 		#include <linux/pp_qos_api.h>
 	#endif
 #endif
 #if IS_ENABLED(CONFIG_INTEL_CBM_SKB) || \
-	LINUX_VERSION_CODE < KERNEL_VERSION(4,19,0)
+	LINUX_VERSION_CODE < KERNEL_VERSION(4, 19, 0)
 	#define DP_SKB_HACK
 #endif
 #include <net/datapath_api_qos.h>
@@ -149,7 +149,7 @@
 
 #define PARSER_FLAG_SIZE   40
 #define PARSER_OFFSET_SIZE 8
-#define DP_PMAC_OPS(gsw, cmd) (dp_gsw_cb)(gsw)->gsw_pmac_ops.cmd
+#define DP_PMAC_OPS(gsw, cmd) ((dp_gsw_cb)(gsw)->gsw_pmac_ops.cmd)
 
 #define PKT_PASER_FLAG_OFFSET   0
 #define PKT_PASER_OFFSET_OFFSET (PARSER_FLAG_SIZE)
@@ -438,7 +438,9 @@ struct dp_subif_info {
 #define DP_POLICY_PER_PORT 4
 	u16 policy_base;
 	u8 policy_num;
-	u16 pool_map; /* pool map: bit 0: POOL SIZE 0, bit 1: POOL_SIZE_1 and so on */
+	u16 pool_map;	/* pool map: bit 0: POOL SIZE 0
+			 *           bit 1: POOL_SIZE_1 and so on
+			 */
 	u8  pkt_only_en;
 	u8  seg_en;
 	u16 gpid;
@@ -514,7 +516,9 @@ struct pmac_port_info {
 			*/
 	u16 policy_base; /* policy base */
 	u8 policy_num;   /* policy number */
-	u16 pool_map; /* pool map: bit 0: POOL SIZE 0, bit 1: POOL_SIZE_1 and so on */
+	u16 pool_map;	/* pool map: bit 0: POOL SIZE 0
+			 *           bit 1: POOL_SIZE_1 and so on
+			 */
 	u32 num_dma_chan; /*For G.INT it's 8 or 16, for other 1*/
 	u32 lct_idx; /* LCT subif register flag */
 	u32 dma_ch_base; /*! Base entry index of dp_dma_chan_tbl */
@@ -867,7 +871,7 @@ extern int dp_init_ok;
 void set_chksum(struct pmac_tx_hdr *pmac, u32 tcp_type,
 		u32 ip_offset, int ip_off_hw_adjust, u32 tcp_h_offset);
 
-#if LINUX_VERSION_CODE < KERNEL_VERSION(4,13,0)
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 13, 0)
 extern int32_t (*qos_mgr_hook_setup_tc)(struct net_device *dev, u32 handle,
 					__be16 protocol,
 					struct tc_to_netdev *tc);
diff --git a/drivers/net/datapath/dpm/datapath_api.c b/drivers/net/datapath/dpm/datapath_api.c
index 02553f231a4a..0f9fbf025937 100644
--- a/drivers/net/datapath/dpm/datapath_api.c
+++ b/drivers/net/datapath/dpm/datapath_api.c
@@ -589,10 +589,8 @@ int32_t dp_register_subif_private(int inst, struct module *owner,
 		 *need to do configuration HW
 		 */
 		if (port_info->status) {
-			if (dp_port_prop[inst].info.
-					subif_platform_set(inst, port_id, i,
-							   &platfrm_data,
-							   flags)) {
+			if (dp_port_prop[inst].info.subif_platform_set(
+				inst, port_id, i, &platfrm_data, flags)) {
 				PR_ERR("subif_platform_set fail\n");
 				goto EXIT;
 			} else {
@@ -961,7 +959,7 @@ int32_t dp_register_dev_ext(int inst, struct module *owner, uint32_t port_id,
 			port_info->status = PORT_ALLOCATED;
 			DP_CB(inst, dev_platform_set)(inst, port_id, data,
 						      flags);
-#if !IS_ENABLED(CONFIG_INTEL_DATAPATH_HAL_GSWIP30)			
+#if !IS_ENABLED(CONFIG_INTEL_DATAPATH_HAL_GSWIP30)
 			if (port_info->umt_param.id)
 				dp_umt_release(&port_info->umt_param, flags);
 #endif
@@ -1014,7 +1012,6 @@ int32_t dp_register_dev_ext(int inst, struct module *owner, uint32_t port_id,
 				       port_id,
 				       cbm_data,
 				       port_info->alloc_flags)) {
-
 		DP_DEBUG(DP_DBG_FLAG_REG, "CBM port alloc failed\n");
 		kfree(cbm_data);
 		DP_LIB_UNLOCK(&dp_lock);
@@ -1498,14 +1495,14 @@ static int dp_register_dc(int inst, uint32_t port_id,
 	umt_param.daddr = (u32)data->umt->umt_msg_paddr;
 
 	if (dp_umt_request(&umt_param, 0)) {
-		PR_ERR("UMT request Fail!! DMA ID %x CQM_PID %d MSG_MODE %d"
+		PR_ERR("UMT request Fail!! DMA ID %x CQM_PID %d MSG_MODE %d "
 		       "PERIOD %d SW_MSG %d DADDR 0x%08x\n", umt_param.dma_id,
 		       umt_param.cqm_dq_pid, umt_param.msg_mode,
 		       umt_param.period, umt_param.sw_msg, umt_param.daddr);
 		return DP_FAILURE;
 	}
 	if (dp_umt_set(&umt_param, 0)) {
-		PR_ERR("UMT port set fail !! DMA ID %x CQM_PID %d MSG_MODE %d"
+		PR_ERR("UMT port set fail !! DMA ID %x CQM_PID %d MSG_MODE %d "
 		       "PERIOD %d SW_MSG %d DADDR 0x%08x\n", umt_param.dma_id,
 		       umt_param.cqm_dq_pid, umt_param.msg_mode,
 		       umt_param.period, umt_param.sw_msg, umt_param.daddr);
@@ -1517,6 +1514,7 @@ static int dp_register_dc(int inst, uint32_t port_id,
 #endif
 	return DP_SUCCESS;
 }
+
 /* return DP_SUCCESS -- found
  * return DP_FAILURE -- not found
  */
diff --git a/drivers/net/datapath/dpm/datapath_instance.c b/drivers/net/datapath/dpm/datapath_instance.c
index 5f0992c8fcee..d687c5c70a32 100644
--- a/drivers/net/datapath/dpm/datapath_instance.c
+++ b/drivers/net/datapath/dpm/datapath_instance.c
@@ -128,11 +128,11 @@ int dp_request_inst(struct dp_inst_info *info, u32 flag)
 	dp_port_prop[i].ops[0] = info->ops[0];
 	dp_port_prop[i].ops[1] = info->ops[1];
 
-	for(j = 0; j < DP_MAX_MAC_HANDLE; j++) {
+	for (j = 0; j < DP_MAX_MAC_HANDLE; j++) {
 		if (info->mac_ops[j])
 			dp_port_prop[i].mac_ops[j] = info->mac_ops[j];
 	}
-	
+
 	dp_port_prop[i].info = hw_cap_list[k].info;
 	dp_port_prop[i].cbm_inst = info->cbm_inst;
 	dp_port_prop[i].qos_inst = info->qos_inst;
@@ -209,7 +209,7 @@ struct dp_dev *dp_dev_lookup(struct hlist_head *head,
 }
 
 #if IS_ENABLED(CONFIG_PPA)
-#if LINUX_VERSION_CODE < KERNEL_VERSION(4,13,0)
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 13, 0)
 static int dp_ndo_setup_tc(struct net_device *dev, u32 handle,
 			   __be16 protocol, struct tc_to_netdev *tc)
 {
@@ -224,8 +224,8 @@ static int dp_ndo_setup_tc(struct net_device *dev, u32 handle,
 }
 #else
 static int dp_ndo_setup_tc(struct net_device *dev,
-				enum tc_setup_type type,
-				void *type_data)
+			   enum tc_setup_type type,
+			   void *type_data)
 {
 	return -1;
 }
diff --git a/drivers/net/datapath/dpm/datapath_instance.h b/drivers/net/datapath/dpm/datapath_instance.h
index ca134659e0e7..3ecc4c623c41 100644
--- a/drivers/net/datapath/dpm/datapath_instance.h
+++ b/drivers/net/datapath/dpm/datapath_instance.h
@@ -25,7 +25,6 @@ extern struct dp_hw_cap hw_cap_list[DP_MAX_HW_CAP];
 		(DP_F_FAST_WLAN | DP_F_FAST_DSL)) && \
 		!((flags) & (DP_TX_CAL_CHKSUM | DP_TX_DSL_FCS)))
 
-
 extern struct hlist_head dp_dev_list[DP_DEV_HASH_SIZE];
 u32 dp_dev_hash(struct net_device *dev, char *subif_name);
 struct dp_dev *dp_dev_lookup(struct hlist_head *head,
diff --git a/drivers/net/datapath/dpm/datapath_ioctl.c b/drivers/net/datapath/dpm/datapath_ioctl.c
index f88bf2d643a2..52b9bd71f4c5 100644
--- a/drivers/net/datapath/dpm/datapath_ioctl.c
+++ b/drivers/net/datapath/dpm/datapath_ioctl.c
@@ -15,7 +15,7 @@ static int get_tsinfo(struct net_device *dev,
 
 	if (dp_get_netif_subifid(dev, NULL, NULL, NULL, &subif, 0)) {
 		PR_ERR("%s dp_get_netif_subifid failed for %s\n",
-				__func__, dev->name);
+		       __func__, dev->name);
 		return -EFAULT;
 	}
 	ops = dp_port_prop[inst].mac_ops[subif.port_id];
diff --git a/drivers/net/datapath/dpm/datapath_misc.c b/drivers/net/datapath/dpm/datapath_misc.c
index 5a7ce1030c61..6fecf3351c5e 100644
--- a/drivers/net/datapath/dpm/datapath_misc.c
+++ b/drivers/net/datapath/dpm/datapath_misc.c
@@ -1109,8 +1109,8 @@ int dp_ingress_ctp_tc_map_set(struct dp_tc_cfg *tc, int flag)
 	mtr_subif.inst =  mtr_subif.subif.inst;
 	if (!dp_port_prop[mtr_subif.inst].info.dp_ctp_tc_map_set)
 		return DP_FAILURE;
-	return dp_port_prop[mtr_subif.inst].info.
-		dp_ctp_tc_map_set(tc, flag, &mtr_subif);
+	return dp_port_prop[mtr_subif.inst].info.dp_ctp_tc_map_set(tc, flag,
+								&mtr_subif);
 }
 EXPORT_SYMBOL(dp_ingress_ctp_tc_map_set);
 
@@ -1204,8 +1204,7 @@ void dp_subif_reclaim(struct rcu_head *rp)
 	struct dp_subif_cache *dp_subif =
 		container_of(rp, struct dp_subif_cache, rcu);
 
-	if (dp_subif->data)
-		kfree(dp_subif->data);
+	kfree(dp_subif->data);
 	kfree(dp_subif);
 }
 
@@ -1284,7 +1283,6 @@ int32_t dp_update_subif(struct net_device *netif, void *data,
 				sizeof(dp_subif->name) - 1);
 		dp_subif->subif_fn = subifid_fn_t;
 		hlist_add_head_rcu(&dp_subif->hlist, &dp_subif_list[idx]);
-		return 0;
 	} else {
 		dp_subif_new = kzalloc(sizeof(*dp_subif), GFP_ATOMIC);
 		if (!dp_subif_new)
@@ -1299,9 +1297,8 @@ int32_t dp_update_subif(struct net_device *netif, void *data,
 		hlist_replace_rcu(&dp_subif->hlist,
 				  &dp_subif_new->hlist);
 		call_rcu_bh(&dp_subif->rcu, dp_subif_reclaim);
-		return 0;
 	}
-	return -1;
+	return 0;
 }
 
 int32_t dp_sync_subifid(struct net_device *dev, char *subif_name,
@@ -1396,7 +1393,8 @@ static int dp_coc_cpufreq_policy_notifier(struct notifier_block *nb,
 {
 	int inst = 0;
 	struct cpufreq_policy *policy = data;
-	DP_DEBUG(DP_DBG_FLAG_COC,"%s; cpu=%d\n",
+
+	DP_DEBUG(DP_DBG_FLAG_COC, "%s; cpu=%d\n",
 		 event ? "CPUFREQ_NOTIFY" : "CPUFREQ_ADJUST",
 		 policy->cpu);
 	if (event != CPUFREQ_ADJUST) {
@@ -1405,8 +1403,7 @@ static int dp_coc_cpufreq_policy_notifier(struct notifier_block *nb,
 		return NOTIFY_DONE;
 	}
 	return
-	dp_port_prop[inst].info.
-		dp_handle_cpufreq_event(POLICY_NOTIFY, policy);
+	dp_port_prop[inst].info.dp_handle_cpufreq_event(POLICY_NOTIFY, policy);
 }
 
 /* keep track of frequency transitions */
@@ -1415,12 +1412,13 @@ static int dp_coc_cpufreq_transition_notifier(struct notifier_block *nb,
 {
 	int inst = 0;
 	struct cpufreq_freqs *freq = data;
+
 	if (event == CPUFREQ_PRECHANGE) {
-		return dp_port_prop[inst].info.
-				dp_handle_cpufreq_event(PRE_CHANGE, freq);
+		return dp_port_prop[inst].info.dp_handle_cpufreq_event(
+							PRE_CHANGE, freq);
 	} else if (event == CPUFREQ_POSTCHANGE) {
-		return dp_port_prop[inst].info.
-				dp_handle_cpufreq_event(POST_CHANGE, freq);
+		return dp_port_prop[inst].info.dp_handle_cpufreq_event(
+							POST_CHANGE, freq);
 	}
 	return NOTIFY_OK;
 }
@@ -1552,7 +1550,8 @@ u32 alloc_dp_port_subif_info(int inst)
 			PR_ERR("Failed for kmalloc: %zu bytes\n",
 			       max_subif * sizeof(struct dp_subif_info));
 			while (--port_id >= 0)
-				kfree(get_dp_port_info(inst, port_id)->subif_info);
+				kfree(get_dp_port_info(inst,
+						       port_id)->subif_info);
 			return DP_FAILURE;
 		}
 	}
diff --git a/drivers/net/datapath/dpm/datapath_proc.c b/drivers/net/datapath/dpm/datapath_proc.c
index 99789aa2ee8e..bb0aabe25662 100644
--- a/drivers/net/datapath/dpm/datapath_proc.c
+++ b/drivers/net/datapath/dpm/datapath_proc.c
@@ -57,6 +57,7 @@ int dump_dc_info(struct seq_file *s, struct pmac_port_info *port)
 {
 	int i = 0;
 	u32 cid, pid, nid;
+
 	for (i = 0; i < port->num_tx_ring; i++) {
 		seq_printf(s, "    DC TxRing:      %d\n", i);
 		seq_printf(s, "      TXIN  DeqRingSize/paddr:      %d/0x%p\n",
@@ -115,7 +116,6 @@ int dump_dc_info(struct seq_file *s, struct pmac_port_info *port)
 	return 0;
 }
 
-
 int proc_port_dump(struct seq_file *s, int pos)
 {
 	int i, j;
@@ -287,17 +287,20 @@ int proc_port_dump(struct seq_file *s, int pos)
 			   dp_deq_port_tbl[tmp_inst][cqm_p].ref_cnt);
 		seq_printf(s, "          : mac_learn_dis:    %d\n",
 			   sif->mac_learn_dis);
-		cid = _DMA_CONTROLLER(dp_deq_port_tbl[tmp_inst][cqm_p].dma_chan);
+		cid = _DMA_CONTROLLER(
+				dp_deq_port_tbl[tmp_inst][cqm_p].dma_chan);
 		pid = _DMA_PORT(dp_deq_port_tbl[tmp_inst][cqm_p].dma_chan);
 		nid = _DMA_CHANNEL(dp_deq_port_tbl[tmp_inst][cqm_p].dma_chan);
 		dma_ch_offset = dp_deq_port_tbl[tmp_inst][cqm_p].dma_ch_offset;
 		if (port->num_dma_chan && dp_dma_chan_tbl[tmp_inst]) {
-			seq_printf(s, "          : tx_dma_ch:    0x%x(ref=%d)\n",
-				   dp_deq_port_tbl[tmp_inst][cqm_p].dma_chan,
-			   atomic_read(&(dp_dma_chan_tbl[tmp_inst] +
-				       dma_ch_offset)->ref_cnt));
-			seq_printf(s, "          : dma-ctrl/port/channel:%d/%d/%d\n",
-				   cid, pid, nid);
+			seq_printf(s,
+				"          : tx_dma_ch:    0x%x(ref=%d)\n",
+				dp_deq_port_tbl[tmp_inst][cqm_p].dma_chan,
+				atomic_read(&(dp_dma_chan_tbl[tmp_inst] +
+						dma_ch_offset)->ref_cnt));
+			seq_printf(s,
+				"          : dma-ctrl/port/channel:%d/%d/%d\n",
+				cid, pid, nid);
 		}
 		seq_printf(s, "          : gpid:           %d\n",
 			   sif->gpid);
@@ -401,8 +404,7 @@ int display_port_info(int inst, u8 pos, int start_vap, int end_vap, u32 flag)
 			     "netif",
 			     (uintptr_t)sif->netif,
 			     "device_name",
-			     sif->netif ? sif->
-			     netif->name : "NULL/DSL",
+			     sif->netif ? sif->netif->name : "NULL/DSL",
 			     "name",
 			     sif->device_name);
 			PR_INFO("          : rx_fn_rxif_pkt =0x%08x\n",
diff --git a/drivers/net/datapath/dpm/datapath_proc_api.c b/drivers/net/datapath/dpm/datapath_proc_api.c
index 5a2c42ac1fbb..7b852e9fd4df 100644
--- a/drivers/net/datapath/dpm/datapath_proc_api.c
+++ b/drivers/net/datapath/dpm/datapath_proc_api.c
@@ -81,6 +81,7 @@ int dp_split_buffer(char *buffer, char *array[], int max_param_num)
 		return 0;
 	while ((array[i] = strsep(&buffer, " ")) != NULL) {
 		size_t len = strlen(array[i]);
+
 		dp_replace_ch(array[i], len, ' ', 0);
 		dp_replace_ch(array[i], len, '\r', 0);
 		dp_replace_ch(array[i], len, '\n', 0);
diff --git a/drivers/net/datapath/dpm/datapath_qos.c b/drivers/net/datapath/dpm/datapath_qos.c
index 1b56ed22a864..8b7da5d8c3dd 100644
--- a/drivers/net/datapath/dpm/datapath_qos.c
+++ b/drivers/net/datapath/dpm/datapath_qos.c
@@ -7,8 +7,8 @@ int dp_node_link_add(struct dp_node_link *info, int flag)
 {
 	if (!dp_port_prop[info->inst].info.dp_qos_platform_set)
 		return DP_FAILURE;
-	return dp_port_prop[info->inst].info.
-		dp_qos_platform_set(NODE_LINK_ADD, info, flag);
+	return dp_port_prop[info->inst].info.dp_qos_platform_set(NODE_LINK_ADD,
+								info, flag);
 }
 EXPORT_SYMBOL(dp_node_link_add);
 
@@ -16,8 +16,8 @@ int dp_node_unlink(struct dp_node_link *info, int flag)
 {
 	if (!dp_port_prop[info->inst].info.dp_qos_platform_set)
 		return DP_FAILURE;
-	return dp_port_prop[info->inst].info.
-		dp_qos_platform_set(NODE_UNLINK, info, flag);
+	return dp_port_prop[info->inst].info.dp_qos_platform_set(NODE_UNLINK,
+								info, flag);
 }
 EXPORT_SYMBOL(dp_node_unlink);
 
@@ -25,8 +25,8 @@ int dp_node_link_get(struct dp_node_link *info, int flag)
 {
 	if (!dp_port_prop[info->inst].info.dp_qos_platform_set)
 		return DP_FAILURE;
-	return dp_port_prop[info->inst].info.
-		dp_qos_platform_set(NODE_LINK_GET, info, flag);
+	return dp_port_prop[info->inst].info.dp_qos_platform_set(NODE_LINK_GET,
+								info, flag);
 }
 EXPORT_SYMBOL(dp_node_link_get);
 
@@ -34,8 +34,8 @@ int dp_node_link_en_set(struct dp_node_link_enable *en, int flag)
 {
 	if (!dp_port_prop[en->inst].info.dp_qos_platform_set)
 		return DP_FAILURE;
-	return dp_port_prop[en->inst].info.
-		dp_qos_platform_set(NODE_LINK_EN_SET, en, flag);
+	return dp_port_prop[en->inst].info.dp_qos_platform_set(NODE_LINK_EN_SET,
+								en, flag);
 }
 EXPORT_SYMBOL(dp_node_link_en_set);
 
@@ -43,8 +43,8 @@ int dp_node_link_en_get(struct dp_node_link_enable *en, int flag)
 {
 	if (!dp_port_prop[en->inst].info.dp_qos_platform_set)
 		return DP_FAILURE;
-	return dp_port_prop[en->inst].info.
-		dp_qos_platform_set(NODE_LINK_EN_GET, en, flag);
+	return dp_port_prop[en->inst].info.dp_qos_platform_set(NODE_LINK_EN_GET,
+								en, flag);
 }
 EXPORT_SYMBOL(dp_node_link_en_get);
 
@@ -52,8 +52,8 @@ int dp_link_add(struct dp_qos_link *cfg, int flag)
 {
 	if (!dp_port_prop[cfg->inst].info.dp_qos_platform_set)
 		return DP_FAILURE;
-	return dp_port_prop[cfg->inst].info.
-		dp_qos_platform_set(LINK_ADD, cfg, flag);
+	return dp_port_prop[cfg->inst].info.dp_qos_platform_set(LINK_ADD,
+								cfg, flag);
 }
 EXPORT_SYMBOL(dp_link_add);
 
@@ -61,8 +61,8 @@ int dp_link_get(struct dp_qos_link *cfg, int flag)
 {
 	if (!dp_port_prop[cfg->inst].info.dp_qos_platform_set)
 		return DP_FAILURE;
-	return dp_port_prop[cfg->inst].info.
-			dp_qos_platform_set(LINK_GET, cfg, flag);
+	return dp_port_prop[cfg->inst].info.dp_qos_platform_set(LINK_GET,
+								cfg, flag);
 }
 EXPORT_SYMBOL(dp_link_get);
 
@@ -70,8 +70,8 @@ int dp_qos_link_prio_set(struct dp_node_prio *info, int flag)
 {
 	if (!dp_port_prop[info->inst].info.dp_qos_platform_set)
 		return DP_FAILURE;
-	return dp_port_prop[info->inst].info.
-			dp_qos_platform_set(LINK_PRIO_SET, info, flag);
+	return dp_port_prop[info->inst].info.dp_qos_platform_set(LINK_PRIO_SET,
+								info, flag);
 }
 EXPORT_SYMBOL(dp_qos_link_prio_set);
 
@@ -79,8 +79,8 @@ int dp_qos_link_prio_get(struct dp_node_prio *info, int flag)
 {
 	if (!dp_port_prop[info->inst].info.dp_qos_platform_set)
 		return DP_FAILURE;
-	return dp_port_prop[info->inst].info.
-			dp_qos_platform_set(LINK_PRIO_GET, info, flag);
+	return dp_port_prop[info->inst].info.dp_qos_platform_set(LINK_PRIO_GET,
+								info, flag);
 }
 EXPORT_SYMBOL(dp_qos_link_prio_get);
 
@@ -88,8 +88,8 @@ int dp_queue_conf_set(struct dp_queue_conf *cfg, int flag)
 {
 	if (!dp_port_prop[cfg->inst].info.dp_qos_platform_set)
 		return DP_FAILURE;
-	return dp_port_prop[cfg->inst].info.
-		dp_qos_platform_set(QUEUE_CFG_SET, cfg, flag);
+	return dp_port_prop[cfg->inst].info.dp_qos_platform_set(QUEUE_CFG_SET,
+								cfg, flag);
 }
 EXPORT_SYMBOL(dp_queue_conf_set);
 
@@ -97,8 +97,8 @@ int dp_queue_conf_get(struct dp_queue_conf *cfg, int flag)
 {
 	if (!dp_port_prop[cfg->inst].info.dp_qos_platform_set)
 		return DP_FAILURE;
-	return dp_port_prop[cfg->inst].info.
-		dp_qos_platform_set(QUEUE_CFG_GET, cfg, flag);
+	return dp_port_prop[cfg->inst].info.dp_qos_platform_set(QUEUE_CFG_GET,
+								cfg, flag);
 }
 EXPORT_SYMBOL(dp_queue_conf_get);
 
@@ -106,15 +106,15 @@ int dp_shaper_conf_set(struct dp_shaper_conf *cfg, int flag)
 {
 	if (!dp_port_prop[cfg->inst].info.dp_qos_platform_set)
 		return DP_FAILURE;
-	return dp_port_prop[cfg->inst].info.
-		dp_qos_platform_set(SHAPER_SET, cfg, flag);
+	return dp_port_prop[cfg->inst].info.dp_qos_platform_set(SHAPER_SET,
+								cfg, flag);
 }
 EXPORT_SYMBOL(dp_shaper_conf_set);
 
 int dp_shaper_conf_get(struct dp_shaper_conf *cfg, int flag)
 {
-	return dp_port_prop[cfg->inst].info.
-		dp_qos_platform_set(SHAPER_GET, cfg, flag);
+	return dp_port_prop[cfg->inst].info.dp_qos_platform_set(SHAPER_GET,
+								cfg, flag);
 }
 EXPORT_SYMBOL(dp_shaper_conf_get);
 
@@ -122,8 +122,8 @@ int dp_node_alloc(struct dp_node_alloc *node, int flag)
 {
 	if (!dp_port_prop[node->inst].info.dp_qos_platform_set)
 		return DP_FAILURE;
-	return dp_port_prop[node->inst].info.
-		dp_qos_platform_set(NODE_ALLOC, node, flag);
+	return dp_port_prop[node->inst].info.dp_qos_platform_set(NODE_ALLOC,
+								node, flag);
 }
 EXPORT_SYMBOL(dp_node_alloc);
 
@@ -131,8 +131,8 @@ int dp_node_free(struct dp_node_alloc *node, int flag)
 {
 	if (!dp_port_prop[node->inst].info.dp_qos_platform_set)
 		return DP_FAILURE;
-	return dp_port_prop[node->inst].info.
-		dp_qos_platform_set(NODE_FREE, node, flag);
+	return dp_port_prop[node->inst].info.dp_qos_platform_set(NODE_FREE,
+								node, flag);
 }
 EXPORT_SYMBOL(dp_node_free);
 
@@ -140,8 +140,8 @@ int dp_node_children_free(struct dp_node_alloc *node, int flag)
 {
 	if (!dp_port_prop[node->inst].info.dp_qos_platform_set)
 		return DP_FAILURE;
-	return dp_port_prop[node->inst].info.
-		dp_qos_platform_set(NODE_CHILDREN_FREE, node, flag);
+	return dp_port_prop[node->inst].info.dp_qos_platform_set(
+					NODE_CHILDREN_FREE, node, flag);
 }
 EXPORT_SYMBOL(dp_node_children_free);
 
@@ -169,8 +169,8 @@ int dp_deq_port_res_get(struct dp_dequeue_res *res, int flag)
 	DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
 		 "dp_deq_port_res_get: dp_port=%d tconf_idx=%d\n",
 		 res->dp_port, res->cqm_deq_idx);
-	return dp_port_prop[res->inst].info.
-		dp_qos_platform_set(DEQ_PORT_RES_GET, res, flag);
+	return dp_port_prop[res->inst].info.dp_qos_platform_set(
+					DEQ_PORT_RES_GET, res, flag);
 }
 EXPORT_SYMBOL(dp_deq_port_res_get);
 
@@ -178,8 +178,8 @@ int dp_counter_mode_set(struct dp_counter_conf *cfg, int flag)
 {
 	if (!dp_port_prop[cfg->inst].info.dp_qos_platform_set)
 		return DP_FAILURE;
-	return dp_port_prop[cfg->inst].info.
-		dp_qos_platform_set(COUNTER_MODE_SET, cfg, flag);
+	return dp_port_prop[cfg->inst].info.dp_qos_platform_set(
+					COUNTER_MODE_SET, cfg, flag);
 }
 EXPORT_SYMBOL(dp_counter_mode_set);
 
@@ -187,8 +187,8 @@ int dp_counter_mode_get(struct dp_counter_conf *cfg, int flag)
 {
 	if (!dp_port_prop[cfg->inst].info.dp_qos_platform_set)
 		return DP_FAILURE;
-	return dp_port_prop[cfg->inst].info.
-		dp_qos_platform_set(COUNTER_MODE_GET, cfg, flag);
+	return dp_port_prop[cfg->inst].info.dp_qos_platform_set(
+					COUNTER_MODE_GET, cfg, flag);
 }
 EXPORT_SYMBOL(dp_counter_mode_get);
 
@@ -196,8 +196,8 @@ int dp_queue_map_set(struct dp_queue_map_set *cfg, int flag)
 {
 	if (!dp_port_prop[cfg->inst].info.dp_qos_platform_set)
 		return DP_FAILURE;
-	return dp_port_prop[cfg->inst].info.
-		dp_qos_platform_set(QUEUE_MAP_SET, cfg, flag);
+	return dp_port_prop[cfg->inst].info.dp_qos_platform_set(QUEUE_MAP_SET,
+								cfg, flag);
 }
 EXPORT_SYMBOL(dp_queue_map_set);
 
@@ -205,8 +205,8 @@ int dp_queue_map_get(struct dp_queue_map_get *cfg, int flag)
 {
 	if (!dp_port_prop[cfg->inst].info.dp_qos_platform_set)
 		return DP_FAILURE;
-	return dp_port_prop[cfg->inst].info.
-		dp_qos_platform_set(QUEUE_MAP_GET, cfg, flag);
+	return dp_port_prop[cfg->inst].info.dp_qos_platform_set(QUEUE_MAP_GET,
+								cfg, flag);
 }
 EXPORT_SYMBOL(dp_queue_map_get);
 
@@ -214,8 +214,8 @@ int dp_children_get(struct dp_node_child *cfg, int flag)
 {
 	if (!dp_port_prop[cfg->inst].info.dp_qos_platform_set)
 		return DP_FAILURE;
-	return dp_port_prop[cfg->inst].info.
-		dp_qos_platform_set(NODE_CHILDREN_GET, cfg, flag);
+	return dp_port_prop[cfg->inst].info.dp_qos_platform_set(
+					NODE_CHILDREN_GET, cfg, flag);
 }
 EXPORT_SYMBOL(dp_children_get);
 
@@ -223,8 +223,8 @@ int dp_qos_level_get(struct dp_qos_level *cfg, int flag)
 {
 	if (!dp_port_prop[cfg->inst].info.dp_qos_platform_set)
 		return DP_FAILURE;
-	return dp_port_prop[cfg->inst].info.
-		dp_qos_platform_set(QOS_LEVEL_GET, cfg, flag);
+	return dp_port_prop[cfg->inst].info.dp_qos_platform_set(QOS_LEVEL_GET,
+								cfg, flag);
 }
 EXPORT_SYMBOL(dp_qos_level_get);
 
@@ -236,8 +236,8 @@ int dp_qos_get_q_logic(struct dp_qos_q_logic *cfg, int flag)
 		cfg->q_logic_id = cfg->q_id; /* For GRX500 */
 		return DP_SUCCESS;
 	}
-	return dp_port_prop[cfg->inst].info.
-		dp_qos_platform_set(QOS_Q_LOGIC, cfg, flag);
+	return dp_port_prop[cfg->inst].info.dp_qos_platform_set(QOS_Q_LOGIC,
+								cfg, flag);
 }
 EXPORT_SYMBOL(dp_qos_get_q_logic);
 
@@ -245,8 +245,8 @@ int dp_qos_global_info_get(struct dp_qos_cfg_info *info, int flag)
 {
 	if (!dp_port_prop[info->inst].info.dp_qos_platform_set)
 		return DP_FAILURE;
-	return dp_port_prop[info->inst].info.
-		dp_qos_platform_set(QOS_GLOBAL_CFG_GET, info, flag);
+	return dp_port_prop[info->inst].info.dp_qos_platform_set(
+					QOS_GLOBAL_CFG_GET, info, flag);
 }
 EXPORT_SYMBOL(dp_qos_global_info_get);
 
@@ -254,7 +254,7 @@ int dp_qos_port_conf_set(struct dp_port_cfg_info *info, int flag)
 {
 	if (!dp_port_prop[info->inst].info.dp_qos_platform_set)
 		return DP_FAILURE;
-	return dp_port_prop[info->inst].info.
-		dp_qos_platform_set(QOS_PORT_CFG_SET, info, flag);
+	return dp_port_prop[info->inst].info.dp_qos_platform_set(
+					QOS_PORT_CFG_SET, info, flag);
 }
 EXPORT_SYMBOL(dp_qos_port_conf_set);
diff --git a/drivers/net/datapath/dpm/datapath_swdev.c b/drivers/net/datapath/dpm/datapath_swdev.c
index 782137186061..e063f7991249 100644
--- a/drivers/net/datapath/dpm/datapath_swdev.c
+++ b/drivers/net/datapath/dpm/datapath_swdev.c
@@ -74,12 +74,11 @@ int dp_get_fid_by_brname(struct net_device *dev, int *inst)
 	struct br_info *br_info;
 
 	br_info = dp_swdev_bridge_entry_lookup(dev->name);
-	if (!br_info) {
+	if (!br_info)
 		return -1;
-	} else {
-		*inst = br_info->inst;
-		return br_info->fid;
-	}
+
+	*inst = br_info->inst;
+	return br_info->fid;
 }
 EXPORT_SYMBOL(dp_get_fid_by_brname);
 
@@ -254,15 +253,14 @@ static int dp_swdev_clr_gswip_cfg(struct bridge_id_entry_item *br_item,
 	if (!br_info)
 		return 0;
 	if (dp_swdev_del_bport_from_list(br_info, br_item->portid)) {
-		if (get_dp_port_info(br_item->inst, br_item->dp_port)->
-							swdev_en == 1) {
-			ret = dp_port_prop[br_item->inst].info.
-				swdev_bridge_port_cfg_reset(br_info,
-							    br_item->inst,
-							    br_item->portid);
+		if (get_dp_port_info(br_item->inst,
+				     br_item->dp_port)->swdev_en == 1) {
+			ret = dp_port_prop[br_item->inst].info.swdev_bridge_port_cfg_reset(
+							br_info, br_item->inst,
+							br_item->portid);
 			if (ret == DEL_BRENTRY) {
-				dp_port_prop[br_item->inst].info.
-					swdev_free_brcfg(br_item->inst, br_item->fid);
+				dp_port_prop[br_item->inst].info.swdev_free_brcfg(
+						br_item->inst, br_item->fid);
 				dp_swdev_remove_bridge_id_entry(br_info);
 				DP_DEBUG(DP_DBG_FLAG_SWDEV,
 					 "rem bport(%d),bridge(%s)\n",
@@ -355,17 +353,16 @@ static int dp_swdev_cfg_gswip(struct bridge_id_entry_item *br_item, u8 *addr)
 
 	DP_DEBUG(DP_DBG_FLAG_SWDEV, "britem flags:%x\n", br_item->flags);
 	if (br_item->flags & ADD_BRENTRY) {
-		if (get_dp_port_info(br_item->inst, br_item->dp_port)->
-							swdev_en == 0) {
+		if (get_dp_port_info(br_item->inst,
+				     br_item->dp_port)->swdev_en == 0) {
 			DP_DEBUG(DP_DBG_FLAG_SWDEV, "swdev disable for bp %d\n",
 				 br_item->portid);
 			return 0;
 		}
 		DP_DEBUG(DP_DBG_FLAG_SWDEV, "Add br entry %s\n",
 			 br_item->br_device_name);
-		if ((dp_port_prop[br_item->inst].info.
-			swdev_bridge_cfg_set(br_item->inst,
-					     br_item->fid) == 0)) {
+		if ((dp_port_prop[br_item->inst].info.swdev_bridge_cfg_set(
+				br_item->inst, br_item->fid) == 0)) {
 			br_info = kmalloc(sizeof(*br_info), GFP_KERNEL);
 			if (!br_info) {
 				PR_ERR
@@ -383,19 +380,18 @@ static int dp_swdev_cfg_gswip(struct bridge_id_entry_item *br_item, u8 *addr)
 			 * the logical i.e. VLAN device.Helpful during
 			 * br/bport delete
 			 */
-			if (br_item->flags & LOGIC_DEV_REGISTER) {
+			if (br_item->flags & LOGIC_DEV_REGISTER)
 				br_info->flag = LOGIC_DEV_REGISTER;
-			}
+
 			strcpy(br_info->br_device_name,
 			       br_item->br_device_name);
 			INIT_LIST_HEAD(&br_info->bp_list);
 			dp_swdev_insert_bridge_id_entry(br_info);
 			dp_swdev_add_bport_to_list(br_info,
 						   br_item->portid);
-			dp_port_prop[br_item->inst].info.
-				swdev_bridge_port_cfg_set(br_info,
-							  br_item->inst,
-							  br_item->portid);
+			dp_port_prop[br_item->inst].info.swdev_bridge_port_cfg_set(
+							br_info, br_item->inst,
+							br_item->portid);
 			br_item->flags &= ~ADD_BRENTRY;
 			DP_DEBUG(DP_DBG_FLAG_SWDEV,
 				 "added bport(%d),bridge(%s)\n",
@@ -411,12 +407,11 @@ static int dp_swdev_cfg_gswip(struct bridge_id_entry_item *br_item, u8 *addr)
 		if (br_item->flags & LOGIC_DEV_REGISTER)
 			br_info->flag = LOGIC_DEV_REGISTER;
 		dp_swdev_add_bport_to_list(br_info, br_item->portid);
-		if (get_dp_port_info(br_item->inst, br_item->dp_port)->
-							swdev_en == 1) {
-			dp_port_prop[br_item->inst].info.
-				swdev_bridge_port_cfg_set(br_info,
-							  br_item->inst,
-							  br_item->portid);
+		if (get_dp_port_info(br_item->inst,
+				     br_item->dp_port)->swdev_en == 1) {
+			dp_port_prop[br_item->inst].info.swdev_bridge_port_cfg_set(
+							br_info, br_item->inst,
+							br_item->portid);
 		}
 		DP_DEBUG(DP_DBG_FLAG_SWDEV, "added bport(%d)\n",
 			 br_item->portid);
@@ -468,9 +463,10 @@ static int dp_swdev_add_if(struct net_device *dev,
 				port = subif.port_id;
 				inst = subif.inst;
 				subif.subif = -1;
-				if (dp_register_subif(get_dp_port_info(inst, port)->owner,
-						      dev, dev->name, &subif,
-						      DP_F_SUBIF_LOGICAL)) {
+				if (dp_register_subif(
+					get_dp_port_info(inst, port)->owner,
+					dev, dev->name, &subif,
+					DP_F_SUBIF_LOGICAL)) {
 					PR_ERR("dp_register_subif fail: %s\n",
 					       dev->name);
 					return -EOPNOTSUPP;
@@ -500,14 +496,15 @@ static int dp_swdev_add_if(struct net_device *dev,
 			br_item->flags = flag;
 		} else {
 			br_item->flags = ADD_BRENTRY | flag;
-			if (get_dp_port_info(br_item->inst, br_item->dp_port)->
-			    swdev_en == 1) {
-				br_id = dp_port_prop[br_item->inst].info.
-					swdev_alloc_bridge_id(br_item->inst);
+			if (get_dp_port_info(br_item->inst,
+					     br_item->dp_port)->swdev_en == 1) {
+				br_id = dp_port_prop[br_item->inst].info.swdev_alloc_bridge_id(
+								br_item->inst);
 				if (br_id) {
-					/* Store bridge information to add in the table.
-					 * This
-					 * info is used during switchdev commit phase
+					/* Store bridge information
+					 * to add in the table.
+					 * This info is used during
+					 * switchdev commit phase
 					 */
 					strcpy(br_item->br_device_name,
 					       br_dev->name);
@@ -633,9 +630,10 @@ static int dp_swdev_del_if(struct net_device *dev,
 			}
 			port = subif.port_id;
 			inst = subif.inst;
-			if (dp_register_subif(get_dp_port_info(inst, port)->owner,
-						dev, dev->name, &subif,
-						DP_F_DEREGISTER)) {
+			if (dp_register_subif(
+					get_dp_port_info(inst, port)->owner,
+					dev, dev->name, &subif,
+					DP_F_DEREGISTER)) {
 				PR_ERR("dp_register_subif fail: %s\n",
 				       dev->name);
 				/*Cannot Return -EOPNOTSUPP
diff --git a/drivers/net/datapath/dpm/gswip30/datapath_coc.c b/drivers/net/datapath/dpm/gswip30/datapath_coc.c
index b8ef4045feaf..b3566e3e8038 100644
--- a/drivers/net/datapath/dpm/gswip30/datapath_coc.c
+++ b/drivers/net/datapath/dpm/gswip30/datapath_coc.c
@@ -397,16 +397,16 @@ int meter_set_default(void)
 
 	for (i = 0; i < PMAC_MAX_NUM; i++) {/*cp green setting to yellow/red*/
 		wred_p.nPortId = i;
-		gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-			     .QoS_WredPortCfgGet, gsw_handle,
-			     &wred_p);
+		gsw_core_api(
+			(dp_gsw_cb)gsw_handle->gsw_qos_ops.QoS_WredPortCfgGet,
+			gsw_handle, &wred_p);
 		wred_p.nYellow_Min = wred_p.nGreen_Min;
 		wred_p.nYellow_Max = wred_p.nGreen_Max;
 		wred_p.nRed_Min = wred_p.nGreen_Min;
 		wred_p.nRed_Max = wred_p.nGreen_Max;
-		gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-			     .QoS_WredPortCfgSet, gsw_handle,
-			     &wred_p);
+		gsw_core_api(
+			(dp_gsw_cb)gsw_handle->gsw_qos_ops.QoS_WredPortCfgSet,
+			gsw_handle, &wred_p);
 	}
 
 	/*#Enable Meter 0, configure the rate
@@ -434,8 +434,9 @@ int meter_set_default(void)
 		meter_port.nMeterId = meter_id;
 		meter_port.eDir = 1;
 		meter_port.nPortIngressId = i;
-		gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-			     .QoS_MeterPortAssign, gsw_handle, &meter_port);
+		gsw_core_api(
+			(dp_gsw_cb)gsw_handle->gsw_qos_ops.QoS_MeterPortAssign,
+			gsw_handle, &meter_port);
 	}
 
 	/*#Enable Port 0 Meter Based Flow control (Bit 2 MFCEN)
@@ -464,17 +465,17 @@ int meter_set_default(void)
 	memset(&wred_q, 0, sizeof(wred_q));
 	for (i = 0; i < 32; i++) {	/*copy green setting to yellow/red */
 		wred_q.nQueueId = i;
-		gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-			     .QoS_WredQueueCfgGet, gsw_handle,
-			     &wred_q);
+		gsw_core_api(
+			(dp_gsw_cb)gsw_handle->gsw_qos_ops.QoS_WredQueueCfgGet,
+			gsw_handle, &wred_q);
 
 		wred_q.nYellow_Min = wred_q.nGreen_Min;
 		wred_q.nYellow_Max = wred_q.nGreen_Max;
 		wred_q.nRed_Min = wred_q.nGreen_Min;
 		wred_q.nRed_Max = wred_q.nGreen_Max;
-		gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-			     .QoS_WredQueueCfgSet, gsw_handle,
-			     &wred_q);
+		gsw_core_api(
+			(dp_gsw_cb)gsw_handle->gsw_qos_ops.QoS_WredQueueCfgSet,
+			gsw_handle, &wred_q);
 	}
 
 	/*Configure Red and Yellow watermark for each queue (Yellow and Red
@@ -514,9 +515,8 @@ static void dp_rmon_polling(unsigned long data)
 	for (i = 0; i < PMAC_MAX_NUM; i++) {
 		memset(&curr, 0, sizeof(curr));
 		curr.nPortId = i;
-		gsw_core_api((dp_gsw_cb)gsw_handle->
-			     gsw_rmon_ops.RMON_Port_Get,
-				gsw_handle, &curr);
+		gsw_core_api((dp_gsw_cb)gsw_handle->gsw_rmon_ops.RMON_Port_Get,
+			     gsw_handle, &curr);
 
 		coc_lock();
 		/*wrapround handling */
diff --git a/drivers/net/datapath/dpm/gswip30/datapath_gswip.c b/drivers/net/datapath/dpm/gswip30/datapath_gswip.c
index 959f8fa53dd2..74ed0b53a0c7 100644
--- a/drivers/net/datapath/dpm/gswip30/datapath_gswip.c
+++ b/drivers/net/datapath/dpm/gswip30/datapath_gswip.c
@@ -64,8 +64,9 @@ int dp_pmac_set_30(int inst, u32 port, dp_pmac_cfg_t *pmac_cfg)
 				igcfg.nTxDmaChanId =
 					pmac_cfg->ig_pmac.tx_dma_chan;
 			}
-			gsw_core_api((dp_gsw_cb)gswr_r->gsw_pmac_ops
-				     .Pmac_Ig_CfgGet, gswr_r, &igcfg);
+			gsw_core_api(
+				(dp_gsw_cb)gswr_r->gsw_pmac_ops.Pmac_Ig_CfgGet,
+				gswr_r, &igcfg);
 
 			/*update igcfg and write back to gsw */
 			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_ERR_DISC)
@@ -164,8 +165,9 @@ int dp_pmac_set_30(int inst, u32 port, dp_pmac_cfg_t *pmac_cfg)
 
 			DP_DEBUG(DP_DBG_FLAG_DBG, "\n");
 
-			gsw_core_api((dp_gsw_cb)gswr_r->gsw_pmac_ops
-				     .Pmac_Ig_CfgSet, gswr_r, &igcfg);
+			gsw_core_api(
+				(dp_gsw_cb)gswr_r->gsw_pmac_ops.Pmac_Ig_CfgSet,
+				gswr_r, &igcfg);
 		}
 
 		kfree(dqport.deq_info);
@@ -285,8 +287,9 @@ int dp_pmac_set_30(int inst, u32 port, dp_pmac_cfg_t *pmac_cfg)
 					 egcfg.bMpe2Flag);
 			}
 #endif
-			gsw_core_api((dp_gsw_cb)gswr_r->gsw_pmac_ops
-				     .Pmac_Eg_CfgSet, gswr_r, &egcfg);
+			gsw_core_api(
+				(dp_gsw_cb)gswr_r->gsw_pmac_ops.Pmac_Eg_CfgSet,
+				gswr_r, &egcfg);
 
 			;
 		}
@@ -308,8 +311,8 @@ int dp_set_gsw_parser_30(u8 flag, u8 cpu, u8 mpe1,
 	GSW_CPU_PortCfg_t param = {0};
 	struct core_ops *gsw_handle = dp_port_prop[0].ops[1]; /*pae*/
 
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_common_ops
-			 .CPU_PortCfgGet, gsw_handle, &param)) {
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_common_ops.CPU_PortCfgGet,
+			 gsw_handle, &param)) {
 		PR_ERR("Failed GSW_CPU_PORT_CFG_GET\n");
 		return -1;
 	}
@@ -331,8 +334,8 @@ int dp_set_gsw_parser_30(u8 flag, u8 cpu, u8 mpe1,
 	if (flag & F_MPE1_MPE2)
 		param.eMPE1MPE2ParserCfg = mpe3;
 
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_common_ops
-			 .CPU_PortCfgSet, gsw_handle, &param)) {
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_common_ops.CPU_PortCfgSet,
+			 gsw_handle, &param)) {
 		PR_ERR("Failed GSW_CPU_PORT_CFG_SET\n");
 		return -1;
 	}
@@ -349,8 +352,8 @@ int dp_get_gsw_parser_30(u8 *cpu, u8 *mpe1, u8 *mpe2,
 	GSW_CPU_PortCfg_t param = {0};
 	struct core_ops *gsw_handle = dp_port_prop[0].ops[1]; /*pae*/
 
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_common_ops
-			 .CPU_PortCfgGet, gsw_handle, &param)) {
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_common_ops.CPU_PortCfgGet,
+			 gsw_handle, &param)) {
 		PR_ERR("Failed GSW_CPU_PORT_CFG_GET\n");
 		return -1;
 	}
diff --git a/drivers/net/datapath/dpm/gswip30/datapath_mib.c b/drivers/net/datapath/dpm/gswip30/datapath_mib.c
index 94e3d6396949..80e6fac5ae55 100644
--- a/drivers/net/datapath/dpm/gswip30/datapath_mib.c
+++ b/drivers/net/datapath/dpm/gswip30/datapath_mib.c
@@ -451,8 +451,9 @@ static int get_gsw_port_rmon(u32 ep, char *gsw_drv_name,
 		return -1;
 	memset(mib, 0, sizeof(*mib));
 	mib->nPortId = ep;
-	ret = gsw_core_api((dp_gsw_cb)dp_port_prop[0].ops[index]->gsw_rmon_ops
-			   .RMON_Port_Get, dp_port_prop[0].ops[index], mib);
+	ret = gsw_core_api(
+		(dp_gsw_cb)dp_port_prop[0].ops[index]->gsw_rmon_ops.RMON_Port_Get,
+		dp_port_prop[0].ops[index], mib);
 	if (ret) {
 		PR_ERR("GSW_RMON_PORT_GET failed(%d) from %s for port %d\n",
 		       ret, gsw_drv_name, ep);
@@ -473,8 +474,9 @@ static int get_gsw_redirect_rmon(u32 ep, int index,
 	}
 
 	memset(mib, 0, sizeof(*mib));
-	ret = gsw_core_api((dp_gsw_cb)dp_port_prop[0].ops[index]->gsw_rmon_ops
-			   .RMON_Redirect_Get, dp_port_prop[0].ops[index], mib);
+	ret = gsw_core_api(
+		(dp_gsw_cb)dp_port_prop[0].ops[index]->gsw_rmon_ops.RMON_Redirect_Get,
+		dp_port_prop[0].ops[index], mib);
 	if (ret) {
 		PR_ERR("GSW_RMON_REDIRECT_GET failed from %s\n",
 		       gsw_drv_name);
@@ -495,9 +497,9 @@ static int get_gsw_itf_rmon(u32 index, int index,
 	}
 	memset(mib, 0, sizeof(*mib));
 	mib->nIfId = index;
-	ret = gsw_core_api((dp_gsw_cb)dp_port_prop[0].ops[index]
-				 ->gsw_rmon_ops.RMON_If_Get,
-				 dp_port_prop[0].ops[index], mib);
+	ret = gsw_core_api(
+		(dp_gsw_cb)dp_port_prop[0].ops[index]->gsw_rmon_ops.RMON_If_Get,
+		dp_port_prop[0].ops[index], mib);
 	if (ret) {
 		PR_ERR
 		    ("GSW_RMON_PORT_GET GSW_RMON_IF_GET from %s: index %d\n",
@@ -541,8 +543,9 @@ int gsw_eth_wan_redirect_status(void)
 	q_cfg.nPortId = WAN_EP;
 	for (i = 0; i <= MAX_CLASS_NUM; i++) {
 		q_cfg.nTrafficClassId = i;
-		ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-				   .QoS_QueuePortGet, gsw_handle, &q_cfg);
+		ret = gsw_core_api(
+			(dp_gsw_cb)gsw_handle->gsw_qos_ops.QoS_QueuePortGet,
+			gsw_handle, &q_cfg);
 		if (ret) {
 			PR_ERR("%s failed(%d) from %s for port %d\n",
 			       "GSW_QOS_QUEUE_PORT_GET",
@@ -630,8 +633,7 @@ static int update_port_mib_lower_lvl(dp_subif_t *subif, u32 flag)
 						    port->vap_mask);
 				ret = tmu_hal_get_qos_m_local(NULL,
 							      &tmp, -1,
-							      &curr->
-								tmu_qos[i],
+							      &curr->tmu_qos[i],
 							      0);
 			}
 			if (ret) /*workaround */
diff --git a/drivers/net/datapath/dpm/gswip30/datapath_misc.c b/drivers/net/datapath/dpm/gswip30/datapath_misc.c
index 229a72f738ad..3a1693c74169 100644
--- a/drivers/net/datapath/dpm/gswip30/datapath_misc.c
+++ b/drivers/net/datapath/dpm/gswip30/datapath_misc.c
@@ -312,16 +312,13 @@ static void dump_tx_dma_desc(struct dma_tx_desc_0 *desc_0,
 		desc_3->field.byte_offset, desc_3->field.qid,
 		desc_3->field.mpoa_pt, desc_3->field.mpoa_mode,
 		desc_3->field.data_len);
-	lookup =
-	    ((desc_0->field.flow_id >> 6) << 12) | ((desc_1->field.
-						     dec) << 11) | ((desc_1->
-								     field.
-								     enc) <<
-								    10) |
-	    ((desc_1->field.mpe2) << 9) | ((desc_1->field.
-					    mpe1) << 8) | ((desc_1->field.
-							    ep) << 4) |
-	    ((desc_1->field.classid) << 0);
+	lookup = ((desc_0->field.flow_id >> 6) << 12) |
+		((desc_1->field.dec) << 11) |
+		((desc_1->field.enc) << 10) |
+		((desc_1->field.mpe2) << 9) |
+		((desc_1->field.mpe1) << 8) |
+		((desc_1->field.ep) << 4) |
+		((desc_1->field.classid) << 0);
 	PR_INFO("  lookup index=0x%x qid=%d\n", lookup,
 		get_lookup_qid_via_index(lookup));
 }
@@ -458,7 +455,7 @@ static int dp_platform_set(int inst, u32 flag)
 }
 
 static int dev_platform_set(int inst, u8 ep, struct dp_dev_data *data,
-			     u32 flags)
+			    u32 flags)
 {
 	return 0;
 }
diff --git a/drivers/net/datapath/dpm/gswip30/datapath_proc.c b/drivers/net/datapath/dpm/gswip30/datapath_proc.c
index 0aa479a2d9cc..9771d4083a29 100644
--- a/drivers/net/datapath/dpm/gswip30/datapath_proc.c
+++ b/drivers/net/datapath/dpm/gswip30/datapath_proc.c
@@ -264,10 +264,8 @@ static ssize_t proc_parser_write(struct file *file, const char *buf,
 			 "flag=0x%x mpe3/2/1/cpu=%d/%d/%d/%d\n", flag, mpe3,
 			 mpe2, mpe1, cpu);
 		dp_set_gsw_parser_30(flag, cpu, mpe1, mpe2, mpe3);
-	} else if (dp_strncmpi(param_list[0],
-				     "refresh",
-					 strlen("refresh"))
-					 == 0) {
+	} else if (dp_strncmpi(param_list[0], "refresh",
+			       strlen("refresh")) == 0) {
 		dp_get_gsw_parser_30(NULL, NULL, NULL, NULL);
 		PR_INFO("value:cpu=%d mpe1=%d mpe2=%d mpe3=%d\n", pinfo[0].v,
 			pinfo[1].v, pinfo[2].v, pinfo[3].v);
@@ -308,8 +306,9 @@ static ssize_t proc_parser_write(struct file *file, const char *buf,
 		pce.action.bRMON_Action = 1;
 		pce.action.nRMON_Id = 0;	/*RMON_UDP_CNTR; */
 
-		if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_tflow_ops
-				 .TFLOW_PceRuleWrite, gsw_handle, &pce)) {
+		if (gsw_core_api(
+			(dp_gsw_cb)gsw_handle->gsw_tflow_ops.TFLOW_PceRuleWrite,
+			gsw_handle, &pce)) {
 			PR_ERR("PCE rule add fail for GSW_PCE_RULE_WRITE\n");
 			return count;
 		}
@@ -322,8 +321,9 @@ static ssize_t proc_parser_write(struct file *file, const char *buf,
 		memset(&pce, 0, sizeof(pce));
 		pce.pattern.nIndex = pce_rule_id;
 		pce.pattern.bEnable = 0;
-		if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_tflow_ops
-				 .TFLOW_PceRuleWrite, gsw_handle, &pce)) {
+		if (gsw_core_api(
+			(dp_gsw_cb)gsw_handle->gsw_tflow_ops.TFLOW_PceRuleWrite,
+			gsw_handle, &pce)) {
 			PR_ERR("PCE rule add fail for GSW_PCE_RULE_WRITE\n");
 			return count;
 		}
@@ -692,9 +692,9 @@ static int proc_gsw_port_rmon_dump(struct seq_file *s, int pos)
 
 		for (i = 0; i < ARRAY_SIZE(gsw_r_rmon_mib); i++) {
 			gsw_r_rmon_mib[i].nPortId = i;
-			ret = gsw_core_api((dp_gsw_cb)gsw_handle
-					   ->gsw_rmon_ops.RMON_Port_Get,
-					   gsw_handle, &gsw_r_rmon_mib[i]);
+			ret = gsw_core_api(
+				(dp_gsw_cb)gsw_handle->gsw_rmon_ops.RMON_Port_Get,
+				gsw_handle, &gsw_r_rmon_mib[i]);
 
 			if (ret != GSW_statusOk) {
 				PR_ERR("RMON_PORT_GET fail for Port %d\n", i);
@@ -704,9 +704,9 @@ static int proc_gsw_port_rmon_dump(struct seq_file *s, int pos)
 
 		/*read pmac rmon redirect mib */
 		memset(&gswr_rmon_redirect, 0, sizeof(gswr_rmon_redirect));
-		ret = gsw_core_api((dp_gsw_cb)gsw_handle
-				   ->gsw_rmon_ops.RMON_Redirect_Get, gsw_handle,
-				   &gswr_rmon_redirect);
+		ret = gsw_core_api(
+			(dp_gsw_cb)gsw_handle->gsw_rmon_ops.RMON_Redirect_Get,
+			gsw_handle, &gswr_rmon_redirect);
 
 		if (ret != GSW_statusOk) {
 			PR_ERR("GSW_RMON_REDIRECT_GET fail for Port %d\n", i);
@@ -717,10 +717,9 @@ static int proc_gsw_port_rmon_dump(struct seq_file *s, int pos)
 		gsw_handle = dp_port_prop[0].ops[GSWIP_L];
 		for (i = 0; i < ARRAY_SIZE(gsw_l_rmon_mib); i++) {
 			gsw_l_rmon_mib[i].nPortId = i;
-			ret = gsw_core_api((dp_gsw_cb)gsw_handle
-					   ->gsw_rmon_ops.RMON_Port_Get,
-					   gsw_handle,
-					   &gsw_l_rmon_mib[i]);
+			ret = gsw_core_api(
+				(dp_gsw_cb)gsw_handle->gsw_rmon_ops.RMON_Port_Get,
+				gsw_handle, &gsw_l_rmon_mib[i]);
 			if (ret != GSW_statusOk) {
 				PR_ERR("RMON_PORT_GET fail for Port %d\n", i);
 				return -1;
@@ -1089,21 +1088,18 @@ static void pmac_eg_cfg(char *param_list[], int num, dp_pmac_cfg_t *pmac_cfg)
 				if (dp_atoi(param_list[i + 1]) > 0) {
 					pmac_cfg->eg_pmac.rm_l2hdr = 1;
 					value = dp_atoi(param_list[i + 1]);
-					egress_entries[j].
-					   egress_callback(pmac_cfg,
-							   value);
+					egress_entries[j].egress_callback(
+							pmac_cfg, value);
 					PR_INFO("egress pmac ep %s config ok\n",
-						egress_entries
-					     [j].name);
+						egress_entries[j].name);
 					break;
 				}
 				pmac_cfg->eg_pmac.rm_l2hdr =
 				    dp_atoi(param_list[i + 1]);
 			} else {
 				value = dp_atoi(param_list[i + 1]);
-				egress_entries[j].
-				    egress_callback(pmac_cfg,
-						    value);
+				egress_entries[j].egress_callback(pmac_cfg,
+								value);
 				PR_INFO("egress pmac ep %s configu ok\n",
 					egress_entries[j].name);
 				break;
@@ -1142,9 +1138,8 @@ ssize_t ep_port_write(struct file *file, const char *buf, size_t count,
 						strlen(ingress_entries[j].name))
 						== 0) {
 					value = dp_atoi(param_list[i + 1]);
-					ingress_entries[j].
-					    ingress_callback(&pmac_cfg,
-							     value);
+					ingress_entries[j].ingress_callback(
+							&pmac_cfg, value);
 					PR_INFO("ingress pmac ep %s configed\n",
 						ingress_entries[j].name);
 					break;
diff --git a/drivers/net/datapath/dpm/gswip30/datapath_rx.c b/drivers/net/datapath/dpm/gswip30/datapath_rx.c
index b7d99bbae1c9..eec5e3afdfad 100644
--- a/drivers/net/datapath/dpm/gswip30/datapath_rx.c
+++ b/drivers/net/datapath/dpm/gswip30/datapath_rx.c
@@ -18,9 +18,9 @@
 #endif
 
 static void rx_dbg(u32 f, struct sk_buff *skb, struct dma_rx_desc_0 *desc0,
-	       struct dma_rx_desc_1 *desc1, struct dma_rx_desc_2 *desc2,
-	       struct dma_rx_desc_3 *desc3, unsigned char *parser,
-	       struct pmac_rx_hdr *pmac, int paser_exist)
+		   struct dma_rx_desc_1 *desc1, struct dma_rx_desc_2 *desc2,
+		   struct dma_rx_desc_3 *desc3, unsigned char *parser,
+		   struct pmac_rx_hdr *pmac, int paser_exist)
 {
 	int inst = 0;
 
@@ -99,7 +99,7 @@ static int dp_handle_lct(struct pmac_port_info *dp_port,
 	if (skb->data[PMAC_SIZE] & 0x1) {
 		/* multicast/broadcast */
 		DP_DEBUG(DP_DBG_FLAG_PAE, "LCT mcast or broadcast\n");
-		if((STATS_GET(sif->rx_flag) <= 0)) {
+		if ((STATS_GET(sif->rx_flag) <= 0)) {
 			UP_STATS(mib->rx_fn_dropped);
 			return 1;
 		}
@@ -119,8 +119,8 @@ static int dp_handle_lct(struct pmac_port_info *dp_port,
 		/* unicast */
 		DP_DEBUG(DP_DBG_FLAG_PAE, "LCT unicast\n");
 		DP_DEBUG(DP_DBG_FLAG_PAE, "unicast pkt sent lct(%s) ret(%d)\n",
-				 skb->dev->name ? skb->dev->name : "NULL", ret);
-		if((STATS_GET(sif->rx_flag) <= 0)) {
+			 skb->dev->name ? skb->dev->name : "NULL", ret);
+		if ((STATS_GET(sif->rx_flag) <= 0)) {
 			UP_STATS(mib->rx_fn_dropped);
 			dev_kfree_skb_any(skb);
 			return 0;
@@ -290,7 +290,7 @@ int32_t dp_rx_30(struct sk_buff *skb, u32 flags)
 			if (dp_port->lct_idx > 0)
 				ret_lct = dp_handle_lct(dp_port, skb, rx_fn);
 			if (ret_lct) {
-				if((STATS_GET(sif->rx_flag) <= 0)) {
+				if ((STATS_GET(sif->rx_flag) <= 0)) {
 					UP_STATS(mib->rx_fn_dropped);
 					goto RX_DROP2;
 				}
@@ -298,7 +298,7 @@ int32_t dp_rx_30(struct sk_buff *skb, u32 flags)
 				UP_STATS(mib->rx_fn_rxif_pkt);
 			}
 		} else {
-			if((STATS_GET(sif->rx_flag) <= 0)) {
+			if ((STATS_GET(sif->rx_flag) <= 0)) {
 				UP_STATS(mib->rx_fn_dropped);
 				goto RX_DROP2;
 			}
@@ -334,5 +334,3 @@ int32_t dp_rx_30(struct sk_buff *skb, u32 flags)
 		dev_kfree_skb_any(skb);
 	return res;
 }
-
-
diff --git a/drivers/net/datapath/dpm/gswip30/datapath_tx.c b/drivers/net/datapath/dpm/gswip30/datapath_tx.c
index 30343dca192c..aa3c734e70ef 100644
--- a/drivers/net/datapath/dpm/gswip30/datapath_tx.c
+++ b/drivers/net/datapath/dpm/gswip30/datapath_tx.c
@@ -109,331 +109,330 @@ void dp_xmit_dbg(
 int32_t dp_xmit_30(struct net_device *rx_if, dp_subif_t *rx_subif,
 		   struct sk_buff *skb, int32_t len, uint32_t flags)
 {
-		struct dma_tx_desc_0 *desc_0;
-		struct dma_tx_desc_1 *desc_1;
-		struct dma_tx_desc_2 *desc_2;
-		struct dma_tx_desc_3 *desc_3;
-		struct pmac_port_info *dp_info;
-		struct pmac_tx_hdr pmac = {0};
-		u32 ip_offset, tcp_h_offset, tcp_type;
-		char tx_chksum_flag = 0; /*check csum cal can be supported or not */
-		char insert_pmac_f = 1; /*flag to insert one pmac */
-		int res = DP_SUCCESS;
-		int ep, vap;
-		enum dp_xmit_errors err_ret = 0;
-		int inst = 0;
-		struct cbm_tx_data data;
+	struct dma_tx_desc_0 *desc_0;
+	struct dma_tx_desc_1 *desc_1;
+	struct dma_tx_desc_2 *desc_2;
+	struct dma_tx_desc_3 *desc_3;
+	struct pmac_port_info *dp_info;
+	struct pmac_tx_hdr pmac = {0};
+	u32 ip_offset, tcp_h_offset, tcp_type;
+	char tx_chksum_flag = 0; /*check csum cal can be supported or not */
+	char insert_pmac_f = 1; /*flag to insert one pmac */
+	int res = DP_SUCCESS;
+	int ep, vap;
+	enum dp_xmit_errors err_ret = 0;
+	int inst = 0;
+	struct cbm_tx_data data;
 #if IS_ENABLED(CONFIG_INTEL_DATAPATH_PTP1588)
-		struct mac_ops *ops;
-		int rec_id = 0;
+	struct mac_ops *ops;
+	int rec_id = 0;
 #endif
-		struct dev_mib *mib;
-	
+	struct dev_mib *mib;
+
 #if IS_ENABLED(CONFIG_INTEL_DATAPATH_EXTRA_DEBUG)
-		if (unlikely(!dp_init_ok)) {
-			err_ret = DP_XMIT_ERR_NOT_INIT;
-			goto lbl_err_ret;
-		}
-		if (unlikely(!rx_subif)) {
-			err_ret = DP_XMIT_ERR_NULL_SUBIF;
-			goto lbl_err_ret;
-		}
-		if (unlikely(!skb)) {
-			err_ret = DP_XMIT_ERR_NULL_SKB;
-			goto lbl_err_ret;
-		}
+	if (unlikely(!dp_init_ok)) {
+		err_ret = DP_XMIT_ERR_NOT_INIT;
+		goto lbl_err_ret;
+	}
+	if (unlikely(!rx_subif)) {
+		err_ret = DP_XMIT_ERR_NULL_SUBIF;
+		goto lbl_err_ret;
+	}
+	if (unlikely(!skb)) {
+		err_ret = DP_XMIT_ERR_NULL_SKB;
+		goto lbl_err_ret;
+	}
 #endif
-		ep = rx_subif->port_id;
-		if (unlikely(ep >= dp_port_prop[inst].info.cap.max_num_dp_ports)) {
-			err_ret = DP_XMIT_ERR_PORT_TOO_BIG;
-			goto lbl_err_ret;
-		}
+	ep = rx_subif->port_id;
+	if (unlikely(ep >= dp_port_prop[inst].info.cap.max_num_dp_ports)) {
+		err_ret = DP_XMIT_ERR_PORT_TOO_BIG;
+		goto lbl_err_ret;
+	}
 #if IS_ENABLED(CONFIG_INTEL_DATAPATH_EXTRA_DEBUG)
-		if (unlikely(in_irq())) {
-			err_ret = DP_XMIT_ERR_IN_IRQ;
-			goto lbl_err_ret;
-		}
+	if (unlikely(in_irq())) {
+		err_ret = DP_XMIT_ERR_IN_IRQ;
+		goto lbl_err_ret;
+	}
 #endif
-		dp_info = get_dp_port_info(inst, ep);
-		vap = GET_VAP(rx_subif->subif, dp_info->vap_offset, dp_info->vap_mask);
-		mib = get_dp_port_subif_mib(get_dp_port_subif(dp_info, vap));
-		if (unlikely(!rx_if && /*For atm pppoa case, rx_if is NULL now */
-			     !(dp_info->alloc_flags & DP_F_FAST_DSL))) {
-			err_ret = DP_XMIT_ERR_NULL_IF;
-			goto lbl_err_ret;
-		}
+	dp_info = get_dp_port_info(inst, ep);
+	vap = GET_VAP(rx_subif->subif, dp_info->vap_offset, dp_info->vap_mask);
+	mib = get_dp_port_subif_mib(get_dp_port_subif(dp_info, vap));
+	if (unlikely(!rx_if && /*For atm pppoa case, rx_if is NULL now */
+			!(dp_info->alloc_flags & DP_F_FAST_DSL))) {
+		err_ret = DP_XMIT_ERR_NULL_IF;
+		goto lbl_err_ret;
+	}
 #if IS_ENABLED(CONFIG_INTEL_DATAPATH_MPE_FASTHOOK_TEST)
-		if (unlikely(ltq_mpe_fasthook_tx_fn))
-			ltq_mpe_fasthook_tx_fn(skb, 0, NULL);
+	if (unlikely(ltq_mpe_fasthook_tx_fn))
+		ltq_mpe_fasthook_tx_fn(skb, 0, NULL);
 #endif
-		if (unlikely(dp_dbg_flag))
-			dp_xmit_dbg("\nOrig", skb, ep, len, flags,
-				    NULL, rx_subif, 0, 0, flags & DP_TX_CAL_CHKSUM);
-	
-		/*No PMAC for WAVE500 and DSL by default except bonding case */
-		if (unlikely(NO_NEED_PMAC(dp_info->alloc_flags)))
-			insert_pmac_f = 0;
-	
-		/**********************************************
-		 *Must put these 4 lines after INSERT_PMAC
-		 *since INSERT_PMAC will change skb if needed
-		 *********************************************/
+	if (unlikely(dp_dbg_flag))
+		dp_xmit_dbg("\nOrig", skb, ep, len, flags,
+			    NULL, rx_subif, 0, 0, flags & DP_TX_CAL_CHKSUM);
+
+	/*No PMAC for WAVE500 and DSL by default except bonding case */
+	if (unlikely(NO_NEED_PMAC(dp_info->alloc_flags)))
+		insert_pmac_f = 0;
+
+	/**********************************************
+	 *Must put these 4 lines after INSERT_PMAC
+	 *since INSERT_PMAC will change skb if needed
+	 *********************************************/
 #ifdef DP_SKB_HACK
-		desc_0 = (struct dma_tx_desc_0 *)&skb->DW0;
-		desc_1 = (struct dma_tx_desc_1 *)&skb->DW1;
-		desc_2 = (struct dma_tx_desc_2 *)&skb->DW2;
-		desc_3 = (struct dma_tx_desc_3 *)&skb->DW3;
+	desc_0 = (struct dma_tx_desc_0 *)&skb->DW0;
+	desc_1 = (struct dma_tx_desc_1 *)&skb->DW1;
+	desc_2 = (struct dma_tx_desc_2 *)&skb->DW2;
+	desc_3 = (struct dma_tx_desc_3 *)&skb->DW3;
 #endif
-		if (flags & DP_TX_CAL_CHKSUM) {
-			int ret_flg;
-	
-			if (!dp_port_prop[inst].info.check_csum_cap()) {
-				err_ret = DP_XMIT_ERR_CSM_NO_SUPPORT;
-				goto lbl_err_ret;
-			}
-			ret_flg = get_offset_clear_chksum(skb, &ip_offset,
-							  &tcp_h_offset, &tcp_type);
-			if (likely(ret_flg == 0))
-				/*HW can support checksum offload*/
-				tx_chksum_flag = 1;
+	if (flags & DP_TX_CAL_CHKSUM) {
+		int ret_flg;
+
+		if (!dp_port_prop[inst].info.check_csum_cap()) {
+			err_ret = DP_XMIT_ERR_CSM_NO_SUPPORT;
+			goto lbl_err_ret;
+		}
+		ret_flg = get_offset_clear_chksum(skb, &ip_offset,
+						  &tcp_h_offset, &tcp_type);
+		if (likely(ret_flg == 0))
+			/*HW can support checksum offload*/
+			tx_chksum_flag = 1;
 #if IS_ENABLED(CONFIG_INTEL_DATAPATH_EXTRA_DEBUG)
-			else if (ret_flg == -1)
-				pr_info_once("packet can't do hw checksum\n");
+		else if (ret_flg == -1)
+			pr_info_once("packet can't do hw checksum\n");
 #endif
-		}
-	
-		/*reset all descriptors as SWAS required since SWAS 3.7 */
-		/*As new SWAS 3.7 required, MPE1/Color/FlowID is set by applications */
-		desc_0->all &= dma_tx_desc_mask0.all;
-		desc_1->all &= dma_tx_desc_mask1.all;
-		/*desc_2->all = 0;*/ /*remove since later it will be set properly */
-		if (desc_3->field.dic) {
-			desc_3->all = 0; /*keep DIC bit to support test tool*/
-			desc_3->field.dic = 1;
+	}
+
+	/*reset all descriptors as SWAS required since SWAS 3.7 */
+	/*As new SWAS 3.7 required, MPE1/Color/FlowID is set by applications */
+	desc_0->all &= dma_tx_desc_mask0.all;
+	desc_1->all &= dma_tx_desc_mask1.all;
+	/*desc_2->all = 0;*/ /*remove since later it will be set properly */
+	if (desc_3->field.dic) {
+		desc_3->all = 0; /*keep DIC bit to support test tool*/
+		desc_3->field.dic = 1;
+	} else {
+		desc_3->all = 0;
+	}
+
+	if (flags & DP_TX_OAM) /* OAM */
+		desc_3->field.pdu_type = 1;
+	desc_1->field.classid = (skb->priority >= 15) ? 15 : skb->priority;
+	desc_2->field.data_ptr = (uint32_t)skb->data;
+
+	/*for ETH LAN/WAN */
+	if (dp_info->alloc_flags & (DP_F_FAST_ETH_LAN | DP_F_FAST_ETH_WAN |
+		DP_F_GPON | DP_F_EPON)) {
+		/*always with pmac*/
+		if (likely(tx_chksum_flag)) {
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_CHECKSUM, &pmac,
+							desc_0, desc_1,
+							dp_info);
+			set_chksum(&pmac, tcp_type, ip_offset,
+				   ip_offset_hw_adjust, tcp_h_offset);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
 		} else {
-			desc_3->all = 0;
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_NORMAL, &pmac,
+							desc_0, desc_1,
+							dp_info);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
 		}
-	
-		if (flags & DP_TX_OAM) /* OAM */
-			desc_3->field.pdu_type = 1;
-		desc_1->field.classid = (skb->priority >= 15) ? 15 : skb->priority;
-		desc_2->field.data_ptr = (uint32_t)skb->data;
-	
-		/*for ETH LAN/WAN */
-		if (dp_info->alloc_flags & (DP_F_FAST_ETH_LAN | DP_F_FAST_ETH_WAN |
-		    DP_F_GPON | DP_F_EPON)) {
-			/*always with pmac*/
-			if (likely(tx_chksum_flag)) {
-				DP_CB(inst, get_dma_pmac_templ)(TEMPL_CHECKSUM, &pmac,
-								desc_0, desc_1,
-								dp_info);
-				set_chksum(&pmac, tcp_type, ip_offset,
-					   ip_offset_hw_adjust, tcp_h_offset);
-				DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
-			} else {
-				DP_CB(inst, get_dma_pmac_templ)(TEMPL_NORMAL, &pmac,
-								desc_0, desc_1,
-								dp_info);
-				DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
-			}
 #if IS_ENABLED(CONFIG_INTEL_DATAPATH_PTP1588)
 #if IS_ENABLED(CONFIG_INTEL_DATAPATH_PTP1588_SW_WORKAROUND)
-			if (dp_info->f_ptp)
+		if (dp_info->f_ptp)
 #else
-			if (dp_info->f_ptp &&
-			    (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP))
+		if (dp_info->f_ptp &&
+			(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP))
 #endif
 		{
 			ops = dp_port_prop[inst].mac_ops[dp_info->port_id];
-				if (!ops) {
-					err_ret = DP_XMIT_PTP_ERR;
-					goto lbl_err_ret;
-				}
-				rec_id = ops->do_tx_hwts(ops, skb);
-				if (rec_id < 0) {
-					err_ret = DP_XMIT_PTP_ERR;
-					goto lbl_err_ret;
-				}
-				DP_CB(inst, get_dma_pmac_templ)(TEMPL_PTP, &pmac,
-								desc_0, desc_1,
-								dp_info);
-				pmac.record_id_msb = rec_id;
+			if (!ops) {
+				err_ret = DP_XMIT_PTP_ERR;
+				goto lbl_err_ret;
 			}
+			rec_id = ops->do_tx_hwts(ops, skb);
+			if (rec_id < 0) {
+				err_ret = DP_XMIT_PTP_ERR;
+				goto lbl_err_ret;
+			}
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_PTP, &pmac,
+							desc_0, desc_1,
+							dp_info);
+			pmac.record_id_msb = rec_id;
+		}
 #endif
-		} else if (dp_info->alloc_flags & DP_F_FAST_DSL) { /*some with pmac*/
-			if (unlikely(flags & DP_TX_CAL_CHKSUM)) { /* w/ pmac*/
-				DP_CB(inst, get_dma_pmac_templ)(TEMPL_CHECKSUM, &pmac,
-								desc_0, desc_1,
-								dp_info);
-				set_chksum(&pmac, tcp_type, ip_offset,
-					   ip_offset_hw_adjust, tcp_h_offset);
-				DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
+	} else if (dp_info->alloc_flags & DP_F_FAST_DSL) { /*some with pmac*/
+		if (unlikely(flags & DP_TX_CAL_CHKSUM)) { /* w/ pmac*/
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_CHECKSUM, &pmac,
+							desc_0, desc_1,
+							dp_info);
+			set_chksum(&pmac, tcp_type, ip_offset,
+				   ip_offset_hw_adjust, tcp_h_offset);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
 #if IS_ENABLED(CONFIG_INTEL_DATAPATH_ACA_CSUM_WORKAROUND)
-				if (aca_portid > 0)
-					desc_1->field.ep = aca_portid;
+			if (aca_portid > 0)
+				desc_1->field.ep = aca_portid;
 #endif
-			} else if (flags & DP_TX_DSL_FCS) {/* after checksum check */
-				/* w/ pmac for FCS purpose*/
-				DP_CB(inst, get_dma_pmac_templ)(TEMPL_OTHERS, &pmac,
-								desc_0, desc_1,
-								dp_info);
-				DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
-				insert_pmac_f = 1;
+		} else if (flags & DP_TX_DSL_FCS) {/* after checksum check */
+			/* w/ pmac for FCS purpose*/
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_OTHERS, &pmac,
+							desc_0, desc_1,
+							dp_info);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
+			insert_pmac_f = 1;
 #if IS_ENABLED(CONFIG_INTEL_DATAPATH_ACA_CSUM_WORKAROUND)
-				if (aca_portid > 0)
-					desc_1->field.ep = aca_portid;
+			if (aca_portid > 0)
+				desc_1->field.ep = aca_portid;
 #endif
-			} else { /*no pmac */
-				DP_CB(inst, get_dma_pmac_templ)(TEMPL_NORMAL, NULL,
-								desc_0, desc_1,
-								dp_info);
-			}
-		} else if (dp_info->alloc_flags & DP_F_FAST_WLAN) {/*some with pmac*/
-			/*normally no pmac. But if need checksum, need pmac*/
-			if (unlikely(tx_chksum_flag)) { /*with pmac*/
-				DP_CB(inst, get_dma_pmac_templ)(TEMPL_CHECKSUM, &pmac,
-								desc_0, desc_1,
-								dp_info);
-				set_chksum(&pmac, tcp_type, ip_offset,
-					   ip_offset_hw_adjust, tcp_h_offset);
-				DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
+		} else { /*no pmac */
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_NORMAL, NULL,
+							desc_0, desc_1,
+							dp_info);
+		}
+	} else if (dp_info->alloc_flags & DP_F_FAST_WLAN) {/*some with pmac*/
+		/*normally no pmac. But if need checksum, need pmac*/
+		if (unlikely(tx_chksum_flag)) { /*with pmac*/
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_CHECKSUM, &pmac,
+							desc_0, desc_1,
+							dp_info);
+			set_chksum(&pmac, tcp_type, ip_offset,
+				   ip_offset_hw_adjust, tcp_h_offset);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
 #if IS_ENABLED(CONFIG_INTEL_DATAPATH_ACA_CSUM_WORKAROUND)
-				if (aca_portid > 0)
-					desc_1->field.ep = aca_portid;
+			if (aca_portid > 0)
+				desc_1->field.ep = aca_portid;
 #endif
-			} else { /*no pmac*/
-				DP_CB(inst, get_dma_pmac_templ)(TEMPL_NORMAL, NULL,
-								desc_0, desc_1,
-								dp_info);
-			}
-		} else if (dp_info->alloc_flags & DP_F_DIRECTLINK) { /*always w/ pmac*/
-			if (unlikely(flags & DP_TX_CAL_CHKSUM)) { /* w/ pmac*/
-				DP_CB(inst, get_dma_pmac_templ)(TEMPL_CHECKSUM, &pmac,
-								desc_0, desc_1,
-								dp_info);
-				set_chksum(&pmac, tcp_type, ip_offset,
-					   ip_offset_hw_adjust, tcp_h_offset);
-				DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
-			} else if (flags & DP_TX_TO_DL_MPEFW) { /*w/ pmac*/
-				/*copy from checksum's pmac template setting,
-				 *but need to reset tcp_chksum in TCP header
-				 */
-				DP_CB(inst, get_dma_pmac_templ)(TEMPL_OTHERS, &pmac,
-								desc_0, desc_1,
-								dp_info);
-				DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
-			} else { /*do like normal directpath with pmac */
-				DP_CB(inst, get_dma_pmac_templ)(TEMPL_NORMAL, &pmac,
-								desc_0, desc_1,
-								dp_info);
-				DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
-			}
-		} else { /*normal directpath: always w/ pmac */
-			if (unlikely(tx_chksum_flag)) {
-				DP_CB(inst, get_dma_pmac_templ)(TEMPL_CHECKSUM,
-								&pmac,
-								desc_0,
-								desc_1,
-								dp_info);
-				set_chksum(&pmac, tcp_type, ip_offset,
-					   ip_offset_hw_adjust, tcp_h_offset);
-				DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
-			} else { /*w/ pmac */
-				DP_CB(inst, get_dma_pmac_templ)(TEMPL_NORMAL, &pmac,
-								desc_0, desc_1,
-								dp_info);
-				DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
-			}
+		} else { /*no pmac*/
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_NORMAL, NULL,
+							desc_0, desc_1,
+							dp_info);
 		}
-		desc_3->field.data_len = skb->len;
-	
-		if (unlikely(dp_dbg_flag)) {
-			if (insert_pmac_f)
-				dp_xmit_dbg("After", skb, ep, len, flags, &pmac,
-					    rx_subif, insert_pmac_f, skb_is_gso(skb),
-					    tx_chksum_flag);
-			else
-				dp_xmit_dbg("After", skb, ep, len, flags, NULL,
-					    rx_subif, insert_pmac_f, skb_is_gso(skb),
-					    tx_chksum_flag);
+	} else if (dp_info->alloc_flags & DP_F_DIRECTLINK) { /*always w/ pmac*/
+		if (unlikely(flags & DP_TX_CAL_CHKSUM)) { /* w/ pmac*/
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_CHECKSUM, &pmac,
+							desc_0, desc_1,
+							dp_info);
+			set_chksum(&pmac, tcp_type, ip_offset,
+				   ip_offset_hw_adjust, tcp_h_offset);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
+		} else if (flags & DP_TX_TO_DL_MPEFW) { /*w/ pmac*/
+			/*copy from checksum's pmac template setting,
+			 *but need to reset tcp_chksum in TCP header
+			 */
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_OTHERS, &pmac,
+							desc_0, desc_1,
+							dp_info);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
+		} else { /*do like normal directpath with pmac */
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_NORMAL, &pmac,
+							desc_0, desc_1,
+							dp_info);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
 		}
-	
-#if IS_ENABLED(CONFIG_LTQ_TOE_DRIVER)
-		if (skb_is_gso(skb)) {
-			res = ltq_tso_xmit(skb, &pmac, sizeof(pmac), 0);
-			UP_STATS(mib->tx_tso_pkt);
-			return res;
+	} else { /*normal directpath: always w/ pmac */
+		if (unlikely(tx_chksum_flag)) {
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_CHECKSUM,
+							&pmac,
+							desc_0,
+							desc_1,
+							dp_info);
+			set_chksum(&pmac, tcp_type, ip_offset,
+				   ip_offset_hw_adjust, tcp_h_offset);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
+		} else { /*w/ pmac */
+			DP_CB(inst, get_dma_pmac_templ)(TEMPL_NORMAL, &pmac,
+							desc_0, desc_1,
+							dp_info);
+			DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
 		}
+	}
+	desc_3->field.data_len = skb->len;
+
+	if (unlikely(dp_dbg_flag)) {
+		if (insert_pmac_f)
+			dp_xmit_dbg("After", skb, ep, len, flags, &pmac,
+				    rx_subif, insert_pmac_f, skb_is_gso(skb),
+				    tx_chksum_flag);
+		else
+			dp_xmit_dbg("After", skb, ep, len, flags, NULL,
+				    rx_subif, insert_pmac_f, skb_is_gso(skb),
+				    tx_chksum_flag);
+	}
+
+#if IS_ENABLED(CONFIG_LTQ_TOE_DRIVER)
+	if (skb_is_gso(skb)) {
+		res = ltq_tso_xmit(skb, &pmac, sizeof(pmac), 0);
+		UP_STATS(mib->tx_tso_pkt);
+		return res;
+	}
 #endif /* CONFIG_LTQ_TOE_DRIVER */
-	
+
 #if IS_ENABLED(CONFIG_INTEL_DATAPATH_EXTRA_DEBUG)
-		if (unlikely(!desc_1->field.ep)) {
-			err_ret = DP_XMIT_ERR_EP_ZERO;
-			goto lbl_err_ret;
-		}
+	if (unlikely(!desc_1->field.ep)) {
+		err_ret = DP_XMIT_ERR_EP_ZERO;
+		goto lbl_err_ret;
+	}
 #endif
-		if (insert_pmac_f) {
-			data.pmac = (u8 *)&pmac;
-			data.pmac_len = sizeof(pmac);
-			data.dp_inst = inst;
-			data.dp_inst = 0;
-		} else {
-			data.pmac = NULL;
-			data.pmac_len = 0;
-			data.dp_inst = inst;
-			data.dp_inst = 0;
-		}
-		res = cbm_cpu_pkt_tx(skb, &data, 0);
-		UP_STATS(mib->tx_cbm_pkt);
-		return res;
-	
-	lbl_err_ret:
-		switch (err_ret) {
-		case DP_XMIT_ERR_NOT_INIT:
-			PR_RATELIMITED("dp_xmit failed for dp no init yet\n");
-			break;
-		case DP_XMIT_ERR_IN_IRQ:
-			PR_RATELIMITED("dp_xmit not allowed in interrupt context\n");
-			break;
-		case DP_XMIT_ERR_NULL_SUBIF:
-			PR_RATELIMITED("dp_xmit failed for rx_subif null\n");
-			UP_STATS(get_dp_port_info(inst, 0)->tx_err_drop);
-			break;
-		case DP_XMIT_ERR_PORT_TOO_BIG:
-			UP_STATS(get_dp_port_info(inst, 0)->tx_err_drop);
-			PR_RATELIMITED("rx_subif->port_id >= max_ports");
-			break;
-		case DP_XMIT_ERR_NULL_SKB:
-			PR_RATELIMITED("skb NULL");
-			UP_STATS(get_dp_port_info(inst, rx_subif->port_id)->
-				 tx_err_drop);
-			break;
-		case DP_XMIT_ERR_NULL_IF:
-			UP_STATS(mib->tx_pkt_dropped);
-			PR_RATELIMITED("rx_if NULL");
-			break;
-		case DP_XMIT_ERR_REALLOC_SKB:
-			PR_INFO_ONCE("dp_create_new_skb failed\n");
-			break;
-		case DP_XMIT_ERR_EP_ZERO:
-			PR_ERR("Why ep zero in dp_xmit for %s\n",
-			       skb->dev ? skb->dev->name : "NULL");
-			break;
-		case DP_XMIT_ERR_GSO_NOHEADROOM:
-			PR_ERR("No enough skb headerroom(GSO). Need tune SKB buffer\n");
-			break;
-		case DP_XMIT_ERR_CSM_NO_SUPPORT:
-			PR_RATELIMITED("dp_xmit not support checksum\n");
-			break;
-		case DP_XMIT_PTP_ERR:
-			break;
-		default:
-			UP_STATS(mib->tx_pkt_dropped);
-			PR_INFO_ONCE("Why come to here:%x\n", dp_info->status);
-		}
-		if (skb)
-			dev_kfree_skb_any(skb);
-		return DP_FAILURE;
+	if (insert_pmac_f) {
+		data.pmac = (u8 *)&pmac;
+		data.pmac_len = sizeof(pmac);
+		data.dp_inst = inst;
+		data.dp_inst = 0;
+	} else {
+		data.pmac = NULL;
+		data.pmac_len = 0;
+		data.dp_inst = inst;
+		data.dp_inst = 0;
+	}
+	res = cbm_cpu_pkt_tx(skb, &data, 0);
+	UP_STATS(mib->tx_cbm_pkt);
+	return res;
 
+lbl_err_ret:
+	switch (err_ret) {
+	case DP_XMIT_ERR_NOT_INIT:
+		PR_RATELIMITED("dp_xmit failed for dp no init yet\n");
+		break;
+	case DP_XMIT_ERR_IN_IRQ:
+		PR_RATELIMITED("dp_xmit not allowed in interrupt context\n");
+		break;
+	case DP_XMIT_ERR_NULL_SUBIF:
+		PR_RATELIMITED("dp_xmit failed for rx_subif null\n");
+		UP_STATS(get_dp_port_info(inst, 0)->tx_err_drop);
+		break;
+	case DP_XMIT_ERR_PORT_TOO_BIG:
+		UP_STATS(get_dp_port_info(inst, 0)->tx_err_drop);
+		PR_RATELIMITED("rx_subif->port_id >= max_ports");
+		break;
+	case DP_XMIT_ERR_NULL_SKB:
+		PR_RATELIMITED("skb NULL");
+		UP_STATS(get_dp_port_info(inst,
+					  rx_subif->port_id)->tx_err_drop);
+		break;
+	case DP_XMIT_ERR_NULL_IF:
+		UP_STATS(mib->tx_pkt_dropped);
+		PR_RATELIMITED("rx_if NULL");
+		break;
+	case DP_XMIT_ERR_REALLOC_SKB:
+		PR_INFO_ONCE("dp_create_new_skb failed\n");
+		break;
+	case DP_XMIT_ERR_EP_ZERO:
+		PR_ERR("Why ep zero in dp_xmit for %s\n",
+		       skb->dev ? skb->dev->name : "NULL");
+		break;
+	case DP_XMIT_ERR_GSO_NOHEADROOM:
+		PR_ERR("No enough skb headerroom(GSO). Need tune SKB buffer\n");
+		break;
+	case DP_XMIT_ERR_CSM_NO_SUPPORT:
+		PR_RATELIMITED("dp_xmit not support checksum\n");
+		break;
+	case DP_XMIT_PTP_ERR:
+		break;
+	default:
+		UP_STATS(mib->tx_pkt_dropped);
+		PR_INFO_ONCE("Why come to here:%x\n", dp_info->status);
+	}
+	if (skb)
+		dev_kfree_skb_any(skb);
+	return DP_FAILURE;
 }
 
diff --git a/drivers/net/datapath/dpm/gswip31/datapath_ext_vlan.c b/drivers/net/datapath/dpm/gswip31/datapath_ext_vlan.c
index f7ff39f14be3..fc19a60327d0 100644
--- a/drivers/net/datapath/dpm/gswip31/datapath_ext_vlan.c
+++ b/drivers/net/datapath/dpm/gswip31/datapath_ext_vlan.c
@@ -252,9 +252,8 @@ static int update_ctp(struct core_ops *ops, struct ext_vlan_info *vlan)
 			 "ingress VLAN operation disabled in ctp\n");
 		alloc.nExtendedVlanBlockId = block;
 		ret = ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
-		if (ret != GSW_statusOk) {
+		if (ret != GSW_statusOk)
 			return -EIO;
-		}
 	}
 	memset(&alloc, 0, sizeof(GSW_EXTENDEDVLAN_alloc_t));
 
@@ -263,7 +262,7 @@ static int update_ctp(struct core_ops *ops, struct ext_vlan_info *vlan)
 	alloc.nNumberOfEntries += vlan->n_vlan1_drop * 2;
 	alloc.nNumberOfEntries += vlan->n_vlan2_drop;
 	if (alloc.nNumberOfEntries == 0) {
-		DP_DEBUG(DP_DBG_FLAG_SWDEV,"nNumberOfEntries == 0\n");
+		DP_DEBUG(DP_DBG_FLAG_SWDEV, "nNumberOfEntries == 0\n");
 		return 0;
 	}
 
@@ -310,9 +309,9 @@ static int update_ctp(struct core_ops *ops, struct ext_vlan_info *vlan)
 	ctp.nIngressExtendedVlanBlockSize = 0;
 	ret = ops->gsw_ctp_ops.CTP_PortConfigSet(ops, &ctp);
 
-	if (ret != GSW_statusOk) {
+	if (ret != GSW_statusOk)
 		return -EIO;
-	}
+
 	return 0;
 UPDATE_ERROR:
 	ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
@@ -379,7 +378,7 @@ static int bp_add_vlan1(struct core_ops *ops, struct vlan1 *vlan,
 
 		if (ret != GSW_statusOk) {
 			PR_ERR("Fail updating Extended VLAN entry (%u, %u).\n",
-				alloc.nExtendedVlanBlockId, i);
+			       alloc.nExtendedVlanBlockId, i);
 			ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
 			return -EIO;
 		}
@@ -468,7 +467,7 @@ static int bp_add_vlan2(struct core_ops *ops, struct vlan2 *vlan,
 
 		if (ret != GSW_statusOk) {
 			PR_ERR("Fail updating Extended VLAN entry (%u, %u).\n",
-				alloc.nExtendedVlanBlockId, i);
+			       alloc.nExtendedVlanBlockId, i);
 			ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
 			return -EIO;
 		}
@@ -539,7 +538,8 @@ static int bp_diff(u32 *bp0, u32 num_bp0, u32 *bp1)
 	u32 i, j;
 
 	for (i = 0; i < num_bp0; i++) {
-		for (j = 0; j < num_bp0 && bp0[j] != bp1[i]; j++);
+		for (j = 0; j < num_bp0 && bp0[j] != bp1[i]; j++)
+			;
 		if (j >= num_bp0)
 			break;
 	}
diff --git a/drivers/net/datapath/dpm/gswip31/datapath_gswip.c b/drivers/net/datapath/dpm/gswip31/datapath_gswip.c
index 8dc69d185c21..e64cc6295fa8 100644
--- a/drivers/net/datapath/dpm/gswip31/datapath_gswip.c
+++ b/drivers/net/datapath/dpm/gswip31/datapath_gswip.c
@@ -140,8 +140,9 @@ int dp_pmac_set_31(int inst, u32 port, dp_pmac_cfg_t *pmac_cfg)
 				igcfg.nTxDmaChanId =
 					pmac_cfg->ig_pmac.tx_dma_chan;
 			}
-			gsw_core_api((dp_gsw_cb)gswr_r->gsw_pmac_ops.
-				     Pmac_Ig_CfgGet, gswr_r, &igcfg);
+			gsw_core_api(
+				(dp_gsw_cb)gswr_r->gsw_pmac_ops.Pmac_Ig_CfgGet,
+				gswr_r, &igcfg);
 
 			/*update igcfg and write back to gsw */
 			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_ERR_DISC)
@@ -241,8 +242,9 @@ int dp_pmac_set_31(int inst, u32 port, dp_pmac_cfg_t *pmac_cfg)
 
 			DP_DEBUG(DP_DBG_FLAG_DBG, "\n");
 
-			gsw_core_api((dp_gsw_cb)gswr_r->gsw_pmac_ops
-				     .Pmac_Ig_CfgSet, gswr_r, &igcfg);
+			gsw_core_api(
+				(dp_gsw_cb)gswr_r->gsw_pmac_ops.Pmac_Ig_CfgSet,
+				gswr_r, &igcfg);
 		}
 
 			kfree(dqport.deq_info);
@@ -264,8 +266,9 @@ int dp_pmac_set_31(int inst, u32 port, dp_pmac_cfg_t *pmac_cfg)
 			egcfg.bProcFlagsSelect = 1;
 			DP_DEBUG(DP_DBG_FLAG_DBG, "bProcFlagsSelect=%u\n",
 				 egcfg.bProcFlagsSelect);
-			gsw_core_api((dp_gsw_cb)gswr_r->gsw_pmac_ops
-				     .Pmac_Eg_CfgGet, gswr_r, &egcfg);
+			gsw_core_api(
+				(dp_gsw_cb)gswr_r->gsw_pmac_ops.Pmac_Eg_CfgGet,
+				gswr_r, &egcfg);
 
 			/*update egcfg and write back to gsw */
 			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_FCS)
@@ -395,8 +398,9 @@ int dp_pmac_set_31(int inst, u32 port, dp_pmac_cfg_t *pmac_cfg)
 					 egcfg.bMpe2Flag);
 			}
 #endif
-			gsw_core_api((dp_gsw_cb)gswr_r->gsw_pmac_ops
-				     .Pmac_Eg_CfgSet, gswr_r, &egcfg);
+			gsw_core_api(
+				(dp_gsw_cb)gswr_r->gsw_pmac_ops.Pmac_Eg_CfgSet,
+				gswr_r, &egcfg);
 		}
 	}
 
@@ -416,8 +420,9 @@ int dp_set_gsw_parser_31(u8 flag, u8 cpu, u8 mpe1,
 	GSW_CPU_PortCfg_t param = {0};
 	struct core_ops *gsw_handle = dp_port_prop[0].ops[0];/*gswip o */
 
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_common_ops
-			 .CPU_PortCfgGet, gsw_handle, &param)) {
+	if (gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_common_ops.CPU_PortCfgGet,
+		gsw_handle, &param)) {
 		PR_ERR("Failed GSW_CPU_PORT_CFG_GET\n");
 		return -1;
 	}
@@ -439,8 +444,9 @@ int dp_set_gsw_parser_31(u8 flag, u8 cpu, u8 mpe1,
 	if (flag & F_MPE1_MPE2)
 		param.eMPE1MPE2ParserCfg = mpe3;
 
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_common_ops
-			 .CPU_PortCfgSet, gsw_handle, &param)) {
+	if (gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_common_ops.CPU_PortCfgSet,
+		gsw_handle, &param)) {
 		PR_ERR("Failed GSW_CPU_PORT_CFG_SET\n");
 		return -1;
 	}
@@ -457,8 +463,9 @@ int dp_get_gsw_parser_31(u8 *cpu, u8 *mpe1, u8 *mpe2,
 	GSW_CPU_PortCfg_t param = {0};
 	struct core_ops *gsw_handle = dp_port_prop[0].ops[0]; /*gswip 0*/
 
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_common_ops
-			 .CPU_PortCfgGet, gsw_handle, &param)) {
+	if (gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_common_ops.CPU_PortCfgGet,
+		gsw_handle, &param)) {
 		PR_ERR("Failed GSW_CPU_PORT_CFG_GET\n");
 		return -1;
 	}
@@ -528,10 +535,10 @@ struct gsw_itf *ctp_port_assign(int inst, u8 ep, int bp_default,
 		ctp_assign.eMode = itf_assign[ep].mode;
 		ctp_assign.nFirstCtpPortId = itf_assign[ep].start;
 		ctp_assign.nNumberOfCtpPort = itf_assign[ep].n;
-		if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_ctp_ops
-				 .CTP_PortAssignmentFree,
-				 gsw_handle,
-				 &ctp_assign) != 0) {
+		if (gsw_core_api(
+			(dp_gsw_cb)gsw_handle->gsw_ctp_ops.CTP_PortAssignmentFree,
+			gsw_handle,
+			&ctp_assign) != 0) {
 			PR_ERR("Failed to allc CTP for ep=%d blk=%d mode=%d\n",
 			       ep, assign->num, assign->emode);
 			return NULL;
@@ -555,10 +562,10 @@ struct gsw_itf *ctp_port_assign(int inst, u8 ep, int bp_default,
 	ctp_assign.nBridgePortId = bp_default;
 	ctp_assign.nFirstCtpPortId = 0;
 	ctp_assign.nNumberOfCtpPort = num;
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_ctp_ops
-			  .CTP_PortAssignmentAlloc,
-			  gsw_handle,
-			  &ctp_assign) != 0) {
+	if (gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_ctp_ops.CTP_PortAssignmentAlloc,
+		gsw_handle,
+		&ctp_assign) != 0) {
 		PR_ERR("Failed CTP Assignment for ep=%d blk size=%d mode=%s\n",
 		       ep, num, ctp_mode_string(assign->emode));
 		return NULL;
@@ -584,7 +591,7 @@ struct gsw_itf *ctp_port_assign(int inst, u8 ep, int bp_default,
 
 int set_port_lookup_mode_31(int inst, u8 ep, u32 flags)
 {
-	int i, alloc_flag;	
+	int i, alloc_flag;
 	struct ctp_assign *assign = &ctp_assign_def;
 	struct pmac_port_info *port_info = get_dp_port_info(inst, ep);
 
@@ -613,8 +620,9 @@ int alloc_bridge_port(int inst, int port_id, int subif_ix,
 	gsw_handle = dp_port_prop[inst].ops[GSWIP_L];
 	/*allocate a free bridge port */
 	memset(&bp, 0, sizeof(bp));
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			   .BridgePort_Alloc, gsw_handle, &bp);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_Alloc,
+		gsw_handle, &bp);
 	if ((ret != GSW_statusOk) ||
 	    (bp.nBridgePortId < 0)) {
 		PR_ERR("Failed to get a bridge port\n");
@@ -629,8 +637,8 @@ int alloc_bridge_port(int inst, int port_id, int subif_ix,
 	/* By default Disable src mac learning for registered
 	 * non CPU bridge port with DP
 	 */
-	if (get_dp_port_subif(get_dp_port_info(inst, port_id), subif_ix)->
-	    mac_learn_dis == DP_MAC_LEARNING_DIS)
+	if (get_dp_port_subif(get_dp_port_info(inst, port_id),
+			      subif_ix)->mac_learn_dis == DP_MAC_LEARNING_DIS)
 		bp_cfg.bSrcMacLearningDisable = 1;
 	else
 		bp_cfg.bSrcMacLearningDisable = 0;
@@ -641,13 +649,15 @@ int alloc_bridge_port(int inst, int port_id, int subif_ix,
 	bp_cfg.nBridgeId = fid;
 
 	SET_BP_MAP(bp_cfg.nBridgePortMap, bp_member); /*CPU*/
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			     .BridgePort_ConfigSet, gsw_handle, &bp_cfg);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigSet,
+		gsw_handle, &bp_cfg);
 	if (ret != GSW_statusOk) {
 		PR_ERR("Failed to set bridge id(%d) and port map for bp= %d\n",
 		       fid, bp_cfg.nBridgePortId);
-		gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			     .BridgePort_Free, gsw_handle, &bp);
+		gsw_core_api(
+			(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_Free,
+			gsw_handle, &bp);
 		return -1;
 	}
 
@@ -656,23 +666,27 @@ int alloc_bridge_port(int inst, int port_id, int subif_ix,
 	 */
 	bp_cfg.nBridgePortId = bp_member;
 	bp_cfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			   .BridgePort_ConfigGet, gsw_handle, &bp_cfg);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigGet,
+		gsw_handle, &bp_cfg);
 	if (ret != GSW_statusOk) {
 		PR_ERR("Failed to get bridge port's member for bridgeport=%d\n",
 		       bp_cfg.nBridgePortId);
-		gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			     .BridgePort_Free, gsw_handle, &bp);
+		gsw_core_api(
+			(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_Free,
+			gsw_handle, &bp);
 		return -1;
 	}
 	SET_BP_MAP(bp_cfg.nBridgePortMap, bp.nBridgePortId);
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			   .BridgePort_ConfigSet, gsw_handle, &bp_cfg);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigSet,
+		gsw_handle, &bp_cfg);
 	if (ret != GSW_statusOk) {
 		PR_ERR("Failed to set bridge port's member for bridgeport=%d\n",
 		       bp_cfg.nBridgePortId);
-		gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			     .BridgePort_Free, gsw_handle, &bp);
+		gsw_core_api(
+			(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_Free,
+			gsw_handle, &bp);
 		return -1;
 	}
 
@@ -703,8 +717,9 @@ int free_bridge_port(int inst, int bp)
 	/*read out this delting bridge port's member*/
 	tmp->nBridgePortId = bp;
 	tmp->eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			   .BridgePort_ConfigGet, gsw_handle, tmp);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigGet,
+		gsw_handle, tmp);
 	if (ret != GSW_statusOk) {
 		PR_ERR("Failed GSW_BRIDGE_PORT_CONFIG_GET: %d\n", bp);
 		goto EXIT;
@@ -719,19 +734,17 @@ int free_bridge_port(int inst, int bp)
 			tmp2->eMask =
 				GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
 			tmp2->nBridgePortId = i * 16 + j;
-			ret = gsw_core_api((dp_gsw_cb)gsw_handle
-					   ->gsw_brdgport_ops
-					   .BridgePort_ConfigGet, gsw_handle,
-					   tmp2);
+			ret = gsw_core_api(
+				(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigGet,
+				gsw_handle, tmp2);
 			if (ret != GSW_statusOk) {
 				PR_ERR("Failed GSW_BRIDGE_PORT_CONFIG_GET\n");
 				goto EXIT;
 			}
 			UNSET_BP_MAP(tmp2->nBridgePortMap, bp);
-			ret = gsw_core_api((dp_gsw_cb)gsw_handle
-					   ->gsw_brdgport_ops
-					   .BridgePort_ConfigSet, gsw_handle,
-					   tmp2);
+			ret = gsw_core_api(
+				(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigSet,
+				gsw_handle, tmp2);
 			if (ret != GSW_statusOk) {
 				PR_ERR("Failed GSW_BRIDGE_PORT_CONFIG_SET\n");
 				goto EXIT;
@@ -743,8 +756,9 @@ int free_bridge_port(int inst, int bp)
 	memset(tmp, 0, sizeof(tmp));
 	tmp->nBridgePortId = bp;
 	tmp->eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			   .BridgePort_Free, gsw_handle, tmp);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_Free,
+		gsw_handle, tmp);
 	if (ret != GSW_statusOk)
 		PR_ERR("Failed to GSW_BRIDGE_PORT_FREE:%d\n", bp);
 FREE_EXIT:
@@ -767,8 +781,9 @@ int dp_gswip_mac_entry_add(int bport, int fid, int inst, u8 *addr)
 	SET_BP_MAP(tmp.nPortMap, bport);
 	tmp.nSubIfId = 0;
 	memcpy(tmp.nMAC, addr, GSW_MAC_ADDR_LEN);
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_swmac_ops.
-			   MAC_TableEntryAdd, gsw_handle, &tmp);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_swmac_ops.MAC_TableEntryAdd,
+		gsw_handle, &tmp);
 	if (ret != GSW_statusOk) {
 		PR_ERR("fail in setting MAC table static add entry\r\n");
 		return -1;
@@ -788,16 +803,18 @@ int dp_gswip_mac_entry_del(int bport, int fid, int inst, u8 *addr)
 	memset(&mac_query, 0, sizeof(mac_query));
 	mac_query.nFId = fid;
 	memcpy(mac_query.nMAC, addr, GSW_MAC_ADDR_LEN);
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_swmac_ops.
-			   MAC_TableEntryQuery, gsw_handle, &tmp);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_swmac_ops.MAC_TableEntryQuery,
+		gsw_handle, &tmp);
 	if (ret != GSW_statusOk) {
 		PR_ERR("fail in getting MAC query entry\r\n");
 		return -1;
 	}
 	tmp.nFId = fid;
 	memcpy(tmp.nMAC, addr, GSW_MAC_ADDR_LEN);
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_swmac_ops.
-			   MAC_TableEntryRemove, gsw_handle, &tmp);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_swmac_ops.MAC_TableEntryRemove,
+		gsw_handle, &tmp);
 	if (ret != GSW_statusOk) {
 		PR_ERR("fail in setting MAC static entry remove\r\n");
 		return -1;
@@ -814,16 +831,16 @@ int cpu_vlan_mod_dis(int inst)
 	ops = dp_port_prop[inst].ops[GSWIP_L];
 
 	cfg.nPortId = 0;
-	ret = gsw_core_api((dp_gsw_cb)ops->gsw_qos_ops.
-			   QoS_PortRemarkingCfgGet, ops, &cfg);
+	ret = gsw_core_api((dp_gsw_cb)ops->gsw_qos_ops.QoS_PortRemarkingCfgGet,
+			   ops, &cfg);
 	if (ret != GSW_statusOk) {
 		PR_ERR("QoS_PortRemarkingCfgGet failed\n");
 		return -1;
 	}
 
 	cfg.bPCP_EgressRemarkingEnable = LTQ_FALSE;
-	ret = gsw_core_api((dp_gsw_cb)ops->gsw_qos_ops.
-			   QoS_PortRemarkingCfgSet, ops, &cfg);
+	ret = gsw_core_api((dp_gsw_cb)ops->gsw_qos_ops.QoS_PortRemarkingCfgSet,
+			   ops, &cfg);
 	if (ret != GSW_statusOk) {
 		PR_ERR("QoS_PortRemarkingCfgSet failed\n");
 		return -1;
@@ -888,8 +905,9 @@ int dp_set_gsw_pmapper_31(int inst, int bport, int lport,
 		 "call switch api mode %d enable %d eMask 0x%x\n",
 		 bp_cfg.ePmapperMappingMode, bp_cfg.bPmapperEnable,
 		 bp_cfg.eMask);
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			     .BridgePort_ConfigSet, gsw_handle, &bp_cfg);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigSet,
+		gsw_handle, &bp_cfg);
 	if (ret != GSW_statusOk) {
 		PR_ERR("fail in setting pmapper\r\n");
 		return -1;
@@ -918,8 +936,9 @@ int dp_get_gsw_pmapper_31(int inst, int bport, int lport,
 	bp_cfg.nDestLogicalPortId = lport;
 	bp_cfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_CTP_MAPPING;
 
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			     .BridgePort_ConfigGet, gsw_handle, &bp_cfg);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigGet,
+		gsw_handle, &bp_cfg);
 	if (ret != GSW_statusOk) {
 		PR_ERR("fail in getting pmapper\r\n");
 		return -1;
@@ -1176,8 +1195,8 @@ int dp_meter_add_31(struct net_device *dev,  struct dp_meter_cfg  *meter,
 			bret = -1;
 			goto err;
 		}
-		port_info =
-		get_dp_port_info(mtr_subif->subif.inst, mtr_subif->subif.port_id);
+		port_info = get_dp_port_info(mtr_subif->subif.inst,
+					     mtr_subif->subif.port_id);
 		if (!port_info) {
 			PR_ERR(" port_info is NULL\n");
 			bret = -1;
diff --git a/drivers/net/datapath/dpm/gswip31/datapath_misc.c b/drivers/net/datapath/dpm/gswip31/datapath_misc.c
index d4513d66aa77..b0f0c75ff3ce 100644
--- a/drivers/net/datapath/dpm/gswip31/datapath_misc.c
+++ b/drivers/net/datapath/dpm/gswip31/datapath_misc.c
@@ -510,7 +510,7 @@ int alloc_q_to_port(struct ppv4_q_sch_port *info, u32 flag)
 		return -1;
 	}
 	subif = get_dp_port_subif(get_dp_port_info(info->inst, info->dp_port),
-			   info->ctp);
+				  info->ctp);
 	subif->qid = q.qid;
 	subif->q_node = q.qid;
 	info->qid = q.qid;
@@ -582,9 +582,9 @@ static int dp_gswip_remark_8P0D_set(int mode, int inst)
 	color_remark.nVal[13] = 11;
 	color_remark.nVal[14] = 13;
 	color_remark.nVal[15] = 15;
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-				.QOS_ColorReMarkingTableSet,
-				gsw_handle, &color_remark)) {
+	if (gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_qos_ops.QOS_ColorReMarkingTableSet,
+		gsw_handle, &color_remark)) {
 		PR_ERR("GSW_QOS_COLOR_REMARKING_CFG_SET failed\n");
 		return -1;
 	}
@@ -614,9 +614,9 @@ static int dp_gswip_remark_7P1D_set(int mode, int inst)
 	color_remark.nVal[13] = 9;
 	color_remark.nVal[14] = 13;
 	color_remark.nVal[15] = 15;
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-				.QOS_ColorReMarkingTableSet,
-				gsw_handle, &color_remark)) {
+	if (gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_qos_ops.QOS_ColorReMarkingTableSet,
+		gsw_handle, &color_remark)) {
 		PR_ERR("GSW_QOS_COLOR_REMARKING_CFG_SET failed\n");
 		return -1;
 	}
@@ -646,9 +646,9 @@ static int dp_gswip_remark_6P2D_set(int mode, int inst)
 	color_remark.nVal[13] = 9;
 	color_remark.nVal[14] = 13;
 	color_remark.nVal[15] = 15;
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-				.QOS_ColorReMarkingTableSet,
-				gsw_handle, &color_remark)) {
+	if (gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_qos_ops.QOS_ColorReMarkingTableSet,
+		gsw_handle, &color_remark)) {
 		PR_ERR("GSW_QOS_COLOR_REMARKING_CFG_SET failed\n");
 		return -1;
 	}
@@ -678,9 +678,9 @@ static int dp_gswip_remark_5P3D_set(int mode, int inst)
 	color_remark.nVal[13] = 9;
 	color_remark.nVal[14] = 13;
 	color_remark.nVal[15] = 15;
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-				.QOS_ColorReMarkingTableSet,
-				gsw_handle, &color_remark)) {
+	if (gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_qos_ops.QOS_ColorReMarkingTableSet,
+		gsw_handle, &color_remark)) {
 		PR_ERR("GSW_QOS_COLOR_REMARKING_CFG_SET failed\n");
 		return -1;
 	}
@@ -710,9 +710,9 @@ static int dp_gswip_remark_dscp_set(int mode, int inst)
 	color_remark.nVal[13] = 36;
 	color_remark.nVal[14] = 36;
 	color_remark.nVal[15] = 36;
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-				.QOS_ColorReMarkingTableSet,
-				gsw_handle, &color_remark)) {
+	if (gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_qos_ops.QOS_ColorReMarkingTableSet,
+		gsw_handle, &color_remark)) {
 		PR_ERR("GSW_QOS_COLOR_REMARKING_CFG_SET failed\n");
 		return -1;
 	}
@@ -854,9 +854,9 @@ static int dp_gswip_color_dscp_set(int mode, int inst)
 	color_mark.nColor[61] = GSW_DROP_PRECEDENCE_YELLOW;
 	color_mark.nColor[62] = GSW_DROP_PRECEDENCE_YELLOW;
 	color_mark.nColor[63] = GSW_DROP_PRECEDENCE_YELLOW;
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-				.QOS_ColorMarkingTableSet,
-				gsw_handle, &color_mark)) {
+	if (gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_qos_ops.QOS_ColorMarkingTableSet,
+		gsw_handle, &color_mark)) {
 		PR_ERR("GSW_QOS_COLOR_MARKING_CFG_SET failed\n");
 		return -1;
 	}
@@ -902,9 +902,9 @@ static int dp_gswip_color_5P3D_set(int mode, int inst)
 	color_mark.nColor[13] = GSW_DROP_PRECEDENCE_YELLOW;
 	color_mark.nColor[14] = GSW_DROP_PRECEDENCE_GREEN;
 	color_mark.nColor[15] = GSW_DROP_PRECEDENCE_YELLOW;
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-				.QOS_ColorMarkingTableSet,
-				gsw_handle, &color_mark)) {
+	if (gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_qos_ops.QOS_ColorMarkingTableSet,
+		gsw_handle, &color_mark)) {
 		PR_ERR("GSW_QOS_COLOR_MARKING_CFG_SET failed\n");
 		return -1;
 	}
@@ -950,9 +950,9 @@ static int dp_gswip_color_6P2D_set(int mode, int inst)
 	color_mark.nColor[13] = GSW_DROP_PRECEDENCE_YELLOW;
 	color_mark.nColor[14] = GSW_DROP_PRECEDENCE_GREEN;
 	color_mark.nColor[15] = GSW_DROP_PRECEDENCE_YELLOW;
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-				.QOS_ColorMarkingTableSet,
-				gsw_handle, &color_mark)) {
+	if (gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_qos_ops.QOS_ColorMarkingTableSet,
+		gsw_handle, &color_mark)) {
 		PR_ERR("GSW_QOS_COLOR_MARKING_CFG_SET failed\n");
 		return -1;
 	}
@@ -998,9 +998,9 @@ static int dp_gswip_color_7P1D_set(int mode, int inst)
 	color_mark.nColor[13] = GSW_DROP_PRECEDENCE_YELLOW;
 	color_mark.nColor[14] = GSW_DROP_PRECEDENCE_GREEN;
 	color_mark.nColor[15] = GSW_DROP_PRECEDENCE_YELLOW;
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-				.QOS_ColorMarkingTableSet,
-				gsw_handle, &color_mark)) {
+	if (gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_qos_ops.QOS_ColorMarkingTableSet,
+		gsw_handle, &color_mark)) {
 		PR_ERR("GSW_QOS_COLOR_MARKING_CFG_SET failed\n");
 		return -1;
 	}
@@ -1046,9 +1046,9 @@ static int dp_gswip_color_8P0D_set(int mode, int inst)
 	color_mark.nColor[13] = GSW_DROP_PRECEDENCE_YELLOW;
 	color_mark.nColor[14] = GSW_DROP_PRECEDENCE_GREEN;
 	color_mark.nColor[15] = GSW_DROP_PRECEDENCE_YELLOW;
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-				.QOS_ColorMarkingTableSet,
-				gsw_handle, &color_mark)) {
+	if (gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_qos_ops.QOS_ColorMarkingTableSet,
+		gsw_handle, &color_mark)) {
 		PR_ERR("GSW_QOS_COLOR_MARKING_CFG_SET failed\n");
 		return -1;
 	}
@@ -1142,10 +1142,10 @@ int dp_platform_queue_set(int inst, u32 flag)
 		DP_DEBUG(DP_DBG_FLAG_QOS, "cpu(%d) deq_port=%d",
 			 i, cpu_data.dq_tx_push_info[i].deq_port);
 		q_port.cqe_deq = cpu_data.dq_tx_push_info[i].deq_port;
-		q_port.tx_pkt_credit = cpu_data.dq_tx_push_info[i].
-							tx_pkt_credit;
-		q_port.tx_ring_addr = cpu_data.dq_tx_push_info[i].
-								txpush_addr_qos;
+		q_port.tx_pkt_credit =
+			cpu_data.dq_tx_push_info[i].tx_pkt_credit;
+		q_port.tx_ring_addr =
+			cpu_data.dq_tx_push_info[i].txpush_addr_qos;
 		q_port.tx_ring_size = cpu_data.dq_tx_push_info[i].tx_ring_size;
 
 		/*Sotre Ring Info */
@@ -1161,8 +1161,7 @@ int dp_platform_queue_set(int inst, u32 flag)
 		DP_DEBUG(DP_DBG_FLAG_QOS, "Store CPU ring info\n");
 		DP_DEBUG(DP_DBG_FLAG_QOS, "  ring_address[%d]=0x%p\n",
 			 q_port.cqe_deq,
-			 (void *)dp_deq_port_tbl[inst][q_port.cqe_deq].
-								txpush_addr);
+			 (void *)dp_deq_port_tbl[inst][q_port.cqe_deq].txpush_addr);
 		DP_DEBUG(DP_DBG_FLAG_QOS, "  ring_address_push[%d]=0x%px\n",
 			 q_port.cqe_deq,
 			 dp_deq_port_tbl[inst][q_port.cqe_deq].txpush_addr_qos);
@@ -1268,16 +1267,16 @@ static int dp_platform_set(int inst, u32 flag)
 #endif
 		/*disable egress VLAN modification for CPU port*/
 		port_remark.nPortId = 0;
-		if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-				 .QoS_PortRemarkingCfgGet,
-				 gsw_handle, &port_remark)) {
+		if (gsw_core_api(
+			(dp_gsw_cb)gsw_handle->gsw_qos_ops.QoS_PortRemarkingCfgGet,
+			gsw_handle, &port_remark)) {
 			PR_ERR("GSW_QOS_PORT_REMARKING_CFG_GET failed\n");
 			return -1;
 		}
 		port_remark.bPCP_EgressRemarkingEnable = 0;
-		if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-				 .QoS_PortRemarkingCfgGet,
-				 gsw_handle, &port_remark)) {
+		if (gsw_core_api(
+			(dp_gsw_cb)gsw_handle->gsw_qos_ops.QoS_PortRemarkingCfgGet,
+			gsw_handle, &port_remark)) {
 			PR_ERR("GSW_QOS_PORT_REMARKING_CFG_GET failed\n");
 			return -1;
 		}
@@ -1650,8 +1649,8 @@ static int subif_hw_set(int inst, int portid, int subif_ix,
 #endif
 	q_port.tx_pkt_credit =
 		dp_deq_port_tbl[inst][q_port.cqe_deq].tx_pkt_credit;
-	q_port.tx_ring_addr = (u32)dp_deq_port_tbl[inst][q_port.cqe_deq].
-								txpush_addr_qos;
+	q_port.tx_ring_addr =
+		(u32)dp_deq_port_tbl[inst][q_port.cqe_deq].txpush_addr_qos;
 	q_port.tx_ring_size =
 		dp_deq_port_tbl[inst][q_port.cqe_deq].tx_ring_size;
 	q_port.inst = inst;
@@ -1813,7 +1812,6 @@ static int subif_hw_set(int inst, int portid, int subif_ix,
 		lookup.mpe1 = 0;
 	}
 	/* Map this port's lookup to its 1st queue only */
-	//lookup.mode = get_dp_port_info(inst, portid)->cqe_lu_mode; /*no need */
 	lookup.ep = portid;
 	lookup.sub_if_id = subif; /* Note:CQM API need full subif(15bits) */
 	/* For 1st subif and mode 0, use CBM_QUEUE_MAP_F_SUBIF_DONTCARE,
diff --git a/drivers/net/datapath/dpm/gswip31/datapath_ppv4_api.c b/drivers/net/datapath/dpm/gswip31/datapath_ppv4_api.c
index 48abdb342413..c580c7a75c6a 100644
--- a/drivers/net/datapath/dpm/gswip31/datapath_ppv4_api.c
+++ b/drivers/net/datapath/dpm/gswip31/datapath_ppv4_api.c
@@ -2167,9 +2167,9 @@ int dp_node_link_get_31(struct dp_node_link *info, int flag)
 
 		arbi = get_parent_arbi(info->inst, node_id, flag);
 
-		if (arbi == DP_FAILURE) {
+		if (arbi == DP_FAILURE)
 			return DP_FAILURE;
-		}
+
 		info->arbi = arbi;
 
 		if (info->arbi == ARBITRATION_WRR) {
@@ -2331,7 +2331,8 @@ static int dp_link_set(struct dp_node_link *info, int parent_node, int flag)
 			sched_cfg->sched_child_prop.priority = info->prio_wfq;
 		}
 
-		sched_cfg->sched_parent_prop.arbitration = arbi_dp2pp(info->arbi);
+		sched_cfg->sched_parent_prop.arbitration =
+						arbi_dp2pp(info->arbi);
 		node_id = info->node_id.sch_id;
 
 		DP_DEBUG(DP_DBG_FLAG_QOS,
@@ -2374,7 +2375,6 @@ static int dp_link_set(struct dp_node_link *info, int parent_node, int flag)
 	return res;
 }
 
-
 /* set_parent_arbi API
  * set arbitration of node_id's all children and return DP_SUCCESS
  * else return DP_FAILURE
@@ -2577,9 +2577,9 @@ int dp_qos_link_prio_get_31(struct dp_node_prio *info, int flag)
 
 		arbi = get_parent_arbi(info->inst, node_id, flag);
 
-		if (arbi == DP_FAILURE) {
+		if (arbi == DP_FAILURE)
 			return DP_FAILURE;
-		}
+
 		info->arbi = arbi;
 		if (info->arbi == ARBITRATION_WRR) {
 			info->prio_wfq =
@@ -2609,9 +2609,9 @@ int dp_qos_link_prio_get_31(struct dp_node_prio *info, int flag)
 
 		arbi = get_parent_arbi(info->inst, info->id.sch_id, flag);
 
-		if (arbi == DP_FAILURE) {
+		if (arbi == DP_FAILURE)
 			return DP_FAILURE;
-		}
+
 		info->arbi = arbi;
 
 		if (info->arbi == ARBITRATION_WRR) {
@@ -2789,8 +2789,8 @@ int dp_node_unlink_31(struct dp_node_link *info, int flag)
 		if (!(priv->qos_queue_stat[info->node_id.q_id].flag &
 		    PP_NODE_ACTIVE)) {
 			PR_ERR("Wrong Queue[%d] Stat(%d):Expect ACTIVE\n",
-				info->node_id.q_id,
-				priv->qos_queue_stat[info->node_id.q_id].flag);
+			       info->node_id.q_id,
+			       priv->qos_queue_stat[info->node_id.q_id].flag);
 		}
 		cqm_qid2ep_map_set(info->node_id.q_id, priv->ppv4_drop_p);
 		DP_DEBUG(DP_DBG_FLAG_QOS, "%s qid:%d, dq_port:%d\n",
@@ -4055,7 +4055,7 @@ int dp_queue_map_get_31(struct dp_queue_map_get *cfg, int flag)
 		return DP_FAILURE;
 	}
 	if ((priv->qos_queue_stat[cfg->q_id].flag == PP_NODE_FREE) &&
-			(cfg->q_id != priv->ppv4_drop_q)) {
+	    (cfg->q_id != priv->ppv4_drop_q)) {
 		PR_ERR("Invalid Queue flag:%d\n",
 		       priv->qos_queue_stat[cfg->q_id].flag);
 		return DP_FAILURE;
@@ -4127,7 +4127,8 @@ int dp_queue_map_set_31(struct dp_queue_map_set *cfg, int flag)
 		PR_ERR("Invalid Queue ID:%d\n", cfg->q_id);
 		return DP_FAILURE;
 	}
-	if ((priv->qos_queue_stat[cfg->q_id].flag == PP_NODE_FREE)  && (cfg->q_id != priv->ppv4_drop_q)) {
+	if ((priv->qos_queue_stat[cfg->q_id].flag == PP_NODE_FREE) &&
+	    (cfg->q_id != priv->ppv4_drop_q)) {
 		PR_ERR("Invalid Queue flag:%d\n",
 		       priv->qos_queue_stat[cfg->q_id].flag);
 		return DP_FAILURE;
diff --git a/drivers/net/datapath/dpm/gswip31/datapath_proc.c b/drivers/net/datapath/dpm/gswip31/datapath_proc.c
index 232e167a1dba..c8905717b996 100644
--- a/drivers/net/datapath/dpm/gswip31/datapath_proc.c
+++ b/drivers/net/datapath/dpm/gswip31/datapath_proc.c
@@ -204,8 +204,9 @@ ssize_t proc_parser_write(struct file *file, const char *buf,
 		pce.action.bRMON_Action = 1;
 		pce.action.nRMON_Id = 0;	/*RMON_UDP_CNTR; */
 
-		if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_tflow_ops
-				 .TFLOW_PceRuleWrite, gsw_handle, &pce)) {
+		if (gsw_core_api(
+			(dp_gsw_cb)gsw_handle->gsw_tflow_ops.TFLOW_PceRuleWrite,
+			gsw_handle, &pce)) {
 			PR_ERR("PCE rule add fail: GSW_PCE_RULE_WRITE\n");
 			return count;
 		}
@@ -216,8 +217,9 @@ ssize_t proc_parser_write(struct file *file, const char *buf,
 		pce_rule_id = dp_atoi(param_list[1]);
 		pce.pattern.nIndex = pce_rule_id;
 		pce.pattern.bEnable = 0;
-		if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_tflow_ops
-				 .TFLOW_PceRuleWrite, gsw_handle, &pce)) {
+		if (gsw_core_api(
+			(dp_gsw_cb)gsw_handle->gsw_tflow_ops.TFLOW_PceRuleWrite,
+			gsw_handle, &pce)) {
 			PR_ERR("PCE rule add fail:GSW_PCE_RULE_WRITE\n");
 			return count;
 		}
@@ -248,8 +250,9 @@ char *get_bp_member_string(int inst, u16 bp, char *buf)
 	bp_cfg.nBridgePortId = bp;
 	bp_cfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP |
 		GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID;
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			   .BridgePort_ConfigGet, gsw_handle, &bp_cfg);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigGet,
+		gsw_handle, &bp_cfg);
 	if (ret != GSW_statusOk) {
 		PR_ERR("Failed to get bridge port's member for bridgeport=%d\n",
 		       bp_cfg.nBridgePortId);
diff --git a/drivers/net/datapath/dpm/gswip31/datapath_rx.c b/drivers/net/datapath/dpm/gswip31/datapath_rx.c
index 3285f7fee5d8..53b7cf00059d 100644
--- a/drivers/net/datapath/dpm/gswip31/datapath_rx.c
+++ b/drivers/net/datapath/dpm/gswip31/datapath_rx.c
@@ -15,9 +15,9 @@
 #include "datapath_misc.h"
 
 static void rx_dbg(u32 f, struct sk_buff *skb, struct dma_rx_desc_0 *desc0,
-	       struct dma_rx_desc_1 *desc1, struct dma_rx_desc_2 *desc2,
-	       struct dma_rx_desc_3 *desc3, unsigned char *parser,
-	       struct pmac_rx_hdr *pmac, int paser_exist)
+		   struct dma_rx_desc_1 *desc1, struct dma_rx_desc_2 *desc2,
+		   struct dma_rx_desc_3 *desc3, unsigned char *parser,
+		   struct pmac_rx_hdr *pmac, int paser_exist)
 
 {
 	int inst = 0;
@@ -97,7 +97,7 @@ static int dp_handle_lct(struct pmac_port_info *dp_port,
 	if (skb->data[PMAC_SIZE] & 0x1) {
 		/* multicast/broadcast */
 		DP_DEBUG(DP_DBG_FLAG_PAE, "LCT mcast or broadcast\n");
-		if((STATS_GET(sif->rx_flag) <= 0)) {
+		if ((STATS_GET(sif->rx_flag) <= 0)) {
 			UP_STATS(mib->rx_fn_dropped);
 			return 1;
 		}
@@ -117,7 +117,7 @@ static int dp_handle_lct(struct pmac_port_info *dp_port,
 		DP_DEBUG(DP_DBG_FLAG_PAE, "LCT unicast\n");
 		DP_DEBUG(DP_DBG_FLAG_PAE, "unicast pkt sent lct(%s)\n",
 			 skb->dev->name ? skb->dev->name : "NULL");
-		if((STATS_GET(sif->rx_flag) <= 0)) {
+		if ((STATS_GET(sif->rx_flag) <= 0)) {
 			UP_STATS(mib->rx_fn_dropped);
 			dev_kfree_skb_any(skb);
 			return 0;
@@ -240,7 +240,8 @@ int32_t dp_rx_31(struct sk_buff *skb, u32 flags)
 		desc_3->all &= dma_rx_desc_mask3.all;
 		skb->priority = desc_1->field.classid;
 		skb->dev = sif->netif; /* note: for DSL ATM case, its driver
-					* will correct it in later stage */
+					* will correct it in later stage
+					*/
 		if (((dp_port->alloc_flags & DP_F_FAST_DSL) == 0) && /*non-dsl*/
 			sif->flags) { /*not de-registered */
 			dev = sif->netif;
diff --git a/drivers/net/datapath/dpm/gswip31/datapath_switchdev.c b/drivers/net/datapath/dpm/gswip31/datapath_switchdev.c
index 7e36b615fa55..8e7b7231bd9e 100644
--- a/drivers/net/datapath/dpm/gswip31/datapath_switchdev.c
+++ b/drivers/net/datapath/dpm/gswip31/datapath_switchdev.c
@@ -48,8 +48,9 @@ int dp_swdev_bridge_port_cfg_set(struct br_info *br_item,
 	DP_DEBUG(DP_DBG_FLAG_SWDEV, "Set current BP=%d inst:%d\n",
 		 brportcfg.nBridgePortId, inst);
 	brportcfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops.
-			   BridgePort_ConfigGet, gsw_handle, &brportcfg);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigGet,
+		gsw_handle, &brportcfg);
 	if (ret != GSW_statusOk) {
 		PR_ERR("fail in getting bridge port config\r\n");
 		return -1;
@@ -64,8 +65,9 @@ int dp_swdev_bridge_port_cfg_set(struct br_info *br_item,
 	brportcfg.nBridgePortId = bport;
 	brportcfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID |
 		GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops.
-			   BridgePort_ConfigSet, gsw_handle, &brportcfg);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigSet,
+		gsw_handle, &brportcfg);
 	if (ret != GSW_statusOk) {
 		PR_ERR("Fail in allocating/configuring bridge port\n");
 		return -1;
@@ -81,10 +83,9 @@ int dp_swdev_bridge_port_cfg_set(struct br_info *br_item,
 				 brportcfg.nBridgePortId, inst);
 			brportcfg.eMask =
 				GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
-			ret = gsw_core_api((dp_gsw_cb)gsw_handle
-					->gsw_brdgport_ops
-					.BridgePort_ConfigGet,
-					gsw_handle, &brportcfg);
+			ret = gsw_core_api(
+				(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigGet,
+				gsw_handle, &brportcfg);
 			if (ret != GSW_statusOk) {
 				PR_ERR
 					("fail in getting br port config\r\n");
@@ -96,10 +97,9 @@ int dp_swdev_bridge_port_cfg_set(struct br_info *br_item,
 			brportcfg.eMask =
 				GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID |
 				GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
-			ret = gsw_core_api((dp_gsw_cb)gsw_handle
-					->gsw_brdgport_ops
-					.BridgePort_ConfigSet,
-					gsw_handle, &brportcfg);
+			ret = gsw_core_api(
+				(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigSet,
+				gsw_handle, &brportcfg);
 			if (ret != GSW_statusOk) {
 				PR_ERR("Fail alloc/cfg bridge port\n");
 				return -1;
@@ -125,8 +125,9 @@ int dp_swdev_bridge_port_cfg_reset(struct br_info *br_item,
 		 brportcfg.nBridgePortId, inst);
 	brportcfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
 	/*Reset other members from current bport map*/
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			   .BridgePort_ConfigGet, gsw_handle, &brportcfg);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigGet,
+		gsw_handle, &brportcfg);
 	if (ret != GSW_statusOk) {
 		/* Note: here may fail if this device is not removed from
 		 * linux bridge via brctl delif but user try to un-regiser
@@ -162,8 +163,9 @@ int dp_swdev_bridge_port_cfg_reset(struct br_info *br_item,
 	brportcfg.nBridgePortId = bport;
 	brportcfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID |
 			  GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			   .BridgePort_ConfigSet, gsw_handle, &brportcfg);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigSet,
+		gsw_handle, &brportcfg);
 	if (ret != GSW_statusOk) {
 		PR_ERR("Fail in configuring GSW_BRIDGE_portConfig_t in %s\r\n",
 		       __func__);
@@ -181,10 +183,9 @@ int dp_swdev_bridge_port_cfg_reset(struct br_info *br_item,
 			brportcfg.eMask =
 				 GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP |
 				 GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID;
-			ret = gsw_core_api((dp_gsw_cb)gsw_handle
-					 ->gsw_brdgport_ops
-					 .BridgePort_ConfigGet,
-					 gsw_handle, &brportcfg);
+			ret = gsw_core_api(
+				(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigGet,
+				gsw_handle, &brportcfg);
 			if (ret != GSW_statusOk) {
 				PR_ERR("failed getting br port cfg\r\n");
 				return -1;
@@ -194,10 +195,9 @@ int dp_swdev_bridge_port_cfg_reset(struct br_info *br_item,
 			brportcfg.eMask =
 				 GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID |
 				 GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
-			ret = gsw_core_api((dp_gsw_cb)gsw_handle
-					 ->gsw_brdgport_ops
-					 .BridgePort_ConfigSet,
-					 gsw_handle, &brportcfg);
+			ret = gsw_core_api(
+				(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigSet,
+				gsw_handle, &brportcfg);
 			if (ret != GSW_statusOk) {
 				PR_ERR("Fail alloc/cfg br port\n");
 				return -1;
@@ -229,8 +229,8 @@ int dp_swdev_bridge_cfg_set(int inst, u16 fid)
 	brcfg.eForwardBroadcast = GSW_BRIDGE_FORWARD_FLOOD;
 	brcfg.eForwardUnknownMulticastNonIp = GSW_BRIDGE_FORWARD_FLOOD;
 	brcfg.eForwardUnknownUnicast = GSW_BRIDGE_FORWARD_FLOOD;
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdg_ops
-			   .Bridge_ConfigSet, gsw_handle, &brcfg);
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdg_ops.Bridge_ConfigSet,
+			   gsw_handle, &brcfg);
 	if (ret != GSW_statusOk) {
 		PR_ERR("Failed to set bridge id(%d)\n", brcfg.nBridgeId);
 		br.nBridgeId = fid;
@@ -309,23 +309,28 @@ int dp_gswip_ext_vlan(int inst, int vap, int ep)
 					 vlan_prop.in_proto, vlan_prop.in_vid);
 				DP_DEBUG(DP_DBG_FLAG_SWDEV,
 					 "VLAN out proto=%x, vid=%d\n",
-					 vlan_prop.out_proto, vlan_prop.out_vid);
-				vlan->vlan2_list[v2].outer_vlan.vid = vlan_prop.out_vid;
+					 vlan_prop.out_proto,
+					 vlan_prop.out_vid);
+				vlan->vlan2_list[v2].outer_vlan.vid =
+							vlan_prop.out_vid;
 				vlan->vlan2_list[v2].outer_vlan.tpid =
-								vlan_prop.out_proto;
+							vlan_prop.out_proto;
 				vlan->vlan2_list[v2].ether_type = 0;
-				vlan->vlan2_list[v2].inner_vlan.vid = vlan_prop.in_vid;
+				vlan->vlan2_list[v2].inner_vlan.vid =
+							vlan_prop.in_vid;
 				vlan->vlan2_list[v2].inner_vlan.tpid =
-								vlan_prop.in_proto;
+							vlan_prop.in_proto;
 				vlan->vlan2_list[v2].bp = tmp->bp;
 				v2 += 1;
 			} else if (vlan_prop.num == 1) {
 				DP_DEBUG(DP_DBG_FLAG_SWDEV,
 					 "outer VLAN proto=%x, vid=%d\n",
-					 vlan_prop.out_proto, vlan_prop.out_vid);
-				vlan->vlan1_list[v1].outer_vlan.vid = vlan_prop.out_vid;
+					 vlan_prop.out_proto,
+					 vlan_prop.out_vid);
+				vlan->vlan1_list[v1].outer_vlan.vid =
+							vlan_prop.out_vid;
 				vlan->vlan1_list[v1].outer_vlan.tpid =
-								vlan_prop.out_proto;
+							vlan_prop.out_proto;
 				vlan->vlan1_list[v1].bp = tmp->bp;
 				v1 += 1;
 			}
diff --git a/drivers/net/datapath/dpm/gswip31/datapath_tc_asym_vlan.c b/drivers/net/datapath/dpm/gswip31/datapath_tc_asym_vlan.c
index 59e3f2f53308..85341fce773f 100644
--- a/drivers/net/datapath/dpm/gswip31/datapath_tc_asym_vlan.c
+++ b/drivers/net/datapath/dpm/gswip31/datapath_tc_asym_vlan.c
@@ -127,11 +127,14 @@ static int update_ctp(struct core_ops *ops,
 	ctpcfg2.nSubIfIdGroup = subifidg;
 	if (ingress) {
 		if (multicast) {
-			ctpcfg1.eMask = GSW_CTP_PORT_CONFIG_MASK_INGRESS_VLAN_IGMP;
+			ctpcfg1.eMask =
+				GSW_CTP_PORT_CONFIG_MASK_INGRESS_VLAN_IGMP;
 			if (!pextvlan) {
-				ctpcfg2.bIngressExtendedVlanIgmpEnable = LTQ_FALSE;
+				ctpcfg2.bIngressExtendedVlanIgmpEnable =
+								LTQ_FALSE;
 			} else {
-				ctpcfg2.bIngressExtendedVlanIgmpEnable = LTQ_TRUE;
+				ctpcfg2.bIngressExtendedVlanIgmpEnable =
+								LTQ_TRUE;
 				ctpcfg2.nIngressExtendedVlanBlockIdIgmp =
 					pextvlan->nExtendedVlanBlockId;
 				ctpcfg2.nIngressExtendedVlanBlockSizeIgmp = 0;
@@ -149,11 +152,14 @@ static int update_ctp(struct core_ops *ops,
 		}
 	} else {
 		if (multicast) {
-			ctpcfg1.eMask = GSW_CTP_PORT_CONFIG_MASK_EGRESS_VLAN_IGMP;
+			ctpcfg1.eMask =
+				GSW_CTP_PORT_CONFIG_MASK_EGRESS_VLAN_IGMP;
 			if (!pextvlan) {
-				ctpcfg2.bEgressExtendedVlanIgmpEnable = LTQ_FALSE;
+				ctpcfg2.bEgressExtendedVlanIgmpEnable =
+								LTQ_FALSE;
 			} else {
-				ctpcfg2.bEgressExtendedVlanIgmpEnable = LTQ_TRUE;
+				ctpcfg2.bEgressExtendedVlanIgmpEnable =
+								LTQ_TRUE;
 				ctpcfg2.nEgressExtendedVlanBlockIdIgmp =
 					pextvlan->nExtendedVlanBlockId;
 				ctpcfg2.nEgressExtendedVlanBlockSizeIgmp = 0;
@@ -182,12 +188,14 @@ static int update_ctp(struct core_ops *ops,
 
 	if (ingress) {
 		if (multicast) {
-			if (ctpcfg1.bIngressExtendedVlanIgmpEnable != LTQ_FALSE) {
+			if (ctpcfg1.bIngressExtendedVlanIgmpEnable !=
+								LTQ_FALSE) {
 				GSW_EXTENDEDVLAN_alloc_t alloc = {0};
 
 				alloc.nExtendedVlanBlockId =
 					ctpcfg1.nIngressExtendedVlanBlockIdIgmp;
-				ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+				ops->gsw_extvlan_ops.ExtendedVlan_Free(ops,
+									&alloc);
 			}
 		} else {
 			if (ctpcfg1.bIngressExtendedVlanEnable != LTQ_FALSE) {
@@ -195,17 +203,20 @@ static int update_ctp(struct core_ops *ops,
 
 				alloc.nExtendedVlanBlockId =
 					ctpcfg1.nIngressExtendedVlanBlockId;
-				ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+				ops->gsw_extvlan_ops.ExtendedVlan_Free(ops,
+									&alloc);
 			}
 		}
 	} else {
 		if (multicast) {
-			if (ctpcfg1.bEgressExtendedVlanIgmpEnable != LTQ_FALSE) {
+			if (ctpcfg1.bEgressExtendedVlanIgmpEnable !=
+								LTQ_FALSE) {
 				GSW_EXTENDEDVLAN_alloc_t alloc = {0};
 
 				alloc.nExtendedVlanBlockId =
 					ctpcfg1.nEgressExtendedVlanBlockIdIgmp;
-				ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+				ops->gsw_extvlan_ops.ExtendedVlan_Free(ops,
+									&alloc);
 			}
 		} else {
 			if (ctpcfg1.bEgressExtendedVlanEnable != LTQ_FALSE) {
@@ -213,7 +224,8 @@ static int update_ctp(struct core_ops *ops,
 
 				alloc.nExtendedVlanBlockId =
 					ctpcfg1.nEgressExtendedVlanBlockId;
-				ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+				ops->gsw_extvlan_ops.ExtendedVlan_Free(ops,
+									&alloc);
 			}
 		}
 	}
@@ -603,11 +615,10 @@ static int ext_vlan_action_cfg(struct core_ops *ops,
 	       sizeof(pcfg->nDscp2PcpMap));
 
 	DP_DEBUG(DP_DBG_FLAG_PAE, "act->ract.act: 0x%02x Bp_dev %p\n",
-		(unsigned int)act->ract.act, act->ract.bp_dev);
+		 (unsigned int)act->ract.act, act->ract.bp_dev);
 
 	/* Reassign Bridge Port */
 	if ((act->ract.act & DP_BP_REASSIGN) && (act->ract.bp_dev)) {
-		
 		ret = dp_get_port_subitf_via_dev(act->ract.bp_dev, &subif);
 		if (ret == DP_FAILURE)
 			return 0;
@@ -620,13 +631,12 @@ static int ext_vlan_action_cfg(struct core_ops *ops,
 
 	/* Reassign traffic class */
 	if (act->ract.act & DP_TC_REASSIGN) {
-		
 		DP_DEBUG(DP_DBG_FLAG_PAE, "New TC %d\n", act->ract.new_tc);
-		
+
 		pcfg->bNewTrafficClassEnable = 1;
 		pcfg->nNewTrafficClass = act->ract.new_tc;
 	}
-	
+
 	/* forward without modification */
 	if ((act->act & DP_VLAN_ACT_FWD))
 		return 0;
@@ -891,7 +901,9 @@ int tc_vlan_set_31(struct core_ops *ops,
 	if ((info->dev_type & 0x01) != 0) {
 		int ret;
 
-		/* Multicast (IGMP Controlled) VLAN is not supported on Bridge Port */
+		/* Multicast (IGMP Controlled) VLAN is not supported
+		 * on Bridge Port
+		 */
 		if ((info->dev_type & 0x02) != 0)
 			return -EINVAL;
 
diff --git a/drivers/net/datapath/dpm/gswip31/datapath_tx.c b/drivers/net/datapath/dpm/gswip31/datapath_tx.c
index 86819e6323cd..fcd0dab02c5a 100644
--- a/drivers/net/datapath/dpm/gswip31/datapath_tx.c
+++ b/drivers/net/datapath/dpm/gswip31/datapath_tx.c
@@ -425,8 +425,8 @@ int32_t dp_xmit_31(struct net_device *rx_if, dp_subif_t *rx_subif,
 		break;
 	case DP_XMIT_ERR_NULL_SKB:
 		PR_RATELIMITED("skb NULL");
-		UP_STATS(get_dp_port_info(inst, rx_subif->port_id)->
-			 tx_err_drop);
+		UP_STATS(get_dp_port_info(inst,
+					  rx_subif->port_id)->tx_err_drop);
 		break;
 	case DP_XMIT_ERR_NULL_IF:
 		UP_STATS(mib->tx_pkt_dropped);
@@ -454,6 +454,4 @@ int32_t dp_xmit_31(struct net_device *rx_if, dp_subif_t *rx_subif,
 	if (skb)
 		dev_kfree_skb_any(skb);
 	return DP_FAILURE;
-
 }
-
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_ext_vlan.c b/drivers/net/datapath/dpm/gswip32/datapath_ext_vlan.c
index 8613354eaadd..eef0c7b7af29 100644
--- a/drivers/net/datapath/dpm/gswip32/datapath_ext_vlan.c
+++ b/drivers/net/datapath/dpm/gswip32/datapath_ext_vlan.c
@@ -252,9 +252,8 @@ static int update_ctp(struct core_ops *ops, struct ext_vlan_info *vlan)
 			 "ingress VLAN operation disabled in ctp\n");
 		alloc.nExtendedVlanBlockId = block;
 		ret = ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
-		if (ret != GSW_statusOk) {
+		if (ret != GSW_statusOk)
 			return -EIO;
-		}
 	}
 	memset(&alloc, 0, sizeof(GSW_EXTENDEDVLAN_alloc_t));
 
@@ -263,7 +262,7 @@ static int update_ctp(struct core_ops *ops, struct ext_vlan_info *vlan)
 	alloc.nNumberOfEntries += vlan->n_vlan1_drop * 2;
 	alloc.nNumberOfEntries += vlan->n_vlan2_drop;
 	if (alloc.nNumberOfEntries == 0) {
-		DP_DEBUG(DP_DBG_FLAG_SWDEV,"nNumberOfEntries == 0\n");
+		DP_DEBUG(DP_DBG_FLAG_SWDEV, "nNumberOfEntries == 0\n");
 		return 0;
 	}
 
@@ -310,9 +309,9 @@ static int update_ctp(struct core_ops *ops, struct ext_vlan_info *vlan)
 	ctp.nIngressExtendedVlanBlockSize = 0;
 	ret = ops->gsw_ctp_ops.CTP_PortConfigSet(ops, &ctp);
 
-	if (ret != GSW_statusOk) {
+	if (ret != GSW_statusOk)
 		return -EIO;
-	}
+
 	return 0;
 UPDATE_ERROR:
 	ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
@@ -379,7 +378,7 @@ static int bp_add_vlan1(struct core_ops *ops, struct vlan1 *vlan,
 
 		if (ret != GSW_statusOk) {
 			PR_ERR("Fail updating Extended VLAN entry (%u, %u).\n",
-				alloc.nExtendedVlanBlockId, i);
+			       alloc.nExtendedVlanBlockId, i);
 			ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
 			return -EIO;
 		}
@@ -468,7 +467,7 @@ static int bp_add_vlan2(struct core_ops *ops, struct vlan2 *vlan,
 
 		if (ret != GSW_statusOk) {
 			PR_ERR("Fail updating Extended VLAN entry (%u, %u).\n",
-				alloc.nExtendedVlanBlockId, i);
+			       alloc.nExtendedVlanBlockId, i);
 			ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
 			return -EIO;
 		}
@@ -539,7 +538,8 @@ static int bp_diff(u32 *bp0, u32 num_bp0, u32 *bp1)
 	u32 i, j;
 
 	for (i = 0; i < num_bp0; i++) {
-		for (j = 0; j < num_bp0 && bp0[j] != bp1[i]; j++);
+		for (j = 0; j < num_bp0 && bp0[j] != bp1[i]; j++)
+			;
 		if (j >= num_bp0)
 			break;
 	}
@@ -548,7 +548,7 @@ static int bp_diff(u32 *bp0, u32 num_bp0, u32 *bp1)
 
 /* Function for VLAN configure */
 int set_gswip_ext_vlan_32(struct core_ops *ops, struct ext_vlan_info *vlan,
-		       int flag)
+			  int flag)
 {
 	static GSW_BRIDGE_portConfig_t bpcfg;
 
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_gswip.c b/drivers/net/datapath/dpm/gswip32/datapath_gswip.c
index 344b91229d17..d78e313a23ac 100644
--- a/drivers/net/datapath/dpm/gswip32/datapath_gswip.c
+++ b/drivers/net/datapath/dpm/gswip32/datapath_gswip.c
@@ -140,8 +140,9 @@ int dp_pmac_set_32(int inst, u32 port, dp_pmac_cfg_t *pmac_cfg)
 				igcfg.nTxDmaChanId =
 					pmac_cfg->ig_pmac.tx_dma_chan;
 			}
-			gsw_core_api((dp_gsw_cb)gswr_r->gsw_pmac_ops.
-				     Pmac_Ig_CfgGet, gswr_r, &igcfg);
+			gsw_core_api(
+				(dp_gsw_cb)gswr_r->gsw_pmac_ops.Pmac_Ig_CfgGet,
+				gswr_r, &igcfg);
 
 			/*update igcfg and write back to gsw */
 			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_ERR_DISC)
@@ -241,8 +242,9 @@ int dp_pmac_set_32(int inst, u32 port, dp_pmac_cfg_t *pmac_cfg)
 
 			DP_DEBUG(DP_DBG_FLAG_DBG, "\n");
 
-			gsw_core_api((dp_gsw_cb)gswr_r->gsw_pmac_ops
-				     .Pmac_Ig_CfgSet, gswr_r, &igcfg);
+			gsw_core_api(
+				(dp_gsw_cb)gswr_r->gsw_pmac_ops.Pmac_Ig_CfgSet,
+				gswr_r, &igcfg);
 		}
 
 			kfree(dqport.deq_info);
@@ -263,8 +265,9 @@ int dp_pmac_set_32(int inst, u32 port, dp_pmac_cfg_t *pmac_cfg)
 			egcfg.nBslTrafficClass = i;
 
 			memset(&pmac_glb, 0, sizeof(pmac_glb));
-			gsw_core_api((dp_gsw_cb)gswr_r->gsw_pmac_ops
-				     .Pmac_Gbl_CfgGet, gswr_r, &pmac_glb);
+			gsw_core_api(
+				(dp_gsw_cb)gswr_r->gsw_pmac_ops.Pmac_Gbl_CfgGet,
+				gswr_r, &pmac_glb);
 			egcfg.bProcFlagsSelect = pmac_glb.bProcFlagsEgCfgEna;
 			DP_DEBUG(DP_DBG_FLAG_DBG, "bProcFlagsSelect=%u\n",
 				 egcfg.bProcFlagsSelect);
@@ -397,8 +400,9 @@ int dp_pmac_set_32(int inst, u32 port, dp_pmac_cfg_t *pmac_cfg)
 					 egcfg.bMpe2Flag);
 			}
 #endif
-			gsw_core_api((dp_gsw_cb)gswr_r->gsw_pmac_ops
-				     .Pmac_Eg_CfgSet, gswr_r, &egcfg);
+			gsw_core_api(
+				(dp_gsw_cb)gswr_r->gsw_pmac_ops.Pmac_Eg_CfgSet,
+				gswr_r, &egcfg);
 			;
 		}
 	}
@@ -419,8 +423,8 @@ int dp_set_gsw_parser_32(u8 flag, u8 cpu, u8 mpe1,
 	GSW_CPU_PortCfg_t param = {0};
 	struct core_ops *gsw_handle = dp_port_prop[0].ops[0];/*gswip o */
 
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_common_ops
-			 .CPU_PortCfgGet, gsw_handle, &param)) {
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_common_ops.CPU_PortCfgGet,
+			 gsw_handle, &param)) {
 		PR_ERR("Failed GSW_CPU_PORT_CFG_GET\n");
 		return -1;
 	}
@@ -442,8 +446,8 @@ int dp_set_gsw_parser_32(u8 flag, u8 cpu, u8 mpe1,
 	if (flag & F_MPE1_MPE2)
 		param.eMPE1MPE2ParserCfg = mpe3;
 
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_common_ops
-			 .CPU_PortCfgSet, gsw_handle, &param)) {
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_common_ops.CPU_PortCfgSet,
+			 gsw_handle, &param)) {
 		PR_ERR("Failed GSW_CPU_PORT_CFG_SET\n");
 		return -1;
 	}
@@ -460,8 +464,8 @@ int dp_get_gsw_parser_32(u8 *cpu, u8 *mpe1, u8 *mpe2,
 	GSW_CPU_PortCfg_t param = {0};
 	struct core_ops *gsw_handle = dp_port_prop[0].ops[0]; /*gswip 0*/
 
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_common_ops
-			 .CPU_PortCfgGet, gsw_handle, &param)) {
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_common_ops.CPU_PortCfgGet,
+			 gsw_handle, &param)) {
 		PR_ERR("Failed GSW_CPU_PORT_CFG_GET\n");
 		return -1;
 	}
@@ -512,7 +516,7 @@ int gsw_mib_reset_32(int dev, u32 flag)
 
 /* Return allocated ctp number */
 struct gsw_itf *ctp_port_assign_32(int inst, u8 ep, int bp_default,
-				u32 flags, struct dp_dev_data *data)
+				   u32 flags, struct dp_dev_data *data)
 {
 	GSW_CTP_portAssignment_t ctp_assign;
 	const struct ctp_assign *assign = &ctp_assign_def;
@@ -531,10 +535,9 @@ struct gsw_itf *ctp_port_assign_32(int inst, u8 ep, int bp_default,
 		ctp_assign.eMode = itf_assign[ep].mode;
 		ctp_assign.nFirstCtpPortId = itf_assign[ep].start;
 		ctp_assign.nNumberOfCtpPort = itf_assign[ep].n;
-		if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_ctp_ops
-				 .CTP_PortAssignmentFree,
-				 gsw_handle,
-				 &ctp_assign) != 0) {
+		if (gsw_core_api(
+			(dp_gsw_cb)gsw_handle->gsw_ctp_ops.CTP_PortAssignmentFree,
+			gsw_handle, &ctp_assign) != 0) {
 			PR_ERR("Failed to allc CTP for ep=%d blk=%d mode=%d\n",
 			       ep, assign->num, assign->emode);
 			return NULL;
@@ -558,10 +561,10 @@ struct gsw_itf *ctp_port_assign_32(int inst, u8 ep, int bp_default,
 	ctp_assign.nBridgePortId = bp_default;
 	ctp_assign.nFirstCtpPortId = 0;
 	ctp_assign.nNumberOfCtpPort = num;
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_ctp_ops
-			  .CTP_PortAssignmentAlloc,
-			  gsw_handle,
-			  &ctp_assign) != 0) {
+	if (gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_ctp_ops.CTP_PortAssignmentAlloc,
+		gsw_handle,
+		&ctp_assign) != 0) {
 		PR_ERR("Failed CTP Assignment for ep=%d blk size=%d mode=%s\n",
 		       ep, num, ctp_mode_string(assign->emode));
 		return NULL;
@@ -607,7 +610,7 @@ int set_port_lookup_mode_32(int inst, u8 ep, u32 flags)
 
 /*Allocate a bridge port with specified FID and hardcoded CPU port member */
 int alloc_bridge_port_32(int inst, int port_id, int subif_ix,
-		      int fid, int bp_member)
+			 int fid, int bp_member)
 {
 	GSW_BRIDGE_portAlloc_t bp = {0};
 	GSW_BRIDGE_portConfig_t bp_cfg = {0};
@@ -617,8 +620,9 @@ int alloc_bridge_port_32(int inst, int port_id, int subif_ix,
 	gsw_handle = dp_port_prop[inst].ops[GSWIP_L];
 	/*allocate a free bridge port */
 	memset(&bp, 0, sizeof(bp));
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			   .BridgePort_Alloc, gsw_handle, &bp);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_Alloc,
+		gsw_handle, &bp);
 	if ((ret != GSW_statusOk) ||
 	    (bp.nBridgePortId < 0)) {
 		PR_ERR("Failed to get a bridge port\n");
@@ -633,8 +637,8 @@ int alloc_bridge_port_32(int inst, int port_id, int subif_ix,
 	/* By default Disable src mac learning for registered
 	 * non CPU bridge port with DP
 	 */
-	if (get_dp_port_subif(get_dp_port_info(inst, port_id), subif_ix)->
-	    mac_learn_dis == DP_MAC_LEARNING_DIS)
+	if (get_dp_port_subif(get_dp_port_info(inst, port_id),
+			      subif_ix)->mac_learn_dis == DP_MAC_LEARNING_DIS)
 		bp_cfg.bSrcMacLearningDisable = 1;
 	else
 		bp_cfg.bSrcMacLearningDisable = 0;
@@ -645,13 +649,15 @@ int alloc_bridge_port_32(int inst, int port_id, int subif_ix,
 	bp_cfg.nBridgeId = fid;
 
 	SET_BP_MAP(bp_cfg.nBridgePortMap, bp_member); /*CPU*/
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			     .BridgePort_ConfigSet, gsw_handle, &bp_cfg);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigSet,
+		gsw_handle, &bp_cfg);
 	if (ret != GSW_statusOk) {
 		PR_ERR("Failed to set bridge id(%d) and port map for bp= %d\n",
 		       fid, bp_cfg.nBridgePortId);
-		gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			     .BridgePort_Free, gsw_handle, &bp);
+		gsw_core_api(
+			(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_Free,
+			gsw_handle, &bp);
 		return -1;
 	}
 
@@ -660,23 +666,27 @@ int alloc_bridge_port_32(int inst, int port_id, int subif_ix,
 	 */
 	bp_cfg.nBridgePortId = bp_member;
 	bp_cfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			   .BridgePort_ConfigGet, gsw_handle, &bp_cfg);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigGet,
+		gsw_handle, &bp_cfg);
 	if (ret != GSW_statusOk) {
 		PR_ERR("Failed to get bridge port's member for bridgeport=%d\n",
 		       bp_cfg.nBridgePortId);
-		gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			     .BridgePort_Free, gsw_handle, &bp);
+		gsw_core_api(
+			(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_Free,
+			gsw_handle, &bp);
 		return -1;
 	}
 	SET_BP_MAP(bp_cfg.nBridgePortMap, bp.nBridgePortId);
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			   .BridgePort_ConfigSet, gsw_handle, &bp_cfg);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigSet,
+		gsw_handle, &bp_cfg);
 	if (ret != GSW_statusOk) {
 		PR_ERR("Failed to set bridge port's member for bridgeport=%d\n",
 		       bp_cfg.nBridgePortId);
-		gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			     .BridgePort_Free, gsw_handle, &bp);
+		gsw_core_api(
+			(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_Free,
+			gsw_handle, &bp);
 		return -1;
 	}
 
@@ -707,8 +717,9 @@ int free_bridge_port_32(int inst, int bp)
 	/*read out this delting bridge port's member*/
 	tmp->nBridgePortId = bp;
 	tmp->eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			   .BridgePort_ConfigGet, gsw_handle, tmp);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigGet,
+		gsw_handle, tmp);
 	if (ret != GSW_statusOk) {
 		PR_ERR("Failed GSW_BRIDGE_PORT_CONFIG_GET: %d\n", bp);
 		goto EXIT;
@@ -723,19 +734,17 @@ int free_bridge_port_32(int inst, int bp)
 			tmp2->eMask =
 				GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
 			tmp2->nBridgePortId = i * 16 + j;
-			ret = gsw_core_api((dp_gsw_cb)gsw_handle
-					   ->gsw_brdgport_ops
-					   .BridgePort_ConfigGet, gsw_handle,
-					   tmp2);
+			ret = gsw_core_api(
+				(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigGet,
+				gsw_handle, tmp2);
 			if (ret != GSW_statusOk) {
 				PR_ERR("Failed GSW_BRIDGE_PORT_CONFIG_GET\n");
 				goto EXIT;
 			}
 			UNSET_BP_MAP(tmp2->nBridgePortMap, bp);
-			ret = gsw_core_api((dp_gsw_cb)gsw_handle
-					   ->gsw_brdgport_ops
-					   .BridgePort_ConfigSet, gsw_handle,
-					   tmp2);
+			ret = gsw_core_api(
+				(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigSet,
+				gsw_handle, tmp2);
 			if (ret != GSW_statusOk) {
 				PR_ERR("Failed GSW_BRIDGE_PORT_CONFIG_SET\n");
 				goto EXIT;
@@ -747,8 +756,9 @@ int free_bridge_port_32(int inst, int bp)
 	memset(tmp, 0, sizeof(*tmp));
 	tmp->nBridgePortId = bp;
 	tmp->eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			   .BridgePort_Free, gsw_handle, tmp);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_Free,
+		gsw_handle, tmp);
 	if (ret != GSW_statusOk)
 		PR_ERR("Failed to GSW_BRIDGE_PORT_FREE:%d\n", bp);
 FREE_EXIT:
@@ -771,8 +781,9 @@ int dp_gswip_mac_entry_add_32(int bport, int fid, int inst, u8 *addr)
 	SET_BP_MAP(tmp.nPortMap, bport);
 	tmp.nSubIfId = 0;
 	memcpy(tmp.nMAC, addr, GSW_MAC_ADDR_LEN);
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_swmac_ops.
-			   MAC_TableEntryAdd, gsw_handle, &tmp);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_swmac_ops.MAC_TableEntryAdd,
+		gsw_handle, &tmp);
 	if (ret != GSW_statusOk) {
 		PR_ERR("fail in setting MAC table static add entry\r\n");
 		return -1;
@@ -792,16 +803,18 @@ int dp_gswip_mac_entry_del_32(int bport, int fid, int inst, u8 *addr)
 	memset(&mac_query, 0, sizeof(mac_query));
 	mac_query.nFId = fid;
 	memcpy(mac_query.nMAC, addr, GSW_MAC_ADDR_LEN);
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_swmac_ops.
-			   MAC_TableEntryQuery, gsw_handle, &tmp);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_swmac_ops.MAC_TableEntryQuery,
+		gsw_handle, &tmp);
 	if (ret != GSW_statusOk) {
 		PR_ERR("fail in getting MAC query entry\r\n");
 		return -1;
 	}
 	tmp.nFId = fid;
 	memcpy(tmp.nMAC, addr, GSW_MAC_ADDR_LEN);
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_swmac_ops.
-			   MAC_TableEntryRemove, gsw_handle, &tmp);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_swmac_ops.MAC_TableEntryRemove,
+		gsw_handle, &tmp);
 	if (ret != GSW_statusOk) {
 		PR_ERR("fail in setting MAC static entry remove\r\n");
 		return -1;
@@ -818,16 +831,18 @@ int cpu_vlan_mod_dis_32(int inst)
 	ops = dp_port_prop[inst].ops[GSWIP_L];
 
 	cfg.nPortId = 0;
-	ret = gsw_core_api((dp_gsw_cb)ops->gsw_qos_ops.
-			   QoS_PortRemarkingCfgGet, ops, &cfg);
+	ret = gsw_core_api(
+		(dp_gsw_cb)ops->gsw_qos_ops.QoS_PortRemarkingCfgGet,
+		ops, &cfg);
 	if (ret != GSW_statusOk) {
 		PR_ERR("QoS_PortRemarkingCfgGet failed\n");
 		return -1;
 	}
 
 	cfg.bPCP_EgressRemarkingEnable = LTQ_FALSE;
-	ret = gsw_core_api((dp_gsw_cb)ops->gsw_qos_ops.
-			   QoS_PortRemarkingCfgSet, ops, &cfg);
+	ret = gsw_core_api(
+		(dp_gsw_cb)ops->gsw_qos_ops.QoS_PortRemarkingCfgSet,
+		ops, &cfg);
 	if (ret != GSW_statusOk) {
 		PR_ERR("QoS_PortRemarkingCfgSet failed\n");
 		return -1;
@@ -892,8 +907,9 @@ int dp_set_gsw_pmapper_32(int inst, int bport, int lport,
 		 "call switch api mode %d enable %d eMask 0x%x\n",
 		 bp_cfg.ePmapperMappingMode, bp_cfg.bPmapperEnable,
 		 bp_cfg.eMask);
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			     .BridgePort_ConfigSet, gsw_handle, &bp_cfg);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigSet,
+		gsw_handle, &bp_cfg);
 	if (ret != GSW_statusOk) {
 		PR_ERR("fail in setting pmapper\r\n");
 		return -1;
@@ -922,8 +938,9 @@ int dp_get_gsw_pmapper_32(int inst, int bport, int lport,
 	bp_cfg.nDestLogicalPortId = lport;
 	bp_cfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_CTP_MAPPING;
 
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			     .BridgePort_ConfigGet, gsw_handle, &bp_cfg);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigGet,
+		gsw_handle, &bp_cfg);
 	if (ret != GSW_statusOk) {
 		PR_ERR("fail in getting pmapper\r\n");
 		return -1;
@@ -1181,7 +1198,8 @@ int dp_meter_add_32(struct net_device *dev,  struct dp_meter_cfg  *meter,
 			goto err;
 		}
 		port_info =
-		get_dp_port_info(mtr_subif->subif.inst, mtr_subif->subif.port_id);
+		get_dp_port_info(mtr_subif->subif.inst,
+				 mtr_subif->subif.port_id);
 		if (!port_info) {
 			PR_ERR(" port_info is NULL\n");
 			bret = -1;
@@ -1541,10 +1559,9 @@ int gpid_port_assign(int inst, u8 ep, u32 flags)
 		gpid_base = get_dp_port_info(inst, ep)->gpid_base;
 		gpid_num = get_dp_port_info(inst, ep)->gpid_num;
 
-		if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_gpid_ops
-				 .LpidToGpid_AssignmentSet,
-				 gsw_handle,
-				 &lp_gp_assign) != 0) {
+		if (gsw_core_api(
+			(dp_gsw_cb)gsw_handle->gsw_gpid_ops.LpidToGpid_AssignmentSet,
+			gsw_handle, &lp_gp_assign) != 0) {
 			PR_ERR("Fail to assign Lpid->Gpid table %d in GSWIP\n",
 			       ep);
 			return DP_FAILURE;
@@ -1553,10 +1570,9 @@ int gpid_port_assign(int inst, u8 ep, u32 flags)
 		gp_lp_assign.nLogicalPortId = ep;
 		gp_lp_assign.nGlobalPortId = gpid_base;
 
-		if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_gpid_ops
-				 .GpidToLpid_AssignmentSet,
-				 gsw_handle,
-				 &gp_lp_assign) != 0) {
+		if (gsw_core_api(
+			(dp_gsw_cb)gsw_handle->gsw_gpid_ops.GpidToLpid_AssignmentSet,
+			gsw_handle, &gp_lp_assign) != 0) {
 			PR_ERR("Fail to assign GPID->LPID table %d in GSWIP\n",
 			       ep);
 			return DP_FAILURE;
@@ -1581,17 +1597,16 @@ int gpid_port_assign(int inst, u8 ep, u32 flags)
 		DP_ERR("Fail to alloc special GPID for dpid=%d\n", ep);
 		return DP_FAILURE;
 	}
-	DP_DEBUG(DP_DBG_FLAG_DBG, "Lpid->Gpid table ep=%d \n", ep);
+	DP_DEBUG(DP_DBG_FLAG_DBG, "Lpid->Gpid table ep=%d\n", ep);
 	PR_INFO("Alloc GPID=%d num=%d spl_GPID=%d\n", gpid_base, gpid_num,
 		gpid_spl);
 	lp_gp_assign.nLogicalPortId = ep;
 	lp_gp_assign.nFirstGlobalPortId = gpid_base;
 	lp_gp_assign.nNumberOfGlobalPort = gpid_num;
 
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_gpid_ops
-			 .LpidToGpid_AssignmentSet,
-			 gsw_handle,
-			 &lp_gp_assign) != 0) {
+	if (gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_gpid_ops.LpidToGpid_AssignmentSet,
+		gsw_handle, &lp_gp_assign) != 0) {
 #if 0
 		PR_ERR("Fail to assign Lpid->Gpid table %d in GSWIP\n",
 		       ep);
@@ -1599,15 +1614,14 @@ int gpid_port_assign(int inst, u8 ep, u32 flags)
 		return DP_FAILURE;
 	}
 
-	DP_DEBUG(DP_DBG_FLAG_DBG, "Gpid->Lpid table ep=%d \n", ep);
+	DP_DEBUG(DP_DBG_FLAG_DBG, "Gpid->Lpid table ep=%d\n", ep);
 
 	gp_lp_assign.nLogicalPortId = ep;
 	gp_lp_assign.nGlobalPortId = gpid_base;
 
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_gpid_ops
-			 .GpidToLpid_AssignmentSet,
-			 gsw_handle,
-			 &gp_lp_assign) != 0) {
+	if (gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_gpid_ops.GpidToLpid_AssignmentSet,
+		gsw_handle, &gp_lp_assign) != 0) {
 		PR_ERR("Fail to assign Gpid->Lpid table %d in GSWIP\n",
 		       ep);
 #if 0
@@ -1638,19 +1652,19 @@ int gpid_port_assign(int inst, u8 ep, u32 flags)
 	get_dp_port_info(inst, ep)->gpid_spl = gpid_spl;
 	if (gpid_spl >= 0)
 		priv->gp_dp_map[gpid_spl].dpid = ep;
-	for (i = 0; i < gpid_num; i++) {
+	for (i = 0; i < gpid_num; i++)
 		priv->gp_dp_map[gpid_base + i].dpid = ep;
-	}
+
 	priv->gp_dp_map[gpid_spl].dpid = ep;
 
 	/* alloc/configure speical GPID for this dp port */
-	PR_INFO("Try to add gpid_spl=%d\n", get_dp_port_info(inst, ep)->gpid_spl);
-	if (dp_add_pp_gpid(inst, ep, 0, get_dp_port_info(inst, ep)->gpid_spl, 1)) {
+	PR_INFO("Try to add gpid_spl=%d\n",
+		get_dp_port_info(inst, ep)->gpid_spl);
+	if (dp_add_pp_gpid(inst, ep, 0,
+			   get_dp_port_info(inst, ep)->gpid_spl, 1)) {
 		DP_ERR("dp_add_pp_gpid for dport/gpid=%d/%d\n", ep,
 		       get_dp_port_info(inst, ep)->gpid_spl);
 		return DP_FAILURE;
 	}
 	return DP_SUCCESS;
 }
-
-
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_lookup_proc.c b/drivers/net/datapath/dpm/gswip32/datapath_lookup_proc.c
index 65d7e25e9fb9..8caf76003eb8 100644
--- a/drivers/net/datapath/dpm/gswip32/datapath_lookup_proc.c
+++ b/drivers/net/datapath/dpm/gswip32/datapath_lookup_proc.c
@@ -412,7 +412,7 @@ int find_pattern(int port_id, struct seq_file *s, int qid)
 
 		if (lookup_match_qid[i] != priv->ppv4_drop_q) {
 			proc_printf(s, "    ");
-			for (j = LOOKUP_FIELD_BITS- 1; j >= 0; j--) {
+			for (j = LOOKUP_FIELD_BITS - 1; j >= 0; j--) {
 				if ((lookup_match_mask[i] >> j) & 1)
 					proc_printf(s, "%5c", 'x');
 				else
@@ -517,7 +517,8 @@ ssize_t proc_get_qid_via_index32(struct file *file, const char *buf,
 		PR_INFO("Get lookup[%05u 0x%04x] ->     queue[%u]\n",
 			lookup_index, lookup_index, qid);
 		return count;
-	} else if (dp_strncmpi(param_list[0], "find", strlen("find") + 1) == 0) {
+	} else if (dp_strncmpi(param_list[0], "find",
+		   strlen("find") + 1) == 0) {
 		/*read out its all flags for specified qid */
 		int i;
 
@@ -526,7 +527,7 @@ ssize_t proc_get_qid_via_index32(struct file *file, const char *buf,
 			find_pattern(i, NULL, qid);
 		return count;
 	} else if (dp_strncmpi(param_list[0], "find2",
-		   strlen("find2")+ 1) == 0) {
+		   strlen("find2") + 1) == 0) {
 		/*read out its all flags for specified qid */
 		qid = dp_atoi(param_list[1]);
 		lookup_table_via_qid(qid);
@@ -540,7 +541,7 @@ ssize_t proc_get_qid_via_index32(struct file *file, const char *buf,
 			old_q, new_q);
 		return count;
 	}  else if (dp_strncmpi(param_list[0], "test",
-		   strlen("test")+ 1) == 0) {
+		   strlen("test") + 1) == 0) {
 		cbm_queue_map_entry_t lookup = {0};
 		int inst = 0;
 		int qid = dp_atoi(param_list[1]);
@@ -557,12 +558,12 @@ ssize_t proc_get_qid_via_index32(struct file *file, const char *buf,
 		lookup.egflag = 0;
 		lookup.ep = 3;
 		cbm_queue_map_set(dp_port_prop[inst].cbm_inst, qid,
-			  &lookup, flag);
+				  &lookup, flag);
 		DP_INFO("cbm_queue_map_set ep=%d egflag=%d qid=%d flag=0x%x\n",
 			lookup.ep, lookup.egflag, qid, flag);
 		lookup.egflag = 1;
 		cbm_queue_map_set(dp_port_prop[inst].cbm_inst, qid,
-			  &lookup, flag);
+				  &lookup, flag);
 		DP_INFO("cbm_queue_map_set ep=%d egflag=%d qid=%d flag=0x%x\n",
 			lookup.ep, lookup.egflag, qid, flag);
 		return count;
@@ -707,7 +708,7 @@ void lookup_table_recursive(int k, int tmp_index, int set_flag, int qid)
 {
 	int i;
 	struct cbm_lookup lookup = {0};
-	
+
 	if (k < 0) {	/*finish recursive and start real read/set action */
 		if (set_flag) {
 			lookup.index = tmp_index;
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_misc.c b/drivers/net/datapath/dpm/gswip32/datapath_misc.c
index 42425c157645..9fc7a11c69b8 100644
--- a/drivers/net/datapath/dpm/gswip32/datapath_misc.c
+++ b/drivers/net/datapath/dpm/gswip32/datapath_misc.c
@@ -90,7 +90,7 @@ static void init_dma_pmac_template(int portid, u32 flags)
 		dp_info->dma3_mask_template[i].all = 0xFFFFFFFF;
 	}
 	if ((flags & DP_F_FAST_ETH_LAN) || (flags & DP_F_FAST_ETH_WAN) ||
-	    (flags & DP_F_GPON) || (flags & DP_F_EPON)|| (flags & DP_F_GINT)) {
+	    (flags & DP_F_GPON) || (flags & DP_F_EPON) || (flags & DP_F_GINT)) {
 		/*always with pmac */
 		for (i = 0; i < MAX_TEMPLATE; i++) {
 			/* Pmac Template */
@@ -177,9 +177,9 @@ static void init_dma_pmac_template(int portid, u32 flags)
 }
 
 void dump_rx_dma_desc_32(struct dma_rx_desc_0 *desc_0,
-		      struct dma_rx_desc_1 *desc_1,
-		      struct dma_rx_desc_2 *desc_2,
-		      struct dma_rx_desc_3 *desc_3)
+			 struct dma_rx_desc_1 *desc_1,
+			 struct dma_rx_desc_2 *desc_2,
+			 struct dma_rx_desc_3 *desc_3)
 {
 	if (!desc_0 || !desc_1 || !desc_2 || !desc_3) {
 		PR_ERR("rx desc_0/1/2/3 NULL\n");
@@ -198,12 +198,12 @@ void dump_rx_dma_desc_32(struct dma_rx_desc_0 *desc_0,
 		" lro_type=%d color=%d port=%d classid=%d\n",
 		desc_1->field.redir, desc_1->field.header_mode,
 		desc_1->field.pmac, desc_1->field.ts, desc_1->field.pre_l2,
-		desc_1->field.classen, desc_1->field.fcs, desc_1->field.pkt_type,
-		desc_1->field.src_pool, desc_1->field.dec, desc_1->field.enc,
-		desc_1->field.lro_type, desc_1->field.color, desc_1->field.ep,
-		desc_1->field.classid);
-	PR_INFO("  DW2: data_ptr=0x%08x ByteOffset=%d\n", desc_2->field.data_ptr,
-		desc_2->field.byte_offset);
+		desc_1->field.classen, desc_1->field.fcs,
+		desc_1->field.pkt_type, desc_1->field.src_pool,
+		desc_1->field.dec, desc_1->field.enc, desc_1->field.lro_type,
+		desc_1->field.color, desc_1->field.ep, desc_1->field.classid);
+	PR_INFO("  DW2: data_ptr=0x%08x ByteOffset=%d\n",
+		desc_2->field.data_ptr, desc_2->field.byte_offset);
 	PR_INFO("  DW3: own=%d c=%d sop=%d eop=%d dic=%d haddr=0x%08x"
 		" sp=%d pool_policy=%d data_len=%d\n",
 		desc_3->field.own, desc_3->field.c, desc_3->field.sop,
@@ -213,9 +213,9 @@ void dump_rx_dma_desc_32(struct dma_rx_desc_0 *desc_0,
 }
 
 void dump_tx_dma_desc_32(struct dma_tx_desc_0 *desc_0,
-		      struct dma_tx_desc_1 *desc_1,
-		      struct dma_tx_desc_2 *desc_2,
-		      struct dma_tx_desc_3 *desc_3)
+			 struct dma_tx_desc_1 *desc_1,
+			 struct dma_tx_desc_2 *desc_2,
+			 struct dma_tx_desc_3 *desc_3)
 {
 	int lookup;
 	int inst = 0;
@@ -236,18 +236,18 @@ void dump_tx_dma_desc_32(struct dma_tx_desc_0 *desc_0,
 		desc_0->field.flow_id, desc_0->field.dw0bit16,
 		"subif", desc_0->field.dest_sub_if_id);
 	PR_INFO("  DW1:redir=%d header_mode=%d pmac=%d ts=%d pre_l2=%d"
-		"classen=%d fcs=%d pkt_type=%d src_pool=%d dec=%d enc=%d"
-		"lro_type=%d color=%d port=%d classid=%d\n",
+		" classen=%d fcs=%d pkt_type=%d src_pool=%d dec=%d enc=%d"
+		" lro_type=%d color=%d port=%d classid=%d\n",
 		desc_1->field.redir, desc_1->field.header_mode,
 		desc_1->field.pmac, desc_1->field.ts, desc_1->field.pre_l2,
-		desc_1->field.classen, desc_1->field.fcs, desc_1->field.pkt_type,
-		desc_1->field.src_pool, desc_1->field.dec, desc_1->field.enc,
-		desc_1->field.lro_type, desc_1->field.color, desc_1->field.ep,
-		desc_1->field.classid);
+		desc_1->field.classen, desc_1->field.fcs,
+		desc_1->field.pkt_type, desc_1->field.src_pool,
+		desc_1->field.dec, desc_1->field.enc, desc_1->field.lro_type,
+		desc_1->field.color, desc_1->field.ep, desc_1->field.classid);
 	PR_INFO("  DW2:data_ptr=0x%08x ByteOffset=%d\n", desc_2->field.data_ptr,
 		desc_2->field.byte_offset);
 	PR_INFO("  DW3:own=%d c=%d sop=%d eop=%d dic=%d haddr=0x%08x"
-		"sp=%d pool_policy=%d data_len=%d\n",
+		" sp=%d pool_policy=%d data_len=%d\n",
 		desc_3->field.own, desc_3->field.c, desc_3->field.sop,
 		desc_3->field.eop, desc_3->field.dic, desc_3->field.haddr,
 		desc_3->field.sp, desc_3->field.pool_policy,
@@ -336,7 +336,7 @@ static void dump_rx_pmac(struct pmac_rx_hdr *pmac)
 		pmac->tcp_type);
 	/*byte 2 */
 	PR_INFO("  byte 2:class=%d res=%d src_dst_subif_id_14_12=%d\n",
-	pmac->class, pmac->res2, pmac->src_dst_subif_id_14_12);
+		pmac->class, pmac->res2, pmac->src_dst_subif_id_14_12);
 	/*byte 3 */
 	PR_INFO("  byte 3:pkt_type=%d ext=%d ins=%d pre_l2=%d oam=%d res32=%d\n",
 		pmac->pkt_type, pmac->ext, pmac->ins, pmac->pre_l2,
@@ -379,7 +379,7 @@ static void dump_tx_pmac(struct pmac_tx_hdr *pmac)
 		pmac->tcp_type);
 	/*byte 2 */
 	PR_INFO("  byte 2:igp_msb=%d res=%d\n", pmac->src_dst_subif_id_14_12,
-	pmac->res2);
+		pmac->res2);
 	/*byte 3 */
 	PR_INFO("  byte 3:%s=%d %s=%d %s=%d %s=%d %s=%d %s=%d %s=%d\n",
 		"pkt_type", pmac->pkt_type,
@@ -467,7 +467,7 @@ int alloc_q_to_port_32(struct ppv4_q_sch_port *info, u32 flag)
 		return -1;
 	}
 	subif = get_dp_port_subif(get_dp_port_info(info->inst, info->dp_port),
-			   info->ctp);
+				  info->ctp);
 	subif->qid = q.qid;
 	subif->q_node = q.qid;
 	info->qid = q.qid;
@@ -539,9 +539,9 @@ static int dp_gswip_remark_8P0D_set(int mode, int inst)
 	color_remark.nVal[13] = 11;
 	color_remark.nVal[14] = 13;
 	color_remark.nVal[15] = 15;
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-				.QOS_ColorReMarkingTableSet,
-				gsw_handle, &color_remark)) {
+	if (gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_qos_ops.QOS_ColorReMarkingTableSet,
+		gsw_handle, &color_remark)) {
 		PR_ERR("GSW_QOS_COLOR_REMARKING_CFG_SET failed\n");
 		return -1;
 	}
@@ -571,9 +571,9 @@ static int dp_gswip_remark_7P1D_set(int mode, int inst)
 	color_remark.nVal[13] = 9;
 	color_remark.nVal[14] = 13;
 	color_remark.nVal[15] = 15;
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-				.QOS_ColorReMarkingTableSet,
-				gsw_handle, &color_remark)) {
+	if (gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_qos_ops.QOS_ColorReMarkingTableSet,
+		gsw_handle, &color_remark)) {
 		PR_ERR("GSW_QOS_COLOR_REMARKING_CFG_SET failed\n");
 		return -1;
 	}
@@ -603,9 +603,9 @@ static int dp_gswip_remark_6P2D_set(int mode, int inst)
 	color_remark.nVal[13] = 9;
 	color_remark.nVal[14] = 13;
 	color_remark.nVal[15] = 15;
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-				.QOS_ColorReMarkingTableSet,
-				gsw_handle, &color_remark)) {
+	if (gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_qos_ops.QOS_ColorReMarkingTableSet,
+		gsw_handle, &color_remark)) {
 		PR_ERR("GSW_QOS_COLOR_REMARKING_CFG_SET failed\n");
 		return -1;
 	}
@@ -635,9 +635,9 @@ static int dp_gswip_remark_5P3D_set(int mode, int inst)
 	color_remark.nVal[13] = 9;
 	color_remark.nVal[14] = 13;
 	color_remark.nVal[15] = 15;
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-				.QOS_ColorReMarkingTableSet,
-				gsw_handle, &color_remark)) {
+	if (gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_qos_ops.QOS_ColorReMarkingTableSet,
+		gsw_handle, &color_remark)) {
 		PR_ERR("GSW_QOS_COLOR_REMARKING_CFG_SET failed\n");
 		return -1;
 	}
@@ -667,9 +667,9 @@ static int dp_gswip_remark_dscp_set(int mode, int inst)
 	color_remark.nVal[13] = 36;
 	color_remark.nVal[14] = 36;
 	color_remark.nVal[15] = 36;
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-				.QOS_ColorReMarkingTableSet,
-				gsw_handle, &color_remark)) {
+	if (gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_qos_ops.QOS_ColorReMarkingTableSet,
+		gsw_handle, &color_remark)) {
 		PR_ERR("GSW_QOS_COLOR_REMARKING_CFG_SET failed\n");
 		return -1;
 	}
@@ -811,9 +811,9 @@ static int dp_gswip_color_dscp_set(int mode, int inst)
 	color_mark.nColor[61] = GSW_DROP_PRECEDENCE_YELLOW;
 	color_mark.nColor[62] = GSW_DROP_PRECEDENCE_YELLOW;
 	color_mark.nColor[63] = GSW_DROP_PRECEDENCE_YELLOW;
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-				.QOS_ColorMarkingTableSet,
-				gsw_handle, &color_mark)) {
+	if (gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_qos_ops.QOS_ColorMarkingTableSet,
+		gsw_handle, &color_mark)) {
 		PR_ERR("GSW_QOS_COLOR_MARKING_CFG_SET failed\n");
 		return -1;
 	}
@@ -859,9 +859,9 @@ static int dp_gswip_color_5P3D_set(int mode, int inst)
 	color_mark.nColor[13] = GSW_DROP_PRECEDENCE_YELLOW;
 	color_mark.nColor[14] = GSW_DROP_PRECEDENCE_GREEN;
 	color_mark.nColor[15] = GSW_DROP_PRECEDENCE_YELLOW;
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-				.QOS_ColorMarkingTableSet,
-				gsw_handle, &color_mark)) {
+	if (gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_qos_ops.QOS_ColorMarkingTableSet,
+		gsw_handle, &color_mark)) {
 		PR_ERR("GSW_QOS_COLOR_MARKING_CFG_SET failed\n");
 		return -1;
 	}
@@ -907,9 +907,9 @@ static int dp_gswip_color_6P2D_set(int mode, int inst)
 	color_mark.nColor[13] = GSW_DROP_PRECEDENCE_YELLOW;
 	color_mark.nColor[14] = GSW_DROP_PRECEDENCE_GREEN;
 	color_mark.nColor[15] = GSW_DROP_PRECEDENCE_YELLOW;
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-				.QOS_ColorMarkingTableSet,
-				gsw_handle, &color_mark)) {
+	if (gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_qos_ops.QOS_ColorMarkingTableSet,
+		gsw_handle, &color_mark)) {
 		PR_ERR("GSW_QOS_COLOR_MARKING_CFG_SET failed\n");
 		return -1;
 	}
@@ -955,9 +955,9 @@ static int dp_gswip_color_7P1D_set(int mode, int inst)
 	color_mark.nColor[13] = GSW_DROP_PRECEDENCE_YELLOW;
 	color_mark.nColor[14] = GSW_DROP_PRECEDENCE_GREEN;
 	color_mark.nColor[15] = GSW_DROP_PRECEDENCE_YELLOW;
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-				.QOS_ColorMarkingTableSet,
-				gsw_handle, &color_mark)) {
+	if (gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_qos_ops.QOS_ColorMarkingTableSet,
+		gsw_handle, &color_mark)) {
 		PR_ERR("GSW_QOS_COLOR_MARKING_CFG_SET failed\n");
 		return -1;
 	}
@@ -1003,9 +1003,9 @@ static int dp_gswip_color_8P0D_set(int mode, int inst)
 	color_mark.nColor[13] = GSW_DROP_PRECEDENCE_YELLOW;
 	color_mark.nColor[14] = GSW_DROP_PRECEDENCE_GREEN;
 	color_mark.nColor[15] = GSW_DROP_PRECEDENCE_YELLOW;
-	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-				.QOS_ColorMarkingTableSet,
-				gsw_handle, &color_mark)) {
+	if (gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_qos_ops.QOS_ColorMarkingTableSet,
+		gsw_handle, &color_mark)) {
 		PR_ERR("GSW_QOS_COLOR_MARKING_CFG_SET failed\n");
 		return -1;
 	}
@@ -1027,6 +1027,7 @@ static int dp_platform_color_table_set(int inst)
 
 	return 0;
 }
+
 #define PER_CPU_PORTS 2 /* cqm dequeue ports per CPU*/
 
 int dp_platform_queue_set_32(int inst, u32 flag)
@@ -1124,8 +1125,8 @@ int dp_platform_queue_set_32(int inst, u32 flag)
 		       "cbm_cpu_port_get");
 		return -1;
 	}
-	gpid_base= alloc_gpid(inst, DP_DYN_GPID,
-			      CPU_GPID_NUM, CPU_PORT);
+	gpid_base = alloc_gpid(inst, DP_DYN_GPID,
+			       CPU_GPID_NUM, CPU_PORT);
 	if (gpid_base == DP_FAILURE) {
 		DP_ERR("alloc_gpid fail for CPU: %d\n", CPU_PORT);
 		return -1;
@@ -1147,13 +1148,14 @@ int dp_platform_queue_set_32(int inst, u32 flag)
 			DP_DEBUG(DP_DBG_FLAG_QOS, "cpu(%d)(%d) deq_port=%d",
 				 i, j, cpu.dq_tx_push_info[i][j].deq_port);
 			q_port.cqe_deq = cpu.dq_tx_push_info[i][j].deq_port;
-			q_port.tx_pkt_credit = cpu.dq_tx_push_info[i][j].
-								tx_pkt_credit;
-			q_port.tx_ring_addr = cpu.dq_tx_push_info[i][j].
-								txpush_addr_qos;
-			q_port.tx_ring_addr_push = cpu.dq_tx_push_info[i][j].
-								txpush_addr;
-			q_port.tx_ring_size = cpu.dq_tx_push_info[i][j].tx_ring_size;
+			q_port.tx_pkt_credit =
+				cpu.dq_tx_push_info[i][j].tx_pkt_credit;
+			q_port.tx_ring_addr =
+				cpu.dq_tx_push_info[i][j].txpush_addr_qos;
+			q_port.tx_ring_addr_push =
+				cpu.dq_tx_push_info[i][j].txpush_addr;
+			q_port.tx_ring_size =
+				cpu.dq_tx_push_info[i][j].tx_ring_size;
 
 			/*Sotre Ring Info */
 			dp_deq_port_tbl[inst][q_port.cqe_deq].tx_pkt_credit =
@@ -1183,9 +1185,11 @@ int dp_platform_queue_set_32(int inst, u32 flag)
 			q_port.dp_port = PMAC_CPU_ID;
 			DP_DEBUG(DP_DBG_FLAG_QOS, "CPU[%d] ring addr=%px\n", i,
 				 cpu.dq_tx_push_info[i][j].txpush_addr);
-			DP_DEBUG(DP_DBG_FLAG_QOS, "CPU[%d] ring addr push=%px\n", i,
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "CPU[%d] ring addr push=%px\n", i,
 				 cpu.dq_tx_push_info[i][j].txpush_addr_qos);
-			q_port.ctp = i; /*fake CTP for CPU port to store its qid*/
+			/* fake CTP for CPU port to store its qid */
+			q_port.ctp = i;
 			DP_DEBUG(DP_DBG_FLAG_QOS, "alloc_q_to_port_32...\n");
 			if (alloc_q_to_port_32(&q_port, 0)) { /* q_port.qid */
 				PR_ERR("alloc_q_to_port_32 fail for dp_port=%d\n",
@@ -1202,42 +1206,46 @@ int dp_platform_queue_set_32(int inst, u32 flag)
 			subif_info->cqm_deq_port = q_port.cqe_deq;
 			subif_info->policy_base = cpu.policy_base[i][j];
 			subif_info->policy_num = cpu.policy_num[i][j];
-			if(dp_add_pp_gpid(inst, CPU_PORT, vap,
-					  gpid_base + vap, 0)) {
+			if (dp_add_pp_gpid(inst, CPU_PORT, vap,
+					   gpid_base + vap, 0)) {
 				DP_ERR("dp_alloc_pp_gpid fail for CPU VAP=%d\n",
 				       vap);
 				return -1;
 			}
 			if (!f_cpu_q && (i == cpu.default_cpu)) {
 				f_cpu_q = 1;
-				/*Map all CPU port's lookup to its 1st queue only */
+				/* Map all CPU port's lookup
+				 * to its 1st queue only
+				 */
 				lookup.ep = PMAC_CPU_ID;
 				lookup.egflag = 0;
-				cbm_queue_map_set(dp_port_prop[inst].cbm_inst,
-						  q_port.qid,
-						  &lookup,
-						  CBM_QUEUE_MAP_F_FLOWID_L_DONTCARE |
-						  CBM_QUEUE_MAP_F_FLOWID_H_DONTCARE |
-						  CBM_QUEUE_MAP_F_SUBIF_DONTCARE |
-						  CBM_QUEUE_MAP_F_EN_DONTCARE |
-						  CBM_QUEUE_MAP_F_DE_DONTCARE |
-						  CBM_QUEUE_MAP_F_MPE1_DONTCARE |
-						  CBM_QUEUE_MAP_F_MPE2_DONTCARE |
-						  CBM_QUEUE_MAP_F_TC_DONTCARE |
-						  CBM_QUEUE_MAP_F_COLOR_DONTCARE);
+				cbm_queue_map_set(
+					dp_port_prop[inst].cbm_inst,
+					q_port.qid,
+					&lookup,
+					CBM_QUEUE_MAP_F_FLOWID_L_DONTCARE |
+					CBM_QUEUE_MAP_F_FLOWID_H_DONTCARE |
+					CBM_QUEUE_MAP_F_SUBIF_DONTCARE |
+					CBM_QUEUE_MAP_F_EN_DONTCARE |
+					CBM_QUEUE_MAP_F_DE_DONTCARE |
+					CBM_QUEUE_MAP_F_MPE1_DONTCARE |
+					CBM_QUEUE_MAP_F_MPE2_DONTCARE |
+					CBM_QUEUE_MAP_F_TC_DONTCARE |
+					CBM_QUEUE_MAP_F_COLOR_DONTCARE);
 				lookup.egflag = 1;
-				cbm_queue_map_set(dp_port_prop[inst].cbm_inst,
-						  q_port.qid,
-						  &lookup,
-						  CBM_QUEUE_MAP_F_FLOWID_L_DONTCARE |
-						  CBM_QUEUE_MAP_F_FLOWID_H_DONTCARE |
-						  CBM_QUEUE_MAP_F_SUBIF_DONTCARE |
-						  CBM_QUEUE_MAP_F_EN_DONTCARE |
-						  CBM_QUEUE_MAP_F_DE_DONTCARE |
-						  CBM_QUEUE_MAP_F_MPE1_DONTCARE |
-						  CBM_QUEUE_MAP_F_MPE2_DONTCARE |
-						  CBM_QUEUE_MAP_F_TC_DONTCARE |
-						  CBM_QUEUE_MAP_F_COLOR_DONTCARE);
+				cbm_queue_map_set(
+					dp_port_prop[inst].cbm_inst,
+					q_port.qid,
+					&lookup,
+					CBM_QUEUE_MAP_F_FLOWID_L_DONTCARE |
+					CBM_QUEUE_MAP_F_FLOWID_H_DONTCARE |
+					CBM_QUEUE_MAP_F_SUBIF_DONTCARE |
+					CBM_QUEUE_MAP_F_EN_DONTCARE |
+					CBM_QUEUE_MAP_F_DE_DONTCARE |
+					CBM_QUEUE_MAP_F_MPE1_DONTCARE |
+					CBM_QUEUE_MAP_F_MPE2_DONTCARE |
+					CBM_QUEUE_MAP_F_TC_DONTCARE |
+					CBM_QUEUE_MAP_F_COLOR_DONTCARE);
 				hostif.inst = inst;
 				hostif.gpid = gpid_base + vap;
 				hostif.qid = q_port.qid;
@@ -1249,18 +1257,20 @@ int dp_platform_queue_set_32(int inst, u32 flag)
 				}
 			}
 			/*Note:
-			 *    CPU port no DMA and don't set en_data.dma_chnl_init to 1
+			 *    CPU port no DMA and
+			 *    don't set en_data.dma_chnl_init to 1
 			 */
 			en_data.cbm_inst = dp_port_prop[inst].cbm_inst;
 			en_data.dp_inst = inst;
 			en_data.deq_port = cpu.dq_tx_push_info[i][j].deq_port;
 			if (cbm_dp_enable(NULL, PMAC_CPU_ID, &en_data, 0, 0)) {
-				PR_ERR("Fail to enable CPU[%d]\n", en_data.deq_port);
+				PR_ERR("Fail to enable CPU[%d]\n",
+				       en_data.deq_port);
 				return -1;
 			}
 		}
 	}
-	DP_INFO("CPU VAP %d are enabled now \n", vap_num);
+	DP_INFO("CPU VAP %d are enabled now\n", vap_num);
 
 	return 0;
 }
@@ -1275,6 +1285,7 @@ static int dp_platform_set(int inst, u32 flag)
 	/* For initialize */
 	if ((flag & DP_PLATFORM_INIT) == DP_PLATFORM_INIT) {
 		struct dp_subif_info *sif;
+
 		dp_port_prop[inst].priv_hal =
 			kzalloc(sizeof(*priv), GFP_KERNEL);
 		if (!dp_port_prop[inst].priv_hal) {
@@ -1316,16 +1327,16 @@ static int dp_platform_set(int inst, u32 flag)
 #endif
 		/*disable egress VLAN modification for CPU port*/
 		port_remark.nPortId = 0;
-		if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-				 .QoS_PortRemarkingCfgGet,
-				 gsw_handle, &port_remark)) {
+		if (gsw_core_api(
+			(dp_gsw_cb)gsw_handle->gsw_qos_ops.QoS_PortRemarkingCfgGet,
+			gsw_handle, &port_remark)) {
 			PR_ERR("GSW_QOS_PORT_REMARKING_CFG_GET failed\n");
 			return -1;
 		}
 		port_remark.bPCP_EgressRemarkingEnable = 0;
-		if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
-				 .QoS_PortRemarkingCfgGet,
-				 gsw_handle, &port_remark)) {
+		if (gsw_core_api(
+			(dp_gsw_cb)gsw_handle->gsw_qos_ops.QoS_PortRemarkingCfgGet,
+			gsw_handle, &port_remark)) {
 			PR_ERR("GSW_QOS_PORT_REMARKING_CFG_GET failed\n");
 			return -1;
 		}
@@ -1626,7 +1637,7 @@ static int subif_hw_set(int inst, int portid, int subif_ix,
 				data->subif_data->ctp_dev->name : "NULL");
 		sif->mac_learn_dis = data->subif_data->mac_learn_disable;
 		bp = alloc_bridge_port_32(inst, portid,
-				       subif_ix, CPU_FID, CPU_BP);
+					  subif_ix, CPU_FID, CPU_BP);
 		if (bp < 0) {
 			PR_ERR("Fail to alloc bridge port\n");
 			return -1;
@@ -1672,7 +1683,8 @@ static int subif_hw_set(int inst, int portid, int subif_ix,
 #if IS_ENABLED(CONFIG_INTEL_DATAPATH_DBG)
 	if (unlikely(dp_dbg_flag & DP_DBG_FLAG_QOS)) {
 		DP_DEBUG(DP_DBG_FLAG_QOS, "cqe_deq=%d\n", q_port.cqe_deq);
-		DP_DEBUG(DP_DBG_FLAG_QOS, "priv=%px deq_port_stat=%px qdev=%px\n",
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "priv=%px deq_port_stat=%px qdev=%px\n",
 			 priv,
 			 priv ? priv->deq_port_stat : NULL,
 			 priv ? priv->qdev : NULL);
@@ -1837,14 +1849,11 @@ static int subif_hw_set(int inst, int portid, int subif_ix,
 		       portid, subif_ix);
 	}
 	if (subif_ix < port_info->gpid_num)
-		sif->gpid = port_info->gpid_base +
-						       subif_ix;
+		sif->gpid = port_info->gpid_base + subif_ix;
 	else
-		sif->gpid = port_info->gpid_base +
-						       port_info->gpid_num - 1;
+		sif->gpid = port_info->gpid_base + port_info->gpid_num - 1;
 
 	/* Map this port's lookup to its 1st queue only */
-	//lookup.mode = get_dp_port_info(inst, portid)->cqe_lu_mode; /*no need */
 	lookup.ep = portid;
 	lookup.sub_if_id = subif; /* Note:CQM API need full subif(15bits) */
 	/* For 1st subif and mode 0, use CBM_QUEUE_MAP_F_SUBIF_DONTCARE,
@@ -2023,7 +2032,7 @@ static int subif_platform_set_unexplicit(int inst, int port_id,
 	} else {
 		logic_dev->bp =
 			alloc_bridge_port_32(inst, port_id, logic_dev->ctp,
-					  CPU_FID, CPU_BP);
+					     CPU_FID, CPU_BP);
 	}
 
 	return 0;
@@ -2082,12 +2091,13 @@ static void set_pmac_subif(struct pmac_tx_hdr *pmac, int32_t subif)
 	pmac->src_dst_subif_id_lsb = subif & 0xff;
 	pmac->src_dst_subif_id_msb =  (subif >> 8) & 0x0f;
 	pmac->src_dst_subif_id_msb |= (((subif & 0x8000) >> 15) << 4);
-	pmac->src_dst_subif_id_14_12 =(subif & 0x7000) >> 12;
+	pmac->src_dst_subif_id_14_12 = (subif & 0x7000) >> 12;
 }
 
 static void get_pmac_subif(struct pmac_rx_hdr *pmac, int32_t *subif)
 {
-	*subif = pmac->src_dst_subif_id_lsb + ((pmac->src_dst_subif_id_msb & 0x0F) << 8);
+	*subif = pmac->src_dst_subif_id_lsb +
+		((pmac->src_dst_subif_id_msb & 0x0F) << 8);
 	*subif |= ((pmac->src_dst_subif_id_msb & 0x10 >> 4) << 15);
 	*subif |= (pmac->src_dst_subif_id_14_12 >> 12);
 }
@@ -2205,7 +2215,8 @@ int register_dp_cap_gswip32(int flag)
 	cap.info.swdev_alloc_bridge_id = dp_swdev_alloc_bridge_id_32;
 	cap.info.swdev_free_brcfg = dp_swdev_free_brcfg_32;
 	cap.info.swdev_bridge_cfg_set = dp_swdev_bridge_cfg_set_32;
-	cap.info.swdev_bridge_port_cfg_reset = dp_swdev_bridge_port_cfg_reset_32;
+	cap.info.swdev_bridge_port_cfg_reset =
+				dp_swdev_bridge_port_cfg_reset_32;
 	cap.info.swdev_bridge_port_cfg_set = dp_swdev_bridge_port_cfg_set_32;
 	cap.info.dp_mac_set = dp_gswip_mac_entry_add_32;
 	cap.info.dp_mac_reset = dp_gswip_mac_entry_del_32;
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_misc.h b/drivers/net/datapath/dpm/gswip32/datapath_misc.h
index f4f141862b7a..9126ba8f440c 100644
--- a/drivers/net/datapath/dpm/gswip32/datapath_misc.h
+++ b/drivers/net/datapath/dpm/gswip32/datapath_misc.h
@@ -115,7 +115,9 @@ struct hal_priv {
 			 * to get physical queue id
 			 */
 	int ppv4_flag[MAX_PPV4_PORTS];
-	struct dp_gpid_map_table gp_dp_map[MAX_GPID]; /* Map Table GPID <-> DPID */
+	struct dp_gpid_map_table gp_dp_map[MAX_GPID];	/* Map Table GPID
+							 *       <-> DPID
+							 */
 };
 
 struct datapath_ctrl {
@@ -134,7 +136,6 @@ struct ctp_assign {
 	u16 swdev_enable; /* To enable or disable switchdev feature */
 };
 
-
 #define SET_PMAC_IGP_EGP(pmac, port_id) ((pmac)->igp_egp = (port_id) & 0xF)
 
 #define SET_PMAC_SUBIF(pmac, subif) do { \
@@ -142,10 +143,11 @@ struct ctp_assign {
 	(pmac)->src_dst_subif_id_msb =  ((subif) >> 8) & 0x1f; \
 } while (0)
 
-int alloc_bridge_port_32(int inst, int portid, int subif, int fid, int bp_member);
+int alloc_bridge_port_32(int inst, int portid, int subif,
+			 int fid, int bp_member);
 int free_bridge_port_32(int inst, int bp);
 struct gsw_itf *ctp_port_assign_32(int inst, u8 ep, int bp_default,
-				u32 flags, struct dp_dev_data *data);
+				   u32 flags, struct dp_dev_data *data);
 int gpid_port_assign(int inst, u8 ep, u32 flags);
 void dp_sys_mib_reset_32(u32 flag);
 int dp_pmac_set_32(int inst, u32 port, dp_pmac_cfg_t *pmac_cfg);
@@ -153,14 +155,14 @@ int dp_set_gsw_parser_32(u8 flag, u8 cpu, u8 mpe1, u8 mpe2, u8 mpe3);
 int dp_get_gsw_parser_32(u8 *cpu, u8 *mpe1, u8 *mpe2, u8 *mpe3);
 int gsw_mib_reset_32(int dev, u32 flag);
 int proc_print_ctp_bp_info_32(struct seq_file *s, int inst,
-			   struct pmac_port_info *port,
-			   int subif_index, u32 flag);
+			      struct pmac_port_info *port,
+			      int subif_index, u32 flag);
 
 #if IS_ENABLED(CONFIG_INTEL_DATAPATH_SWITCHDEV)
 int dp_gswip_mac_entry_add_32(int bport, int fid, int inst, u8 *addr);
 int dp_gswip_mac_entry_del_32(int bport, int fid, int inst, u8 *addr);
 int set_gswip_ext_vlan_32(struct core_ops *ops, struct ext_vlan_info *vlan,
-		       int flag);
+			  int flag);
 #endif
 int qos_platform_set_32(int cmd_id, void *node, int flag);
 int dp_node_alloc_32(struct dp_node_alloc *node, int flag);
@@ -203,10 +205,10 @@ int dp_qos_global_info_get_32(struct dp_qos_cfg_info *info, int flag);
 int dp_qos_port_conf_set_32(struct dp_port_cfg_info *info, int flag);
 int32_t dp_rx_32(struct sk_buff *skb, u32 flags);
 int32_t dp_xmit_32(struct net_device *rx_if, dp_subif_t *rx_subif,
-		struct sk_buff *skb, int32_t len, uint32_t flags);
+		   struct sk_buff *skb, int32_t len, uint32_t flags);
 void set_chksum(struct pmac_tx_hdr *pmac, u32 tcp_type,
-		       u32 ip_offset, int ip_off_hw_adjust,
-		       u32 tcp_h_offset);
+		u32 ip_offset, int ip_off_hw_adjust,
+		u32 tcp_h_offset);
 
 int dp_lan_wan_bridging(int port_id, struct sk_buff *skb);
 
@@ -237,12 +239,12 @@ ssize_t proc_get_qid_via_index(struct file *file, const char *buf,
 			       size_t count, loff_t *ppos);
 int datapath_debugfs_init(struct datapath_ctrl *pctrl);
 int get_q_mib_32(int inst, int qid,
-	      u32 *total_accept,
-	      u32 *total_drop,
-	      u32 *red_drop);
+		 u32 *total_accept,
+		 u32 *total_drop,
+		 u32 *red_drop);
 int get_p_mib_32(int inst, int pid,
-	      u32 *green /* bytes*/,
-	      u32 *yellow /*bytes*/);
+		 u32 *green /* bytes*/,
+		 u32 *yellow /*bytes*/);
 int cpu_vlan_mod_dis_32(int inst);
 int set_port_lookup_mode_32(int inst, u8 ep, u32 flags);
 int tc_vlan_set_32(struct core_ops *ops, struct dp_tc_vlan *vlan,
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_ppv4.c b/drivers/net/datapath/dpm/gswip32/datapath_ppv4.c
index 58312446b3c8..041bb6e83a26 100644
--- a/drivers/net/datapath/dpm/gswip32/datapath_ppv4.c
+++ b/drivers/net/datapath/dpm/gswip32/datapath_ppv4.c
@@ -14,37 +14,36 @@
 int (*qos_queue_remove_32)(struct pp_qos_dev *qos_dev, unsigned int id);
 int (*qos_queue_allocate_32)(struct pp_qos_dev *qos_dev, unsigned int *id);
 int (*qos_queue_info_get_32)(struct pp_qos_dev *qos_dev, unsigned int id,
-			  struct pp_qos_queue_info *info);
+			     struct pp_qos_queue_info *info);
 int (*qos_port_remove_32)(struct pp_qos_dev *qos_dev, unsigned int id);
 int (*qos_sched_allocate_32)(struct pp_qos_dev *qos_dev, unsigned int *id);
 int (*qos_sched_remove_32)(struct pp_qos_dev *qos_dev, unsigned int id);
 int (*qos_port_allocate_32)(struct pp_qos_dev *qos_dev,
-			 unsigned int physical_id,
-			 unsigned int *id);
+			    unsigned int physical_id, unsigned int *id);
 int (*qos_port_set_32)(struct pp_qos_dev *qos_dev, unsigned int id,
-		    const struct pp_qos_port_conf *conf);
+		       const struct pp_qos_port_conf *conf);
 void (*qos_port_conf_set_default_32)(struct pp_qos_port_conf *conf);
 void (*qos_queue_conf_set_default_32)(struct pp_qos_queue_conf *conf);
 int (*qos_queue_set_32)(struct pp_qos_dev *qos_dev, unsigned int id,
-		     const struct pp_qos_queue_conf *conf);
+			const struct pp_qos_queue_conf *conf);
 void (*qos_sched_conf_set_default_32)(struct pp_qos_sched_conf *conf);
 int (*qos_sched_set_32)(struct pp_qos_dev *qos_dev, unsigned int id,
-		     const struct pp_qos_sched_conf *conf);
+			const struct pp_qos_sched_conf *conf);
 int (*qos_queue_conf_get_32)(struct pp_qos_dev *qos_dev, unsigned int id,
-			  struct pp_qos_queue_conf *conf);
+			     struct pp_qos_queue_conf *conf);
 int (*qos_queue_flush_32)(struct pp_qos_dev *qos_dev, unsigned int id);
 int (*qos_sched_conf_get_32)(struct pp_qos_dev *qos_dev, unsigned int id,
-			  struct pp_qos_sched_conf *conf);
+			     struct pp_qos_sched_conf *conf);
 int (*qos_sched_get_queues_32)(struct pp_qos_dev *qos_dev, unsigned int id,
-			    u16 *queue_ids, unsigned int size,
-			    unsigned int *queues_num);
+			       u16 *queue_ids, unsigned int size,
+			       unsigned int *queues_num);
 int (*qos_port_get_queues_32)(struct pp_qos_dev *qos_dev, unsigned int id,
-			   u16 *queue_ids, unsigned int size,
-				  unsigned int *queues_num);
+			      u16 *queue_ids, unsigned int size,
+			      unsigned int *queues_num);
 int (*qos_port_conf_get_32)(struct pp_qos_dev *qdev, unsigned int id,
-			 struct pp_qos_port_conf *conf);
+			    struct pp_qos_port_conf *conf);
 int (*qos_port_info_get_32)(struct pp_qos_dev *qdev, unsigned int id,
-			 struct pp_qos_port_info *info);
+			    struct pp_qos_port_info *info);
 struct pp_qos_dev *(*qos_dev_open_32)(unsigned int id);
 
 #if IS_ENABLED(CONFIG_INTEL_DATAPATH_DUMMY_QOS)
@@ -329,7 +328,6 @@ void init_qos_fn_32(void)
 #endif /*CONFIG_INTEL_DATAPATH_DUMMY_QOS*/
 }
 
-
 int dp_pp_alloc_port_32(struct ppv4_port *info)
 {
 	int qos_p_id = 0;
@@ -338,8 +336,8 @@ int dp_pp_alloc_port_32(struct ppv4_port *info)
 	struct pp_qos_dev *qos_dev = priv->qdev;
 
 	if (qos_port_allocate_32(qos_dev,
-			      info->cqm_deq_port,
-			      &qos_p_id)) {
+				 info->cqm_deq_port,
+				 &qos_p_id)) {
 		PR_ERR("Failed to alloc QoS for deq_port=%d\n",
 		       info->cqm_deq_port);
 		return -1;
@@ -490,19 +488,19 @@ int init_ppv4_qos_32(int inst, int flag)
 	dp_deq_port_tbl[inst][idx].tx_ring_size = flush_port->tx_ring_size;
 	dp_deq_port_tbl[inst][idx].dp_port = 0;/* dummy one */
 	DP_DEBUG(DP_DBG_FLAG_QOS,
-			 "DP Flush port[%d]: ring addr/push=0x%px/0x%px size=%d pkt_credit=%d\n",
-			 priv->cqm_drop_p,
-			 dp_deq_port_tbl[inst][idx].txpush_addr_qos,
-		dp_deq_port_tbl[inst][idx].txpush_addr,
-		dp_deq_port_tbl[inst][idx].tx_ring_size,
-		dp_deq_port_tbl[inst][idx].tx_pkt_credit);
+		 "DP Flush port[%d]: ring addr/push=0x%px/0x%px size=%d pkt_credit=%d\n",
+		 priv->cqm_drop_p,
+		 dp_deq_port_tbl[inst][idx].txpush_addr_qos,
+		 dp_deq_port_tbl[inst][idx].txpush_addr,
+		 dp_deq_port_tbl[inst][idx].tx_ring_size,
+		 dp_deq_port_tbl[inst][idx].tx_pkt_credit);
 #if IS_ENABLED(CONFIG_INTEL_DATAPATH_QOS_HAL)
 	DP_DEBUG(DP_DBG_FLAG_DBG, "priv=%px deq_port_stat=%px q_dev=%px\n",
 		 priv, priv ? priv->deq_port_stat : NULL,
 		 priv ? priv->qdev : NULL);
 	if (qos_port_allocate_32(priv->qdev,
-			      priv->cqm_drop_p,
-			      &priv->ppv4_drop_p)) {
+				 priv->cqm_drop_p,
+				 &priv->ppv4_drop_p)) {
 		PR_ERR("Failed to alloc  qos drop port=%d\n",
 		       priv->cqm_drop_p);
 		goto EXIT;
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_ppv4.h b/drivers/net/datapath/dpm/gswip32/datapath_ppv4.h
index 153f92478d66..d13085f2b270 100644
--- a/drivers/net/datapath/dpm/gswip32/datapath_ppv4.h
+++ b/drivers/net/datapath/dpm/gswip32/datapath_ppv4.h
@@ -31,7 +31,6 @@ struct pp_qos_dev {
 #define DEF_QRED_SLOP_YELLOW 70 /* yellow slop in queue */
 #define DEF_WRED_RATIO       5
 
-
 #define HAL(inst) ((struct hal_priv *)dp_port_prop[inst].priv_hal)
 #define PARENT(x) (x.queue_child_prop.parent)
 #define PARENT_S(x) (x.sched_child_prop.parent)
@@ -166,39 +165,42 @@ struct dp_lookup_entry {
 
 void init_qos_fn_32(void);
 extern int (*qos_queue_remove_32)(struct pp_qos_dev *qos_dev, unsigned int id);
-extern int (*qos_queue_allocate_32)(struct pp_qos_dev *qos_dev, unsigned int *id);
+extern int (*qos_queue_allocate_32)(struct pp_qos_dev *qos_dev,
+				    unsigned int *id);
 extern int (*qos_queue_info_get_32)(struct pp_qos_dev *qos_dev, unsigned int id,
-				 struct pp_qos_queue_info *info);
+				    struct pp_qos_queue_info *info);
 extern int (*qos_port_remove_32)(struct pp_qos_dev *qos_dev, unsigned int id);
-extern int (*qos_sched_allocate_32)(struct pp_qos_dev *qos_dev, unsigned int *id);
+extern int (*qos_sched_allocate_32)(struct pp_qos_dev *qos_dev,
+				    unsigned int *id);
 extern int (*qos_sched_remove_32)(struct pp_qos_dev *qos_dev, unsigned int id);
 extern int (*qos_port_allocate_32)(struct pp_qos_dev *qos_dev,
-				unsigned int physical_id,
-				unsigned int *id);
+				   unsigned int physical_id, unsigned int *id);
 extern int (*qos_port_set_32)(struct pp_qos_dev *qos_dev, unsigned int id,
-			   const struct pp_qos_port_conf *conf);
+			      const struct pp_qos_port_conf *conf);
 extern void (*qos_port_conf_set_default_32)(struct pp_qos_port_conf *conf);
 extern void (*qos_queue_conf_set_default_32)(struct pp_qos_queue_conf *conf);
 extern int (*qos_queue_set_32)(struct pp_qos_dev *qos_dev, unsigned int id,
-			    const struct pp_qos_queue_conf *conf);
+			       const struct pp_qos_queue_conf *conf);
 extern void (*qos_sched_conf_set_default_32)(struct pp_qos_sched_conf *conf);
 extern int (*qos_sched_set_32)(struct pp_qos_dev *qos_dev, unsigned int id,
-			    const struct pp_qos_sched_conf *conf);
+			       const struct pp_qos_sched_conf *conf);
 extern int (*qos_queue_conf_get_32)(struct pp_qos_dev *qos_dev, unsigned int id,
-				 struct pp_qos_queue_conf *conf);
+				    struct pp_qos_queue_conf *conf);
 extern int (*qos_queue_flush_32)(struct pp_qos_dev *qos_dev, unsigned int id);
 extern int (*qos_sched_conf_get_32)(struct pp_qos_dev *qos_dev, unsigned int id,
-				 struct pp_qos_sched_conf *conf);
-extern int (*qos_sched_get_queues_32)(struct pp_qos_dev *qos_dev, unsigned int id,
-				   u16 *queue_ids, unsigned int size,
-				   unsigned int *queues_num);
-extern int (*qos_port_get_queues_32)(struct pp_qos_dev *qos_dev, unsigned int id,
-				  u16 *queue_ids, unsigned int size,
-				  unsigned int *queues_num);
+				    struct pp_qos_sched_conf *conf);
+extern int (*qos_sched_get_queues_32)(struct pp_qos_dev *qos_dev,
+				      unsigned int id, u16 *queue_ids,
+				      unsigned int size,
+				      unsigned int *queues_num);
+extern int (*qos_port_get_queues_32)(struct pp_qos_dev *qos_dev,
+				     unsigned int id, u16 *queue_ids,
+				     unsigned int size,
+				     unsigned int *queues_num);
 extern int (*qos_port_conf_get_32)(struct pp_qos_dev *qdev, unsigned int id,
-				struct pp_qos_port_conf *conf);
+				   struct pp_qos_port_conf *conf);
 extern int (*qos_port_info_get_32)(struct pp_qos_dev *qdev, unsigned int id,
-				struct pp_qos_port_info *info);
+				   struct pp_qos_port_info *info);
 extern struct pp_qos_dev *(*qos_dev_open_32)(unsigned int id);
 int dp_map_to_drop_q_32(int inst, int q_id, struct dp_lookup_entry *lookup);
 int dp_pp_alloc_port_32(struct ppv4_port *info);
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_ppv4_api.c b/drivers/net/datapath/dpm/gswip32/datapath_ppv4_api.c
index f32806242604..d7786a2fefda 100644
--- a/drivers/net/datapath/dpm/gswip32/datapath_ppv4_api.c
+++ b/drivers/net/datapath/dpm/gswip32/datapath_ppv4_api.c
@@ -57,7 +57,6 @@ static void dp_wred_def(struct pp_qos_queue_conf *conf)
 #endif
 }
 
-
 int qos_platform_set_32(int cmd_id, void *node, int flag)
 {
 	struct dp_node_link *node_link = (struct dp_node_link *)node;
@@ -161,10 +160,12 @@ int qos_platform_set_32(int cmd_id, void *node, int flag)
 		res = dp_qos_global_info_get_32((struct dp_qos_cfg_info *)node,
 						flag);
 	case QOS_Q_LOGIC:
-		res = dp_get_queue_logic_32((struct dp_qos_q_logic*)node, flag);
+		res = dp_get_queue_logic_32((struct dp_qos_q_logic *)node,
+					    flag);
 		break;
 	case QOS_PORT_CFG_SET:
-		res = dp_qos_port_conf_set_32((struct dp_port_cfg_info *)node, flag);
+		res = dp_qos_port_conf_set_32((struct dp_port_cfg_info *)node,
+					      flag);
 		break;
 	default:
 		PR_ERR("no support yet cmd_id %d\n", cmd_id);
@@ -398,8 +399,9 @@ static int queue_flush_32(int inst, int node_id, int flag)
 		dp_wred_def(&tmp_q_cfg);
 		tmp_q_cfg.queue_child_prop.parent = priv->ppv4_drop_p;
 		if (qos_queue_set_32(priv->qdev, node_id, &tmp_q_cfg)) {
-			PR_ERR("qos_queue_set_32 fail for queue=%d to parent=%d\n",
-			       qid, tmp_q_cfg.queue_child_prop.parent);
+			PR_ERR(
+			    "qos_queue_set_32 fail for queue=%d to parent=%d\n",
+			    qid, tmp_q_cfg.queue_child_prop.parent);
 			goto EXIT;
 		}
 		DP_DEBUG(DP_DBG_FLAG_QOS,
@@ -1109,14 +1111,14 @@ static int dp_link_unset(struct dp_node_link *info, int flag)
 		goto EXIT;
 	} else if (info->node_type == DP_NODE_SCH) {
 		if (qos_sched_conf_get_32(priv->qdev, info->node_id.sch_id,
-				       &sched_cfg)) {
+					  &sched_cfg)) {
 			PR_ERR("failed to qos_queue_conf_get_32\n");
 			return DP_FAILURE;
 		}
 		if (sched_cfg.sched_child_prop.parent ==
 						*(int *)&info->p_node_id) {
 			if (qos_sched_remove_32(priv->qdev,
-					     info->node_id.sch_id)) {
+						info->node_id.sch_id)) {
 				PR_ERR("failed to qos_sched_remove_32\n");
 				return DP_FAILURE;
 			}
@@ -1289,8 +1291,9 @@ static int dp_node_alloc_global_pool(struct dp_node_alloc *node, int flag)
 		q_conf->queue_child_prop.parent = priv->ppv4_tmp_p;
 #endif /* WORKAROUND_DROP_PORT */
 		if (qos_queue_set_32(priv->qdev, id, q_conf)) {
-			PR_ERR("qos_queue_set_32 fail for queue=%d to parent=%d\n",
-			       id, q_conf->queue_child_prop.parent);
+			PR_ERR(
+			    "qos_queue_set_32 fail for queue=%d to parent=%d\n",
+			    id, q_conf->queue_child_prop.parent);
 			goto EXIT;
 		}
 		DP_DEBUG(DP_DBG_FLAG_QOS,
@@ -1390,8 +1393,7 @@ static int dp_alloc_qos_port(struct dp_node_alloc *node, int flag)
 	/* Configure QOS dequeue port */
 	qos_port_conf_set_default_32(&port_cfg);
 	port_cfg.ring_address =
-		(unsigned long)dp_deq_port_tbl[inst][cqm_deq_port].
-							txpush_addr_qos;
+		(unsigned long)dp_deq_port_tbl[inst][cqm_deq_port].txpush_addr_qos;
 	port_cfg.ring_size = dp_deq_port_tbl[inst][cqm_deq_port].tx_ring_size;
 	port_cfg.credit = dp_deq_port_tbl[inst][cqm_deq_port].tx_pkt_credit;
 	if (port_cfg.credit)
@@ -1889,7 +1891,8 @@ int dp_node_free_32(struct dp_node_alloc *node, int flag)
 			goto EXIT;
 		}
 
-		if (qos_queue_conf_get_32(priv->qdev, t->node_id, &t->queue_cfg)) {
+		if (qos_queue_conf_get_32(priv->qdev, t->node_id,
+					  &t->queue_cfg)) {
 			res = node_stat_update(node->inst, t->node_id,
 					       DP_NODE_RST);
 			if (res == DP_FAILURE) {
@@ -1950,7 +1953,8 @@ int dp_node_free_32(struct dp_node_alloc *node, int flag)
 		if (t->resv_flag & PP_NODE_RESERVE) {
 			qos_queue_conf_set_default_32(&t->tmp_q);
 			t->tmp_q.queue_child_prop.parent = priv->ppv4_drop_p;
-			if (qos_queue_set_32(priv->qdev, t->node_id, &t->tmp_q)) {
+			if (qos_queue_set_32(priv->qdev, t->node_id,
+					     &t->tmp_q)) {
 				PR_ERR("qos_queue_set_32 %s=%d to parent=%d\n",
 				       "fail to reserve queue", t->node_id,
 				       t->tmp_q.queue_child_prop.parent);
@@ -1981,7 +1985,8 @@ int dp_node_free_32(struct dp_node_alloc *node, int flag)
 			goto EXIT;
 		}
 
-		if (qos_sched_conf_get_32(priv->qdev, t->sch_id, &t->sched_cfg)) {
+		if (qos_sched_conf_get_32(priv->qdev, t->sch_id,
+					  &t->sched_cfg)) {
 			if (node_stat_update(node->inst, t->sch_id,
 					     DP_NODE_RST | P_FLAG)) {
 				PR_ERR("node_stat_update failed\n");
@@ -2033,7 +2038,8 @@ int dp_node_free_32(struct dp_node_alloc *node, int flag)
 		if (t->resv_flag & PP_NODE_RESERVE) {
 			qos_sched_conf_set_default_32(&t->tmp_sch);
 			t->tmp_sch.sched_child_prop.parent = priv->ppv4_drop_p;
-			if (qos_sched_set_32(priv->qdev, t->sch_id, &t->tmp_sch)) {
+			if (qos_sched_set_32(priv->qdev, t->sch_id,
+					     &t->tmp_sch)) {
 				PR_ERR("qos_sched_set_32 %s=%d to parent=%d\n",
 				       "fail to reserve SCH", t->sch_id,
 				       t->tmp_sch.sched_child_prop.parent);
@@ -2163,7 +2169,7 @@ int dp_node_link_get_32(struct dp_node_link *info, int flag)
 		return DP_SUCCESS;
 	} else if (info->node_type == DP_NODE_SCH) {
 		if (qos_sched_conf_get_32(priv->qdev, info->node_id.sch_id,
-				       &sched_cfg)) {
+					  &sched_cfg)) {
 			PR_ERR("failed to qos_sched_conf_get_32\n");
 			return DP_FAILURE;
 		}
@@ -2234,7 +2240,8 @@ static int dp_link_set(struct dp_node_link *info, int parent_node, int flag)
 		qos_sched_conf_set_default_32(sched_cfg);
 		sched_cfg->sched_child_prop.parent = parent_node;
 		sched_cfg->sched_child_prop.priority = info->prio_wfq;
-		sched_cfg->sched_parent_prop.arbitration = arbi_dp2pp_32(info->arbi);
+		sched_cfg->sched_parent_prop.arbitration =
+						arbi_dp2pp_32(info->arbi);
 		node_id = info->node_id.sch_id;
 
 		DP_DEBUG(DP_DBG_FLAG_QOS,
@@ -2449,7 +2456,7 @@ int dp_qos_link_prio_set_32(struct dp_node_prio *info, int flag)
 			return DP_FAILURE;
 		}
 		if (qos_sched_conf_get_32(priv->qdev, info->id.sch_id,
-				       &sched_cfg)) {
+					  &sched_cfg)) {
 			PR_ERR("fail to get sched prio and parent\n");
 			return DP_FAILURE;
 		}
@@ -2517,9 +2524,9 @@ int dp_qos_link_prio_get_32(struct dp_node_prio *info, int flag)
 
 		arbi = get_parent_arbi(info->inst, node_id, flag);
 
-		if (arbi == DP_FAILURE) {
+		if (arbi == DP_FAILURE)
 			return DP_FAILURE;
-		}
+
 		info->arbi = arbi;
 		info->prio_wfq = queue_cfg.queue_child_prop.priority;
 		return DP_SUCCESS;
@@ -2536,16 +2543,16 @@ int dp_qos_link_prio_get_32(struct dp_node_prio *info, int flag)
 			return DP_FAILURE;
 		}
 		if (qos_sched_conf_get_32(priv->qdev, info->id.sch_id,
-				       &sched_cfg)) {
+					  &sched_cfg)) {
 			PR_ERR("fail to get sched arbi and prio values!\n");
 			return DP_FAILURE;
 		}
 
 		arbi = get_parent_arbi(info->inst, info->id.sch_id, flag);
 
-		if (arbi == DP_FAILURE) {
+		if (arbi == DP_FAILURE)
 			return DP_FAILURE;
-		}
+
 		info->arbi = arbi;
 		info->prio_wfq = sched_cfg.sched_child_prop.priority;
 		return DP_SUCCESS;
@@ -2613,8 +2620,8 @@ int dp_deq_port_res_get_32(struct dp_dequeue_res *res, int flag)
 			continue;
 		q_num = 0;
 		if (qos_port_get_queues_32(priv->qdev,
-					priv->deq_port_stat[k].node_id,
-					q_ids, q_size, &q_num)) {
+					   priv->deq_port_stat[k].node_id,
+					   q_ids, q_size, &q_num)) {
 			PR_ERR("qos_port_get_queues_32: port[%d/%d]\n",
 			       k,
 			       priv->deq_port_stat[k].node_id);
@@ -2631,7 +2638,7 @@ int dp_deq_port_res_get_32(struct dp_dequeue_res *res, int flag)
 		for (i = 0; (i < q_num) && (idx < res->q_res_size); i++) {
 			memset(&t.q_info, 0, sizeof(t.q_info));
 			if (qos_queue_info_get_32(priv->qdev,
-					       q_ids[i], &t.q_info)) {
+						  q_ids[i], &t.q_info)) {
 				PR_ERR("qos_port_info_get_32 fail:q[/%d]\n",
 				       q_ids[i]);
 				continue;
@@ -2643,7 +2650,7 @@ int dp_deq_port_res_get_32(struct dp_dequeue_res *res, int flag)
 			res->q_res[idx].sch_lvl = 0;
 			memset(&t.q_conf, 0, sizeof(t.q_conf));
 			if (qos_queue_conf_get_32(priv->qdev,
-					       q_ids[i], &t.q_conf)) {
+						  q_ids[i], &t.q_conf)) {
 				PR_ERR("qos_port_conf_get_32 fail:q[/%d]\n",
 				       q_ids[i]);
 				continue;
@@ -2668,10 +2675,11 @@ int dp_deq_port_res_get_32(struct dp_dequeue_res *res, int flag)
 				j++;
 				res->q_res[idx].sch_lvl = j;
 				/* get next parent */
-				if (qos_sched_conf_get_32(priv->qdev,
-						       p_id, &t.sched_conf)) {
-					PR_ERR("qos_sched_conf_get_32 %s[/%d]\n",
-					       "fail:sch", p_id);
+				if (qos_sched_conf_get_32(priv->qdev, p_id,
+							  &t.sched_conf)) {
+					PR_ERR(
+					    "qos_sched_conf_get_32 %s[/%d]\n",
+					    "fail:sch", p_id);
 					break;
 				}
 				p_id = t.sched_conf.sched_child_prop.parent;
@@ -2715,8 +2723,8 @@ int dp_node_unlink_32(struct dp_node_link *info, int flag)
 		if (!(priv->qos_queue_stat[info->node_id.q_id].flag &
 		    PP_NODE_ACTIVE)) {
 			PR_ERR("Wrong Queue[%d] Stat(%d):Expect ACTIVE\n",
-				info->node_id.q_id,
-				priv->qos_queue_stat[info->node_id.q_id].flag);
+			       info->node_id.q_id,
+			       priv->qos_queue_stat[info->node_id.q_id].flag);
 		}
 		if (qos_queue_conf_get_32(priv->qdev, node_id, &queue_cfg) == 0)
 			queue_flush_32(info->inst, node_id, 0);
@@ -2725,14 +2733,14 @@ int dp_node_unlink_32(struct dp_node_link *info, int flag)
 								PP_NODE_ACTIVE))
 			PR_ERR("Wrong Sched FLAG Expect ACTIVE\n");
 		if (qos_sched_conf_get_32(priv->qdev, info->node_id.sch_id,
-				       &sched_cfg))
+					  &sched_cfg))
 			return DP_FAILURE;
 		if (qos_sched_get_queues_32(priv->qdev, info->node_id.sch_id,
-					 queue_buf, queue_size, &queue_num))
+					    queue_buf, queue_size, &queue_num))
 			return DP_FAILURE;
 		for (i = 0; i < queue_num; i++) {
 			if (qos_queue_conf_get_32(priv->qdev, queue_buf[i],
-					       &queue_cfg))
+						  &queue_cfg))
 				continue;
 			queue_flush_32(info->inst, queue_buf[i], 0);
 		}
@@ -2785,7 +2793,7 @@ int dp_node_link_add_32(struct dp_node_link *info, int flag)
 
 	if ((!info->dp_port) && (info->dp_port != DP_PORT(info).dp_port)) {
 		PR_ERR("Fix wrong dp_port from %d to %d\n",
-			info->dp_port, DP_PORT(info).dp_port);
+		       info->dp_port, DP_PORT(info).dp_port);
 		info->dp_port = DP_PORT(info).dp_port;
 	}
 	t = kzalloc(sizeof(*t), GFP_ATOMIC);
@@ -2859,7 +2867,7 @@ int dp_node_link_add_32(struct dp_node_link *info, int flag)
 
 		t->node_id = priv->qos_queue_stat[info->node_id.q_id].node_id;
 		if (qos_queue_conf_get_32(priv->qdev, t->node_id,
-				       &t->queue_cfg) == 0) {
+					  &t->queue_cfg) == 0) {
 			t->queue_num = 1;
 			t->queue_buf[0] = t->node_id;
 			/* save original block/suspend status */
@@ -2933,22 +2941,22 @@ int dp_node_link_add_32(struct dp_node_link *info, int flag)
 		}
 		if ((t->f_child_free == 0) &&
 		    qos_sched_conf_get_32(priv->qdev, info->node_id.sch_id,
-				       &t->sched_cfg) == 0) {
+					  &t->sched_cfg) == 0) {
 			DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
 				 "info->node_id.sch_id %d\n",
 				 info->node_id.sch_id);
 			if (qos_sched_get_queues_32(priv->qdev,
-						 info->node_id.sch_id,
-						 t->queue_buf, t->queue_size,
-						 &t->queue_num)) {
+						    info->node_id.sch_id,
+						    t->queue_buf, t->queue_size,
+						    &t->queue_num)) {
 				PR_ERR("Can not get queues:%d\n",
 				       info->node_id.sch_id);
 				goto EXIT_ERR;
 			}
 			for (i = 0; i < t->queue_num; i++) {
 				if (qos_queue_conf_get_32(priv->qdev,
-						       t->queue_buf[i],
-						       &t->queue_cfg))
+							  t->queue_buf[i],
+							  &t->queue_cfg))
 					continue;
 				if (t->queue_cfg.blocked == 0)
 					t->q_orig_block[i] =
@@ -2988,7 +2996,7 @@ int dp_node_link_add_32(struct dp_node_link *info, int flag)
 		    (t->q_orig_suspend[i] < 0))/* non-valid suspend stat */
 			continue;
 		if (qos_queue_conf_get_32(priv->qdev, t->queue_buf[i],
-				       &t->queue_cfg))
+					  &t->queue_cfg))
 			continue;
 		t->f_restore = 0;
 		if (t->q_orig_block[i] >= 0) {
@@ -3004,7 +3012,8 @@ int dp_node_link_add_32(struct dp_node_link *info, int flag)
 
 		if (!t->f_restore)
 			continue;
-		if (qos_queue_set_32(priv->qdev, t->queue_buf[i], &t->queue_cfg)) {
+		if (qos_queue_set_32(priv->qdev, t->queue_buf[i],
+				     &t->queue_cfg)) {
 			PR_ERR("qos_queue_set_32 fail for q[/%d]\n",
 			       t->queue_buf[i]);
 			res = DP_FAILURE;
@@ -3269,7 +3278,8 @@ int dp_node_link_en_set_32(struct dp_node_link_enable *en, int flag)
 		}
 
 		if (qos_queue_conf_get_32(priv->qdev, node_id, &queue_cfg)) {
-			PR_ERR("qos_queue_conf_get_32 fail: q[%d]\n", en->id.q_id);
+			PR_ERR("qos_queue_conf_get_32 fail: q[%d]\n",
+			       en->id.q_id);
 			return DP_FAILURE;
 		}
 		if (en->en & DP_NODE_EN) {
@@ -3305,7 +3315,8 @@ int dp_node_link_en_set_32(struct dp_node_link_enable *en, int flag)
 			return DP_FAILURE;
 		}
 
-		if (qos_sched_conf_get_32(priv->qdev, en->id.sch_id, &sched_cfg)) {
+		if (qos_sched_conf_get_32(priv->qdev, en->id.sch_id,
+					  &sched_cfg)) {
 			PR_ERR("qos_sched_conf_get_32 fail: sch[%d]\n",
 			       en->id.sch_id);
 			return DP_FAILURE;
@@ -3421,7 +3432,7 @@ int dp_node_link_en_get_32(struct dp_node_link_enable *en, int flag)
 		DP_DEBUG(DP_DBG_FLAG_QOS,
 			 "en->id.sch_id=%d\n", en->id.sch_id);
 		if (qos_sched_conf_get_32(priv->qdev, en->id.sch_id,
-				       &sched_conf)) {
+					  &sched_conf)) {
 			PR_ERR("qos_sched_conf_get_32 fail: sched[/%d]\n",
 			       en->id.sch_id);
 			return DP_FAILURE;
@@ -3498,7 +3509,7 @@ int dp_link_get_32(struct dp_qos_link *cfg, int flag)
 			cfg->sch[i + 1].id =
 				priv->qos_sch_stat[node_id].parent.node_id;
 			if (qos_sched_conf_get_32(priv->qdev, cfg->sch[i].id,
-					       &sched_cfg)) {
+						  &sched_cfg)) {
 				PR_ERR("dp_link_get:sched[/%d] conf get fail\n",
 				       cfg->sch[i].id);
 				return DP_FAILURE;
@@ -3755,8 +3766,9 @@ int dp_shaper_conf_set_32(struct dp_shaper_conf *cfg, int flag)
 		}
 
 		if (qos_sched_conf_get_32(priv->qdev, cfg->id.sch_id,
-				       &sched_cfg)) {
-			PR_ERR("qos_sched_conf_get_32 fail:%d\n", cfg->id.sch_id);
+					  &sched_cfg)) {
+			PR_ERR("qos_sched_conf_get_32 fail:%d\n",
+			       cfg->id.sch_id);
 			return DP_FAILURE;
 		}
 
@@ -3820,7 +3832,8 @@ int dp_shaper_conf_set_32(struct dp_shaper_conf *cfg, int flag)
 		}
 
 		if (qos_port_set_32(priv->qdev, node_id, &port_cfg)) {
-			PR_ERR("qos_port_set_32 fail:%d\n", cfg->id.cqm_deq_port);
+			PR_ERR("qos_port_set_32 fail:%d\n",
+			       cfg->id.cqm_deq_port);
 			return DP_FAILURE;
 		}
 		return DP_SUCCESS;
@@ -3892,8 +3905,9 @@ int dp_shaper_conf_get_32(struct dp_shaper_conf *cfg, int flag)
 		}
 
 		if (qos_sched_conf_get_32(priv->qdev, cfg->id.sch_id,
-				       &sched_cfg)) {
-			PR_ERR("qos_sched_conf_get_32 fail:%d\n", cfg->id.sch_id);
+					  &sched_cfg)) {
+			PR_ERR("qos_sched_conf_get_32 fail:%d\n",
+			       cfg->id.sch_id);
 			return DP_FAILURE;
 		}
 
@@ -4468,8 +4482,9 @@ int dp_node_reserve_32(int inst, int ep, struct dp_port_data *data, int flags)
 		q_conf.queue_child_prop.parent = priv->ppv4_drop_p;
 		if (qos_queue_set_32(priv->qdev, id, &q_conf)) {
 			res = DP_FAILURE;
-			PR_ERR("qos_queue_set_32 fail for queue=%d to parent=%d\n",
-			       id, q_conf.queue_child_prop.parent);
+			PR_ERR(
+			    "qos_queue_set_32 fail for queue=%d to parent=%d\n",
+			    id, q_conf.queue_child_prop.parent);
 			goto FREE_EXIT;
 		}
 		DP_DEBUG(DP_DBG_FLAG_QOS,
@@ -4553,7 +4568,6 @@ int dp_get_q_logic_32(int inst, int qid)
 	return pp_qos_queue_id_get(priv->qdev, qid);
 }
 
-
 int dp_get_queue_logic_32(struct dp_qos_q_logic *cfg, int flag)
 {
 	struct hal_priv *priv = HAL(cfg->inst);
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_ppv4_session.c b/drivers/net/datapath/dpm/gswip32/datapath_ppv4_session.c
index 80d73db66358..e80c7d2e6847 100644
--- a/drivers/net/datapath/dpm/gswip32/datapath_ppv4_session.c
+++ b/drivers/net/datapath/dpm/gswip32/datapath_ppv4_session.c
@@ -24,10 +24,11 @@ s32 pp_port_add(u16 port_id, struct pp_port_cfg *cfg)
 {
 	return 0;
 }
+
 s32 pp_session_create(struct pp_sess_create_args *args, u32 *sess_id,
 		      struct pp_request *req)
 {
-	static int local_sess_id = 0;
+	static int local_sess_id;
 	*sess_id = local_sess_id++;
 
 	return 0;
@@ -66,8 +67,6 @@ void init_gpid_map_table(int inst)
 		if (i >= DP_SPL_GPID_START)
 			priv->gp_dp_map[i].dpid = (12 + (i-DP_SPL_GPID_START));
 	}
-
-	return;
 }
 
 static void __mark_alloc_gpid(int inst, int base, int end, int dpid)
@@ -166,7 +165,6 @@ bool is_stream_port(int alloc_flag)
 	return false;
 }
 
-
 int get_subif_size(u32 vap_mask)
 {
 	u32 i;
@@ -174,14 +172,14 @@ int get_subif_size(u32 vap_mask)
 
 	for (i = 0; i < sizeof(i); i++)
 		if (vap_mask & (1 << i))
-			num ++;
+			num++;
 
 	return num;
 }
 
 /* dp_add_pp_gpid: to configure normal GPID or special GPID
  * Note: try to get all GPID related configuration via dpid/vap
-         if spl_gpid is 1, vap is not valid
+ *       if spl_gpid is 1, vap is not valid
  * If success, return DP_SUCCESS.
  * else return -1 /DP_FAILURE
  */
@@ -282,7 +280,8 @@ int dp_add_pp_gpid(int inst, int dpid, int vap, int gpid, int spl_gpid)
 			cfg.tx.tailroom_size = STREAM_TAILROOM;
 		}
 		cfg.tx.max_pkt_size = STREAM_MAX_PKT_ZIE;
-		cfg.tx.min_pkt_len = PP_MIN_TX_PKT_LEN_NONE; //PP_MIN_TX_PKT_LEN_64B;
+		// PP_MIN_TX_PKT_LEN_64B
+		cfg.tx.min_pkt_len = PP_MIN_TX_PKT_LEN_NONE;
 	} else { /*to CPU */
 		cfg.tx.max_pkt_size = CPU_MAX_PKT_ZIE;
 		cfg.tx.headroom_size = CPU_HEADERROOM;
@@ -307,7 +306,6 @@ int dp_add_pp_gpid(int inst, int dpid, int vap, int gpid, int spl_gpid)
 	PR_INFO("cfg.tx.pkt_only_en=%d\n", cfg.tx.pkt_only_en);
 	PR_INFO("cfg.tx.seg_en=%d\n", cfg.tx.seg_en);
 
-
 	if (pp_port_add(gpid, &cfg)) {
 		PR_ERR("failed to create gpid: %d\n", gpid);
 		return DP_FAILURE;
@@ -321,7 +319,6 @@ int dp_del_pp_gpid(int inst, int dpid, int vap, int gpid, int spl_gpid)
 	return DP_SUCCESS;
 }
 
-
 /* dp_add_default_egress_sess: Add default egress session based on
  *                             special GPID, class/subif only
  * This API will be used only for CPU TX path to memory port for
@@ -341,9 +338,9 @@ int dp_add_default_egress_sess(struct dp_session *sess, int flag)
 	args.color = PP_COLOR_GREEN;
 	args.flags = 0;
 	args.dst_q = dp_get_q_logic_32(sess->inst, sess->qid);
-	for (i =0; i < ARRAY_SIZE(args.sgc); i++)
+	for (i = 0; i < ARRAY_SIZE(args.sgc); i++)
 		args.sgc[i] = PP_SGC_INVALID;
-	for (i =0; i < ARRAY_SIZE(args.tbm); i++)
+	for (i = 0; i < ARRAY_SIZE(args.tbm); i++)
 		args.tbm[i] = PP_TBM_INVALID;
 	args.ud_sz = 0;
 	args.tmp_ud_sz = 0; /* 1 means 1 template of UD,
@@ -395,23 +392,23 @@ int dp_add_hostif(int inst, int dpid, int vap)
 
 	hif.cls.port = get_dp_port_subif(port_info, vap)->gpid;
 	hif.dp.color = PP_COLOR_GREEN;  //??? enough
-	for (i =0; i < ARRAY_SIZE(hif.dp.sgc); i++)
+	for (i = 0; i < ARRAY_SIZE(hif.dp.sgc); i++)
 		hif.dp.sgc[i] = PP_SGC_INVALID;
-	for (i =0; i < ARRAY_SIZE(hif.dp.tbm); i++)
+	for (i = 0; i < ARRAY_SIZE(hif.dp.tbm); i++)
 		hif.dp.tbm[i] = PP_TBM_INVALID;
 
 	/* low priority */
-	hif.cls.tc_bitmap = BIT(0) | BIT(1);  /*need check GSWIP implementation ?*/
+	/* need check GSWIP implementation ? */
+	hif.cls.tc_bitmap = BIT(0) | BIT(1);
 	/*collect all CPU low priority queue/port */
-	for (i =0; (i < ARRAY_SIZE(hif.dp.eg)) && (i < MAX_SUBIFS); i++) {
+	for (i = 0; (i < ARRAY_SIZE(hif.dp.eg)) && (i < MAX_SUBIFS); i++) {
 		struct dp_subif_info *sif;
 
 		sif = get_dp_port_subif(cpu_info, 2 * i + 1);
 		if (sif->flags) { /* vaid VAP */
 			hif.dp.eg[i].qos_q = dp_get_q_logic_32(inst, sif->qid);
 			hif.dp.eg[i].pid = sif->gpid;
-		}
-		else {
+		} else {
 			hif.dp.eg[i].qos_q = PP_QOS_INVALID_ID;
 			hif.dp.eg[i].pid = PP_PORT_INVALID;
 		}
@@ -424,23 +421,23 @@ int dp_add_hostif(int inst, int dpid, int vap)
 		DP_INFO("hif.dp.eg[%d].pid=%u\n", i, hif.dp.eg[i].pid);
 		DP_INFO("hif.dp.eg[%d].qos_q=%u\n", i, hif.dp.eg[i].qos_q);
 	}
-	ret= pp_hostif_add(&hif);
+	ret = pp_hostif_add(&hif);
 	if (ret)
 		DP_ERR("hostif_add fail:dpid/gpid=%u/%u vap/tc=%d/%u\n",
 		       dpid, hif.cls.port, vap, hif.cls.tc_bitmap);
 
 	/* high priority */
-	hif.cls.tc_bitmap = BIT(2) | BIT(3); /*need check GSWIP implementation ?*/
+	/* need check GSWIP implementation ? */
+	hif.cls.tc_bitmap = BIT(2) | BIT(3);
 	/*collect all CPU low priority queue/port */
-	for (i =0; (i < ARRAY_SIZE(hif.dp.eg)) && (i < MAX_SUBIFS); i++) {
+	for (i = 0; (i < ARRAY_SIZE(hif.dp.eg)) && (i < MAX_SUBIFS); i++) {
 		struct dp_subif_info *sif;
 
 		sif = get_dp_port_subif(cpu_info, 2 * i);
 		if (sif->flags) { /* Valid VAP */
 			hif.dp.eg[i].qos_q = dp_get_q_logic_32(inst, sif->qid);
 			hif.dp.eg[i].pid = sif->gpid;
-		}
-		else {
+		} else {
 			hif.dp.eg[i].qos_q = PP_QOS_INVALID_ID;
 			hif.dp.eg[i].pid  = PP_PORT_INVALID;
 		}
@@ -453,7 +450,7 @@ int dp_add_hostif(int inst, int dpid, int vap)
 		DP_INFO("hif.dp.eg[%d].pid=%u\n", i, hif.dp.eg[i].pid);
 		DP_INFO("hif.dp.eg[%d].qos_q=%u\n", i, hif.dp.eg[i].qos_q);
 	}
-	ret= pp_hostif_add(&hif);
+	ret = pp_hostif_add(&hif);
 	if (ret)
 		DP_ERR("hostif_add fail:dpid/gpid=%d/%d vap/tc=%d/%d\n",
 		       dpid, hif.cls.port, vap, hif.cls.tc_bitmap);
@@ -461,7 +458,6 @@ int dp_add_hostif(int inst, int dpid, int vap)
 	return DP_SUCCESS;
 }
 
-
 /* dp_add_dflt_hostif: create default hostif
  * This API is for default setting in case not match any exception sessions
  */
@@ -479,13 +475,13 @@ int dp_add_dflt_hostif(struct dp_dflt_hostif *hostif, int flag)
 	dp.eg[0].pid = hostif->gpid;
 	dp.color = PP_COLOR_GREEN;
 
-	for (i =1; i < ARRAY_SIZE(dp.eg); i++) {
+	for (i = 1; i < ARRAY_SIZE(dp.eg); i++) {
 		dp.eg[i].qos_q = PP_QOS_INVALID_ID;
 		dp.eg[i].pid = PP_PORT_INVALID;
 	}
-	for (i =0; i < ARRAY_SIZE(dp.sgc); i++)
+	for (i = 0; i < ARRAY_SIZE(dp.sgc); i++)
 		dp.sgc[i] = PP_SGC_INVALID;
-	for (i =0; i < ARRAY_SIZE(dp.tbm); i++)
+	for (i = 0; i < ARRAY_SIZE(dp.tbm); i++)
 		dp.tbm[i] = PP_TBM_INVALID;
 
 	return pp_hostif_dflt_set(&dp);
@@ -507,7 +503,7 @@ int dp_subif_pp_set(int inst, int portid, int vap,
 		gpid = port_info->gpid_base + num;
 	}
 	PR_INFO("dp_subif_pp_set=%d\n", gpid);
-	if (dp_add_pp_gpid(inst, portid, vap, gpid, 0) == DP_FAILURE){
+	if (dp_add_pp_gpid(inst, portid, vap, gpid, 0) == DP_FAILURE) {
 		DP_ERR("dp_add_pp_gpid for dport/vap=%d/%d\n", portid, vap);
 		return -1;
 	}
@@ -515,8 +511,7 @@ int dp_subif_pp_set(int inst, int portid, int vap,
 
 	/* need increase GPID reference counter and store DPID
 	 * Also need update to pmac_port_info array
-	*/
+	 */
 	dp_add_hostif(inst, portid, vap);
 	return 0;
 }
-
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_ppv4_session.h b/drivers/net/datapath/dpm/gswip32/datapath_ppv4_session.h
index e22d1fb03a43..2d9481b0a556 100644
--- a/drivers/net/datapath/dpm/gswip32/datapath_ppv4_session.h
+++ b/drivers/net/datapath/dpm/gswip32/datapath_ppv4_session.h
@@ -14,9 +14,9 @@
 #define DP_CLASS_SIZE    4
 
 enum GPID_TYPE {
-	DP_RES_GPID = 0,  /* 0 - 15,    Dont Use, as it overlaps with LPID */
-	DP_DYN_GPID, 	  /* 16 - 239,  Dynamically allocated by DP */
-	DP_SPL_GPID 	  /* 240 - 255, 16 Special GPID per DPID */
+	DP_RES_GPID = 0, /*   0 -  15, Dont Use, as it overlaps with LPID */
+	DP_DYN_GPID,     /*  16 - 239, Dynamically allocated by DP */
+	DP_SPL_GPID      /* 240 - 255, 16 Special GPID per DPID */
 };
 
 #define DP_DYN_GPID_START	16
@@ -25,16 +25,17 @@ enum GPID_TYPE {
 #define DP_SPL_GPID_END		255
 
 #define IS_SPECIAL_GPID(gpid)	\
-	((gpid >= DP_SPL_GPID_START) && (gpid <= DP_SPL_GPID_END))?1:0
+	(((gpid >= DP_SPL_GPID_START) && (gpid <= DP_SPL_GPID_END)) ? 1 : 0)
 /* Get Special GPID via DPID with fixed algo:
-   DPID    special_gpid
-   15      255
-   14      254
-   13      253
-   ...
-   10 .... 250
+ * DPID    special_gpid
+ * 15      255
+ * 14      254
+ * 13      253
+ * ...
+ * 10 .... 250
  */
-#define SPL_GPID_VIA_DPID(dpid) (DP_SPL_GPID_END - (DP_DYN_GPID_START - 1 - dpid))
+#define SPL_GPID_VIA_DPID(dpid)	\
+	(DP_SPL_GPID_END - (DP_DYN_GPID_START - 1 - dpid))
 struct dp_dflt_hostif {
 	int inst;
 	int qid;
@@ -42,7 +43,6 @@ struct dp_dflt_hostif {
 	int color;
 };
 
-
 struct dp_session {
 	int inst; /* reserved for future */
 	int in_port; /* ingress GPID: Special GPID for this DC LPID.*/
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_proc.c b/drivers/net/datapath/dpm/gswip32/datapath_proc.c
index 3baae69e70a8..85e34f5fcca1 100644
--- a/drivers/net/datapath/dpm/gswip32/datapath_proc.c
+++ b/drivers/net/datapath/dpm/gswip32/datapath_proc.c
@@ -204,8 +204,9 @@ ssize_t proc_parser_write(struct file *file, const char *buf,
 		pce.action.bRMON_Action = 1;
 		pce.action.nRMON_Id = 0;	/*RMON_UDP_CNTR; */
 
-		if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_tflow_ops
-				 .TFLOW_PceRuleWrite, gsw_handle, &pce)) {
+		if (gsw_core_api(
+			(dp_gsw_cb)gsw_handle->gsw_tflow_ops.TFLOW_PceRuleWrite,
+			gsw_handle, &pce)) {
 			PR_ERR("PCE rule add fail: GSW_PCE_RULE_WRITE\n");
 			return count;
 		}
@@ -216,8 +217,9 @@ ssize_t proc_parser_write(struct file *file, const char *buf,
 		pce_rule_id = dp_atoi(param_list[1]);
 		pce.pattern.nIndex = pce_rule_id;
 		pce.pattern.bEnable = 0;
-		if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_tflow_ops
-				 .TFLOW_PceRuleWrite, gsw_handle, &pce)) {
+		if (gsw_core_api(
+			(dp_gsw_cb)gsw_handle->gsw_tflow_ops.TFLOW_PceRuleWrite,
+			gsw_handle, &pce)) {
 			PR_ERR("PCE rule add fail:GSW_PCE_RULE_WRITE\n");
 			return count;
 		}
@@ -248,8 +250,9 @@ char *get_bp_member_string_32(int inst, u16 bp, char *buf)
 	bp_cfg.nBridgePortId = bp;
 	bp_cfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP |
 		GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID;
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			   .BridgePort_ConfigGet, gsw_handle, &bp_cfg);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigGet,
+		gsw_handle, &bp_cfg);
 	if (ret != GSW_statusOk) {
 		PR_ERR("Failed to get bridge port's member for bridgeport=%d\n",
 		       bp_cfg.nBridgePortId);
@@ -264,8 +267,8 @@ char *get_bp_member_string_32(int inst, u16 bp, char *buf)
 
 /* proc_print_ctp_bp_info_32 is an callback API, not a standalone proc API */
 int proc_print_ctp_bp_info_32(struct seq_file *s, int inst,
-			   struct pmac_port_info *port,
-			   int subif_index, u32 flag)
+			      struct pmac_port_info *port,
+			      int subif_index, u32 flag)
 {
 	struct logic_dev *tmp;
 	struct dp_subif_info *sif = get_dp_port_subif(port, subif_index);
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_rx.c b/drivers/net/datapath/dpm/gswip32/datapath_rx.c
index d556ef98f5e6..c24fd7a9b5a1 100644
--- a/drivers/net/datapath/dpm/gswip32/datapath_rx.c
+++ b/drivers/net/datapath/dpm/gswip32/datapath_rx.c
@@ -23,13 +23,12 @@ void rx_dbg_32(u32 f, struct sk_buff *skb, struct dma_rx_desc_0 *desc0,
 {
 	int inst = 0;
 
-	DP_DEBUG(DP_DBG_FLAG_DUMP_RX, "\nDPID=%d GPID=%d\n",dpid, gpid);
+	DP_DEBUG(DP_DBG_FLAG_DUMP_RX, "\nDPID=%d GPID=%d\n", dpid, gpid);
 	DP_DEBUG(DP_DBG_FLAG_DUMP_RX,
 		 "\ndp_rx:skb->data=%px Loc=%x offset=%d skb->len=%d\n",
 		 skb->data, desc2->field.data_ptr,
 		 desc2->field.byte_offset, skb->len);
 
-
 	if ((f) & DP_DBG_FLAG_DUMP_RX_DESCRIPTOR)
 		dp_port_prop[inst].info.dump_rx_dma_desc(desc0, desc1,
 				desc2, desc3);
@@ -44,7 +43,6 @@ void rx_dbg_32(u32 f, struct sk_buff *skb, struct dma_rx_desc_0 *desc0,
 		dp_dump_raw_data((char *)skb->data,
 				 skb->len,
 				 "Original Data");
-
 }
 
 int32_t dp_rx_32(struct sk_buff *skb, u32 flags)
@@ -102,7 +100,7 @@ int32_t dp_rx_32(struct sk_buff *skb, u32 flags)
 	}
 
 	prel2_len += (desc_1->field.pre_l2 * 16);
-	if(prel2_len)
+	if (prel2_len)
 		pre_l2 = (u32 *)(skb->data + pmac_len);
 
 	/* only for those packet from PPv4 should  call pp hook ? */
@@ -113,7 +111,8 @@ int32_t dp_rx_32(struct sk_buff *skb, u32 flags)
 		goto RX_DROP2;
 	}
 
-	if (desc_1->field.ep < (get_dp_port_info(inst, 0)->gpid_base + get_dp_port_info(inst, 0)->gpid_num))
+	if (desc_1->field.ep < (get_dp_port_info(inst, 0)->gpid_base +
+				get_dp_port_info(inst, 0)->gpid_num))
 		gpid = pp_desc->ud.rx_port;
 
 	dpid = get_dpid_from_gpid(0, gpid);
@@ -125,9 +124,13 @@ int32_t dp_rx_32(struct sk_buff *skb, u32 flags)
 
 	if (unlikely(!dpid)) { /*Normally shouldnot go to here */
 		DP_ERR("Impossible: DPID Invalid (0), Desc rx'd: D0: %08x D1: %08x D2: %08x D3: %08x\n",
-		       *(u32 *)desc_0, *(u32 *)desc_1, *(u32 *)desc_2, *(u32 *)desc_3);
+		       *(u32 *)desc_0, *(u32 *)desc_1,
+		       *(u32 *)desc_2, *(u32 *)desc_3);
 		DP_ERR("QoS Descriptor at buf_base %px Desc rx'd: D0: %08x D1: %08x D2: %08x D3: %08x\n",
-                skb->buf_base, *(skb->buf_base), *(skb->buf_base + sizeof(u32)), *(skb->buf_base + (2 * sizeof(u32))), *(skb->buf_base + (3 * sizeof(u32))));
+		       skb->buf_base, *(skb->buf_base),
+		       *(skb->buf_base + sizeof(u32)),
+		       *(skb->buf_base + (2 * sizeof(u32))),
+		       *(skb->buf_base + (3 * sizeof(u32))));
 		goto RX_DROP;
 	}
 
@@ -153,6 +156,7 @@ int32_t dp_rx_32(struct sk_buff *skb, u32 flags)
 
 		if (atomic_inc_and_test(&p_subif->f_dfl_sess[classid]) == 1) {
 			struct dp_session sess;
+
 			sess.inst = inst;
 			sess.in_port = gpid;
 			sess.eg_port = p_subif->gpid;
@@ -163,7 +167,7 @@ int32_t dp_rx_32(struct sk_buff *skb, u32 flags)
 			sess.sig = pp_desc->ud.hash_sig;
 			if (dp_add_default_egress_sess(&sess, 0)) {
 				atomic_dec(&p_subif->f_dfl_sess[classid]);
-				PR_ERR("Fail to create default egress \n");
+				PR_ERR("Fail to create default egress\n");
 				goto RX_DROP;
 			}
 		} else {
@@ -172,10 +176,8 @@ int32_t dp_rx_32(struct sk_buff *skb, u32 flags)
 
 		cbm_data.dp_inst = inst;
 		cbm_data.f_byqos = 1;
-		res = cbm_cpu_pkt_tx(skb, &cbm_data, 0); /* no need to insert pmac
-						      * since it should be
-						      * already there
-						      */
+		/* no need to insert pmac since it should be already there */
+		res = cbm_cpu_pkt_tx(skb, &cbm_data, 0);
 		UP_STATS(mib->tx_cbm_pkt);
 		return res;
 	}
@@ -213,8 +215,8 @@ int32_t dp_rx_32(struct sk_buff *skb, u32 flags)
 		}
 
 		if (unlikely(dp_dbg_flag)) {
-			DP_DEBUG(DP_DBG_FLAG_DUMP_RX, "DPID=%d GPID=%d vap=%d\n",
-				 dpid, gpid, vap);
+			DP_DEBUG(DP_DBG_FLAG_DUMP_RX,
+				 "DPID=%d GPID=%d vap=%d\n", dpid, gpid, vap);
 
 			if (dp_dbg_flag & DP_DBG_FLAG_DUMP_RX_DATA) {
 				if (pmac_len) {
@@ -225,9 +227,8 @@ int32_t dp_rx_32(struct sk_buff *skb, u32 flags)
 				} else {
 					data_offset = skb->data;
 					data_len = skb->len;
-					dp_dump_raw_data(data_offset,
-						 	data_len,
-						 	"Data to top drv");
+					dp_dump_raw_data(data_offset, data_len,
+							 "Data to top drv");
 				}
 			}
 
@@ -237,7 +238,6 @@ int32_t dp_rx_32(struct sk_buff *skb, u32 flags)
 					desc_2, desc_3);
 		}
 
-
 		/*
 		 * If switch h/w acceleration is enabled,setting of this bit
 		 * avoid forwarding duplicate packets from linux
@@ -249,11 +249,11 @@ int32_t dp_rx_32(struct sk_buff *skb, u32 flags)
 
 #endif
 
-		/*Remove PMAC from SKB */
-        if (pmac_len)
-		    skb_pull(skb, pmac_len);
+		/* Remove PMAC from SKB */
+		if (pmac_len)
+			skb_pull(skb, pmac_len);
 
-		if((STATS_GET(p_subif->rx_flag) <= 0)) {
+		if ((STATS_GET(p_subif->rx_flag) <= 0)) {
 			UP_STATS(mib->rx_fn_dropped);
 			goto RX_DROP2;
 		}
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_switchdev.c b/drivers/net/datapath/dpm/gswip32/datapath_switchdev.c
index cc0e2de45325..5886337b714c 100644
--- a/drivers/net/datapath/dpm/gswip32/datapath_switchdev.c
+++ b/drivers/net/datapath/dpm/gswip32/datapath_switchdev.c
@@ -48,8 +48,9 @@ int dp_swdev_bridge_port_cfg_set(struct br_info *br_item,
 	DP_DEBUG(DP_DBG_FLAG_SWDEV, "Set current BP=%d inst:%d\n",
 		 brportcfg.nBridgePortId, inst);
 	brportcfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops.
-			   BridgePort_ConfigGet, gsw_handle, &brportcfg);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigGet,
+		gsw_handle, &brportcfg);
 	if (ret != GSW_statusOk) {
 		PR_ERR("fail in getting bridge port config\r\n");
 		return -1;
@@ -64,8 +65,9 @@ int dp_swdev_bridge_port_cfg_set(struct br_info *br_item,
 	brportcfg.nBridgePortId = bport;
 	brportcfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID |
 		GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops.
-			   BridgePort_ConfigSet, gsw_handle, &brportcfg);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigSet,
+		gsw_handle, &brportcfg);
 	if (ret != GSW_statusOk) {
 		PR_ERR("Fail in allocating/configuring bridge port\n");
 		return -1;
@@ -81,10 +83,9 @@ int dp_swdev_bridge_port_cfg_set(struct br_info *br_item,
 				 brportcfg.nBridgePortId, inst);
 			brportcfg.eMask =
 				GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
-			ret = gsw_core_api((dp_gsw_cb)gsw_handle
-					->gsw_brdgport_ops
-					.BridgePort_ConfigGet,
-					gsw_handle, &brportcfg);
+			ret = gsw_core_api(
+				(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigGet,
+				gsw_handle, &brportcfg);
 			if (ret != GSW_statusOk) {
 				PR_ERR
 					("fail in getting br port config\r\n");
@@ -96,10 +97,9 @@ int dp_swdev_bridge_port_cfg_set(struct br_info *br_item,
 			brportcfg.eMask =
 				GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID |
 				GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
-			ret = gsw_core_api((dp_gsw_cb)gsw_handle
-					->gsw_brdgport_ops
-					.BridgePort_ConfigSet,
-					gsw_handle, &brportcfg);
+			ret = gsw_core_api(
+				(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigSet,
+				gsw_handle, &brportcfg);
 			if (ret != GSW_statusOk) {
 				PR_ERR("Fail alloc/cfg bridge port\n");
 				return -1;
@@ -110,7 +110,7 @@ int dp_swdev_bridge_port_cfg_set(struct br_info *br_item,
 }
 
 int dp_swdev_bridge_port_cfg_reset_32(struct br_info *br_item,
-				   int inst, int bport)
+				      int inst, int bport)
 {
 	GSW_BRIDGE_portConfig_t brportcfg;
 	struct bridge_member_port *bport_list = NULL;
@@ -125,8 +125,9 @@ int dp_swdev_bridge_port_cfg_reset_32(struct br_info *br_item,
 		 brportcfg.nBridgePortId, inst);
 	brportcfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
 	/*Reset other members from current bport map*/
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			   .BridgePort_ConfigGet, gsw_handle, &brportcfg);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigGet,
+		gsw_handle, &brportcfg);
 	if (ret != GSW_statusOk) {
 		/* Note: here may fail if this device is not removed from
 		 * linux bridge via brctl delif but user try to un-regiser
@@ -162,8 +163,9 @@ int dp_swdev_bridge_port_cfg_reset_32(struct br_info *br_item,
 	brportcfg.nBridgePortId = bport;
 	brportcfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID |
 			  GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
-			   .BridgePort_ConfigSet, gsw_handle, &brportcfg);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigSet,
+		gsw_handle, &brportcfg);
 	if (ret != GSW_statusOk) {
 		PR_ERR("Fail in configuring GSW_BRIDGE_portConfig_t in %s\r\n",
 		       __func__);
@@ -181,10 +183,9 @@ int dp_swdev_bridge_port_cfg_reset_32(struct br_info *br_item,
 			brportcfg.eMask =
 				 GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP |
 				 GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID;
-			ret = gsw_core_api((dp_gsw_cb)gsw_handle
-					 ->gsw_brdgport_ops
-					 .BridgePort_ConfigGet,
-					 gsw_handle, &brportcfg);
+			ret = gsw_core_api(
+				(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigGet,
+				gsw_handle, &brportcfg);
 			if (ret != GSW_statusOk) {
 				PR_ERR("failed getting br port cfg\r\n");
 				return -1;
@@ -194,10 +195,9 @@ int dp_swdev_bridge_port_cfg_reset_32(struct br_info *br_item,
 			brportcfg.eMask =
 				 GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID |
 				 GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
-			ret = gsw_core_api((dp_gsw_cb)gsw_handle
-					 ->gsw_brdgport_ops
-					 .BridgePort_ConfigSet,
-					 gsw_handle, &brportcfg);
+			ret = gsw_core_api(
+				(dp_gsw_cb)gsw_handle->gsw_brdgport_ops.BridgePort_ConfigSet,
+				gsw_handle, &brportcfg);
 			if (ret != GSW_statusOk) {
 				PR_ERR("Fail alloc/cfg br port\n");
 				return -1;
@@ -229,8 +229,9 @@ int dp_swdev_bridge_cfg_set_32(int inst, u16 fid)
 	brcfg.eForwardBroadcast = GSW_BRIDGE_FORWARD_FLOOD;
 	brcfg.eForwardUnknownMulticastNonIp = GSW_BRIDGE_FORWARD_FLOOD;
 	brcfg.eForwardUnknownUnicast = GSW_BRIDGE_FORWARD_FLOOD;
-	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdg_ops
-			   .Bridge_ConfigSet, gsw_handle, &brcfg);
+	ret = gsw_core_api(
+		(dp_gsw_cb)gsw_handle->gsw_brdg_ops.Bridge_ConfigSet,
+		gsw_handle, &brcfg);
 	if (ret != GSW_statusOk) {
 		PR_ERR("Failed to set bridge id(%d)\n", brcfg.nBridgeId);
 		br.nBridgeId = fid;
@@ -309,23 +310,28 @@ int dp_gswip_ext_vlan_32(int inst, int vap, int ep)
 					 vlan_prop.in_proto, vlan_prop.in_vid);
 				DP_DEBUG(DP_DBG_FLAG_SWDEV,
 					 "VLAN out proto=%x, vid=%d\n",
-					 vlan_prop.out_proto, vlan_prop.out_vid);
-				vlan->vlan2_list[v2].outer_vlan.vid = vlan_prop.out_vid;
+					 vlan_prop.out_proto,
+					 vlan_prop.out_vid);
+				vlan->vlan2_list[v2].outer_vlan.vid =
+							vlan_prop.out_vid;
 				vlan->vlan2_list[v2].outer_vlan.tpid =
-								vlan_prop.out_proto;
+							vlan_prop.out_proto;
 				vlan->vlan2_list[v2].ether_type = 0;
-				vlan->vlan2_list[v2].inner_vlan.vid = vlan_prop.in_vid;
+				vlan->vlan2_list[v2].inner_vlan.vid =
+							vlan_prop.in_vid;
 				vlan->vlan2_list[v2].inner_vlan.tpid =
-								vlan_prop.in_proto;
+							vlan_prop.in_proto;
 				vlan->vlan2_list[v2].bp = tmp->bp;
 				v2 += 1;
 			} else if (vlan_prop.num == 1) {
 				DP_DEBUG(DP_DBG_FLAG_SWDEV,
 					 "outer VLAN proto=%x, vid=%d\n",
-					 vlan_prop.out_proto, vlan_prop.out_vid);
-				vlan->vlan1_list[v1].outer_vlan.vid = vlan_prop.out_vid;
+					 vlan_prop.out_proto,
+					 vlan_prop.out_vid);
+				vlan->vlan1_list[v1].outer_vlan.vid =
+							vlan_prop.out_vid;
 				vlan->vlan1_list[v1].outer_vlan.tpid =
-								vlan_prop.out_proto;
+							vlan_prop.out_proto;
 				vlan->vlan1_list[v1].bp = tmp->bp;
 				v1 += 1;
 			}
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_switchdev.h b/drivers/net/datapath/dpm/gswip32/datapath_switchdev.h
index e6b21b306504..7512dde30c87 100644
--- a/drivers/net/datapath/dpm/gswip32/datapath_switchdev.h
+++ b/drivers/net/datapath/dpm/gswip32/datapath_switchdev.h
@@ -11,9 +11,9 @@
 
 int dp_swdev_alloc_bridge_id_32(int inst);
 int dp_swdev_bridge_port_cfg_set_32(struct br_info *br_item,
-				 int inst, int bport);
+				    int inst, int bport);
 int dp_swdev_bridge_port_cfg_reset_32(struct br_info *br_item,
-				   int inst, int bport);
+				      int inst, int bport);
 int dp_swdev_bridge_cfg_set_32(int inst, u16 fid);
 int dp_swdev_free_brcfg_32(int inst, u16 fid);
 int dp_gswip_ext_vlan_32(int inst, int vap, int ep);
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_tc_asym_vlan.c b/drivers/net/datapath/dpm/gswip32/datapath_tc_asym_vlan.c
index 832d47acb371..b9a1c74934b3 100644
--- a/drivers/net/datapath/dpm/gswip32/datapath_tc_asym_vlan.c
+++ b/drivers/net/datapath/dpm/gswip32/datapath_tc_asym_vlan.c
@@ -127,11 +127,14 @@ static int update_ctp(struct core_ops *ops,
 	ctpcfg2.nSubIfIdGroup = subifidg;
 	if (ingress) {
 		if (multicast) {
-			ctpcfg1.eMask = GSW_CTP_PORT_CONFIG_MASK_INGRESS_VLAN_IGMP;
+			ctpcfg1.eMask =
+				GSW_CTP_PORT_CONFIG_MASK_INGRESS_VLAN_IGMP;
 			if (!pextvlan) {
-				ctpcfg2.bIngressExtendedVlanIgmpEnable = LTQ_FALSE;
+				ctpcfg2.bIngressExtendedVlanIgmpEnable =
+								LTQ_FALSE;
 			} else {
-				ctpcfg2.bIngressExtendedVlanIgmpEnable = LTQ_TRUE;
+				ctpcfg2.bIngressExtendedVlanIgmpEnable =
+								LTQ_TRUE;
 				ctpcfg2.nIngressExtendedVlanBlockIdIgmp =
 					pextvlan->nExtendedVlanBlockId;
 				ctpcfg2.nIngressExtendedVlanBlockSizeIgmp = 0;
@@ -149,11 +152,14 @@ static int update_ctp(struct core_ops *ops,
 		}
 	} else {
 		if (multicast) {
-			ctpcfg1.eMask = GSW_CTP_PORT_CONFIG_MASK_EGRESS_VLAN_IGMP;
+			ctpcfg1.eMask =
+				GSW_CTP_PORT_CONFIG_MASK_EGRESS_VLAN_IGMP;
 			if (!pextvlan) {
-				ctpcfg2.bEgressExtendedVlanIgmpEnable = LTQ_FALSE;
+				ctpcfg2.bEgressExtendedVlanIgmpEnable =
+								LTQ_FALSE;
 			} else {
-				ctpcfg2.bEgressExtendedVlanIgmpEnable = LTQ_TRUE;
+				ctpcfg2.bEgressExtendedVlanIgmpEnable =
+								LTQ_TRUE;
 				ctpcfg2.nEgressExtendedVlanBlockIdIgmp =
 					pextvlan->nExtendedVlanBlockId;
 				ctpcfg2.nEgressExtendedVlanBlockSizeIgmp = 0;
@@ -182,12 +188,14 @@ static int update_ctp(struct core_ops *ops,
 
 	if (ingress) {
 		if (multicast) {
-			if (ctpcfg1.bIngressExtendedVlanIgmpEnable != LTQ_FALSE) {
+			if (ctpcfg1.bIngressExtendedVlanIgmpEnable !=
+								LTQ_FALSE) {
 				GSW_EXTENDEDVLAN_alloc_t alloc = {0};
 
 				alloc.nExtendedVlanBlockId =
 					ctpcfg1.nIngressExtendedVlanBlockIdIgmp;
-				ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+				ops->gsw_extvlan_ops.ExtendedVlan_Free(ops,
+									&alloc);
 			}
 		} else {
 			if (ctpcfg1.bIngressExtendedVlanEnable != LTQ_FALSE) {
@@ -195,17 +203,20 @@ static int update_ctp(struct core_ops *ops,
 
 				alloc.nExtendedVlanBlockId =
 					ctpcfg1.nIngressExtendedVlanBlockId;
-				ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+				ops->gsw_extvlan_ops.ExtendedVlan_Free(ops,
+									&alloc);
 			}
 		}
 	} else {
 		if (multicast) {
-			if (ctpcfg1.bEgressExtendedVlanIgmpEnable != LTQ_FALSE) {
+			if (ctpcfg1.bEgressExtendedVlanIgmpEnable !=
+								LTQ_FALSE) {
 				GSW_EXTENDEDVLAN_alloc_t alloc = {0};
 
 				alloc.nExtendedVlanBlockId =
 					ctpcfg1.nEgressExtendedVlanBlockIdIgmp;
-				ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+				ops->gsw_extvlan_ops.ExtendedVlan_Free(ops,
+									&alloc);
 			}
 		} else {
 			if (ctpcfg1.bEgressExtendedVlanEnable != LTQ_FALSE) {
@@ -213,7 +224,8 @@ static int update_ctp(struct core_ops *ops,
 
 				alloc.nExtendedVlanBlockId =
 					ctpcfg1.nEgressExtendedVlanBlockId;
-				ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+				ops->gsw_extvlan_ops.ExtendedVlan_Free(ops,
+									&alloc);
 			}
 		}
 	}
@@ -862,7 +874,9 @@ int tc_vlan_set_32(struct core_ops *ops,
 	if ((info->dev_type & 0x01) != 0) {
 		int ret;
 
-		/* Multicast (IGMP Controlled) VLAN is not supported on Bridge Port */
+		/* Multicast (IGMP Controlled) VLAN
+		 * is not supported on Bridge Port
+		 */
 		if ((info->dev_type & 0x02) != 0)
 			return -EINVAL;
 
diff --git a/drivers/net/datapath/dpm/gswip32/datapath_tx.c b/drivers/net/datapath/dpm/gswip32/datapath_tx.c
index bc040266987f..9976b159cfbf 100644
--- a/drivers/net/datapath/dpm/gswip32/datapath_tx.c
+++ b/drivers/net/datapath/dpm/gswip32/datapath_tx.c
@@ -34,7 +34,6 @@ static int cqm_preprocess(struct sk_buff *skb, struct dp_tx_common *cmn,
 		return DP_XMIT_ERR_EP_ZERO;
 	}
 #endif
-
 	tx->desc_1->field.pmac = !!(cmn->flags & DP_TX_FLAG_INSERT_PMAC);
 	if (is_stream_port(tx->port->alloc_flags))
 		cmn->flags |= DP_TX_FLAG_STREAM_PORT;
@@ -174,9 +173,8 @@ int32_t dp_xmit_32(struct net_device *rx_if, dp_subif_t *rx_subif,
 	desc_3->all = (desc_3->all &
 		       tx.port->dma3_mask_template[TEMPL_NORMAL].all);
 
-	if (desc_3->field.dic) {
+	if (desc_3->field.dic)
 		desc_3->field.dic = 1;
-	}
 
 	desc_1->field.classid = (skb->priority >= 15) ? 15 : skb->priority;
 	desc_2->all = ((uintptr_t)skb->data) & 0xFFFFFFFF;
diff --git a/include/net/datapath_api.h b/include/net/datapath_api.h
index b586b60b95e4..666edb8a4650 100644
--- a/include/net/datapath_api.h
+++ b/include/net/datapath_api.h
@@ -104,7 +104,7 @@
 			   *   For GRX500/PRX300, only support 1 ring
 			   *   For LGM, maximum up to 2 rings
 			   */
-#define DP_TX_RING_NUM 8 /*!< maximum number of ACA RXOUT ring
+#define DP_TX_RING_NUM 8  /*!< maximum number of ACA RXOUT ring
 			   *   For GRX500/PRX300, only support 1 ring
 			   *   For 5G, it needs to support up to 16 ring.
 			   */
@@ -394,8 +394,8 @@ typedef struct dp_subif {
 			       *   valid for dp_get_netif_subifid only
 			       */
 	u16 dfl_eg_sess[DP_DFL_SESS_NUM]; /*!< [out] default egress session id
-			*   This is for CPU TX to DC only
-			*/
+					   *   This is for CPU TX to DC only
+					   */
 } dp_subif_t;
 
 typedef dp_subif_t PPA_SUBIF; /*!< @brief structure type dp_subif PPA_SUBIF*/
@@ -847,7 +847,8 @@ struct dp_subif_data {
 				     */
 	#define DP_MAC_LEARNING_EN 0
 	#define DP_MAC_LEARNING_DIS 1
-	u16 mac_learn_disable; /*!< [in] To enable or disable mac learning for subif
+	u16 mac_learn_disable; /*!< [in] To enable or disable
+				* mac learning for subif
 				*/
 };
 
@@ -860,10 +861,10 @@ enum dp_port_data_flag {
 	DP_F_DATA_RESV_SCH = BIT(4), /*!< reserve QOS scheduler */
 	DP_F_DATA_FCS_DISABLE = BIT(5), /*!< Disable FCS for PON port on SOC */
 	DP_F_DATA_NO_CQM_DEQ = BIT(6), /*!< No mapped CQM dequeue port needed,
-				    *   instead DC device directly dequeue
-				    *   packet from PP QOS port via credit left
-				    *   and credit add
-				    */
+					*   instead DC device directly dequeue
+					*   packet from PP QOS port via credit
+					*   left and credit add
+					*/
 	DP_F_DATA_CONTINOUS_Q_RESV = BIT(7) /*!< reserve continous physical
 					     *   queue ID
 					     */
@@ -959,8 +960,8 @@ struct dp_buf_info {
 	int inst; /*!< [in] DP instance ID */
 	int dp_port; /*!< [in] DP port ID */
 	struct dp_dc_buf rx[DP_RX_RING_NUM]; /* [in] buffers in the rx ring
-						 * to free
-						 */
+					      * to free
+					      */
 	struct dp_dc_buf tx; /* [in] buffers in the rx ring to free */
 };
 
@@ -978,8 +979,10 @@ int dp_free_dc_buf(struct dp_buf_info *buf, int flag);
 /**
  * @brief dp_gpid_tx_info
  */
-#define DP_DFT_MAX_PKT_LEN 1600 /*!< Default maximal packet length ??? Not sure need or not */
-#define DP_DFT_MIN_PKT_LEN 60 /*!< Default minimal packet length ??? Not sure need or not */
+/* FIXME: ??? Not sure need or not */
+#define DP_DFT_MAX_PKT_LEN 1600 /*!< Default maximal packet length */
+/* FIXME: ??? Not sure need or not */
+#define DP_DFT_MIN_PKT_LEN 60 /*!< Default minimal packet length */
 /*! @addtogroup Datapath_Driver_Structures */
 /*! @brief  PPA Sub-interface Data structure
  *@param port_id  Datapath Port Id corresponds to PMAC Port Id
@@ -1036,7 +1039,7 @@ struct dp_rx_ring {
 				*   For LGM: it is CQM enqueue register address
 				*   If NULL, it means not valid
 				*/
-	void *out_enq_vaddr;	/*!< [out] rxout ring virtual address
+	void *out_enq_vaddr;   /*!< [out] rxout ring virtual address
 				*   For software testing or debugging
 				*   For GRX350/PRX300, it is DMA coherent
 				*     virtual address of DMA Descriptor base
@@ -1062,7 +1065,7 @@ struct dp_rx_ring {
 			       *   Note:
 			       *   1. GRX350/PRX300: not support
 			       */
-	void *in_alloc_vaddr;	/*!< [out] rxin ring buf allocation virtual
+	void *in_alloc_vaddr;  /*!< [out] rxin ring buf allocation virtual
 				*     address. It is for 4 ring case only
 				*   For software testing or debugging
 				*   Note:
@@ -1088,18 +1091,18 @@ struct dp_rx_ring {
 	void *pkt_base_paddr; /*!< [out] packet list base physical address,
 			       *    which stored  @prefill_pkt_num of packet
 			       *    physical addressin
-			       * For LGM, normally no need and caller just set it
-			       *     to 0.
+			       * For LGM, normally no need and caller just
+			       *     set it to 0.
 			       * For GRX350/PRX300, Normally needed to pre-fill
-			       *     The buffer size should based on @rx_pkt_size
-			       *     requirement
+			       *     The buffer size should based on
+			       *     @rx_pkt_size requirement
 			       */
-	void *pkt_base_vaddr;	/*!< [out] packet list base virtual address,
+	void *pkt_base_vaddr;  /*!< [out] packet list base virtual address,
 				*    which stored @prefill_pkt_num of packet
 				*    physical addresses
 				*    For software testing or debugging
 				*/
-	void *pkt_list_vaddr;	/*!< [out] virtual address of 2nd pkt list,
+	void *pkt_list_vaddr;  /*!< [out] virtual address of 2nd pkt list,
 				*    which stored @prefill_pkt_num of packet
 				*    virtual addresses
 				*    For software testing or debugging
@@ -1121,9 +1124,9 @@ struct dp_tx_ring {
 	void *in_deq_paddr; /*!< [out] txin ring/dequeue physical base
 			     *   address
 			     */
-	void *in_deq_vaddr;	/*!< [out] txin ring/dequeue virtual base address
-					* For software testing or debugging
-					*/
+	void *in_deq_vaddr; /*!< [out] txin ring/dequeue virtual base address
+			     * For software testing or debugging
+			     */
 	int out_free_ring_size;/*!< [out] txout ring/free buffer burst size,
 				*     the number of buffer can be freed in
 				*     in one free operation.
@@ -1131,9 +1134,9 @@ struct dp_tx_ring {
 	void *out_free_paddr; /*!< [out] txout/free buffer
 			       *     physica address
 			       */
-	void *out_free_vaddr;	/*!< [out] txout/free buffer virtual base address
-					* For software testing or debugging
-					*/
+	void *out_free_vaddr; /*!< [out] txout/free buffer virtual base address
+			       * For software testing or debugging
+			       */
 	u32 num_tx_pkt; /*!< [in] nuber of packet */
 	int tx_pkt_size; /*!< [in] maximum packet size
 			  *   requirement to the packet
@@ -1224,30 +1227,30 @@ struct dp_umt {
  *  applications
  */
 struct dp_dev_data {
-	u8 num_rx_ring;   /*!< [in] number of rx ring from DC device to Host.
-			    *   num_rx_ring requirement:
-			    *   @num_rings <= @DP_RX_RING_NUM
-			    *   GRX350/PRX300:1 rx ring
-			    *   LGM: up to 2 rx ring, like Docsis can use 2 rings
-			    *   For two ring case:
-			    *    1st rxout ring without qos
-			    *    2nd rxout ring with qos
-			    */
-	u8 num_tx_ring;   /*!< [in] number of tx ring from Host to DC device
-			    *   num_rx_ring requirement:
-			    *   @num_rings <= @DP_TX_RING_NUM
-			    *   Normally it is 1 TX ring only.
-			    *   But for 5G, it can support up to 8 TX ring
-			    *   For docsis, alhtough it is 16 dequeue port to WIB.
-			    *   But the final ring only 1, ie, WIB to Dcosis
-			    */
+	u8 num_rx_ring; /*!< [in] number of rx ring from DC device to Host.
+			 *   num_rx_ring requirement:
+			 *   @num_rings <= @DP_RX_RING_NUM
+			 *   GRX350/PRX300:1 rx ring
+			 *   LGM: up to 2 rx ring, like Docsis can use 2 rings
+			 *   For two ring case:
+			 *    1st rxout ring without qos
+			 *    2nd rxout ring with qos
+			 */
+	u8 num_tx_ring; /*!< [in] number of tx ring from Host to DC device
+			 *   num_rx_ring requirement:
+			 *   @num_rings <= @DP_TX_RING_NUM
+			 *   Normally it is 1 TX ring only.
+			 *   But for 5G, it can support up to 8 TX ring
+			 *   For docsis, alhtough it is 16 dequeue port to WIB.
+			 *   But the final ring only 1, ie, WIB to Dcosis
+			 */
 	u8 num_umt_port;   /*!< [in] number of UMT port.
 			    *    Normally is 1 only. But Docsis can use up to 2
 			    */
 	struct dp_rx_ring rx_ring[DP_RX_RING_NUM]; /*!< [in/out] DC rx ring info
 						    */
 	struct dp_tx_ring tx_ring[DP_TX_RING_NUM]; /*!< [in/out] DC tx ring info
-						     */
+						    */
 	struct dp_gpid_tx_info gpid_info; /*!< [in] for GPID tx information
 					   *   Valid only if @f_gpid valid.
 					   */
@@ -1537,7 +1540,8 @@ int dp_coc_new_stat_req(int new_state, uint32_t flag);
  */
 /*! DP's submodule to call it */
 /*int dp_set_rmon_threshold(struct dp_coc_threshold *threshold,
-			  uint32_t flags);*/
+ *			    uint32_t flags);
+ */
 #endif /*! CONFIG_INTEL_DATAPATH_CPUFREQ*/
 
 /*! get port flag. for TMU proc file cat /proc/tmu/queue1 and /proc/tmu/eqt */
@@ -1742,30 +1746,30 @@ struct dp_spl_conn {
 	int dp_port; /*!< [out] dp_port ID, normally it is CPU 0.
 		      *   if -1, then not applicable for this special connect
 		      */
-	u8 num_rx_ring;   /*!< [in] number of rx ring from DC device to Host.
-		    *   num_rx_ring requirement:
-		    *   @num_rings <= @DP_RX_RING_NUM
-		    *   GRX350/PRX300:1 rx ring
-		    *   LGM: up to 2 rx ring, like Docsis can use 2 rings
-		    *   For two ring case:
-		    *    1st rxout ring without qos
-		    *    2nd rxout ring with qos
-		    */
-	u8 num_tx_ring;   /*!< [in] number of tx ring from Host to DC device
-			    *   num_rx_ring requirement:
-			    *   @num_rings <= @DP_TX_RING_NUM
-			    *   Normally it is 1 TX ring only.
-			    *   But for 5G, it can support up to 8 TX ring
-			    *   For docsis, alhtough it is 16 dequeue port to WIB.
-			    *   But the final ring only 1, ie, WIB to Dcosis
-			    */
-	u8 num_umt_port;   /*!< [in] number of UMT port.
-			     *    Normally is 1 only. But Docsis can use up to 2
-			     */
+	u8 num_rx_ring; /*!< [in] number of rx ring from DC device to Host.
+			 *   num_rx_ring requirement:
+			 *   @num_rings <= @DP_RX_RING_NUM
+			 *   GRX350/PRX300:1 rx ring
+			 *   LGM: up to 2 rx ring, like Docsis can use 2 rings
+			 *   For two ring case:
+			 *    1st rxout ring without qos
+			 *    2nd rxout ring with qos
+			 */
+	u8 num_tx_ring; /*!< [in] number of tx ring from Host to DC device
+			 *   num_rx_ring requirement:
+			 *   @num_rings <= @DP_TX_RING_NUM
+			 *   Normally it is 1 TX ring only.
+			 *   But for 5G, it can support up to 8 TX ring
+			 *   For docsis, alhtough it is 16 dequeue port to WIB.
+			 *   But the final ring only 1, ie, WIB to Dcosis
+			 */
+	u8 num_umt_port; /*!< [in] number of UMT port.
+			  *    Normally is 1 only. But Docsis can use up to 2
+			  */
 	struct dp_rx_ring rx_ring[DP_RX_RING_NUM]; /*!< [in/out] DC rx ring info
 						    */
 	struct dp_tx_ring tx_ring[DP_TX_RING_NUM]; /*!< [in/out] DC tx ring info
-						     */
+						    */
 	struct dp_gpid_tx_info gpid_info; /*!< [in] for GPID tx information
 					   *   Valid only if @f_gpid valid.
 					   */
diff --git a/include/net/datapath_api_qos.h b/include/net/datapath_api_qos.h
index f9bce8a38c50..a44c7e1de261 100644
--- a/include/net/datapath_api_qos.h
+++ b/include/net/datapath_api_qos.h
@@ -1090,7 +1090,8 @@ enum dp_col_marking {
 	/*!< Mark all to green */
 	DP_NO_MARKING,
 	/*!< Internal marking derives the color of the packet from internal
-	     data flow instead of using VLAN tag or DSCP */
+	 *   data flow instead of using VLAN tag or DSCP
+	 */
 	DP_INTERNAL,
 	/*!< Drop eligible bit color marking */
 	DP_DEI,
@@ -1125,11 +1126,11 @@ enum dp_meter_traffic_type {
 };
 
 /*!
-* @struct dp_meter_cfg
-*
-* @brief This structure defines the exact meter configuration
-*
-*/
+ * @struct dp_meter_cfg
+ *
+ * @brief This structure defines the exact meter configuration
+ *
+ */
 struct dp_meter_cfg {
 	/*!< meter for ingressing traffic */
 #define DP_DIR_INGRESS 0
@@ -1164,11 +1165,11 @@ struct dp_meter_cfg {
 	 /*!< color marking mode */
 	enum dp_col_marking mode;
 	 /*!< traffic flow type for bridge/PCE rule mode only */
-	union dp_pce{
+	union dp_pce {
 		enum dp_meter_traffic_type flow;
 		/*!< PCE table rule index */
 		u32 pce_idx;
-	}dp_pce;
+	} dp_pce;
 };
 
 /*!< API dp_meter_alloc: allocate a meter resource
