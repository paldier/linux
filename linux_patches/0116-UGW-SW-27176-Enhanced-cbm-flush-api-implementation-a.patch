From 435ef10e18693d90af854ebcf88a18d7beec0343 Mon Sep 17 00:00:00 2001
From: agaraix <anath.bandux.garai@intel.com>
Date: Tue, 3 Jul 2018 19:07:00 +0530
Subject: [PATCH] UGW_SW-27176 : Enhanced cbm flush api implementation and
 aligned dc api header for wlan recovery.

---
 drivers/net/ethernet/lantiq/cqm/grx500/cbm.c | 458 +++++++++++++++++----------
 include/net/directconnect_dp_api.h           |  32 +-
 include/net/lantiq_cbm_api.h                 |   1 +
 3 files changed, 324 insertions(+), 167 deletions(-)

diff --git a/drivers/net/ethernet/lantiq/cqm/grx500/cbm.c b/drivers/net/ethernet/lantiq/cqm/grx500/cbm.c
index 5967a94ff359..cb59dd8b916e 100644
--- a/drivers/net/ethernet/lantiq/cqm/grx500/cbm.c
+++ b/drivers/net/ethernet/lantiq/cqm/grx500/cbm.c
@@ -36,6 +36,7 @@
 #define CBM_QDQCNT_BASE	(g_base_addr.cbm_qdqcnt_addr_base)
 #define CBM_DMADESC_BASE	(g_base_addr.cbm_dmadesc_addr_base)
 #define SIZE_DONT_CARE 0
+#define CHECK_WHILE_LOOP 1
 static struct cbm_ctrl g_cbm_ctrl;
 struct clk *g_cbm_clk;
 static void *std_buf_base;
@@ -99,7 +100,6 @@ LIST_HEAD(pmac_mapping_list);
 /*!< spin lock for cbm port mapping list*/
 spinlock_t cbm_port_mapping;
 
-
 #define SBID_START 16
 #define NKEYS (sizeof(device_table) / sizeof(struct t_symstruct))
 
@@ -669,13 +669,14 @@ static void *cbm_buffer_alloc_grx500(u32 pid, u32 flag)
 		 (i++) < DEFAULT_WAIT_CYCLES);
 
 	if ((buf_addr & 0xFFFFF800) == 0xFFFFF800) {
-		cbm_err("alloc buffer fail for portid: %d type %d", pid, flag);
+		cbm_err("alloc buffer fail for portid: %d type %d ofsc %d\n", pid, flag, cbm_get_std_free_count());
 		local_irq_restore(sys_flag);
 		return NULL;
 	}
 	local_irq_restore(sys_flag);
 	return (void *)__va(buf_addr);
 }
+
 static void *cbm_buffer_alloc_grx500_by_size(u32 pid, u32 flag, u32 size)
 {
 	if ((flag & CBM_PORT_F_STANDARD_BUF) || (flag & CBM_PORT_F_JUMBO_BUF))
@@ -750,7 +751,7 @@ void *data, u32 frag_size, gfp_t priority)
 				CBM_STD_BUF_SIZE : CONFIG_CBM_JBO_PKT_SIZE;
 	if (frag_size > buf_size) {
 		LOGF_KLOG_ERROR("frag_size = %d received in buffer = %x \n",
-							frag_size, (unsigned int)data);
+				frag_size, (unsigned int)data);
 		LOGF_KLOG_ERROR("packet length exceeds the buffer size\n");
 		return NULL;
 	}
@@ -770,7 +771,7 @@ static void init_fsqm_by_idx(int idx)
 /* FSQM buffer initialization
  */
 static void init_fsqm_buf_std(unsigned int std_base_addr,
-		       unsigned int size)
+			      unsigned int size)
 {
 	int i;
 	int minlsa, maxlsa;
@@ -859,7 +860,6 @@ static void init_fsqm_buf_jumbo(unsigned int jbo_base_addr, unsigned int size)
 
 static void init_fsqm(void)
 {
-	
 	buf_addr_adjust((u32)std_buf_base,
 			CONFIG_CBM_STD_BUF_POOL_SIZE,
 			&g_cbm_buff.std_buf_addr,
@@ -906,25 +906,6 @@ static void cbm_intr_mapping_init(void)
 #endif
 }
 
-
-#if 0
-static void cbm_intr_mapping_uninit(void)
-{
-	int i;
-
-	for (i = 0; i < CBM_MAX_INT_LINES / 2; i++) {
-		/*enq intr attach to first 4 lines */
-		xrx500_cbm_w32((CBM_BASE + CBM_INT_LINE(i, igp_irnen)),
-			       0 << i);
-		/*LS intr attach to last 4 lines */
-		xrx500_cbm_w32((CBM_BASE +
-			       CBM_INT_LINE(i + (CBM_MAX_INT_LINES / 2),
-					    cbm_irnen)),
-			       0 << (i + 8));
-	}
-	xrx500_cbm_w32((CBM_BASE + CBM_INT_LINE(0, cbm_irnen)), 0x0);
-}
-#endif
 /* CBM basic initialization
  */
 static void init_cbm_basic(void)
@@ -932,8 +913,8 @@ static void init_cbm_basic(void)
 	int jsel = 0;
 	unsigned int std_buf_addr = g_cbm_buff.std_buf_addr;
 	unsigned int jbo_buf_addr = g_cbm_buff.jbo_buf_addr;
-	pr_info("PHY ADDR STD 0xl%x\n", __pa(std_buf_addr));
-	pr_info("PHY ADDR JBO 0xl%x\n", __pa(jbo_buf_addr));
+	pr_info("PHY ADDR STD 0x%lx\n", __pa(std_buf_addr));
+	pr_info("PHY ADDR JBO 0x%lx\n", __pa(jbo_buf_addr));
 	xrx500_cbm_w32((CBM_BASE + CBM_SBA_0), __pa(std_buf_addr));
 	xrx500_cbm_w32((CBM_BASE + CBM_JBA_0), __pa(jbo_buf_addr));
 	xrx500_cbm_w32((CBM_BASE + CBM_SBA_1), CBM_IOCU_ADDR(std_buf_addr));
@@ -956,14 +937,6 @@ static void init_cbm_basic(void)
 
 /* CBM Enqueue port Initialization
  */
-#if 0
-static void init_cbm_eqm_scpu_port(void)
-{
-	xrx500_cbm_w32((CBM_EQM_BASE + CFG_CPU_IGP_4), 0x303);
-	xrx500_cbm_w32((CBM_EQM_BASE + POCC_CPU_IGP_4), 0);
-}
-#endif
-
 static void init_cbm_eqm_cpu_port(int idx)
 {
 	pr_info("%d\n", idx);
@@ -1112,18 +1085,6 @@ static void init_cbm_dqm_cpu_port(int idx)
 #endif
 }
 
-#if 0
-static void init_cbm_dqm_wlan_cpu_port(int idx)
-{
-	int ep_map = ((idx << CFG_CPU_EGP_0_EPMAP_POS) &
-		       CFG_CPU_EGP_0_EPMAP_MASK);
-
-	/*struct cbm_dqm_cpu_egp_reg  cbm_dqm_cpu;*/
-	xrx500_cbm_w32((CBM_DQM_BASE + CBM_DQM_CPU_PORT(idx, cfg)),
-		       0x107 | ep_map);
-}
-#endif
-
 static void init_cbm_dqm_scpu_port(int idx)
 {
 	int ep_map = ((idx << CFG_CPU_EGP_5_EPMAP_POS) &
@@ -1194,8 +1155,8 @@ static void cbm_qid_reg_set(struct cbm_qidt_elm *qidt_elm, u8 qid_val)
 }
 
 static void cbm_qidt_set(const struct cbm_qidt_elm *qid_set,
-		  const struct cbm_qidt_mask *qid_mask,
-		  u8 qid_val)
+			 const struct cbm_qidt_mask *qid_mask,
+			 u8 qid_val)
 {
 	int cls_idx = 0, ep_idx = 0, fl_idx = 0, fh_idx = 0, dec_idx = 0,
 	enc_idx = 0, mpe1_idx = 0, mpe2_idx = 0, state;
@@ -1205,21 +1166,21 @@ static void cbm_qidt_set(const struct cbm_qidt_elm *qid_set,
 
 	pr_debug("%d\n", qid_val);
 	pr_debug("%u %u %u %u %u %u %u %u\n", qid_set->clsid,
-		qid_set->ep,
-		qid_set->mpe1,
-		qid_set->mpe2,
-		qid_set->enc,
-		qid_set->dec,
-		qid_set->flowidh,
-		qid_set->flowidl);
+		 qid_set->ep,
+		 qid_set->mpe1,
+		 qid_set->mpe2,
+		 qid_set->enc,
+		 qid_set->dec,
+		 qid_set->flowidh,
+		 qid_set->flowidl);
 	pr_debug("%u %u %u %u %u %u %u %u\n", qid_mask->classid_mask,
-		qid_mask->ep_mask,
-		qid_mask->mpe1_mask,
-		qid_mask->mpe2_mask,
-		qid_mask->enc_mask,
-		qid_mask->dec_mask,
-		qid_mask->flowid_hmask,
-		qid_mask->flowid_lmask);
+		 qid_mask->ep_mask,
+		 qid_mask->mpe1_mask,
+		 qid_mask->mpe2_mask,
+		 qid_mask->enc_mask,
+		 qid_mask->dec_mask,
+		 qid_mask->flowid_hmask,
+		 qid_mask->flowid_lmask);
 #if 1
 	state = STATE_CLASS;
 	while (state != STATE_NONE) {
@@ -1373,7 +1334,7 @@ static void cbm_qidt_set(const struct cbm_qidt_elm *qid_set,
 		break;
 		};
 		}
-#endif	
+#endif
 }
 
 static s32
@@ -1567,7 +1528,7 @@ void dump_cbm_desc(struct cbm_desc *desc, int detail)
  * Caller must have checked the parameter validity etc before calling this API
  */
 static int cpu_enqueue_hw(u32 pid, struct cbm_desc *desc, void *data_pointer,
-		   int flags)
+			  int flags)
 {
 	unsigned long sys_flag;
 	local_irq_save(sys_flag);
@@ -1807,7 +1768,7 @@ cbm_cpu_pkt_tx_grx500(
 			pr_err("%s: cbm buffer alloc failed ..\n", __func__);
 			dev_kfree_skb_any(skb);
 			return CBM_FAILURE;
-		} 
+		}
 		if (cbm_linearise_buf(skb, data, buf_size, (new_buf + CBM_FIXED_RX_OFFSET))) {
 			pr_err("Error in linearising\n");
 			cbm_buffer_free(smp_processor_id(), (void *)new_buf, 0);
@@ -1816,7 +1777,6 @@ cbm_cpu_pkt_tx_grx500(
 		tmp_data_ptr = new_buf + CBM_FIXED_RX_OFFSET;
 		if (data && data->pmac)
 			skb->len += data->pmac_len;
-
 	} else {
 		if (data && data->pmac) {
 			skb_push(skb, data->pmac_len);
@@ -1828,7 +1788,7 @@ cbm_cpu_pkt_tx_grx500(
 		skb->head = NULL;
 	}
 	if (setup_desc((struct cbm_desc *)&desc, tmp_data_ptr,
-		       skb->len,
+		       (skb->len < (ETH_ZLEN + 8)) ? (ETH_ZLEN + 8) : skb->len,
 		       skb->DW1, skb->DW0)){
 		pr_err("cbm setup desc failed..\n");
 		dev_kfree_skb_any(skb);
@@ -2194,20 +2154,17 @@ static int do_dq_cbm_poll(struct napi_struct *napi, int budget)
 				LOGF_KLOG_DEV_ERROR(g_cbm_ctrl.dev, "%s DMA map failed\n", __func__);
 				break;
 			}
-			
-
 			result = check_ptr_valid_type(data_ptr);
 			real_buf_start = (result == SBA0_STD) ?
 			(data_ptr & 0xfffff800) :
 			(data_ptr & g_cbm_ctrl.jumbo_size_mask);
-		
 			new_offset = (data_ptr - real_buf_start);
 			temp_len = data_len + new_offset + data_offset;
 			real_len = SKB_DATA_ALIGN(temp_len)
 			+ SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
 			skb = build_skb_grx500(real_buf_start,
-			real_len,
-			GFP_ATOMIC);
+					       real_len,
+					       GFP_ATOMIC);
 			if (skb) {
 				skb_reserve(skb, new_offset + data_offset);
 				/* Call the datapath library Rx function */
@@ -2333,8 +2290,8 @@ static void do_cbm_tasklet(unsigned long cpu)
 			real_len = SKB_DATA_ALIGN(temp_len)
 			+ SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
 			skb = build_skb_grx500((u32 *)real_buf_start,
-			real_len,
-			GFP_ATOMIC);
+					       real_len,
+					       GFP_ATOMIC);
 			if (skb) {
 				skb_reserve(skb, new_offset + data_offset);
 				/* Call the datapath library Rx function */
@@ -2770,7 +2727,10 @@ s32 cbm_port_id,
 u32 flags)
 {
 	int result = cbm_port_id;
-
+	int i = 0;
+	u32 reg;
+	u32 data_pointer = 0;
+	u32 cfg;
 	if (flags & CBM_PORT_F_DEQUEUE_PORT) {
 		if ((cbm_port_id >= 5) && (cbm_port_id <= 22))
 		result = 5;
@@ -2779,29 +2739,67 @@ u32 flags)
 		case 24:
 		case 25:
 		case 26:
+			cfg = xrx500_cbm_r32(CBM_DQM_BASE + CBM_DQM_CPU_PORT(result, cfg));
+			rmb();
+			xrx500_cbm_w32((CBM_DQM_BASE + CBM_DQM_CPU_PORT(result, cfg)), 0);
+			wmb();
 			xrx500_cbm_w32((CBM_DQM_BASE + CBM_DQM_CPU_PORT(cbm_port_id, dptr)), 0x1f);
 			xrx500_cbm_w32((CBM_DQM_BASE + CBM_DQM_CPU_PORT(cbm_port_id, resv2[0])), 0x0);
 			xrx500_cbm_w32((CBM_DQM_BASE + CBM_DQM_CPU_PORT(cbm_port_id, dqpc)), 0x0);
+			wmb();
 #ifdef CONFIG_LTQ_UMT_SW_MODE
 			umt_reset_port_dq_idx(cbm_port_id);
 #endif
 			xrx500_cbm_w32((CBM_DQM_BASE + CBM_DQM_CPU_PORT(cbm_port_id, bprc)), 0x0);
+			wmb();
+			for (i = 0; i < dqm_port_info[cbm_port_id].deq_info.num_desc; i++) {
+				reg = xrx500_cbm_r32(CBM_DQM_BASE + CBM_DQM_SCPU_PORT(cbm_port_id, desc[i].desc0) + 12);
+				rmb();
+				if (reg & 0x80000000) {
+					data_pointer = xrx500_cbm_r32(CBM_DQM_BASE + CBM_DQM_SCPU_PORT(cbm_port_id, desc[i].desc0) + 8);
+					cbm_buffer_free_grx500(smp_processor_id(), (void *)data_pointer, 1);
+				}
+				xrx500_cbm_w32(CBM_DQM_BASE + CBM_DQM_SCPU_PORT(cbm_port_id, desc[i].desc0) + 0, 0);
+				xrx500_cbm_w32(CBM_DQM_BASE + CBM_DQM_SCPU_PORT(cbm_port_id, desc[i].desc0) + 4, 0);
+				xrx500_cbm_w32(CBM_DQM_BASE + CBM_DQM_SCPU_PORT(cbm_port_id, desc[i].desc0) + 8, 0);
+				xrx500_cbm_w32(CBM_DQM_BASE + CBM_DQM_SCPU_PORT(cbm_port_id, desc[i].desc0) + 12, 0);
+			}
+			xrx500_cbm_w32((CBM_DQM_BASE + CBM_DQM_CPU_PORT(result, cfg)), cfg);
+			wmb();
 		break;
 		case 0:
 		case 1:
 		case 2:
 		case 3:
+			cfg = xrx500_cbm_r32(CBM_DQM_BASE + CBM_DQM_CPU_PORT(result, cfg));
+			rmb();
+			xrx500_cbm_w32((CBM_DQM_BASE + CBM_DQM_CPU_PORT(result, cfg)), 0);
+			wmb();
 			xrx500_cbm_w32((CBM_DQM_BASE + CBM_DQM_CPU_PORT(cbm_port_id, dptr)), 0x1);
 			xrx500_cbm_w32((CBM_DQM_BASE + CBM_DQM_CPU_PORT(cbm_port_id, dqpc)), 0x0);
 			xrx500_cbm_w32((CBM_DQM_BASE + CBM_DQM_CPU_PORT(cbm_port_id, bprc)), 0x0);
+			xrx500_cbm_w32((CBM_DQM_BASE + CBM_DQM_CPU_PORT(result, cfg)), cfg);
+			wmb();
 		break;
 		case 5:
+			cfg = xrx500_cbm_r32(CBM_DQM_BASE + CBM_DQM_CPU_PORT(cbm_port_id, cfg));
+			rmb();
+			xrx500_cbm_w32((CBM_DQM_BASE + CBM_DQM_CPU_PORT(cbm_port_id, cfg)), 0);
+			wmb();
 			xrx500_cbm_w32((CBM_DQM_BASE + CBM_DQM_CPU_PORT(cbm_port_id, dptr)), 0x1);
 			xrx500_cbm_w32((CBM_DQM_BASE + CBM_DQM_CPU_PORT(cbm_port_id, dqpc)), 0x0);
+			xrx500_cbm_w32((CBM_DQM_BASE + CBM_DQM_CPU_PORT(cbm_port_id, cfg)), cfg);
+			wmb();
 		break;
 		case 23:
+			cfg = xrx500_cbm_r32(CBM_DQM_BASE + CBM_DQM_CPU_PORT(result, cfg));
+			rmb();
+			xrx500_cbm_w32((CBM_DQM_BASE + CBM_DQM_CPU_PORT(result, cfg)), 0);
+			wmb();
 			xrx500_cbm_w32((CBM_DQM_BASE + CBM_DQM_CPU_PORT(cbm_port_id, dptr)), 0x1f);
 			xrx500_cbm_w32((CBM_DQM_BASE + CBM_DQM_CPU_PORT(cbm_port_id, dqpc)), 0x0);
+			xrx500_cbm_w32(CBM_DQM_BASE + CBM_DQM_CPU_PORT(result, cfg), cfg);
+			wmb();
 		}
 	} else {
 		return CBM_FAILURE;
@@ -2907,7 +2905,7 @@ dp_port_alloc(
 	} else {
 		if ((pmac[0] != CBM_PMAC_DYNAMIC) && (pmac[0] != port_id)) {
 			LOGF_KLOG_ERROR("%s Error in the mapping table\r\n",
-			__func__);
+					__func__);
 			return DP_FAILURE;
 		}
 		local_entry.pmac = port_id;
@@ -3235,6 +3233,67 @@ void cbm_restore_qmap(int enable)
 	LOGF_KLOG_DEBUG("%s:count %d\n", __func__, count);
 }
 
+void cbm_restore_qmap_strict(int enable, int qid)
+{
+	u8 *ptr, *ptr1;
+	u32 i, found = 0, count = 0;
+	u32 tmp_drop_flag;
+
+	for (i = 0; i < 0x1000; i++) {
+		tmp_drop_flag = g_cbm_qidt_mirror[i].qidt_drop_flag;
+		ptr = (u8 *)&g_cbm_qidt_mirror[i].qidt_drop_flag;
+		ptr1 = (u8 *)&g_cbm_qidt_mirror[i].qidt_shadow;
+		if (enable) {
+			if (ptr[3] && (ptr1[3] == qid)) {
+				cbm_qtable[ptr1[3]].refcnt++;
+				found++;
+				count++;
+				tmp_drop_flag &= 0x01010100;
+			}
+			if (ptr[2] && (ptr1[2] == qid)) {
+				cbm_qtable[ptr1[2]].refcnt++;
+				found++;
+				count++;
+				tmp_drop_flag &= 0x01010001;
+			}
+			if (ptr[1] && (ptr1[1] == qid)) {
+				cbm_qtable[ptr1[1]].refcnt++;
+				found++;
+				count++;
+				tmp_drop_flag &= 0x01000101;
+			}
+			if (ptr[0] && (ptr1[0] == qid)) {
+				cbm_qtable[ptr1[0]].refcnt++;
+				found++;
+				count++;
+				tmp_drop_flag &= 0x00010101;
+			}
+		} else {
+			if (ptr[3] || ptr[2] || ptr[1] || ptr[0])
+				found = 1;
+			if (ptr[3]) {
+				count++;
+			}
+			if (ptr[2]) {
+				count++;
+			}
+			if (ptr[1]) {
+				count++;
+			}
+			if (ptr[0]) {
+				count++;
+			}
+		}
+		if (found) {
+			g_cbm_qidt_mirror[i].qidt_drop_flag = tmp_drop_flag;
+			if (enable)
+				xrx500_cbm_w32((CBM_QIDT_BASE + i * 4), g_cbm_qidt_mirror[i].qidt_shadow);
+		}
+		found = 0;
+	}
+	LOGF_KLOG_DEBUG("%s:count %d\n", __func__, count);
+}
+
 static s32
 dp_enable(
 	struct module *owner,
@@ -3256,7 +3315,7 @@ dp_enable(
 	qidt_set.ep = port_id;
 	qidt_mask.ep_mask = 0;
 	if ((dp_port_resources_get(&port_id, &num, &res,
-				       alloc_flags) != 0) && (!res)) {
+				   alloc_flags) != 0) && (!res)) {
 		LOGF_KLOG_ERROR("Error getting resources for port_id %d\n", port_id);
 		return CBM_FAILURE;
 	}
@@ -3278,7 +3337,7 @@ dp_enable(
 		/*Hard coded 2 physical ports mapped,
 		 *port 20, port 21
 		 */
-		/*enable the queue for direcpath rx and checksum ports 
+		/*enable the queue for direcpath rx and checksum ports
 		 *when a directpath port is enabled
 		 */
 		direct_dp_enable(port_id, flags, local_entry->egp_type);
@@ -3344,7 +3403,7 @@ void cbm_qid_schedule_out(s32 cbm_port_id, s32 tmu_queue_id, s32 enable)
 				tmu_sched_blk_in_enable(qlink.sbin, enable);
 				queue_flush_buff.qlink_sbin[j] = qlink.sbin;
 				#endif
-				udelay(10000);
+				/*usleep_range(10000, 10500);*/
 			}
 		}
 	} else {
@@ -3359,7 +3418,7 @@ void cbm_qid_schedule_out(s32 cbm_port_id, s32 tmu_queue_id, s32 enable)
 				/*Disable the scheduler block input*/
 				tmu_sched_blk_in_enable(queue_flush_buff.qlink_sbin[j], enable);
 				#endif
-				udelay(10000);
+				/*usleep_range(10000, 10500);*/
 			}
 		}
 	}
@@ -3370,32 +3429,37 @@ static s32 cbm_empty_queue(s32 cbm_port_id, u32 qocc, u32 port_type)
 	int i;
 	int no_packet = 0;
 	u32 reg = 0;
-	u32 data_pointer = 0;
-
+	#ifdef CHECK_WHILE_LOOP
+	int iter = 0;
+	#endif
 	i = 0;
 	while (qocc) {
+		#ifdef CHECK_WHILE_LOOP
+		iter++;
+		if (iter > 30000) {
+			pr_info_once("%s >30k iter qocc %d", __func__, qocc);
+		} else if (iter > 10000) {
+			pr_info_once("%s >10k iter qocc %d", __func__, qocc);
+		} else if (iter > 5000) {
+			pr_info_once("%s >5k iter qocc %d", __func__, qocc);
+		} else if (iter > 500) {
+			pr_info_once("%s >500 iter qocc %d", __func__, qocc);
+		}
+		#endif
 		for (i = 0; i < dqm_port_info[cbm_port_id].deq_info.num_desc;) {
 			switch (port_type) {
 			case DQM_DMA_TYPE:
 				reg = xrx500_cbm_r32(CBM_DMADESC_BASE + CBM_DQM_DMA_DESC(cbm_port_id, i) + 0x8);
 				break;
-			case DQM_WAVE_TYPE:
-				reg = xrx500_cbm_r32(CBM_DQM_BASE + CBM_DQM_SCPU_PORT(cbm_port_id, desc[i].desc0) + 12);
-				break;
 			default:
 				LOGF_KLOG_ERROR("Unknown port type\n");
 			}
-			/*finish reading the register*/
 			rmb();
 			if (reg & 0x80000000) {
 				switch (port_type) {
 				case DQM_DMA_TYPE:
 					xrx500_cbm_w32((CBM_DMADESC_BASE + CBM_DQM_DMA_DESC(cbm_port_id, i) + 0x8), (reg & 0x7fffffff) | 0x40000000);
 					break;
-				case DQM_WAVE_TYPE:
-					data_pointer = xrx500_cbm_r32(CBM_DQM_BASE + CBM_DQM_SCPU_PORT(cbm_port_id, desc[i].desc0) + 8);
-					xrx500_cbm_w32((CBM_DQM_BASE + CBM_DQM_SCPU_PORT(cbm_port_id, scpu_ptr_rtn[i])), data_pointer);
-					break;
 				default:
 					LOGF_KLOG_ERROR("Unknown port type\n");
 				}
@@ -3414,28 +3478,91 @@ static s32 cbm_empty_queue(s32 cbm_port_id, u32 qocc, u32 port_type)
 			}
 		}
 EXIT:
-		if ((qocc <= 33) && no_packet)
+		if ((qocc <= 33) && no_packet) {
 			return qocc;
+		}
 	}
 	return 0;
 }
 
-static s32 cbm_queue_empty_process(u32 port_type, s32 cbm_port_id, s32 tmu_queue_id)
+static s32 cbm_wave_empty(s32 cbm_port_id, s32 tmu_queue_id)
 {
 	u32 wq, qrth, qocc_temp, qavg;
-	int result = 0;
-	int no_packet = 0;
-
+	int count = 0, pkt_deq = 0;
+	int dqpc = 0, i;
+	u32 reg = 0;
+	u32 data_pointer = 0;
+	#ifdef CHECK_WHILE_LOOP
+	int iter = 0;
+	#endif
 	tmu_qoct_read(tmu_queue_id, &wq, &qrth, &qocc_temp, &qavg);
-	qocc_temp += 33;
-	switch (port_type) {
-	case DQM_WAVE_TYPE:
+	dqpc = xrx500_cbm_r32((CBM_DQM_BASE + CBM_DQM_CPU_PORT(cbm_port_id, dqpc)));
+	if ((qocc_temp == 0) || (dqpc != 0x20)) {
+		pr_info("qocc is %d, dont flush desc fifo %d\n", qocc_temp, dqpc);
+		return CBM_FAILURE;
+	}
+	if ((qocc_temp != 0) && (dqpc == 0x20)) {
 		xrx500_cbm_w32((CBM_DQM_BASE + CBM_DQM_CPU_PORT(cbm_port_id, cfg)), 0x107 | ((cbm_port_id << CFG_CPU_EGP_0_EPMAP_POS) & CFG_CPU_EGP_0_EPMAP_MASK));
+		count = qocc_temp + dqpc;
+		while (pkt_deq < count) {
+			#ifdef CHECK_WHILE_LOOP
+			iter++;
+			if (iter > 30000) {
+				pr_info_once("%s >30k iter qocc %d", __func__, count);
+			} else if (iter > 10000) {
+				pr_info_once("%s >10k iter qocc %d", __func__, count);
+			} else if (iter > 5000) {
+				pr_info_once("%s >5k iter qocc %d", __func__, count);
+			} else if (iter > 500) {
+				pr_info_once("%s >500 iter qocc %d", __func__, count);
+			}
+			#endif
+			for (i = 0; i < dqm_port_info[cbm_port_id].deq_info.num_desc;) {
+				reg = xrx500_cbm_r32(CBM_DQM_BASE + CBM_DQM_SCPU_PORT(cbm_port_id, desc[i].desc0) + 12);
+				rmb();
+				if (reg & 0x80000000) {
+					data_pointer = xrx500_cbm_r32(CBM_DQM_BASE + CBM_DQM_SCPU_PORT(cbm_port_id, desc[i].desc0) + 8);
+					xrx500_cbm_w32((CBM_DQM_BASE + CBM_DQM_SCPU_PORT(cbm_port_id, scpu_ptr_rtn[i])), data_pointer);
+					wmb();
+					pkt_deq++;
+					i++;
+				}
+				if (pkt_deq >= count)
 		break;
-	default:
-		break;
+			}
+		}
+		xrx500_cbm_w32((CBM_DQM_BASE + CBM_DQM_CPU_PORT(cbm_port_id, cfg)), 0x10F | ((cbm_port_id << CFG_CPU_EGP_0_EPMAP_POS) & CFG_CPU_EGP_0_EPMAP_MASK));
 	}
+	return CBM_SUCCESS;
+}
+
+static  s32 cbm_queue_empty_process(s32 port_type, s32 cbm_port_id, s32 tmu_queue_id)
+{
+	u32 wq, qrth, qocc_temp, qavg;
+	int result = 0;
+	int no_packet = 0;
+	#ifdef CHECK_WHILE_LOOP
+	int iter = 0;
+	#endif
+	tmu_qoct_read(tmu_queue_id, &wq, &qrth, &qocc_temp, &qavg);
+	qocc_temp += 33;
 	while (1) {
+		#ifdef CHECK_WHILE_LOOP
+		iter++;
+		if (iter > 30000) {
+			tmu_qoct_read(tmu_queue_id, &wq, &qrth, &qocc_temp, &qavg);
+			pr_info_once("%s >30k iter qocc %d", __func__, qocc_temp);
+		} else if (iter > 10000) {
+			tmu_qoct_read(tmu_queue_id, &wq, &qrth, &qocc_temp, &qavg);
+			pr_info_once("%s >10k iter qocc %d", __func__, qocc_temp);
+		} else if (iter > 5000) {
+			tmu_qoct_read(tmu_queue_id, &wq, &qrth, &qocc_temp, &qavg);
+			pr_info_once("%s >5k iter qocc %d", __func__, qocc_temp);
+		} else if (iter > 500) {
+			tmu_qoct_read(tmu_queue_id, &wq, &qrth, &qocc_temp, &qavg);
+			pr_info_once("%s >500 iter qocc %d", __func__, qocc_temp);
+	}
+		#endif
 		result = cbm_empty_queue(cbm_port_id, qocc_temp, port_type);
 		if (!result) {
 			no_packet = 0;
@@ -3443,13 +3570,6 @@ static s32 cbm_queue_empty_process(u32 port_type, s32 cbm_port_id, s32 tmu_queue
 			if (qocc_temp && (qocc_temp != 32767)) {
 				continue;
 			} else {
-				switch (port_type) {
-				case DQM_WAVE_TYPE:
-				xrx500_cbm_w32((CBM_DQM_BASE + CBM_DQM_CPU_PORT(cbm_port_id, cfg)), 0x10F | ((cbm_port_id << CFG_CPU_EGP_0_EPMAP_POS) & CFG_CPU_EGP_0_EPMAP_MASK));
-				break;
-				default:
-				break;
-				}
 				return result;
 			}
 		} else {
@@ -3457,13 +3577,6 @@ static s32 cbm_queue_empty_process(u32 port_type, s32 cbm_port_id, s32 tmu_queue
 			qocc_temp = result;
 			if (no_packet > 10) {
 				tmu_qoct_read(tmu_queue_id, &wq, &qrth, &qocc_temp, &qavg);
-				switch (port_type) {
-				case DQM_WAVE_TYPE:
-				xrx500_cbm_w32((CBM_DQM_BASE + CBM_DQM_CPU_PORT(cbm_port_id, cfg)), 0x10F | ((cbm_port_id << CFG_CPU_EGP_0_EPMAP_POS) & CFG_CPU_EGP_0_EPMAP_MASK));
-				break;
-				default:
-				break;
-				}
 				return result;
 			}
 		}
@@ -3483,6 +3596,9 @@ u32 flags)
 	s32 pos = 3;
 	int count1 = 0;
 	#endif
+	#ifdef CHECK_WHILE_LOOP
+	int iter = 0;
+	#endif
 	int chan;
 	u32 wq, qrth, qocc, qavg;
 	u32 qocc_temp;
@@ -3503,41 +3619,46 @@ u32 flags)
 		if ((port_type == DQM_LDMA_TYPE) ||
 		    (port_type == DQM_WAVE_TYPE)) {
 			if (flags & CBM_Q_F_FORCE_FLUSH) {
-				cbm_queue_empty_process(
-				port_type,
-				cbm_port_id,
-				tmu_queue_id);
+				if (port_type == DQM_WAVE_TYPE) {
+					cbm_wave_empty(cbm_port_id, tmu_queue_id);
+				} else {
+				cbm_queue_empty_process(port_type, cbm_port_id, tmu_queue_id);
+				}
 			} else {
 				while (qocc_temp) {
+					#ifdef CHECK_WHILE_LOOP
+					iter++;
+					if (iter > 30000) {
+						tmu_qoct_read(tmu_queue_id, &wq, &qrth, &qocc_temp, &qavg);
+						pr_info_once("%s >30k iter qocc %d", __func__, qocc_temp);
+					} else if (iter > 10000) {
+						tmu_qoct_read(tmu_queue_id, &wq, &qrth, &qocc_temp, &qavg);
+						pr_info_once("%s >10k iter qocc %d", __func__, qocc_temp);
+					} else if (iter > 5000) {
+						tmu_qoct_read(tmu_queue_id, &wq, &qrth, &qocc_temp, &qavg);
+						pr_info_once("%s >5k iter qocc %d", __func__, qocc_temp);
+					} else if (iter > 500) {
+						tmu_qoct_read(tmu_queue_id, &wq, &qrth, &qocc_temp, &qavg);
+						pr_info_once("%s >500 iter qocc %d", __func__, qocc_temp);
+					}
+					#endif
 					usleep_range(10000, 10100);
-					tmu_qoct_read(
-					tmu_queue_id,
-					&wq,
-					&qrth,
-					&qocc_temp,
-					&qavg);
+					tmu_qoct_read(tmu_queue_id, &wq, &qrth, &qocc_temp, &qavg);
+					/*LOGF_KLOG_INFO("----->qocc_temp =%d Q %d\n", qocc_temp, tmu_queue_id);*/
 				}
 				usleep_range(10000, 10100);
 			}
 		} else {
-			cbm_queue_empty_process(
-			port_type,
-			cbm_port_id,
-			tmu_queue_id);
+			cbm_queue_empty_process(port_type, cbm_port_id, tmu_queue_id);
 			}
 		if (tmu_queue_id != -1) {
-			tmu_qoct_read(
-			tmu_queue_id,
-			&wq,
-			&qrth,
-			&qocc_temp,
-			&qavg);
+			tmu_qoct_read(tmu_queue_id, &wq, &qrth, &qocc_temp, &qavg);
 				LOGF_KLOG_ERROR("out QOCC %d\n", qocc_temp);
 			cbm_qid_schedule_out(cbm_port_id, tmu_queue_id, 1);
 		}
 		if (chan)
 			ltq_dma_chan_on(chan);
-	} else if (port_type == DQM_CPU_TYPE) {
+	} else if ((port_type == DQM_CPU_TYPE)) {
 		if (tmu_queue_id != -1) {
 		/*Disassociate other queues and stop scheduling them*/
 			cbm_qid_schedule_out(cbm_port_id, tmu_queue_id, 0);
@@ -3559,7 +3680,7 @@ u32 flags)
 						reg = xrx500_cbm_r32(CBM_DQM_BASE + CBM_DQM_CPU_PORT(cbm_port_id, desc0.desc0) + 12);
 						reg1 = xrx500_cbm_r32(CBM_DQM_BASE + CBM_DQM_CPU_PORT(cbm_port_id, desc1.desc0) + 12);
 						if ((reg & 0x80000000) || (reg1 & 0x80000000)) {
-							udelay(10);
+							usleep_range(10, 15);
 							goto DEQ_PKTS;
 						}
 					}
@@ -3598,7 +3719,7 @@ u32 flags)
 			reg = xrx500_cbm_r32(CBM_DQM_BASE + CBM_DQM_CPU_PORT(cbm_port_id, desc0.desc0) + 12);
 			reg1 = xrx500_cbm_r32(CBM_DQM_BASE + CBM_DQM_CPU_PORT(cbm_port_id, desc1.desc0) + 12);
 			if ((reg & 0x80000000) || (reg1 & 0x80000000)) {
-				udelay(10);
+				usleep_range(10, 15);
 				goto LS_DEQ_PKTS;
 			}
 		#endif
@@ -3684,12 +3805,27 @@ u32 flags)
 
 	switch (op_type) {
 	case 1:
-		/*For the Datapath port (EP i), go through all entries in the
-		 *CBM Queue Map table in DDR, and mark with
-		 *CBM_F_QUEUE_DROP flag. For each such entry, program the CBM
-		 *Queue Map table entry in CBM to map
-		 *to the Global Drop Queue
-		 */
+		tmu_queue_list(tmu_port, tmp_q_buff2, ARRAY_SIZE(tmp_q_buff2), &q_buff_num,
+			       NULL);
+		LOGF_KLOG_INFO(" q_buff_num: %d\n", q_buff_num);
+		if (flags & CBM_Q_F_RESTORE_ONLY) {
+			#if 1
+			/*restore all entries in the queue map table for that
+			EP to map to original queues, and remove the
+CBM_F_QUEUE_DROP flag from the DDR table*/
+			spin_lock_irqsave(&cbm_qidt_lock, sys_flag);
+			for (j = 0; j < q_buff_num; j++) {
+				pr_info("###restore only####\n");
+				cbm_restore_qmap_strict(1, tmp_q_buff2[j]);
+				/*LOGF_KLOG_INFO(" j: %d\n", j);*/
+			}
+			spin_unlock_irqrestore(&cbm_qidt_lock, sys_flag);
+			#endif
+		}
+		/*spin_lock_irqsave(&cbm_qidt_lock, sys_flag);*/
+		/*For the Datapath port (EP i), go through all entries in the CBM Queue Map table in DDR, and mark with
+		CBM_F_QUEUE_DROP flag. For each such entry, program the CBM Queue Map table entry in CBM to map
+		to the Global Drop Queue*/
 		if (flags & CBM_Q_F_DISABLE) {
 			spin_lock_irqsave(&cbm_qidt_lock, sys_flag);
 			cbm_qidt_set(&qidt_set,
@@ -3701,8 +3837,6 @@ u32 flags)
 		 *(by EP), disable/enable the TMU queue by calling
 		 *the API of the TMU driver
 		 */
-		tmu_queue_list(tmu_port, tmp_q_buff2, ARRAY_SIZE(tmp_q_buff2),
-			       &q_buff_num, NULL);
 		/*LOGF_KLOG_INFO(" q_buff_num: %d\n", q_buff_num);*/
 		for (j = 0; j < q_buff_num; j++) {
 			cbm_queue_sched_mgmt(tmu_port, tmp_q_buff2[j], flags);
@@ -3713,7 +3847,10 @@ u32 flags)
 		 *flag from the DDR table
 		 */
 		spin_lock_irqsave(&cbm_qidt_lock, sys_flag);
+		for (j = 0; j < q_buff_num; j++) {
+			pr_info("enable %d remap %d\n", !(flags & CBM_Q_F_DISABLE),  remap_to_qid);
 		(flags & CBM_Q_F_DISABLE) ?  : cbm_restore_qmap(1);
+		}
 		spin_unlock_irqrestore(&cbm_qidt_lock, sys_flag);
 	break;
 	case 3:
@@ -3789,8 +3926,6 @@ u32 flags)
 	return result;
 }
 
-
-
 static s32 enqueue_port_resources_get(
 cbm_eq_port_res_t *res,
 u32 flags)
@@ -3841,7 +3976,7 @@ u32 flags)
 				}
 
 				if (j > res->num_eq_ports)
-					BUG_ON("exceeds the number of resources\r\n");
+					WARN_ON("exceeds the number of resources\r\n");
 			}
 		}
 	}
@@ -3899,8 +4034,6 @@ dequeue_port_resources_get(
 	return 0;
 }
 
-
-
 s32 get_egress_port_info(u32 cbm_port, u32 *tx_ch,
 			 u32 *flags)
 {
@@ -4208,7 +4341,7 @@ s32 port_id,
 int8_t ovh
 )
 {
-	BUG_ON(!((port_id >= 0) && (port_id <= 15)));
+	WARN_ON(!((port_id >= 0) && (port_id <= 15)));
 	if (cbm_rev <= CBM_SOC_REV_1) {
 		set_val(CBM_EQM_BASE
 		+ CBM_EQM_CPU_PORT(port_id, cfg),
@@ -4227,7 +4360,7 @@ s32 port_id,
 int8_t *ovh
 )
 {
-	BUG_ON(!((port_id >= 0) && (port_id <= 15)));
+	WARN_ON(!((port_id >= 0) && (port_id <= 15)));
 	if (cbm_rev <= CBM_SOC_REV_1) {
 		*ovh = (xrx500_cbm_r32(CBM_EQM_BASE
 		+ CBM_EQM_CPU_PORT(port_id, cfg)) & CFG_CPU_IGP_0_OVH_MASK) >> CFG_CPU_IGP_0_OVH_POS;
@@ -4244,7 +4377,7 @@ igp_delay_set(
 	)
 {
 	pr_info("port%d delay%d\n", cbm_port_id, delay);
-	BUG_ON(!((cbm_port_id >= 0) && (cbm_port_id <= 15)));
+	WARN_ON(!((cbm_port_id >= 0) && (cbm_port_id <= 15)));
 
 	if (cbm_rev > CBM_SOC_REV_1) {
 		xrx500_cbm_w32(CBM_EQM_BASE
@@ -4260,7 +4393,7 @@ igp_delay_get(
 	s32 *delay
 	)
 {
-	BUG_ON(!((cbm_port_id >= 0) && (cbm_port_id <= 15)));
+	WARN_ON(!((cbm_port_id >= 0) && (cbm_port_id <= 15)));
 	if (cbm_rev > CBM_SOC_REV_1) {
 		*delay = xrx500_cbm_r32(CBM_EQM_BASE
 		+ CBM_EQM_CPU_PORT(cbm_port_id, dcntr)
@@ -4277,7 +4410,7 @@ queue_delay_enable_set(
 {
 	u32 regval, i;
 
-	BUG_ON(!((queue >= 0) && (queue <= 256)));
+	WARN_ON(!((queue >= 0) && (queue <= 256)));
 	regval = xrx500_cbm_r32(CBM_EQM_BASE + CBM_EQM_CTRL);
 	regval |= BIT(8) | BIT(6);
 	regval |= 3 << CBM_EQM_CTRL_L_POS;
@@ -4377,7 +4510,7 @@ static s32 enqueue_dma_port_stats_get(
 	if ((cbm_port_id >= 5) && (cbm_port_id <= 15)) {
 		*enq_ctr = xrx500_cbm_r32(CBM_EQM_BASE +
 					  CBM_EQM_CPU_PORT(cbm_port_id,
-					  eqpc));
+							   eqpc));
 		*occupancy_ctr = xrx500_cbm_r32(CBM_EQM_BASE
 		+ CBM_EQM_CPU_PORT(cbm_port_id,
 		pocc));
@@ -5057,7 +5190,6 @@ static const struct cbm_ops l_cbm_ops = {
 	.get_lookup_qid_via_bits = get_lookup_qid_via_bits_grx500,
 	.cbm_setup_DMA_p2p = setup_DMA_p2p,
 	.cbm_turn_on_DMA_p2p = turn_on_DMA_p2p,
-	
 };
 
 static const struct of_device_id cbm_xrx500_match[] = {
@@ -5129,9 +5261,9 @@ static int cbm_xrx500_probe(struct platform_device *pdev)
 		return -ENOMEM;
 	}
 	std_buf_base = dma_alloc_attrs(&pdev->dev,
-					CONFIG_CBM_STD_BUF_POOL_SIZE,
-					&g_cbm_ctrl.dma_handle_std, GFP_KERNEL,
-					DMA_ATTR_NON_CONSISTENT);
+				       CONFIG_CBM_STD_BUF_POOL_SIZE,
+				       &g_cbm_ctrl.dma_handle_std, GFP_KERNEL,
+				       DMA_ATTR_NON_CONSISTENT);
 	if (!std_buf_base)
 		panic("no memory for CBM standard buffers!!");
 	else
@@ -5140,9 +5272,9 @@ static int cbm_xrx500_probe(struct platform_device *pdev)
 			(unsigned int)std_buf_base);
 
 	jbo_buf_base = dma_alloc_attrs(&pdev->dev,
-					CONFIG_CBM_JBO_BUF_SIZE,
-					&g_cbm_ctrl.dma_handle_jbo, GFP_KERNEL,
-				DMA_ATTR_NON_CONSISTENT);
+				       CONFIG_CBM_JBO_BUF_SIZE,
+				       &g_cbm_ctrl.dma_handle_jbo, GFP_KERNEL,
+				       DMA_ATTR_NON_CONSISTENT);
 	if (!jbo_buf_base)
 		panic("no memory for CBM Jumbo buffers!!");
 	else
diff --git a/include/net/directconnect_dp_api.h b/include/net/directconnect_dp_api.h
index 3bf68df7e478..ea1d56eb005b 100644
--- a/include/net/directconnect_dp_api.h
+++ b/include/net/directconnect_dp_api.h
@@ -52,7 +52,7 @@
 /**
   \brief DC DP API version code
  */
-#define DC_DP_API_VERSION_CODE        0x040101
+#define DC_DP_API_VERSION_CODE        0x040103
 
 /**
   \brief DC DP API version
@@ -80,6 +80,16 @@
 #define DC_DP_F_SWPATH           0x00000040
 
 /**
+  \brief Alloc flag as FASTPATH DSL
+ */
+#define DC_DP_F_FAST_DSL         0x00000100
+
+/**
+  \brief Alloc flag as shared resource
+ */
+#define DC_DP_F_SHARED_RES       0x00000200
+
+/**
   \brief Register device flag; Specify if peripheral driver want to allocate SW Tx ring
  */
 #define DC_DP_F_ALLOC_SW_TX_RING		0x00000010
@@ -317,12 +327,12 @@
 /**
   \brief Register subif flags; Specify to register already registered subif/vap as LitePath Partial offload
  */
-#define DC_DP_F_REGISTER_LITEPATH       0x00000100
+#define DC_DP_F_REGISTER_LITEPATH			0x00000100
 
 /**
   \brief Register subif flags; De-register already registered LitePath subif/logical netdev from LitePath
  */
-#define DC_DP_F_DEREGISTER_LITEPATH	0x00000200
+#define DC_DP_F_DEREGISTER_LITEPATH			0x00000200
 
 /**
   \brief Xmit flags: Specify it as dc_dp_xmit() flags value if xmit to litepath
@@ -405,6 +415,16 @@
 #define DC_DP_F_MC_UPDATE_MAC_ADDR		0x08
 
 /**
+  \brief Multicast module cleanup request (if applicable to peripheral driver).
+ */
+#define DC_DP_F_MC_FW_RESET			0x10
+
+/**
+  \brief Multicast module re-learning request (if applicable to peripheral driver).
+ */
+#define DC_DP_F_MC_NEW_STA			0x20
+
+/**
   \brief A new multicast group membership add request.
  */
 #define DC_DP_MC_F_ADD				0x01
@@ -1072,6 +1092,7 @@ dc_dp_get_host_capability (
   \param[in] port_id  Optional port_id number requested. Usually, 0 and allocated by driver
   \param[in] flags  : Use Datapath driver flags for Datapath Port Alloc
   -  DC_DP_F_FASTPATH : Allocate the port as h/w acclerable
+  -  DC_DP_F_FAST_DSL : Allocate the DSL port as h/w acclerable
   -  DC_DP_F_LITEPATH : Allocate the port as partial accelerable/offload
   -  DC_DP_F_SWPATH : Allocate the port as non-accelerable
   -  DC_DP_F_DEREGISTER : Deallocate the already allocated port
@@ -1119,6 +1140,7 @@ dc_dp_reserve_fastpath (
   - DC_DP_F_DONT_ALLOC_HW_TX_RING : Specify it if peripheral driver don't want to allocate hw tx ring
   - DC_DP_F_DONT_ALLOC_HW_RX_RING : Specify it if peripheral driver don't want to allocate hw rx ring
   - DC_DP_F_QOS : Specify it if peripheral/dev QoS class is required in DC peripheral
+  - DC_DP_F_SHARED_RES : Specify it if peripheral driver want to re-use/update dc resources, multi-port/bodning case respectively
   \return 0 if OK / -1 if error
   \note This is the first registration to be invoked by any DC peripheral. Subsequently additional registrations like Multicast, Ring-Recovery or Power Saving (PS) to be done.
  */
@@ -1462,8 +1484,10 @@ dc_dp_disconn_if (
   \param[in] cb  Multicast callback function.
   \param[in] flags  :
   DC_DP_F_MC_REGISTER - Register a DirectConnect interface.
-  DC_DP_F_MC_UPDATE_MAC_ADDR - Register a DirectConnect interface to get mcast SSM support.
   DC_DP_F_MC_DEREGISTER - De-register already registered DirectConnect interface.
+  DC_DP_F_MC_UPDATE_MAC_ADDR - Register a DirectConnect interface to get mcast SSM support.
+  DC_DP_F_MC_FW_RESET - Cleanup request on already learned entries on the registered DirectConnect interface.
+  DC_DP_F_MC_NEW_STA - Re-learn request on the registered DirectConnect interface.
   \return 0 if OK / -1 if error
   \note It can be skipped for specific peripheral driver if there is no notion
   of host connect/disconnect on the peripheral network/interface.
diff --git a/include/net/lantiq_cbm_api.h b/include/net/lantiq_cbm_api.h
index 8a53e7bfe68d..ca6a81bc3ddc 100644
--- a/include/net/lantiq_cbm_api.h
+++ b/include/net/lantiq_cbm_api.h
@@ -198,6 +198,7 @@
 #define CBM_Q_F_CKSUM 0x40000000
 #define CBM_Q_F_FLUSH 0x80000000
 #define CBM_Q_F_FORCE_FLUSH 0x00000001
+#define CBM_Q_F_RESTORE_ONLY 0x00000002
 
 #define CQM_MAX_CPU 4
 
