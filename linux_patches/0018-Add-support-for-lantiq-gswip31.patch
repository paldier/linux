From 4fa758852d8bb5f7e609ec3f436f7d0ae3b770ac Mon Sep 17 00:00:00 2001
From: Hua Ma <hua.ma@linux.intel.com>
Date: Thu, 21 Jun 2018 17:37:49 +0800
Subject: [PATCH] Add support for lantiq gswip31

---
 .../net/ethernet/lantiq/datapath/gswip31/Kconfig   |   49 +
 .../net/ethernet/lantiq/datapath/gswip31/Makefile  |   19 +
 .../lantiq/datapath/gswip31/datapath_coc.c         |  235 +
 .../lantiq/datapath/gswip31/datapath_ext_vlan.c    |  623 +++
 .../lantiq/datapath/gswip31/datapath_gswip.c       |  981 +++++
 .../datapath/gswip31/datapath_gswip_simulate.c     |  984 +++++
 .../datapath/gswip31/datapath_gswip_simulate.h     |   39 +
 .../lantiq/datapath/gswip31/datapath_lookup_proc.c |  644 +++
 .../lantiq/datapath/gswip31/datapath_mib.c         |  388 ++
 .../lantiq/datapath/gswip31/datapath_mib.h         |   19 +
 .../lantiq/datapath/gswip31/datapath_misc.c        | 1390 ++++++
 .../lantiq/datapath/gswip31/datapath_misc.h        |  219 +
 .../lantiq/datapath/gswip31/datapath_ppv4.c        |  822 ++++
 .../lantiq/datapath/gswip31/datapath_ppv4.h        |  207 +
 .../lantiq/datapath/gswip31/datapath_ppv4_api.c    | 4561 ++++++++++++++++++++
 .../lantiq/datapath/gswip31/datapath_proc.c        | 2514 +++++++++++
 .../lantiq/datapath/gswip31/datapath_proc.h        |    9 +
 .../lantiq/datapath/gswip31/datapath_switchdev.c   |  389 ++
 .../lantiq/datapath/gswip31/datapath_switchdev.h   |   20 +
 include/net/datapath_api_gswip31.h                 |  319 ++
 20 files changed, 14431 insertions(+)

diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/Kconfig b/drivers/net/ethernet/lantiq/datapath/gswip31/Kconfig
new file mode 100644
index 000000000000..a727f749b2c5
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/gswip31/Kconfig
@@ -0,0 +1,49 @@
+menuconfig LTQ_DATAPATH_HAL_GSWIP31
+	bool "Datapath HAL_GSWIP31"
+	default y
+	depends on LTQ_DATAPATH && FALCONMX_CQM
+	---help---
+	  Datapath Lib is to provide common rx/tx wrapper Lib without taking
+	  care of much HW knowledge and also provide common interface for legacy
+	  devices and different HW like to CBM or LRO.
+	  Take note: All devices need to register to datapath Lib first
+
+if LTQ_DATAPATH_HAL_GSWIP31
+config LTQ_DATAPATH_HAL_GSWIP31_MIB
+	bool "Datapath aggregated mib support"
+	depends on LTQ_DATAPATH_HAL_GSWIP30 && SOC_GRX500 && LTQ_TMU && LTQ_PPA_TMU_MIB_SUPPORT
+	default y
+	---help---
+	  It is to aggregate GSWIP-L/R, TMU and driver's MIB counter
+
+config LTQ_DATAPATH_HAL_GSWIP30_CPUFREQ
+	bool "Datapath DFS(COC) support"
+	depends on LTQ_DATAPATH && LTQ_CPUFREQ && LTQ_ETHSW_API
+	default y
+	---help---
+	  It is to support DFS(COC) in Datapath
+
+config LTQ_DATAPATH_DDR_SIMULATE_GSWIP31
+	bool "Datapath Simulation GSWIP3.1 based on GRX500 board"
+	default n
+	depends on LTQ_DATAPATH_DBG
+	---help---
+		Datapath Debug Tool for GSWIP DDR simulation
+		Only for debugging purpose
+		By default it should be disabled.
+
+config LTQ_DATAPATH_DUMMY_QOS
+	bool "datapath dummy QOS based on slim QOS driver or real QOS API with falcon_test API"
+	default y
+	depends on LTQ_PPV4_QOS_SLIM || (LTQ_PPV4_QOS || LTQ_PPV4)
+
+config LTQ_DATAPATH_DUMMY_QOS_VIA_FALCON_TEST
+	bool "datapath dummy QOS via ppv4 qos driver's falcon_test api, like slim driver"
+	default y
+	depends on (LTQ_PPV4_QOS || LTQ_PPV4) && !LTQ_PPV4_QOS_SLIM && LTQ_DATAPATH_DUMMY_QOS
+
+config LTQ_DATAPATH_QOS_HAL
+	bool "datapath QOS hal"
+	default n
+	depends on (LTQ_PPV4_QOS || LTQ_PPV4) && !LTQ_DATAPATH_DUMMY_QOS_VIA_FALCON_TEST
+endif
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/Makefile b/drivers/net/ethernet/lantiq/datapath/gswip31/Makefile
new file mode 100644
index 000000000000..2ce79f19ed43
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/gswip31/Makefile
@@ -0,0 +1,19 @@
+ifneq ($(CONFIG_LTQ_DATAPATH_DDR_SIMULATE_GSWIP31),)
+obj-$(CONFIG_LTQ_DATAPATH) += datapath_gswip_simulate.o
+endif
+
+obj-$(CONFIG_LTQ_DATAPATH) += datapath_misc.o datapath_gswip.o datapath_proc.o
+obj-$(CONFIG_LTQ_DATAPATH) += datapath_ppv4.o
+obj-$(CONFIG_LTQ_DATAPATH) += datapath_lookup_proc.o datapath_ppv4_api.o
+
+ifneq ($(CONFIG_LTQ_DATAPATH_HAL_GSWIP31_MIB),)
+obj-$(CONFIG_LTQ_DATAPATH) += datapath_mib.o
+endif
+
+ifneq ($(CONFIG_LTQ_DATAPATH_HAL_GSWIP31_COC),)
+obj-$(CONFIG_LTQ_DATAPATH) += datapath_coc.o
+endif
+
+ifneq ($(CONFIG_LTQ_DATAPATH_SWITCHDEV),)
+obj-$(CONFIG_LTQ_DATAPATH) += datapath_switchdev.o datapath_ext_vlan.o datapath_tc_asym_vlan.o
+endif
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_coc.c b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_coc.c
new file mode 100644
index 000000000000..a3645dea361b
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_coc.c
@@ -0,0 +1,235 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/module.h>
+#include <linux/types.h>	/* size_t */
+#include <linux/timer.h>
+#include <net/datapath_api.h>
+#include <net/datapath_proc_api.h>
+#include "../datapath.h"
+
+#define DP_MODULE  LTQ_CPUFREQ_MODULE_DP
+#define DP_ID 0	 /* this ID should represent the Datapath interface No. */
+static struct timer_list dp_coc_timer;
+static u32 polling_period;	/*seconds */
+static int rmon_timer_en;
+static spinlock_t dp_coc_lock;
+
+/* threshold data for D0:D3 */
+struct ltq_cpufreq_threshold rmon_threshold = { 0, 30, 20, 10 }; /*packets */
+
+/* driver is busy and needs highest performance */
+static int dp_coc_init_stat;	/*DP COC is Initialized or not */
+static int dp_coc_ena;		/*DP COC is enabled or not */
+enum ltq_cpufreq_state dp_coc_ps_curr = LTQ_CPUFREQ_PS_UNDEF;/*current state*/
+/*new statue wanted to switch to */
+enum ltq_cpufreq_state dp_coc_ps_new = LTQ_CPUFREQ_PS_UNDEF;
+
+static GSW_RMON_Port_cnt_t rmon_last[PMAC_MAX_NUM];
+static u64 last_rmon_rx;
+
+/*meter */
+#define PCE_OVERHD 20
+static u32 meter_id;
+/*3 ~ 4 packet size */
+static u32 meter_ncbs = 0x8000 + (1514 + PCE_OVERHD) * 3 + 200;
+/*1 ~ 2 packet size */
+static u32 meter_nebs = 0x8000 + (1514 + PCE_OVERHD) * 1 + 200;
+/*k bits */
+static u32 meter_nrate[4] = { 0/*D0 */, 700/*D1*/, 600/*D2*/, 500/*D3*/};
+
+static int dp_coc_cpufreq_notifier(struct notifier_block *nb,
+				   unsigned long val, void *data);
+static int dp_coc_stateget(enum ltq_cpufreq_state *state);
+static int dp_coc_fss_ena(int ena);
+static int apply_meter_rate(u32 rate, enum ltq_cpufreq_state new_state);
+static int enable_meter_interrupt(void);
+static int clear_meter_interrupt(void);
+
+int dp_set_meter_rate(enum ltq_cpufreq_state stat, unsigned int rate)
+{/*set the rate for upscaling to D0 from specified stat */
+	if (stat == LTQ_CPUFREQ_PS_D1)
+		meter_nrate[1] = rate;
+	else if (stat == LTQ_CPUFREQ_PS_D2)
+		meter_nrate[2] = rate;
+	else if (stat == LTQ_CPUFREQ_PS_D3)
+		meter_nrate[3] = rate;
+	else
+		return -1;
+	if (dp_coc_ps_curr == stat)
+		apply_meter_rate(-1, stat);
+	return 0;
+}
+EXPORT_SYMBOL(dp_set_meter_rate);
+
+static struct notifier_block dp_coc_cpufreq_notifier_block = {
+	.notifier_call = dp_coc_cpufreq_notifier
+};
+
+static inline void coc_lock(void)
+{
+}
+
+static inline void coc_unlock(void)
+{
+	spin_unlock_bh(&dp_coc_lock);
+}
+
+struct ltq_cpufreq_module_info dp_coc_feature_fss = {
+	.name = "Datapath FSS",
+	.pmcuModule = DP_MODULE,
+	.pmcuModuleNr = DP_ID,
+	.powerFeatureStat = 1,
+	.ltq_cpufreq_state_get = dp_coc_stateget,
+	.ltq_cpufreq_pwr_feature_switch = dp_coc_fss_ena,
+};
+
+#if defined(CONFIG_LTQ_DATAPATH_DBG) && CONFIG_LTQ_DATAPATH_DBG
+static char *get_sub_module_str(uint32_t flag)
+{
+	if (flag == DP_COC_REQ_DP)
+		return "DP";
+	else if (flag == DP_COC_REQ_ETHERNET)
+		return "Ethernet";
+	else if (flag == DP_COC_REQ_VRX318)
+		return "VRX318";
+	else
+		return "Unknown";
+}
+#endif
+
+static char *get_coc_stat_string(enum ltq_cpufreq_state stat)
+{
+	if (stat == LTQ_CPUFREQ_PS_D0)
+		return "D0";
+	else if (stat == LTQ_CPUFREQ_PS_D1)
+		return "D1";
+	else if (stat == LTQ_CPUFREQ_PS_D2)
+		return "D2";
+	else if (stat == LTQ_CPUFREQ_PS_D3)
+		return "D3";
+	else if (stat == LTQ_CPUFREQ_PS_D0D3)
+		return "D0D3-NotCare";
+	else if (stat == LTQ_CPUFREQ_PS_BOOST)
+		return "Boost";
+	else
+		return "Undef";
+}
+
+static void dp_rmon_polling(unsigned long data)
+{
+}
+
+static int dp_coc_stateget(enum ltq_cpufreq_state *state)
+{
+}
+
+static int dp_coc_fss_ena(int ena)
+{
+}
+
+void update_rmon_last(void)
+{
+}
+
+int update_coc_rmon_timer(enum ltq_cpufreq_state new_state, uint32_t flag)
+{
+	return 0;
+}
+
+static int update_coc_cfg(enum ltq_cpufreq_state new_state,
+			  enum ltq_cpufreq_state old_state, uint32_t flag)
+	return 0;
+}
+
+static int dp_coc_prechange(enum ltq_cpufreq_module module,
+			    enum ltq_cpufreq_state new_state,
+			    enum ltq_cpufreq_state old_state, u8 flags)
+{
+	return 0;
+}
+
+static int dp_coc_postchange(enum ltq_cpufreq_module module,
+			     enum ltq_cpufreq_state new_state,
+			     enum ltq_cpufreq_state old_state, u8 flags)
+{
+	return 0;
+}
+
+/* keep track of frequency transitions */
+static int dp_coc_cpufreq_notifier(struct notifier_block *nb,
+				   unsigned long val, void *data)
+{
+	return 0;
+}
+
+void proc_coc_read(struct seq_file *s)
+{
+}
+
+int dp_set_rmon_threshold(struct ltq_cpufreq_threshold *threshold,
+			  uint32_t flags)
+{
+	return 0;
+}
+EXPORT_SYMBOL(dp_set_rmon_threshold);
+
+ssize_t proc_coc_write(struct file *file, const char *buf, size_t count,
+		       loff_t *ppos)
+{
+	return count;
+}
+
+int clear_meter_interrupt(void)
+{
+	return 0;
+}
+
+int enable_meter_interrupt(void)
+{
+	return 0;
+}
+
+/* rate      0: disable meter
+ * -1: enable meter
+ * others: really change rate.
+ */
+int apply_meter_rate(u32 rate, enum ltq_cpufreq_state new_state)
+{
+	return 0;
+}
+
+int meter_set_default(void)
+{
+	return 0;
+}
+
+/* For Datapth's sub-module to request power state change, esp for
+ *  Ethernet/VRX318 driver to call it if there is state change needed.
+ *   The flag can be:
+ *     DP_COC_REQ_DP
+ *     DP_COC_REQ_ETHERNET
+ *     DP_COC_REQ_VRX318
+ */
+int dp_coc_new_stat_req(enum ltq_cpufreq_state new_state, uint32_t flag)
+{
+	return 0;
+}
+EXPORT_SYMBOL(dp_coc_new_stat_req);
+
+int dp_coc_cpufreq_init(void)
+{
+	return 0;
+}
+
+int dp_coc_cpufreq_exit(void)
+{
+	return 0;
+}
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_ext_vlan.c b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_ext_vlan.c
new file mode 100644
index 000000000000..85bc8fb92ef0
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_ext_vlan.c
@@ -0,0 +1,623 @@
+/*
+ * Copyright (C) Intel Corporation
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#ifdef NON_LINUX
+#include <stdlib.h>
+#include <errno.h>
+#include <string.h>
+#include "lantiq_gsw.h"
+#include "lantiq_gsw_api.h"
+#include "gsw_flow_ops.h"
+/* Adaption */
+#define kfree(x)	free(x)
+#define kmalloc(x, y)	malloc(x)
+#else
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/etherdevice.h>
+#include <net/datapath_api.h>
+#include "../datapath.h"
+#endif
+
+/* Structure only used by function set_gswip_ext_vlan */
+struct priv_ext_vlan {
+	/* number of bridge ports has VLAN */
+	u32 num_bp;
+	/* bridge port list (vlan1_list + vlan2_list) */
+	u32 bp[1];
+};
+
+/* Supporting Functions */
+static int update_vlan0(struct core_ops *ops, u32 bp,
+			GSW_EXTENDEDVLAN_config_t *vcfg)
+{
+	GSW_return_t ret;
+	u32 block = vcfg->nExtendedVlanBlockId;
+
+	memset(vcfg, 0, sizeof(*vcfg));
+	vcfg->nExtendedVlanBlockId = block;
+	vcfg->nEntryIndex = 0;
+	vcfg->sFilter.sOuterVlan.eType = GSW_EXTENDEDVLAN_FILTER_TYPE_NO_TAG;
+	vcfg->sFilter.eEtherType = GSW_EXTENDEDVLAN_FILTER_ETHERTYPE_NO_FILTER;
+	vcfg->sTreatment.bReassignBridgePort = LTQ_TRUE;
+	vcfg->sTreatment.nNewBridgePortId = bp;
+	ret = ops->gsw_extvlan_ops.ExtendedVlan_Set(ops, vcfg);
+
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	memset(vcfg, 0, sizeof(*vcfg));
+	vcfg->nExtendedVlanBlockId = block;
+	vcfg->nEntryIndex = 1;
+	vcfg->sFilter.sOuterVlan.eType = GSW_EXTENDEDVLAN_FILTER_TYPE_DEFAULT;
+	vcfg->sFilter.sInnerVlan.eType = GSW_EXTENDEDVLAN_FILTER_TYPE_NO_TAG;
+	vcfg->sFilter.eEtherType = GSW_EXTENDEDVLAN_FILTER_ETHERTYPE_NO_FILTER;
+	vcfg->sTreatment.bReassignBridgePort = LTQ_TRUE;
+	vcfg->sTreatment.nNewBridgePortId = bp;
+	ret = ops->gsw_extvlan_ops.ExtendedVlan_Set(ops, vcfg);
+
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	memset(vcfg, 0, sizeof(*vcfg));
+	vcfg->nExtendedVlanBlockId = block;
+	vcfg->nEntryIndex = 2;
+	vcfg->sFilter.sInnerVlan.eType = GSW_EXTENDEDVLAN_FILTER_TYPE_DEFAULT;
+	vcfg->sFilter.eEtherType = GSW_EXTENDEDVLAN_FILTER_ETHERTYPE_NO_FILTER;
+	vcfg->sTreatment.bReassignBridgePort = LTQ_TRUE;
+	vcfg->sTreatment.nNewBridgePortId = bp;
+	ret = ops->gsw_extvlan_ops.ExtendedVlan_Set(ops, vcfg);
+
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	return 0;
+}
+
+static int update_vlan1(struct core_ops *ops, int base, int num,
+			struct vlan1 *vlan_list, int drop,
+			GSW_EXTENDEDVLAN_config_t *vcfg)
+{
+	GSW_return_t ret;
+	u32 block = vcfg->nExtendedVlanBlockId;
+	int i;
+
+	for (i = 0; i < num; i++, base += 2) {
+		memset(vcfg, 0, sizeof(*vcfg));
+		vcfg->nExtendedVlanBlockId = block;
+		vcfg->nEntryIndex = base;
+
+		vcfg->sFilter.sOuterVlan.eType =
+			GSW_EXTENDEDVLAN_FILTER_TYPE_NORMAL;
+		vcfg->sFilter.sOuterVlan.bPriorityEnable = LTQ_FALSE;
+		vcfg->sFilter.sOuterVlan.bVidEnable = LTQ_TRUE;
+		vcfg->sFilter.sOuterVlan.nVidVal = vlan_list[i].outer_vlan.vid;
+		vcfg->sFilter.sOuterVlan.eTpid = vlan_list[i].outer_vlan.tpid;
+		vcfg->sFilter.sOuterVlan.eDei =
+			GSW_EXTENDEDVLAN_FILTER_DEI_NO_FILTER;
+
+		vcfg->sFilter.sInnerVlan.eType =
+			GSW_EXTENDEDVLAN_FILTER_TYPE_NO_TAG;
+
+		vcfg->sFilter.eEtherType = vlan_list[i].ether_type;
+
+		if (drop) {
+			vcfg->sTreatment.eRemoveTag =
+				GSW_EXTENDEDVLAN_TREATMENT_DISCARD_UPSTREAM;
+		} else {
+			vcfg->sTreatment.eRemoveTag =
+				GSW_EXTENDEDVLAN_TREATMENT_REMOVE_1_TAG;
+			vcfg->sTreatment.bReassignBridgePort = LTQ_TRUE;
+			vcfg->sTreatment.nNewBridgePortId = vlan_list[i].bp;
+		}
+
+		ret = ops->gsw_extvlan_ops.ExtendedVlan_Set(ops, vcfg);
+
+		if (ret != GSW_statusOk)
+			return -EIO;
+
+		memset(vcfg, 0, sizeof(*vcfg));
+		vcfg->nExtendedVlanBlockId = block;
+		vcfg->nEntryIndex = base + 1;
+
+		vcfg->sFilter.sOuterVlan.eType =
+			GSW_EXTENDEDVLAN_FILTER_TYPE_NORMAL;
+		vcfg->sFilter.sOuterVlan.bPriorityEnable = LTQ_FALSE;
+		vcfg->sFilter.sOuterVlan.bVidEnable = LTQ_TRUE;
+		vcfg->sFilter.sOuterVlan.nVidVal = vlan_list[i].outer_vlan.vid;
+		vcfg->sFilter.sOuterVlan.eTpid = vlan_list[i].outer_vlan.tpid;
+		vcfg->sFilter.sOuterVlan.eDei =
+			GSW_EXTENDEDVLAN_FILTER_DEI_NO_FILTER;
+
+		vcfg->sFilter.sInnerVlan.eType =
+			GSW_EXTENDEDVLAN_FILTER_TYPE_NO_FILTER;
+
+		vcfg->sFilter.eEtherType = vlan_list[i].ether_type;
+
+		if (drop) {
+			vcfg->sTreatment.eRemoveTag =
+				GSW_EXTENDEDVLAN_TREATMENT_DISCARD_UPSTREAM;
+		} else {
+			vcfg->sTreatment.eRemoveTag =
+				GSW_EXTENDEDVLAN_TREATMENT_REMOVE_1_TAG;
+			vcfg->sTreatment.bReassignBridgePort = LTQ_TRUE;
+			vcfg->sTreatment.nNewBridgePortId = vlan_list[i].bp;
+		}
+
+		ret = ops->gsw_extvlan_ops.ExtendedVlan_Set(ops, vcfg);
+
+		if (ret != GSW_statusOk)
+			return -EIO;
+	}
+
+	return 0;
+}
+
+static int update_vlan2(struct core_ops *ops, int base, int num,
+			struct vlan2 *vlan_list, int drop,
+			GSW_EXTENDEDVLAN_config_t *vcfg)
+{
+	GSW_return_t ret;
+	u32 block = vcfg->nExtendedVlanBlockId;
+	int i;
+
+	for (i = 0; i < num; i++, base++) {
+		memset(vcfg, 0, sizeof(*vcfg));
+		vcfg->nExtendedVlanBlockId = block;
+		vcfg->nEntryIndex = base;
+
+		vcfg->sFilter.sOuterVlan.eType =
+			GSW_EXTENDEDVLAN_FILTER_TYPE_NORMAL;
+		vcfg->sFilter.sOuterVlan.bPriorityEnable = LTQ_FALSE;
+		vcfg->sFilter.sOuterVlan.bVidEnable = LTQ_TRUE;
+		vcfg->sFilter.sOuterVlan.nVidVal = vlan_list[i].outer_vlan.vid;
+		vcfg->sFilter.sOuterVlan.eTpid = vlan_list[i].outer_vlan.tpid;
+		vcfg->sFilter.sOuterVlan.eDei =
+			GSW_EXTENDEDVLAN_FILTER_DEI_NO_FILTER;
+
+		vcfg->sFilter.sInnerVlan.eType =
+			GSW_EXTENDEDVLAN_FILTER_TYPE_NORMAL;
+		vcfg->sFilter.sInnerVlan.bPriorityEnable = LTQ_FALSE;
+		vcfg->sFilter.sInnerVlan.bVidEnable = LTQ_TRUE;
+		vcfg->sFilter.sInnerVlan.nVidVal = vlan_list[i].inner_vlan.vid;
+		vcfg->sFilter.sInnerVlan.eTpid = vlan_list[i].inner_vlan.tpid;
+		vcfg->sFilter.sInnerVlan.eDei =
+			GSW_EXTENDEDVLAN_FILTER_DEI_NO_FILTER;
+
+		vcfg->sFilter.eEtherType = vlan_list[i].ether_type;
+
+		if (drop) {
+			vcfg->sTreatment.eRemoveTag =
+				GSW_EXTENDEDVLAN_TREATMENT_DISCARD_UPSTREAM;
+		} else {
+			vcfg->sTreatment.eRemoveTag =
+				GSW_EXTENDEDVLAN_TREATMENT_REMOVE_2_TAG;
+			vcfg->sTreatment.bReassignBridgePort = LTQ_TRUE;
+			vcfg->sTreatment.nNewBridgePortId = vlan_list[i].bp;
+		}
+
+		ret = ops->gsw_extvlan_ops.ExtendedVlan_Set(ops, vcfg);
+
+		if (ret != GSW_statusOk)
+			return -EIO;
+	}
+
+	return 0;
+}
+
+static int update_ctp(struct core_ops *ops, struct ext_vlan_info *vlan)
+{
+	static GSW_CTP_portConfig_t ctp;
+	static GSW_EXTENDEDVLAN_config_t vcfg;
+	GSW_return_t ret;
+	ltq_bool_t enable;
+	u32 block;
+	GSW_EXTENDEDVLAN_alloc_t alloc;
+	int i;
+
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "bp=%d v1=%d v1_d=%d v2=%d v2_d=%d\n",
+		 vlan->bp, vlan->n_vlan1, vlan->n_vlan1_drop,
+		vlan->n_vlan2, vlan->n_vlan2_drop);
+	memset(&ctp, 0, sizeof(ctp));
+	memset(&alloc, 0, sizeof(GSW_EXTENDEDVLAN_alloc_t));
+
+	ctp.nLogicalPortId = vlan->logic_port;
+	ctp.nSubIfIdGroup = vlan->subif_grp;
+	ctp.eMask = GSW_CTP_PORT_CONFIG_MASK_INGRESS_VLAN;
+	ret = ops->gsw_ctp_ops.CTP_PortConfigGet(ops, &ctp);
+
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	enable = ctp.bIngressExtendedVlanEnable;
+	block = ctp.nIngressExtendedVlanBlockId;
+
+	/* disable previous vlan block operation,if enabled and
+	 * free the previous allocated vlan blocks
+	 * so that new vlan block can be allocated and configured to ctp
+	 */
+	if (enable) {
+		ctp.bIngressExtendedVlanEnable = LTQ_FALSE;
+		ret = ops->gsw_ctp_ops.CTP_PortConfigSet(ops, &ctp);
+		if (ret != GSW_statusOk) {
+			PR_INFO("Fail:Ingress VLan operate disable in ctp\n");
+			return -EIO;
+		}
+		PR_INFO("ingress VLan operation disabled in ctp\n");
+		alloc.nExtendedVlanBlockId = block;
+		ret = ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+		if (ret != GSW_statusOk) {
+			PR_INFO("VLAN Free fail\n");
+			return -EIO;
+			PR_INFO("VLAN Free Success\n");
+		}
+	}
+	memset(&alloc, 0, sizeof(GSW_EXTENDEDVLAN_alloc_t));
+
+	alloc.nNumberOfEntries += vlan->n_vlan1 * 2;
+	alloc.nNumberOfEntries += vlan->n_vlan2;
+	alloc.nNumberOfEntries += vlan->n_vlan1_drop * 2;
+	alloc.nNumberOfEntries += vlan->n_vlan2_drop;
+	if (alloc.nNumberOfEntries == 0) {
+		PR_INFO("nNumberOfEntries == 0 , returning to caller\n");
+		return 0;
+	}
+
+	alloc.nNumberOfEntries += 3;
+	ret = ops->gsw_extvlan_ops.ExtendedVlan_Alloc(ops, &alloc);
+
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	vcfg.nExtendedVlanBlockId = alloc.nExtendedVlanBlockId;
+	ret = update_vlan0(ops, vlan->bp, &vcfg);
+
+	if (ret < 0)
+		goto UPDATE_ERROR;
+
+	i = 3;
+	ret = update_vlan2(ops, i, vlan->n_vlan2, vlan->vlan2_list, 0, &vcfg);
+
+	if (ret < 0)
+		goto UPDATE_ERROR;
+
+	i += vlan->n_vlan2;
+	ret = update_vlan2(ops, i, vlan->n_vlan2_drop,
+			   vlan->vlan2_drop_list, 1, &vcfg);
+
+	if (ret < 0)
+		goto UPDATE_ERROR;
+
+	i += vlan->n_vlan2_drop;
+	ret = update_vlan1(ops, i, vlan->n_vlan1, vlan->vlan1_list, 0, &vcfg);
+
+	if (ret < 0)
+		goto UPDATE_ERROR;
+
+	i += vlan->n_vlan1;
+	ret = update_vlan1(ops, i, vlan->n_vlan1_drop,
+			   vlan->vlan1_drop_list, 1, &vcfg);
+
+	if (ret < 0)
+		goto UPDATE_ERROR;
+
+	ctp.bIngressExtendedVlanEnable = LTQ_TRUE;
+	ctp.nIngressExtendedVlanBlockId = alloc.nExtendedVlanBlockId;
+	ctp.nIngressExtendedVlanBlockSize = 0;
+	ret = ops->gsw_ctp_ops.CTP_PortConfigSet(ops, &ctp);
+
+	if (ret != GSW_statusOk) {
+		PR_INFO("Enable ingress vlan in ctp fail\n");
+		return -EIO;
+	}
+	PR_INFO("Enable ingress vlan in ctp success\n");
+	return 0;
+UPDATE_ERROR:
+	ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+	return ret;
+}
+
+static int bp_add_vlan1(struct core_ops *ops, struct vlan1 *vlan,
+			GSW_BRIDGE_portConfig_t *bpcfg,
+			GSW_EXTENDEDVLAN_config_t *vcfg)
+{
+	int ret;
+	GSW_EXTENDEDVLAN_alloc_t alloc = {0};
+	GSW_ExtendedVlanFilterType_t types[6] = {
+		GSW_EXTENDEDVLAN_FILTER_TYPE_NO_TAG,
+		GSW_EXTENDEDVLAN_FILTER_TYPE_NO_FILTER,
+		GSW_EXTENDEDVLAN_FILTER_TYPE_DEFAULT,
+		GSW_EXTENDEDVLAN_FILTER_TYPE_NO_TAG,
+		GSW_EXTENDEDVLAN_FILTER_TYPE_NO_FILTER,
+		GSW_EXTENDEDVLAN_FILTER_TYPE_DEFAULT
+	};
+	u32 i;
+
+	memset(bpcfg, 0, sizeof(*bpcfg));
+	bpcfg->nBridgePortId = vlan->bp;
+	bpcfg->eMask = GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_VLAN;
+	ret = ops->gsw_brdgport_ops.BridgePort_ConfigGet(ops, bpcfg);
+
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	/* This bridge port should have no egress VLAN configured yet */
+	if (bpcfg->bEgressExtendedVlanEnable != LTQ_FALSE)
+		return -EINVAL;
+
+	alloc.nNumberOfEntries = ARRAY_SIZE(types) / 2;
+	ret = ops->gsw_extvlan_ops.ExtendedVlan_Alloc(ops, &alloc);
+
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	for (i = 0; i < alloc.nNumberOfEntries; i++) {
+		memset(vcfg, 0, sizeof(*vcfg));
+		vcfg->nExtendedVlanBlockId = alloc.nExtendedVlanBlockId;
+		vcfg->nEntryIndex = i;
+		vcfg->sFilter.sOuterVlan.eType = types[i * 2];
+		vcfg->sFilter.sInnerVlan.eType = types[i * 2 + 1];
+		/* filter ether_type as ingress */
+		vcfg->sFilter.eEtherType = vlan->ether_type;
+
+		vcfg->sTreatment.bAddOuterVlan = LTQ_TRUE;
+		vcfg->sTreatment.sOuterVlan.ePriorityMode =
+			GSW_EXTENDEDVLAN_TREATMENT_PRIORITY_VAL;
+		vcfg->sTreatment.sOuterVlan.ePriorityVal = 0;
+		vcfg->sTreatment.sOuterVlan.eVidMode =
+			GSW_EXTENDEDVLAN_TREATMENT_VID_VAL;
+		vcfg->sTreatment.sOuterVlan.eVidVal = vlan->outer_vlan.vid;
+		vcfg->sTreatment.sOuterVlan.eTpid = vlan->outer_vlan.tpid;
+		vcfg->sTreatment.sOuterVlan.eDei =
+			GSW_EXTENDEDVLAN_TREATMENT_DEI_0;
+
+		vcfg->sTreatment.bAddInnerVlan = LTQ_FALSE;
+
+		ret = ops->gsw_extvlan_ops.ExtendedVlan_Set(ops, vcfg);
+
+		if (ret != GSW_statusOk) {
+			PR_INFO("Fail updating Extended VLAN entry (%u, %u).\n",
+				alloc.nExtendedVlanBlockId, i);
+			ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+			return -EIO;
+		}
+	}
+
+	bpcfg->bEgressExtendedVlanEnable = LTQ_TRUE;
+	bpcfg->nEgressExtendedVlanBlockId = alloc.nExtendedVlanBlockId;
+	bpcfg->nEgressExtendedVlanBlockSize = 0;
+	ret = ops->gsw_brdgport_ops.BridgePort_ConfigSet(ops, bpcfg);
+
+	if (ret != GSW_statusOk) {
+		PR_INFO("Failed in attaching Extended VLAN to Bridge Port.\n");
+		ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+		return -EIO;
+	} else {
+		return 0;
+	}
+}
+
+static int bp_add_vlan2(struct core_ops *ops, struct vlan2 *vlan,
+			GSW_BRIDGE_portConfig_t *bpcfg,
+			GSW_EXTENDEDVLAN_config_t *vcfg)
+{
+	int ret;
+	GSW_EXTENDEDVLAN_alloc_t alloc = {0};
+	GSW_ExtendedVlanFilterType_t types[6] = {
+		GSW_EXTENDEDVLAN_FILTER_TYPE_NO_TAG,
+		GSW_EXTENDEDVLAN_FILTER_TYPE_NO_FILTER,
+		GSW_EXTENDEDVLAN_FILTER_TYPE_DEFAULT,
+		GSW_EXTENDEDVLAN_FILTER_TYPE_NO_TAG,
+		GSW_EXTENDEDVLAN_FILTER_TYPE_NO_FILTER,
+		GSW_EXTENDEDVLAN_FILTER_TYPE_DEFAULT
+	};
+	u32 i;
+
+	memset(bpcfg, 0, sizeof(*bpcfg));
+	bpcfg->nBridgePortId = vlan->bp;
+	bpcfg->eMask = GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_VLAN;
+	ret = ops->gsw_brdgport_ops.BridgePort_ConfigGet(ops, bpcfg);
+
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	/* This bridge port should have no egress VLAN configured yet */
+	if (bpcfg->bEgressExtendedVlanEnable != LTQ_FALSE)
+		return -EINVAL;
+
+	alloc.nNumberOfEntries = ARRAY_SIZE(types) / 2;
+	ret = ops->gsw_extvlan_ops.ExtendedVlan_Alloc(ops, &alloc);
+
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	for (i = 0; i < alloc.nNumberOfEntries; i++) {
+		memset(vcfg, 0, sizeof(*vcfg));
+		vcfg->nExtendedVlanBlockId = alloc.nExtendedVlanBlockId;
+		vcfg->nEntryIndex = i;
+		vcfg->sFilter.sOuterVlan.eType = types[i * 2];
+		vcfg->sFilter.sInnerVlan.eType = types[i * 2 + 1];
+		/* filter ether_type as ingress */
+		vcfg->sFilter.eEtherType = vlan->ether_type;
+
+		vcfg->sTreatment.bAddOuterVlan = LTQ_TRUE;
+		vcfg->sTreatment.sOuterVlan.ePriorityMode =
+			GSW_EXTENDEDVLAN_TREATMENT_PRIORITY_VAL;
+		vcfg->sTreatment.sOuterVlan.ePriorityVal = 0;
+		vcfg->sTreatment.sOuterVlan.eVidMode =
+			GSW_EXTENDEDVLAN_TREATMENT_VID_VAL;
+		vcfg->sTreatment.sOuterVlan.eVidVal = vlan->outer_vlan.vid;
+		vcfg->sTreatment.sOuterVlan.eTpid = vlan->outer_vlan.tpid;
+		vcfg->sTreatment.sOuterVlan.eDei =
+			GSW_EXTENDEDVLAN_TREATMENT_DEI_0;
+
+		vcfg->sTreatment.bAddInnerVlan = LTQ_TRUE;
+		vcfg->sTreatment.sInnerVlan.ePriorityMode =
+			GSW_EXTENDEDVLAN_TREATMENT_PRIORITY_VAL;
+		vcfg->sTreatment.sInnerVlan.ePriorityVal = 0;
+		vcfg->sTreatment.sInnerVlan.eVidMode =
+			GSW_EXTENDEDVLAN_TREATMENT_VID_VAL;
+		vcfg->sTreatment.sInnerVlan.eVidVal = vlan->inner_vlan.vid;
+		vcfg->sTreatment.sInnerVlan.eTpid = vlan->inner_vlan.tpid;
+		vcfg->sTreatment.sInnerVlan.eDei =
+			GSW_EXTENDEDVLAN_TREATMENT_DEI_0;
+
+		ret = ops->gsw_extvlan_ops.ExtendedVlan_Set(ops, vcfg);
+
+		if (ret != GSW_statusOk) {
+			PR_INFO("Fail updating Extended VLAN entry (%u, %u).\n",
+				alloc.nExtendedVlanBlockId, i);
+			ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+			return -EIO;
+		}
+	}
+
+	bpcfg->bEgressExtendedVlanEnable = LTQ_TRUE;
+	bpcfg->nEgressExtendedVlanBlockId = alloc.nExtendedVlanBlockId;
+	bpcfg->nEgressExtendedVlanBlockSize = 0;
+	ret = ops->gsw_brdgport_ops.BridgePort_ConfigSet(ops, bpcfg);
+
+	if (ret != GSW_statusOk) {
+		PR_INFO("Failed in attaching Extended VLAN to Bridge Port.\n");
+		ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+		return -EIO;
+	} else {
+		return 0;
+	}
+}
+
+static int bp_add_vlan(struct core_ops *ops, struct ext_vlan_info *vlan,
+		       int idx, GSW_BRIDGE_portConfig_t *bpcfg)
+{
+	static GSW_EXTENDEDVLAN_config_t vcfg;
+
+	if (idx < vlan->n_vlan1)
+		return bp_add_vlan1(ops, vlan->vlan1_list + idx, bpcfg, &vcfg);
+	else
+		return bp_add_vlan2(ops,
+				    vlan->vlan2_list + (idx - vlan->n_vlan1),
+				    bpcfg, &vcfg);
+}
+
+static int bp_rm_vlan(struct core_ops *ops, u32 bp,
+		      GSW_BRIDGE_portConfig_t *bpcfg)
+{
+	int ret;
+	GSW_EXTENDEDVLAN_alloc_t alloc = {0};
+
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "bp=%d\n", bp);
+	memset(bpcfg, 0, sizeof(*bpcfg));
+	bpcfg->nBridgePortId = bp;
+	bpcfg->eMask = GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_VLAN;
+	ret = ops->gsw_brdgport_ops.BridgePort_ConfigGet(ops, bpcfg);
+
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	if (bpcfg->bEgressExtendedVlanEnable == LTQ_FALSE)
+		return 0;
+
+	alloc.nExtendedVlanBlockId = bpcfg->nEgressExtendedVlanBlockId;
+	bpcfg->bEgressExtendedVlanEnable = LTQ_FALSE;
+	ret = ops->gsw_brdgport_ops.BridgePort_ConfigSet(ops, bpcfg);
+
+	if (ret != GSW_statusOk)
+		return -EIO;
+
+	ret = ops->gsw_extvlan_ops.ExtendedVlan_Free(ops, &alloc);
+
+	if (ret != GSW_statusOk)
+		return -EIO;
+	else
+		return 0;
+}
+
+/* Function for VLAN configure */
+int set_gswip_ext_vlan(struct core_ops *ops, struct ext_vlan_info *vlan,
+		       int flag)
+{
+	static GSW_BRIDGE_portConfig_t bpcfg;
+
+	int ret;
+	struct priv_ext_vlan *old_priv, *new_priv;
+	size_t new_priv_size;
+	int i, j;
+
+	/* Assumptions:
+	 * 1. Every time this function is called, one and only one "vlan" is
+	 *    added or removed. No replacement happens.
+	 * 2. The content of 2 vlan_list (not vlan_drop_list) keeps sequence.
+	 *    Only one new item is inserted or removed. No re-ordering happens.
+	 * 3. Bridge ports are not freed before this function is called. This
+	 *    is not compulsary. Just in case any resource free function
+	 *    such as free VLAN block, disable VLAN from bridge port, fails.
+	 * 4. In egress, assume packet at bridge port has no VLAN before
+	 *    Extended VLAN processing to save 2 Extended VLAN entries. This
+	 *    can be changed later if required.
+	 */
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "ext vlan info bp=%d logic=%d subif=%d\n",
+		 vlan->bp, vlan->logic_port, vlan->subif_grp);
+	ret = update_ctp(ops, vlan);
+
+	if (ret < 0)
+		return ret;
+
+	/* prepare new private data */
+	new_priv_size = sizeof(*new_priv);
+	new_priv_size += sizeof(new_priv->bp[0])
+			 * (vlan->n_vlan1 + vlan->n_vlan2);
+	new_priv = kmalloc(new_priv_size, GFP_KERNEL);
+
+	if (!new_priv)
+		return -ENOMEM;
+
+	new_priv->num_bp = (u32)(vlan->n_vlan1 + vlan->n_vlan2);
+	new_priv->bp[0] = vlan->bp;
+
+	for (i = 0, j = 1; i < vlan->n_vlan1; i++, j++)
+		new_priv->bp[j] = vlan->vlan1_list[i].bp;
+
+	for (i = 0; i < vlan->n_vlan2; i++, j++)
+		new_priv->bp[j] = vlan->vlan2_list[i].bp;
+
+	/* remember pointer to old private data */
+	old_priv = vlan->priv;
+
+	if (!old_priv) {
+		/* no old private data, vlan is added */
+		ret = bp_add_vlan(ops, vlan, 0, &bpcfg);
+	} else if (old_priv->num_bp < new_priv->num_bp) {
+		/* vlan added */
+		for (i = 0;
+		     ((u32)i < old_priv->num_bp) &&
+		     (old_priv->bp[i] == new_priv->bp[i]);
+		     i++)
+			;
+
+		ret = bp_add_vlan(ops, vlan, i, &bpcfg);
+	} else if (old_priv->num_bp > new_priv->num_bp) {
+		/* vlan removed */
+		for (i = 0;
+		     ((u32)i < new_priv->num_bp) &&
+		     (old_priv->bp[i] == new_priv->bp[i]);
+		     i++)
+			;
+
+		bp_rm_vlan(ops, old_priv->bp[i], &bpcfg);
+		ret = 0;
+	} else {
+		ret = 0;
+	}
+
+	/* return new private data in vlan structure */
+	vlan->priv = new_priv;
+
+	/* free old private data if any */
+	kfree(old_priv);
+	return ret;
+}
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_gswip.c b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_gswip.c
new file mode 100644
index 000000000000..055d096c5b26
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_gswip.c
@@ -0,0 +1,981 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/etherdevice.h>
+#include <net/lantiq_cbm_api.h>
+#include <net/datapath_api.h>
+#include "../datapath.h"
+#include "datapath_misc.h"
+
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_DDR_SIMULATE_GSWIP31)
+#include "datapath_gswip_simulate.h"
+#endif
+
+struct ctp_assign {
+	u32 flag; /*Datapath Device Flag */
+	GSW_LogicalPortMode_t emode; /*mapped GSWIP CTP flag */
+	u16 num; /*Max CTP allowed for that GSWIP logical port*/
+	u32 vap_offset; /*VAP offset */
+	u32 vap_mask;  /*VAP Mask after shift vap_offset bits */
+	u32 lookup_mode; /*CQE lookup mode  */
+};
+
+static struct ctp_assign ctp_assign_info[] = {
+	/*note: multiple flags must put first */
+	{DP_F_GPON, GSW_LOGICAL_PORT_GPON, 256, 0, 0xFF, CQE_LU_MODE1},
+	{DP_F_EPON, GSW_LOGICAL_PORT_EPON, 256, 0, 0xFF, CQE_LU_MODE1},
+	{DP_F_GINT, GSW_LOGICAL_PORT_GINT, 8, 0, 0xFF, CQE_LU_MODE1},
+/*#define DP_ETH_TEST*/
+#ifndef DP_ETH_TEST
+	{DP_F_FAST_ETH_WAN, GSW_LOGICAL_PORT_8BIT_WLAN, 8, 8, 0xF,
+		CQE_LU_MODE2},
+	{DP_F_FAST_ETH_LAN | DP_F_ALLOC_EXPLICIT_SUBIFID,
+		GSW_LOGICAL_PORT_8BIT_WLAN, 8, 8, 0xF, CQE_LU_MODE2},
+	{DP_F_FAST_ETH_LAN, GSW_LOGICAL_PORT_8BIT_WLAN, 2, 8, 0xF,
+		CQE_LU_MODE2},
+#else /*testing only */
+	{DP_F_FAST_ETH_WAN, GSW_LOGICAL_PORT_OTHER, 1, 8, 0xF, CQE_LU_MODE2},
+	{DP_F_FAST_ETH_LAN | DP_F_ALLOC_EXPLICIT_SUBIFID,
+		GSW_LOGICAL_PORT_OTHER, 1, 8, 0xF, CQE_LU_MODE2},
+	{DP_F_FAST_ETH_LAN, GSW_LOGICAL_PORT_OTHER, 1, 8, 0xF, CQE_LU_MODE2},
+#endif
+	{DP_F_FAST_WLAN, GSW_LOGICAL_PORT_8BIT_WLAN, 16, 8, 0xF, CQE_LU_MODE2},
+	{DP_F_FAST_WLAN_EXT, GSW_LOGICAL_PORT_9BIT_WLAN, 8, 9, 0x7,
+		CQE_LU_MODE2}
+};
+
+static struct ctp_assign ctp_assign_def = {0, GSW_LOGICAL_PORT_8BIT_WLAN, 8, 8,
+					   0xF, CQE_LU_MODE2};
+
+static struct gsw_itf itf_assign[PMAC_MAX_NUM] = {0};
+
+static char *ctp_mode_string(GSW_LogicalPortMode_t type)
+{
+	if (type == GSW_LOGICAL_PORT_8BIT_WLAN)
+		return "8BIT_WLAN";
+	if (type == GSW_LOGICAL_PORT_9BIT_WLAN)
+		return "9BIT_WLAN";
+	if (type == GSW_LOGICAL_PORT_GPON)
+		return "GPON";
+	if (type == GSW_LOGICAL_PORT_EPON)
+		return "EPON";
+	if (type == GSW_LOGICAL_PORT_GINT)
+		return "GINT";
+	if (type == GSW_LOGICAL_PORT_OTHER)
+		return "OTHER";
+	return "Undef";
+}
+
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_DDR_SIMULATE_GSWIP31)
+GSW_return_t gsw_core_api_ddr_simu31(dp_gsw_cb func, void *ops, void *param)
+{
+	if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_brdgport_ops.BridgePort_Alloc) {
+		return BridgePortAlloc(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_brdgport_ops.BridgePort_ConfigGet) {
+		return BridgePortConfigGet(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_brdgport_ops.BridgePort_ConfigSet) {
+		return BridgePortConfigSet(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_brdgport_ops.BridgePort_Free) {
+		return BridgePortFree(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_brdg_ops.Bridge_Alloc) {
+		return BridgeAlloc(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_brdg_ops.Bridge_ConfigSet) {
+		return BridgeConfigSet(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_brdg_ops.Bridge_ConfigGet) {
+		return BridgeConfigGet(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_brdg_ops.Bridge_Free) {
+		return BridgeFree(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_ctp_ops.CTP_PortAssignmentAlloc) {
+		return CTP_PortAssignmentAlloc
+			(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_ctp_ops.CTP_PortAssignmentFree) {
+		return CTP_PortAssignmentFree
+			(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_ctp_ops.CTP_PortAssignmentSet) {
+		return CTP_PortAssignmentSet
+			(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_ctp_ops.CTP_PortAssignmentGet) {
+		return CTP_PortAssignmentGet
+			(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_ctp_ops.CTP_PortConfigSet) {
+		return CtpPortConfigSet(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+	    gsw_ctp_ops.CTP_PortConfigGet) {
+		return CtpPortConfigGet(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+		   gsw_swmac_ops.MAC_TableEntryAdd) {
+		return MacTableAdd(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+		   gsw_swmac_ops.MAC_TableEntryRead) {
+		return MacTableRead(param);
+	} else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+		   gsw_swmac_ops.MAC_TableEntryRemove) {
+		return MacTableRemove(param);
+	}  else if (func == (dp_gsw_cb)dp_port_prop[0].ops[0]->
+		    gsw_swmac_ops.MAC_TableEntryQuery) {
+		return MacTableQuery(param);
+	}
+	return GSW_SIMUTE_DDR_NOT_MATCH;
+}
+#endif
+
+/*This API is only for GSWIP-R PMAC modification, not for GSWIP-L */
+int dp_pmac_set_31(int inst, u32 port, dp_pmac_cfg_t *pmac_cfg)
+{
+	GSW_PMAC_Eg_Cfg_t egcfg;
+	GSW_PMAC_Ig_Cfg_t igcfg;
+	int i, j, k;
+	u32 flag = 0;
+	cbm_dq_port_res_t dqport;
+	s32 ret;
+	u32 pmacid;
+	struct core_ops *gswr_r;
+	GSW_PMAC_Glbl_Cfg_t pmac_glb;
+
+	if (!pmac_cfg || !port) {
+		PR_ERR("dp_pmac_set:wrong parameter(pmac_cfg/port NULL)\n");
+		return -1;
+	}
+
+	if (!pmac_cfg->ig_pmac_flags && !pmac_cfg->eg_pmac_flags)
+		return 0;
+	if (port == 2)
+		pmacid = 1;
+	else
+		pmacid = 0;
+	memset(&dqport, 0, sizeof(cbm_dq_port_res_t));
+
+	/* Get GSWIP device handler */
+	gswr_r = dp_port_prop[inst].ops[0];
+
+	/*set ingress port via DMA tx channel */
+	if (pmac_cfg->ig_pmac_flags) {
+		/*Read back igcfg from gsw first */
+		ret = cbm_dequeue_port_resources_get(port, &dqport, flag);
+
+		if (ret == -1) {
+			PR_ERR("cbm_dequeue_port_resources_get failed\n");
+			return -1;
+		}
+
+		memset(&igcfg, 0, sizeof(GSW_PMAC_Ig_Cfg_t));
+
+		for (i = 0; i < dqport.num_deq_ports; i++) {
+			igcfg.nPmacId = pmacid;
+			igcfg.nTxDmaChanId = dqport.deq_info[i].dma_tx_chan;
+			if (igcfg.nTxDmaChanId == (u8)-1) {
+				igcfg.nTxDmaChanId =
+					pmac_cfg->ig_pmac.tx_dma_chan;
+			}
+			gsw_core_api((dp_gsw_cb)gswr_r->gsw_pmac_ops.
+				     Pmac_Ig_CfgGet, gswr_r, &igcfg);
+
+			/*update igcfg and write back to gsw */
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_ERR_DISC)
+				igcfg.bErrPktsDisc =
+					pmac_cfg->ig_pmac.err_disc;
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_PRESENT)
+				igcfg.bPmacPresent = pmac_cfg->ig_pmac.pmac;
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_SUBIF)
+				//igcfg.bSubIdDefault =
+				igcfg.eSubId =
+					pmac_cfg->ig_pmac.def_pmac_subifid;
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_SPID)
+				igcfg.bSpIdDefault =
+					pmac_cfg->ig_pmac.def_pmac_src_port;
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_CLASSENA)
+				igcfg.bClassEna =
+					pmac_cfg->ig_pmac.def_pmac_en_tc;
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_CLASS)
+				igcfg.bClassDefault =
+					pmac_cfg->ig_pmac.def_pmac_tc;
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_PMAPENA)
+				igcfg.bPmapEna =
+					pmac_cfg->ig_pmac.def_pmac_en_pmap;
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_PMAP)
+				igcfg.bPmapDefault =
+					pmac_cfg->ig_pmac.def_pmac_pmap;
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_PMACHDR1)
+				igcfg.defPmacHdr[0] =
+					pmac_cfg->ig_pmac.def_pmac_hdr[0];
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_PMACHDR2)
+				igcfg.defPmacHdr[1] =
+					pmac_cfg->ig_pmac.def_pmac_hdr[1];
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_PMACHDR3)
+				igcfg.defPmacHdr[2] =
+					pmac_cfg->ig_pmac.def_pmac_hdr[2];
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_PMACHDR4)
+				igcfg.defPmacHdr[3] =
+					pmac_cfg->ig_pmac.def_pmac_hdr[3];
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_PMACHDR5)
+				igcfg.defPmacHdr[4] =
+					pmac_cfg->ig_pmac.def_pmac_hdr[4];
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_PMACHDR6)
+				igcfg.defPmacHdr[5] =
+					pmac_cfg->ig_pmac.def_pmac_hdr[5];
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_PMACHDR7)
+				igcfg.defPmacHdr[6] =
+					pmac_cfg->ig_pmac.def_pmac_hdr[6];
+
+			if (pmac_cfg->ig_pmac_flags & IG_PMAC_F_PMACHDR8)
+				igcfg.defPmacHdr[7] =
+					pmac_cfg->ig_pmac.def_pmac_hdr[7];
+
+			DP_DEBUG(DP_DBG_FLAG_DBG,
+				 "\nPMAC %d igcfg configuration:\n", port);
+			DP_DEBUG(DP_DBG_FLAG_DBG, "igcfg.nPmacId=%d\n",
+				 igcfg.nPmacId);
+			DP_DEBUG(DP_DBG_FLAG_DBG, "igcfg.nTxDmaChanId=%d\n",
+				 igcfg.nTxDmaChanId);
+			DP_DEBUG(DP_DBG_FLAG_DBG, "igcfg.bErrPktsDisc=%d\n",
+				 igcfg.bErrPktsDisc);
+			DP_DEBUG(DP_DBG_FLAG_DBG, "igcfg.bPmapDefault=%d\n",
+				 igcfg.bPmapDefault);
+			DP_DEBUG(DP_DBG_FLAG_DBG, "igcfg.bPmapEna=%d\n",
+				 igcfg.bPmapEna);
+			DP_DEBUG(DP_DBG_FLAG_DBG, "igcfg.bClassDefault=%d\n",
+				 igcfg.bClassDefault);
+			DP_DEBUG(DP_DBG_FLAG_DBG, "igcfg.bClassEna=%d\n",
+				 igcfg.bClassEna);
+			DP_DEBUG(DP_DBG_FLAG_DBG, "igcfg.bSubIdDefault=%d\n",
+				 igcfg.eSubId);
+			DP_DEBUG(DP_DBG_FLAG_DBG, "igcfg.bSpIdDefault=%d\n",
+				 igcfg.bSpIdDefault);
+			DP_DEBUG(DP_DBG_FLAG_DBG, "igcfg.bPmacPresent=%d\n",
+				 igcfg.bPmacPresent);
+			DP_DEBUG(DP_DBG_FLAG_DBG, "igcfg.defPmacHdr=");
+
+			for (k = 0;
+					k <
+					sizeof(igcfg.defPmacHdr) /
+					sizeof(igcfg.defPmacHdr[0]); k++)
+				DP_DEBUG(DP_DBG_FLAG_DBG, "0x%x ",
+					 igcfg.defPmacHdr[k]);
+
+			DP_DEBUG(DP_DBG_FLAG_DBG, "\n");
+
+			gsw_core_api((dp_gsw_cb)gswr_r->gsw_pmac_ops
+				     .Pmac_Ig_CfgSet, gswr_r, &igcfg);
+		}
+
+			kfree(dqport.deq_info);
+	}
+
+	/*set egress port via pmac port id */
+	if (!pmac_cfg->eg_pmac_flags)
+		return 0;
+
+	for (i = 0; i <= 15; i++) {	/*traffic class */
+		for (j = 0; j <= 3; j++) {	/* flow */
+			/*read back egcfg first from gsw */
+			memset(&egcfg, 0, sizeof(GSW_PMAC_Eg_Cfg_t));
+			egcfg.nPmacId = pmacid;
+			egcfg.nDestPortId = port;
+			egcfg.nTrafficClass = i;
+			egcfg.nFlowIDMsb = j;
+			egcfg.nBslTrafficClass = i;
+
+			memset(&pmac_glb, 0, sizeof(pmac_glb));
+			gsw_core_api((dp_gsw_cb)gswr_r->gsw_pmac_ops
+				     .Pmac_Gbl_CfgGet, gswr_r, &pmac_glb);
+			egcfg.bProcFlagsSelect = pmac_glb.bProcFlagsEgCfgEna;
+			DP_DEBUG(DP_DBG_FLAG_DBG, "bProcFlagsSelect=%u\n",
+				 egcfg.bProcFlagsSelect);
+
+			/*update egcfg and write back to gsw */
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_FCS)
+				egcfg.bFcsEna = pmac_cfg->eg_pmac.fcs;
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_L2HDR_RM) {
+				egcfg.bRemL2Hdr = pmac_cfg->eg_pmac.rm_l2hdr;
+				egcfg.numBytesRem =
+				    pmac_cfg->eg_pmac.num_l2hdr_bytes_rm;
+			}
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_PMAC)
+				egcfg.bPmacEna = pmac_cfg->eg_pmac.pmac;
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_RXID)
+				egcfg.nRxDmaChanId =
+				    pmac_cfg->eg_pmac.rx_dma_chan;
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_REDIREN)
+				egcfg.bRedirEnable = pmac_cfg->eg_pmac.redir;
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_BSLSEG)
+				egcfg.bBslSegmentDisable =
+					pmac_cfg->eg_pmac.bsl_seg;
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_RESDW1EN)
+				egcfg.bResDW1Enable =
+					pmac_cfg->eg_pmac.res_endw1;
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_RESDW1)
+				egcfg.nResDW1 = pmac_cfg->eg_pmac.res_dw1;
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_RES1DW0EN)
+				egcfg.bRes1DW0Enable =
+					pmac_cfg->eg_pmac.res1_endw0;
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_RES1DW0)
+				egcfg.nRes1DW0 = pmac_cfg->eg_pmac.res1_dw0;
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_RES2DW0EN)
+				egcfg.bRes2DW0Enable =
+					pmac_cfg->eg_pmac.res2_endw0;
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_RES2DW0)
+				egcfg.nRes2DW0 = pmac_cfg->eg_pmac.res2_dw0;
+
+			/*if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_TCENA)
+			 *    egcfg.bTCEnable = pmac_cfg->eg_pmac.tc_enable;
+			 */
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_DECFLG)
+				egcfg.bDecFlag = pmac_cfg->eg_pmac.dec_flag;
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_ENCFLG)
+				egcfg.bEncFlag = pmac_cfg->eg_pmac.enc_flag;
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_MPE1FLG)
+				egcfg.bMpe1Flag = pmac_cfg->eg_pmac.mpe1_flag;
+
+			if (pmac_cfg->eg_pmac_flags & EG_PMAC_F_MPE2FLG)
+				egcfg.bMpe2Flag = pmac_cfg->eg_pmac.mpe2_flag;
+#if defined(CONFIG_LTQ_DATAPATH_DBG) && CONFIG_LTQ_DATAPATH_DBG
+			if (dp_dbg_flag) {
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "\nPMAC %d egcfg configuration:\n",
+					 port);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.nPmacId=%d\n",
+					 egcfg.nPmacId);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.nRxDmaChanId=%d\n",
+					 egcfg.nRxDmaChanId);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.bRemL2Hdr=%d\n",
+					 egcfg.bRemL2Hdr);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.numBytesRem=%d\n",
+					 egcfg.numBytesRem);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.bFcsEna=%d\n", egcfg.bFcsEna);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.bPmacEna=%d\n",
+					 egcfg.bPmacEna);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.bResDW1Ena=%d\n",
+					 egcfg.bResDW1Enable);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.nResDW1=%d\n", egcfg.nResDW1);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.bRes1DW0Ena=%d\n",
+					 egcfg.bRes1DW0Enable);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.nRes1DW0=%d\n",
+					 egcfg.nRes1DW0);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.bRes2DW0Ena=%d\n",
+					 egcfg.bRes2DW0Enable);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.nRes2DW0=%d\n",
+					 egcfg.nRes2DW0);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.nDestPortId=%d\n",
+					 egcfg.nDestPortId);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.bTCEnable=%d\n",
+					 egcfg.bTCEnable);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.nTrafficClass=%d\n",
+					 egcfg.nTrafficClass);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.nBslTrafficClass=%d\n",
+					 egcfg.nBslTrafficClass);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.nFlowIDMsb=%d\n",
+					 egcfg.nFlowIDMsb);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.bDecFlag=%d\n",
+					 egcfg.bDecFlag);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.bEncFlag=%d\n",
+					 egcfg.bEncFlag);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.bMpe1Flag=%d\n",
+					 egcfg.bMpe1Flag);
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "egcfg.bMpe2Flag=%d\n",
+					 egcfg.bMpe2Flag);
+			}
+#endif
+			gsw_core_api((dp_gsw_cb)gswr_r->gsw_pmac_ops
+				     .Pmac_Eg_CfgSet, gswr_r, &egcfg);
+			;
+		}
+	}
+
+	return 0;
+}
+
+/*flag: bit 0 for cpu
+ * bit 1 for mpe1,bit 2 for mpe2, bit 3 for mpe3;
+ */
+#define GSW_L_BASE_ADDR        (0xBC000000)
+#define GSW_R_BASE_ADDR        (0xBA000000)
+#define FDMA_PASR_ADDR         (0xA47)
+
+int dp_set_gsw_parser_31(u8 flag, u8 cpu, u8 mpe1,
+			 u8 mpe2, u8 mpe3)
+{
+	GSW_CPU_PortCfg_t param = {0};
+	struct core_ops *gsw_handle = dp_port_prop[0].ops[0];/*gswip o */
+
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_common_ops
+			 .CPU_PortCfgGet, gsw_handle, &param)) {
+		PR_ERR("Failed GSW_CPU_PORT_CFG_GET\n");
+		return -1;
+	}
+	DP_DEBUG(DP_DBG_FLAG_DBG, "old flag=0x%x cpu=%d mpe1/2/3=%d/%d/%d\n",
+		 flag, param.eNoMPEParserCfg,
+		 param.eMPE1ParserCfg, param.eMPE2ParserCfg,
+		 param.eMPE1MPE2ParserCfg);
+	DP_DEBUG(DP_DBG_FLAG_DBG, "new flag=0x%x cpu=%d mpe1/2/3=%d/%d/%d\n",
+		 flag, cpu, mpe1, mpe2, mpe3);
+	if (flag & F_MPE_NONE)
+		param.eNoMPEParserCfg = cpu;
+
+	if (flag & F_MPE1_ONLY)
+		param.eMPE1ParserCfg = mpe1;
+
+	if (flag & F_MPE2_ONLY)
+		param.eMPE2ParserCfg = mpe2;
+
+	if (flag & F_MPE1_MPE2)
+		param.eMPE1MPE2ParserCfg = mpe3;
+
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_common_ops
+			 .CPU_PortCfgSet, gsw_handle, &param)) {
+		PR_ERR("Failed GSW_CPU_PORT_CFG_SET\n");
+		return -1;
+	}
+	dp_parser_info_refresh(param.eNoMPEParserCfg,
+			       param.eMPE1ParserCfg,
+			       param.eMPE2ParserCfg,
+			       param.eMPE1MPE2ParserCfg, 0);
+	return 0;
+}
+
+int dp_get_gsw_parser_31(u8 *cpu, u8 *mpe1, u8 *mpe2,
+			 u8 *mpe3)
+{
+	GSW_CPU_PortCfg_t param = {0};
+	struct core_ops *gsw_handle = dp_port_prop[0].ops[0]; /*gswip 0*/
+
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_common_ops
+			 .CPU_PortCfgGet, gsw_handle, &param)) {
+		PR_ERR("Failed GSW_CPU_PORT_CFG_GET\n");
+		return -1;
+	}
+	dp_parser_info_refresh(param.eNoMPEParserCfg,
+			       param.eMPE1ParserCfg,
+			       param.eMPE2ParserCfg,
+			       param.eMPE1MPE2ParserCfg, 1);
+
+	if (cpu) {
+		*cpu = param.eNoMPEParserCfg;
+		DP_DEBUG(DP_DBG_FLAG_DBG, "  cpu=%d\n", *cpu);
+	}
+
+	if (mpe1) {
+		*mpe1 = param.eMPE1ParserCfg;
+		DP_DEBUG(DP_DBG_FLAG_DBG, "  mpe1=%d\n", *mpe1);
+	}
+
+	if (mpe2) {
+		*mpe2 = param.eMPE2ParserCfg;
+		DP_DEBUG(DP_DBG_FLAG_DBG, "  mpe2=%d\n", *mpe2);
+	}
+
+	if (mpe3) {
+		*mpe3 = param.eMPE1MPE2ParserCfg;
+		DP_DEBUG(DP_DBG_FLAG_DBG, "  mpe3=%d\n", *mpe3);
+	}
+	return 0;
+}
+
+int gsw_mib_reset_31(int dev, u32 flag)
+{
+	struct core_ops *gsw_handle;
+	GSW_return_t ret;
+	GSW_RMON_clear_t rmon_clear;
+
+	gsw_handle = dp_port_prop[0].ops[0];
+	rmon_clear.eRmonType = GSW_RMON_ALL_TYPE;
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_rmon_ops.RMON_Clear,
+			   gsw_handle, &rmon_clear);
+
+	if (ret != GSW_statusOk) {
+		PR_ERR("R:GSW_RMON_CLEAR failed for GSW_RMON_ALL_TYPE\n");
+		return -1;
+	}
+	return ret;
+}
+
+/* Return allocated ctp number */
+struct gsw_itf *ctp_port_assign(int inst, u8 ep, int bp_default,
+				u32 flags)
+{
+	GSW_CTP_portAssignment_t ctp_assign;
+	struct ctp_assign *assign = &ctp_assign_def;
+	int i;
+	struct core_ops *gsw_handle;
+
+	memset(&ctp_assign, 0, sizeof(ctp_assign));
+	gsw_handle = dp_port_prop[inst].ops[GSWIP_L];
+
+	if (flags & DP_F_DEREGISTER) {
+		PR_ERR("Need to Free CTP Port here for ep=%d\n", ep);
+		ctp_assign.nLogicalPortId = ep;
+		ctp_assign.eMode = itf_assign[ep].mode;
+		ctp_assign.nFirstCtpPortId = itf_assign[ep].start;
+		ctp_assign.nNumberOfCtpPort = itf_assign[ep].n;
+		if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_ctp_ops
+				 .CTP_PortAssignmentFree,
+				 gsw_handle,
+				 &ctp_assign) != 0) {
+			PR_ERR("Failed to allc CTP for ep=%d blk=%d mode=%d\n",
+			       ep, assign->num, assign->emode);
+			return NULL;
+		}
+		return NULL;
+	}
+
+	for (i = 0; i < ARRAY_SIZE(ctp_assign_info); i++) {
+		if ((ctp_assign_info[i].flag & flags) ==
+			ctp_assign_info[i].flag) {
+			assign = &ctp_assign_info[i];
+			break;
+		}
+	}
+	ctp_assign.nLogicalPortId = ep;
+	ctp_assign.eMode = assign->emode;
+	ctp_assign.nBridgePortId = bp_default;
+	ctp_assign.nFirstCtpPortId = 0;
+	ctp_assign.nNumberOfCtpPort = assign->num;
+	dp_port_info[inst][ep].cqe_lu_mode = assign->lookup_mode;
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_ctp_ops
+			  .CTP_PortAssignmentAlloc,
+			  gsw_handle,
+			  &ctp_assign) != 0) {
+		PR_ERR("Failed CTP Assignment for ep=%d blk size=%d mode=%s\n",
+		       ep, assign->num, ctp_mode_string(assign->emode));
+		return NULL;
+	}
+
+	DP_DEBUG(DP_DBG_FLAG_DBG, "assign ep=%d with eMode=%d\n",
+		 ep, assign->emode);
+	itf_assign[ep].mode = assign->emode;
+	itf_assign[ep].n = ctp_assign.nNumberOfCtpPort;
+	itf_assign[ep].start = ctp_assign.nFirstCtpPortId;
+	itf_assign[ep].end = ctp_assign.nFirstCtpPortId +
+		 ctp_assign.nNumberOfCtpPort - 1;
+	itf_assign[ep].ep = ep;
+	dp_port_info[inst][ep].ctp_max = ctp_assign.nNumberOfCtpPort;
+	dp_port_info[inst][ep].vap_offset = assign->vap_offset;
+	dp_port_info[inst][ep].vap_mask = assign->vap_mask;
+	return &itf_assign[ep];
+}
+
+/*Allocate a bridge port with specified FID and hardcoded CPU port member */
+int alloc_bridge_port(int inst, int port_id, int subif_ix,
+		      int fid, int bp_member)
+{
+	GSW_BRIDGE_portAlloc_t bp = {0};
+	GSW_BRIDGE_portConfig_t bp_cfg = {0};
+	struct core_ops *gsw_handle;
+	GSW_return_t ret;
+
+	gsw_handle = dp_port_prop[inst].ops[GSWIP_L];
+	/*allocate a free bridge port */
+	memset(&bp, 0, sizeof(bp));
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			   .BridgePort_Alloc, gsw_handle, &bp);
+	if ((ret != GSW_statusOk) ||
+	    (bp.nBridgePortId < 0)) {
+		PR_ERR("Failed to get a bridge port\n");
+		return -1;
+	}
+	/*set this new bridge port with specified bridge ID(fid)
+	 *and bridge port map
+	 */
+	bp_cfg.nBridgePortId = bp.nBridgePortId;
+	bp_cfg.nDestLogicalPortId = port_id;
+	bp_cfg.nDestSubIfIdGroup = subif_ix;
+	/* By default Disable src mac learning for registered
+	 * non CPU bridge port with DP
+	 */
+	bp_cfg.bSrcMacLearningDisable = 1;
+	bp_cfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID |
+		GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP |
+		GSW_BRIDGE_PORT_CONFIG_MASK_MC_SRC_MAC_LEARNING |
+		GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_CTP_MAPPING;
+	bp_cfg.nBridgeId = fid;
+
+	SET_BP_MAP(bp_cfg.nBridgePortMap, bp_member); /*CPU*/
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			     .BridgePort_ConfigSet, gsw_handle, &bp_cfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("Failed to set bridge id(%d) and port map for bp= %d\n",
+		       fid, bp_cfg.nBridgePortId);
+		gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			     .BridgePort_Free, gsw_handle, &bp);
+		return -1;
+	}
+
+	/*ADD this bridge port to CPU bridge port's member.
+	 *Need read back first
+	 */
+	bp_cfg.nBridgePortId = bp_member;
+	bp_cfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			   .BridgePort_ConfigGet, gsw_handle, &bp_cfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("Failed to get bridge port's member for bridgeport=%d\n",
+		       bp_cfg.nBridgePortId);
+		gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			     .BridgePort_Free, gsw_handle, &bp);
+		return -1;
+	}
+	SET_BP_MAP(bp_cfg.nBridgePortMap, bp.nBridgePortId);
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			   .BridgePort_ConfigSet, gsw_handle, &bp_cfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("Failed to set bridge port's member for bridgeport=%d\n",
+		       bp_cfg.nBridgePortId);
+		gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			     .BridgePort_Free, gsw_handle, &bp);
+		return -1;
+	}
+
+	return bp.nBridgePortId;
+}
+
+/*Free one GSWIP bridge port
+ *First read out its port member
+ *according to this port memeber, from this deleing bridge port
+ *from its member's member Free this bridge port
+ */
+int free_bridge_port(int inst, int bp)
+{
+	GSW_BRIDGE_portConfig_t *tmp = NULL, *tmp2 = NULL;
+	int i, j;
+	GSW_return_t ret;
+	struct core_ops *gsw_handle;
+
+	gsw_handle = dp_port_prop[inst].ops[GSWIP_L];
+	if (bp == CPU_BP)
+		return 0;
+
+	tmp = kzalloc(sizeof(*tmp), GFP_KERNEL);
+	tmp2 = kzalloc(sizeof(*tmp2), GFP_KERNEL);
+	if (!tmp || !tmp2)
+		goto FREE_EXIT;
+
+	/*read out this delting bridge port's member*/
+	tmp->nBridgePortId = bp;
+	tmp->eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			   .BridgePort_ConfigGet, gsw_handle, tmp);
+	if (ret != GSW_statusOk) {
+		PR_ERR("Failed GSW_BRIDGE_PORT_CONFIG_GET: %d\n", bp);
+		goto EXIT;
+	}
+	/*remove this delting bridgeport from other bridge port's member*/
+	for (i = 0; i < ARRAY_SIZE(tmp->nBridgePortMap); i++) {
+		for (j = 0; j < 16 /*u16*/; j++) {
+			if (!(tmp->nBridgePortMap[i] & (1 << j)))
+				continue; /*not memmber bit set */
+			memset(tmp2->nBridgePortMap, 0,
+			       sizeof(tmp2->nBridgePortMap));
+			tmp2->eMask =
+				GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
+			tmp2->nBridgePortId = i * 16 + j;
+			ret = gsw_core_api((dp_gsw_cb)gsw_handle
+					   ->gsw_brdgport_ops
+					   .BridgePort_ConfigGet, gsw_handle,
+					   tmp2);
+			if (ret != GSW_statusOk) {
+				PR_ERR("Failed GSW_BRIDGE_PORT_CONFIG_GET\n");
+				goto EXIT;
+			}
+			UNSET_BP_MAP(tmp2->nBridgePortMap, bp);
+			ret = gsw_core_api((dp_gsw_cb)gsw_handle
+					   ->gsw_brdgport_ops
+					   .BridgePort_ConfigSet, gsw_handle,
+					   tmp2);
+			if (ret != GSW_statusOk) {
+				PR_ERR("Failed GSW_BRIDGE_PORT_CONFIG_SET\n");
+				goto EXIT;
+			}
+		}
+	}
+EXIT:
+	/*FRee thie bridge port */
+	memset(tmp, 0, sizeof(tmp));
+	tmp->nBridgePortId = bp;
+	tmp->eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			   .BridgePort_Free, gsw_handle, tmp);
+	if (ret != GSW_statusOk)
+		PR_ERR("Failed to GSW_BRIDGE_PORT_FREE:%d\n", bp);
+FREE_EXIT:
+	kfree(tmp);
+	kfree(tmp2);
+	return 0;
+}
+
+int dp_gswip_mac_entry_add(int bport, int fid, int inst, u8 *addr)
+{
+	GSW_MAC_tableAdd_t tmp;
+	GSW_return_t ret;
+	struct core_ops *gsw_handle;
+
+	gsw_handle = dp_port_prop[inst].ops[GSWIP_L];
+	memset(&tmp, 0, sizeof(tmp));
+	tmp.bStaticEntry = 1;
+	tmp.nFId = fid;
+	tmp.nPortId = bport;
+	SET_BP_MAP(tmp.nPortMap, bport);
+	tmp.nSubIfId = 0;
+	memcpy(tmp.nMAC, addr, GSW_MAC_ADDR_LEN);
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_swmac_ops.
+			   MAC_TableEntryAdd, gsw_handle, &tmp);
+	if (ret != GSW_statusOk) {
+		PR_ERR("fail in setting MAC table static add entry\r\n");
+		return -1;
+	}
+	return 0;
+}
+
+int dp_gswip_mac_entry_del(int bport, int fid, int inst, u8 *addr)
+{
+	GSW_MAC_tableRemove_t tmp;
+	GSW_MAC_tableQuery_t mac_query;
+	GSW_return_t ret;
+	struct core_ops *gsw_handle;
+
+	gsw_handle = dp_port_prop[inst].ops[GSWIP_L];
+	memset(&tmp, 0, sizeof(tmp));
+	memset(&mac_query, 0, sizeof(mac_query));
+	mac_query.nFId = fid;
+	memcpy(mac_query.nMAC, addr, GSW_MAC_ADDR_LEN);
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_swmac_ops.
+			   MAC_TableEntryQuery, gsw_handle, &tmp);
+	if (ret != GSW_statusOk) {
+		PR_ERR("fail in getting MAC query entry\r\n");
+		return -1;
+	}
+	tmp.nFId = fid;
+	memcpy(tmp.nMAC, addr, GSW_MAC_ADDR_LEN);
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_swmac_ops.
+			   MAC_TableEntryRemove, gsw_handle, &tmp);
+	if (ret != GSW_statusOk) {
+		PR_ERR("fail in setting MAC static entry remove\r\n");
+		return -1;
+	}
+	return 0;
+}
+
+int cpu_vlan_mod_dis(int inst)
+{
+	GSW_QoS_portRemarkingCfg_t cfg = {0};
+	struct core_ops *ops;
+	int ret;
+
+	ops = dp_port_prop[inst].ops[GSWIP_L];
+
+	cfg.nPortId = 0;
+	ret = gsw_core_api((dp_gsw_cb)ops->gsw_qos_ops.
+			   QoS_PortRemarkingCfgGet, ops, &cfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("QoS_PortRemarkingCfgGet failed\n");
+		return -1;
+	}
+
+	cfg.bPCP_EgressRemarkingEnable = LTQ_FALSE;
+	ret = gsw_core_api((dp_gsw_cb)ops->gsw_qos_ops.
+			   QoS_PortRemarkingCfgSet, ops, &cfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("QoS_PortRemarkingCfgSet failed\n");
+		return -1;
+	}
+
+	return 0;
+}
+
+int dp_set_gsw_pmapper_31(int inst, int bport, int lport,
+			  struct dp_pmapper *mapper, u32 flag)
+{
+	GSW_BRIDGE_portConfig_t bp_cfg = {0};
+	struct core_ops *gsw_handle;
+	GSW_return_t ret;
+	int i, index;
+	int ctp;
+	struct pmac_port_info *port_info = &dp_port_info[inst][lport];
+
+	gsw_handle = dp_port_prop[inst].ops[GSWIP_L];
+
+	DP_DEBUG(DP_DBG_FLAG_DBG,
+		 "set pmapper bport %d inst %d lport %d\n",
+		 bport, inst, lport);
+	bp_cfg.nBridgePortId = bport;
+	bp_cfg.nDestLogicalPortId = lport;
+	bp_cfg.bPmapperEnable = 1;
+	bp_cfg.ePmapperMappingMode = mapper->mode;
+
+	/* copy the sub if information in all pmapper list*/
+	if (mapper->def_ctp != DP_PMAPPER_DISCARD_CTP) {
+		ctp = GET_VAP(mapper->def_ctp, port_info->vap_offset,
+			      port_info->vap_mask);
+	} else {
+		ctp = PMAPPER_DISC_CTP;
+	}
+	bp_cfg.sPmapper.nDestSubIfIdGroup[0] = ctp;
+
+	for (i = 0; i < DP_PMAP_PCP_NUM; i++) {
+		if (mapper->pcp_map[i] != DP_PMAPPER_DISCARD_CTP) {
+			ctp = GET_VAP(mapper->pcp_map[i],
+				      port_info->vap_offset,
+				      port_info->vap_mask);
+		} else {
+			ctp = PMAPPER_DISC_CTP;
+		}
+		bp_cfg.sPmapper.nDestSubIfIdGroup[1 + i] = ctp;
+	}
+	for (i = 0; i < DP_PMAP_DSCP_NUM; i++) {
+		if (mapper->dscp_map[i] != DP_PMAPPER_DISCARD_CTP) {
+			ctp = GET_VAP(mapper->dscp_map[i],
+				      port_info->vap_offset,
+				      port_info->vap_mask);
+		} else {
+			ctp = PMAPPER_DISC_CTP;
+		}
+		index = 1 + DP_PMAP_PCP_NUM + i;
+		bp_cfg.sPmapper.nDestSubIfIdGroup[index] = ctp;
+	}
+	bp_cfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_CTP_MAPPING;
+
+	DP_DEBUG(DP_DBG_FLAG_DBG,
+		 "call switch api mode %d enable %d eMask 0x%x\n",
+		 bp_cfg.ePmapperMappingMode, bp_cfg.bPmapperEnable,
+		 bp_cfg.eMask);
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			     .BridgePort_ConfigSet, gsw_handle, &bp_cfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("fail in setting pmapper\r\n");
+		return -1;
+	}
+	return 0;
+}
+
+int dp_get_gsw_pmapper_31(int inst, int bport, int lport,
+			  struct dp_pmapper *mapper, u32 flag)
+{
+	GSW_BRIDGE_portConfig_t bp_cfg = {0};
+	struct core_ops *gsw_handle;
+	GSW_return_t ret;
+	int i, index;
+	struct hal_priv *priv;
+	u16 dest;
+	struct pmac_port_info *info = &dp_port_info[inst][lport];
+
+	priv = (struct hal_priv *)dp_port_prop[inst].priv_hal;
+	gsw_handle = dp_port_prop[inst].ops[GSWIP_L];
+
+	DP_DEBUG(DP_DBG_FLAG_DBG,
+		 "get bport %d inst %d lport %d\n",
+		 bport, inst, lport);
+	bp_cfg.nBridgePortId = bport;
+	bp_cfg.nDestLogicalPortId = lport;
+	bp_cfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_CTP_MAPPING;
+
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			     .BridgePort_ConfigGet, gsw_handle, &bp_cfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("fail in getting pmapper\r\n");
+		return -1;
+	}
+
+	DP_DEBUG(DP_DBG_FLAG_DBG,
+		 "after call switch api mode %d enable %d\n",
+		 bp_cfg.ePmapperMappingMode, bp_cfg.bPmapperEnable);
+
+	if (!bp_cfg.bPmapperEnable) {
+		PR_ERR("pmapper not enabled\r\n");
+		return -1;
+	}
+	mapper->pmapper_id = bp_cfg.sPmapper.nPmapperId;
+	mapper->mode = bp_cfg.ePmapperMappingMode;
+
+	dest = bp_cfg.sPmapper.nDestSubIfIdGroup[0];
+	if (dest == PMAPPER_DISC_CTP)
+		mapper->def_ctp = DP_PMAPPER_DISCARD_CTP;
+	else
+		mapper->def_ctp = SET_VAP(dest, info->vap_offset,
+					  info->vap_mask);
+	for (i = 0; i < DP_PMAP_PCP_NUM; i++) {
+		dest = bp_cfg.sPmapper.nDestSubIfIdGroup[1 + i];
+		if (dest == PMAPPER_DISC_CTP)
+			mapper->pcp_map[i] = DP_PMAPPER_DISCARD_CTP;
+		else
+			mapper->pcp_map[i] = SET_VAP(dest, info->vap_offset,
+						     info->vap_mask);
+	}
+	for (i = 0; i < DP_PMAP_DSCP_NUM; i++) {
+		index = 1 + DP_PMAP_PCP_NUM + i;
+		dest = bp_cfg.sPmapper.nDestSubIfIdGroup[index];
+		if (dest == PMAPPER_DISC_CTP)
+			mapper->dscp_map[i] = DP_PMAPPER_DISCARD_CTP;
+		else
+			mapper->dscp_map[i] = SET_VAP(dest, info->vap_offset,
+						      info->vap_mask);
+	}
+	return 0;
+}
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_gswip_simulate.c b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_gswip_simulate.c
new file mode 100644
index 000000000000..42f14318ee73
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_gswip_simulate.c
@@ -0,0 +1,984 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/etherdevice.h>
+#include <net/lantiq_cbm_api.h>
+#include <net/datapath_api.h>
+#include "../datapath.h"
+#include "datapath_misc.h"
+
+#define MAX_CPT_PORT 288
+u8 ctp_assign_f[MAX_CPT_PORT] = {0};
+GSW_CTP_portAssignment_t ctp_assign[MAX_CPT_PORT] = {0};
+
+#define MAX_PMAPPER 2336
+u8 pmapper_f[MAX_PMAPPER] = {0};
+
+u32  Pmapper_Alloc(int size)
+{
+	u32 i, j;
+
+	for (i = 1; i < MAX_PMAPPER; i++) {
+		if (pmapper_f[i])
+			continue;
+		for (j = 0; j < size; j++)
+			if (pmapper_f[i + j])
+				continue;
+
+		for (j = 0; j < size; j++)
+			pmapper_f[i] = 1;
+		return i;
+	}
+	return -1;
+}
+
+int Pmapper_Free(u32 offset, int size)
+{
+	int i;
+
+	for (i = 0; i < size; i++)
+		pmapper_f[i + offset] = 0;
+	return -1;
+}
+
+int CTP_PortAssignmentAlloc(GSW_CTP_portAssignment_t *param)
+{
+	int i, j;
+
+	for (i = 1; i < MAX_CPT_PORT; i++) {
+		if (ctp_assign_f[i])
+			continue;
+		for (j = 0; j < param->nNumberOfCtpPort; j++)
+			if (ctp_assign_f[i + j])
+				continue;
+		param->nFirstCtpPortId = i;
+		for (j = 0; j < param->nNumberOfCtpPort; j++) {
+			ctp_assign_f[i + j] = 1;
+			ctp_assign[i + j].eMode = param->eMode;
+			ctp_assign[i + j].nBridgePortId =
+				param->nBridgePortId;
+			ctp_assign[i + j].nFirstCtpPortId =
+				param->nFirstCtpPortId;
+			ctp_assign[i + j].nLogicalPortId =
+				param->nLogicalPortId;
+			ctp_assign[i + j].nNumberOfCtpPort =
+				param->nNumberOfCtpPort;
+		}
+		return 0;
+	}
+	return -1;
+}
+
+int CTP_PortAssignmentFree(GSW_CTP_portAssignment_t *param)
+{
+	int i;
+
+	for (i = 0; i < param->nNumberOfCtpPort; i++)
+		ctp_assign_f[i + param->nNumberOfCtpPort] = 0;
+	return 0;
+}
+
+int CTP_PortAssignmentSet(GSW_CTP_portAssignment_t *param)
+{
+	u32 idx = param->nFirstCtpPortId;
+	int i;
+
+	for (i = 0; i < param->nNumberOfCtpPort; i++) {
+		ctp_assign[idx + i].eMode = param->eMode;
+		ctp_assign[idx + i].nBridgePortId = param->nBridgePortId;
+		ctp_assign[idx + i].nFirstCtpPortId = param->nFirstCtpPortId;
+		ctp_assign[idx + i].nLogicalPortId = param->nLogicalPortId;
+		ctp_assign[idx + i].nNumberOfCtpPort = param->nNumberOfCtpPort;
+	}
+	return 0;
+}
+
+int CTP_PortAssignmentGet(GSW_CTP_portAssignment_t *param)
+{
+	int i;
+
+	for (i = 0; i < MAX_CPT_PORT; i++) {
+		if (!ctp_assign_f[i])
+			continue;
+		if (ctp_assign[i].nLogicalPortId != param->nLogicalPortId)
+			continue;
+
+		param->eMode = ctp_assign[i].eMode;
+		param->nBridgePortId = ctp_assign[i].nBridgePortId;
+		param->nFirstCtpPortId = ctp_assign[i].nFirstCtpPortId;
+		param->nNumberOfCtpPort = ctp_assign[i].nNumberOfCtpPort;
+		return 0;
+	}
+	return -1;
+}
+
+GSW_CTP_portConfig_t ctp_port_cfg[MAX_CPT_PORT] = {0};
+int CtpPortConfigSet(GSW_CTP_portConfig_t *param)
+{
+	GSW_CTP_portAssignment_t ctp_get;
+	int ret;
+	u32 ctp_port;
+
+	ctp_get.nLogicalPortId = param->nLogicalPortId;
+	ret = CTP_PortAssignmentGet(&ctp_get);
+	if (ret) {
+		PR_ERR("CTP_PortAssignmentGet fail for bp=%d\n",
+		       ctp_get.nLogicalPortId);
+		return GSW_statusErr;
+	}
+	ctp_port = ctp_get.nFirstCtpPortId  + param->nSubIfIdGroup;
+	if ((ctp_port >= MAX_CPT_PORT) ||
+	    (ctp_port < ctp_get.nFirstCtpPortId)) {
+		PR_ERR("CtpPortConfigSet wrong ctp_port %d (%d ~ %d)\n",
+		       ctp_port, ctp_get.nFirstCtpPortId, MAX_CPT_PORT);
+		return GSW_statusErr;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_BRIDGE_PORT_ID)
+		ctp_port_cfg[ctp_port].nBridgePortId = param->nBridgePortId;
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_FORCE_TRAFFIC_CLASS) {
+		ctp_port_cfg[ctp_port].nDefaultTrafficClass =
+			param->nDefaultTrafficClass;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_INGRESS_VLAN) {
+		ctp_port_cfg[ctp_port].bIngressExtendedVlanEnable =
+			param->bIngressExtendedVlanEnable;
+		ctp_port_cfg[ctp_port].nIngressExtendedVlanBlockId =
+			param->nIngressExtendedVlanBlockId;
+	}
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_INGRESS_VLAN_IGMP) {
+		ctp_port_cfg[ctp_port].bIngressExtendedVlanIgmpEnable = param->
+			bIngressExtendedVlanIgmpEnable;
+		ctp_port_cfg[ctp_port].nIngressExtendedVlanBlockIdIgmp = param->
+			nIngressExtendedVlanBlockIdIgmp;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_EGRESS_VLAN) {
+		ctp_port_cfg[ctp_port].bEgressExtendedVlanEnable = param->
+			bEgressExtendedVlanEnable;
+		ctp_port_cfg[ctp_port].nEgressExtendedVlanBlockId = param->
+			nEgressExtendedVlanBlockId;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_EGRESS_VLAN_IGMP) {
+		ctp_port_cfg[ctp_port].bEgressExtendedVlanIgmpEnable = param->
+			bEgressExtendedVlanIgmpEnable;
+		ctp_port_cfg[ctp_port].nEgressExtendedVlanBlockIdIgmp = param->
+			nEgressExtendedVlanBlockIdIgmp;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_INRESS_NTO1_VLAN)
+		ctp_port_cfg[ctp_port].bIngressNto1VlanEnable = param->
+			bIngressNto1VlanEnable;
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_EGRESS_NTO1_VLAN)
+		ctp_port_cfg[ctp_port].eMask = param->eMask;
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_INGRESS_METER) {
+		ctp_port_cfg[ctp_port].bIngressMeteringEnable = param->
+			bIngressMeteringEnable;
+		ctp_port_cfg[ctp_port].nIngressTrafficMeterId = param->
+			nIngressTrafficMeterId;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_EGRESS_METER) {
+		ctp_port_cfg[ctp_port].bEgressMeteringEnable =
+			param->bEgressMeteringEnable;
+		ctp_port_cfg[ctp_port].nEgressTrafficMeterId =
+			param->nEgressTrafficMeterId;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_BRIDGING_BYPASS) {
+		ctp_port_cfg[ctp_port].bBridgingBypass =
+			param->bBridgingBypass;
+		ctp_port_cfg[ctp_port].nDestLogicalPortId =
+			param->nDestLogicalPortId;
+		ctp_port_cfg[ctp_port].ePmapperMappingMode =
+			param->ePmapperMappingMode;
+		ctp_port_cfg[ctp_port].nDestSubIfIdGroup =
+			param->nDestSubIfIdGroup;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_INGRESS_MARKING) {
+		ctp_port_cfg[ctp_port].eIngressMarkingMode =
+			param->eIngressMarkingMode;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_EGRESS_MARKING) {
+		ctp_port_cfg[ctp_port].eEgressMarkingMode =
+			param->eEgressMarkingMode;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_EGRESS_REMARKING) {
+		ctp_port_cfg[ctp_port].eEgressRemarkingMode =
+			param->eEgressRemarkingMode;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_EGRESS_MARKING_OVERRIDE) {
+		ctp_port_cfg[ctp_port].bEgressMarkingOverrideEnable = param->
+			bEgressMarkingOverrideEnable;
+		ctp_port_cfg[ctp_port].eEgressMarkingModeOverride = param->
+			eEgressMarkingModeOverride;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_FLOW_ENTRY) {
+		ctp_port_cfg[ctp_port].nFirstFlowEntryIndex =
+			param->nFirstFlowEntryIndex;
+		ctp_port_cfg[ctp_port].nNumberOfFlowEntries =
+			param->nNumberOfFlowEntries;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_LOOPBACK_AND_MIRROR) {
+		ctp_port_cfg[ctp_port].bIngressLoopbackEnable = param->
+			bIngressLoopbackEnable;
+		ctp_port_cfg[ctp_port].bEgressLoopbackEnable =
+			param->bEgressLoopbackEnable;
+		ctp_port_cfg[ctp_port].bIngressMirrorEnable =
+			param->bIngressMirrorEnable;
+		ctp_port_cfg[ctp_port].bEgressMirrorEnable =
+			param->bEgressMirrorEnable;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_LOOPBACK_AND_MIRROR) {
+		ctp_port_cfg[ctp_port].bIngressDaSaSwapEnable = param->
+			bIngressDaSaSwapEnable;
+		ctp_port_cfg[ctp_port].bEgressDaSaSwapEnable =
+			param->bEgressDaSaSwapEnable;
+	}
+
+	return 0;
+}
+
+int CtpPortConfigGet(GSW_CTP_portConfig_t *param)
+{
+	GSW_CTP_portAssignment_t ctp_get;
+	int ret;
+	u32 ctp_port;
+
+	ctp_get.nLogicalPortId = param->nLogicalPortId;
+	ret = CTP_PortAssignmentGet(&ctp_get);
+	if (ret) {
+		PR_ERR("CtpPortConfigGet returns ERROR\n");
+		return GSW_statusErr;
+	}
+	ctp_port = ctp_get.nFirstCtpPortId  + param->nSubIfIdGroup;
+	if ((ctp_port >= MAX_CPT_PORT) ||
+	    (ctp_port < ctp_get.nFirstCtpPortId)) {
+		PR_ERR("CtpPortConfigGet wrong ctp_port %d (%d ~ %d)\n",
+		       ctp_port, ctp_get.nFirstCtpPortId, MAX_CPT_PORT);
+		return GSW_statusErr;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_BRIDGE_PORT_ID)
+		param->nBridgePortId = ctp_port_cfg[ctp_port].nBridgePortId;
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_FORCE_TRAFFIC_CLASS) {
+		param->nDefaultTrafficClass =
+			ctp_port_cfg[ctp_port].nDefaultTrafficClass;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_INGRESS_VLAN) {
+		param->bIngressExtendedVlanEnable = ctp_port_cfg[ctp_port].
+			bIngressExtendedVlanEnable;
+		param->nIngressExtendedVlanBlockId = ctp_port_cfg[ctp_port].
+			nIngressExtendedVlanBlockId;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_INGRESS_VLAN_IGMP) {
+		param->bIngressExtendedVlanIgmpEnable = ctp_port_cfg[ctp_port].
+			bIngressExtendedVlanIgmpEnable;
+		param->nIngressExtendedVlanBlockIdIgmp = ctp_port_cfg[ctp_port].
+			nIngressExtendedVlanBlockIdIgmp;
+		/*param->nIngressExtendedVlanBlockSizeIgmp =
+		 *ctp_port_cfg[ctp_port].nIngressExtendedVlanBlockSizeIgmp;
+		 */
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_EGRESS_VLAN) {
+		param->bEgressExtendedVlanEnable = ctp_port_cfg[ctp_port].
+			bEgressExtendedVlanEnable;
+		param->nEgressExtendedVlanBlockId = ctp_port_cfg[ctp_port].
+			nEgressExtendedVlanBlockId;
+		/*param->nEgressExtendedVlanBlockSize = ctp_port_cfg[ctp_port].
+		 *nEgressExtendedVlanBlockSize;
+		 */
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_EGRESS_VLAN_IGMP) {
+		param->bEgressExtendedVlanIgmpEnable = ctp_port_cfg[ctp_port].
+			bEgressExtendedVlanIgmpEnable;
+		param->nEgressExtendedVlanBlockIdIgmp = ctp_port_cfg[ctp_port].
+			nEgressExtendedVlanBlockIdIgmp;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_INRESS_NTO1_VLAN)
+		param->bIngressNto1VlanEnable = ctp_port_cfg[ctp_port].
+			bIngressNto1VlanEnable;
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_MASK_EGRESS_NTO1_VLAN)
+		param->eMask = ctp_port_cfg[ctp_port].eMask;
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_INGRESS_METER) {
+		param->bIngressMeteringEnable = ctp_port_cfg[ctp_port].
+			bIngressMeteringEnable;
+		param->nIngressTrafficMeterId = ctp_port_cfg[ctp_port].
+			nIngressTrafficMeterId;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_EGRESS_METER) {
+		param->bEgressMeteringEnable =
+			ctp_port_cfg[ctp_port].bEgressMeteringEnable;
+		param->nEgressTrafficMeterId =
+			ctp_port_cfg[ctp_port].nEgressTrafficMeterId;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_BRIDGING_BYPASS) {
+		param->bBridgingBypass =
+			ctp_port_cfg[ctp_port].bBridgingBypass;
+		param->nDestLogicalPortId =
+			ctp_port_cfg[ctp_port].nDestLogicalPortId;
+		param->ePmapperMappingMode =
+			ctp_port_cfg[ctp_port].ePmapperMappingMode;
+		param->nDestSubIfIdGroup =
+			ctp_port_cfg[ctp_port].nDestSubIfIdGroup;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_INGRESS_MARKING) {
+		param->eIngressMarkingMode =
+			ctp_port_cfg[ctp_port].eIngressMarkingMode;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_EGRESS_MARKING) {
+		param->eEgressMarkingMode =
+			ctp_port_cfg[ctp_port].eEgressMarkingMode;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_EGRESS_REMARKING) {
+		param->eEgressRemarkingMode =
+			ctp_port_cfg[ctp_port].eEgressRemarkingMode;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_EGRESS_MARKING_OVERRIDE) {
+		param->bEgressMarkingOverrideEnable = ctp_port_cfg[ctp_port].
+			bEgressMarkingOverrideEnable;
+		param->eEgressMarkingModeOverride = ctp_port_cfg[ctp_port].
+			eEgressMarkingModeOverride;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_FLOW_ENTRY) {
+		param->nFirstFlowEntryIndex =
+			ctp_port_cfg[ctp_port].nFirstFlowEntryIndex;
+		param->nNumberOfFlowEntries =
+			ctp_port_cfg[ctp_port].nNumberOfFlowEntries;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_LOOPBACK_AND_MIRROR) {
+		param->bIngressLoopbackEnable = ctp_port_cfg[ctp_port].
+			bIngressLoopbackEnable;
+		param->bEgressLoopbackEnable = ctp_port_cfg[ctp_port].
+			bEgressLoopbackEnable;
+		param->bIngressMirrorEnable = ctp_port_cfg[ctp_port].
+			bIngressMirrorEnable;
+		param->bEgressMirrorEnable = ctp_port_cfg[ctp_port].
+			bEgressMirrorEnable;
+	}
+
+	if (param->eMask & GSW_CTP_PORT_CONFIG_LOOPBACK_AND_MIRROR) {
+		param->bIngressDaSaSwapEnable = ctp_port_cfg[ctp_port].
+			bIngressDaSaSwapEnable;
+		param->bEgressDaSaSwapEnable = ctp_port_cfg[ctp_port].
+			bEgressDaSaSwapEnable;
+	}
+	return 0;
+}
+
+#define MAX_BRIDGE_PORT 120
+u8 bridge_port_f[MAX_BRIDGE_PORT];
+GSW_BRIDGE_portConfig_t bridge_port[MAX_BRIDGE_PORT];
+
+#define MAX_BRIDGE	64
+u8 bridge_f[MAX_BRIDGE_PORT];
+GSW_BRIDGE_config_t bridge[MAX_BRIDGE];
+
+int BridgePortAlloc(GSW_BRIDGE_portConfig_t *param)
+{
+	int i;
+
+	for (i = 1; i < MAX_BRIDGE_PORT; i++) {
+		if (bridge_port_f[i])
+			continue;
+		bridge_port_f[i] = 1;
+		param->nBridgePortId = i;
+		return 0;
+	}
+
+	return -1;
+}
+
+int BridgePortFree(GSW_BRIDGE_portConfig_t *param)
+{
+	bridge_port_f[param->nBridgePortId] = 0;
+	return 0;
+}
+
+int BridgeAlloc(GSW_BRIDGE_alloc_t *param)
+{
+	int i;
+
+	for (i = 1; i < MAX_BRIDGE_PORT; i++) {
+		if (bridge_f[i])
+			continue;
+		bridge_f[i] = 1;
+		param->nBridgeId = i;
+		return 0;
+	}
+
+	return -1;
+}
+
+int BridgeFree(GSW_BRIDGE_alloc_t *param)
+{
+	bridge_f[param->nBridgeId] = 0;
+	return 0;
+}
+
+int BridgePortConfigSet(GSW_BRIDGE_portConfig_t *param)
+{
+	int i = param->nBridgePortId;
+	//int k;
+	bridge_port[i].nBridgePortId = param->nBridgePortId;
+
+	/*If Bridge Port ID is invalid ,find a free Bridge
+	 *port configuration table
+	 *index and allocate
+	 *New Bridge Port configuration table index
+	 */
+	if (param->nBridgePortId >= MAX_BRIDGE_PORT) {
+		PR_ERR("wrong bridge port %d\n", param->nBridgePortId);
+		return GSW_statusErr;
+	}
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID)
+		bridge_port[i].nBridgeId = param->nBridgeId;
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_INGRESS_VLAN) {
+		bridge_port[i].bIngressExtendedVlanEnable = param->
+			bIngressExtendedVlanEnable;
+		if (param->bIngressExtendedVlanEnable)
+			bridge_port[i].nIngressExtendedVlanBlockId = param->
+				nIngressExtendedVlanBlockId;
+		/*bridge_port[i].nIngressExtendedVlanBlockSize = param->
+		 *nIngressExtendedVlanBlockSize;
+		 */
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_VLAN) {
+		bridge_port[i].bEgressExtendedVlanEnable = param->
+			bEgressExtendedVlanEnable;
+		bridge_port[i].nEgressExtendedVlanBlockId = param->
+			nEgressExtendedVlanBlockId;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_INGRESS_MARKING)
+		bridge_port[i].eIngressMarkingMode = param->eIngressMarkingMode;
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_REMARKING) {
+		bridge_port[i].eEgressRemarkingMode = param->
+			eEgressRemarkingMode;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_INGRESS_METER) {
+		bridge_port[i].bIngressMeteringEnable = param->
+			bIngressMeteringEnable;
+		bridge_port[i].nIngressTrafficMeterId = param->
+			nIngressTrafficMeterId;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_SUB_METER) {
+		memcpy(bridge_port[i].bEgressSubMeteringEnable, param->
+		       bEgressSubMeteringEnable,
+		       sizeof(bridge_port[i].bEgressSubMeteringEnable));
+		memcpy(bridge_port[i].nEgressTrafficSubMeterId, param->
+		       nEgressTrafficSubMeterId,
+		       sizeof(bridge_port[i].nEgressTrafficSubMeterId));
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_CTP_MAPPING) {
+#define PMAPPER_BLOCK_SIZE 36
+		bridge_port[i].bPmapperEnable = param->bPmapperEnable;
+		bridge_port[i].nDestLogicalPortId = param->nDestLogicalPortId;
+		bridge_port[i].ePmapperMappingMode = param->ePmapperMappingMode;
+		if (bridge_port[i].bPmapperEnable) {
+			if (!bridge_port[i].sPmapper.nPmapperId) {
+				bridge_port[i].sPmapper.nPmapperId =
+					Pmapper_Alloc(PMAPPER_BLOCK_SIZE);
+				param->sPmapper.nPmapperId =
+					bridge_port[i].sPmapper.nPmapperId;
+			}
+		} else if (bridge_port[i].sPmapper.nPmapperId)
+			Pmapper_Free(bridge_port[i].sPmapper.nPmapperId,
+				     PMAPPER_BLOCK_SIZE);
+	}
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP)
+		memcpy(bridge_port[i].nBridgePortMap, param->nBridgePortMap,
+		       sizeof(bridge_port[i].nBridgePortMap));
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_MC_DEST_IP_LOOKUP) {
+		bridge_port[i].bMcDestIpLookupDisable = param->
+			bMcDestIpLookupDisable;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_MC_SRC_IP_LOOKUP) {
+		bridge_port[i].bMcSrcIpLookupEnable = param->
+			bMcSrcIpLookupEnable;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_MC_DEST_MAC_LOOKUP) {
+		bridge_port[i].bDestMacLookupDisable = param->
+			bDestMacLookupDisable;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_MC_SRC_MAC_LEARNING) {
+		bridge_port[i].bSrcMacLearningDisable = param->
+			bSrcMacLearningDisable;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_MAC_SPOOFING) {
+		bridge_port[i].bMacSpoofingDetectEnable = param->
+			bMacSpoofingDetectEnable;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_PORT_LOCK)
+		bridge_port[i].bPortLockEnable = param->bPortLockEnable;
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_INGRESS_VLAN_FILTER) {
+		bridge_port[i].bBypassEgressVlanFilter1 = param->
+			bBypassEgressVlanFilter1;
+		bridge_port[i].bIngressVlanFilterEnable = param->
+			bIngressVlanFilterEnable;
+		bridge_port[i].nIngressVlanFilterBlockId = param->
+			nIngressVlanFilterBlockId;
+		/*bridge_port[i].nIngressVlanFilterBlockSize = param->
+		 *nIngressVlanFilterBlockSize;
+		 */
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_VLAN_FILTER1) {
+		bridge_port[i].bEgressVlanFilter1Enable = param->
+			bEgressVlanFilter1Enable;
+		bridge_port[i].nEgressVlanFilter1BlockId = param->
+			nEgressVlanFilter1BlockId;
+		/*bridge_port[i].nEgressVlanFilter1BlockSize = param->
+		 *nEgressVlanFilter1BlockSize;
+		 */
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_VLAN_FILTER2) {
+		bridge_port[i].bEgressVlanFilter2Enable = param->
+			bEgressVlanFilter2Enable;
+		bridge_port[i].nEgressVlanFilter2BlockId = param->
+			nEgressVlanFilter2BlockId;
+		/*bridge_port[i].nEgressVlanFilter2BlockSize = param->
+		 *nEgressVlanFilter2BlockSize;
+		 */
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_MAC_LEARNING_LIMIT) {
+		bridge_port[i].bMacLearningLimitEnable = param->
+			bMacLearningLimitEnable;
+		bridge_port[i].nMacLearningLimit = param->nMacLearningLimit;
+	}
+	return 0;
+}
+
+int BridgePortConfigGet(GSW_BRIDGE_portConfig_t *param)
+{
+	int i = param->nBridgePortId;
+	//int k;
+
+	if (param->nBridgePortId >= MAX_BRIDGE_PORT) {
+		PR_ERR("wrong bridge port %d\n", param->nBridgePortId);
+		return GSW_statusErr;
+	}
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID)
+		param->nBridgeId = bridge_port[i].nBridgeId;
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_INGRESS_VLAN) {
+		param->bIngressExtendedVlanEnable = bridge_port[i].
+			bIngressExtendedVlanEnable;
+
+		param->nIngressExtendedVlanBlockId = bridge_port[i].
+			nIngressExtendedVlanBlockId;
+		/*param->nIngressExtendedVlanBlockSize = bridge_port[i].
+		 *nIngressExtendedVlanBlockSize;
+		 */
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_VLAN) {
+		param->bEgressExtendedVlanEnable = bridge_port[i].
+			bEgressExtendedVlanEnable;
+		param->nEgressExtendedVlanBlockId = bridge_port[i].
+			nEgressExtendedVlanBlockId;
+		/*param->nEgressExtendedVlanBlockSize = bridge_port[i].
+		 *nEgressExtendedVlanBlockSize;
+		 */
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_INGRESS_MARKING)
+		param->eIngressMarkingMode = bridge_port[i].eIngressMarkingMode;
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_REMARKING) {
+		param->eEgressRemarkingMode = bridge_port[i].
+			eEgressRemarkingMode;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_INGRESS_METER) {
+		param->bIngressMeteringEnable = bridge_port[i].
+			bIngressMeteringEnable;
+		param->nIngressTrafficMeterId = bridge_port[i].
+			nIngressTrafficMeterId;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_SUB_METER) {
+		memcpy(param->bEgressSubMeteringEnable, bridge_port[i].
+		       bEgressSubMeteringEnable,
+		       sizeof(bridge_port[i].bEgressSubMeteringEnable));
+		memcpy(param->nEgressTrafficSubMeterId, bridge_port[i].
+		       nEgressTrafficSubMeterId,
+		       sizeof(bridge_port[i].nEgressTrafficSubMeterId));
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_CTP_MAPPING) {
+#define PMAPPER_BLOCK_SIZE 36
+		param->bPmapperEnable = bridge_port[i].bPmapperEnable;
+		param->nDestLogicalPortId = bridge_port[i].nDestLogicalPortId;
+		param->ePmapperMappingMode = bridge_port[i].ePmapperMappingMode;
+		memcpy(&param->sPmapper, &bridge_port[i].sPmapper,
+		       sizeof(param->sPmapper));
+	}
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP) {
+		memcpy(param->nBridgePortMap, bridge_port[i].nBridgePortMap,
+		       sizeof(bridge_port[i].nBridgePortMap));
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_MC_DEST_IP_LOOKUP) {
+		param->bMcDestIpLookupDisable = bridge_port[i].
+			bMcDestIpLookupDisable;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_MC_SRC_IP_LOOKUP) {
+		param->bMcSrcIpLookupEnable = bridge_port[i].
+			bMcSrcIpLookupEnable;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_MC_DEST_MAC_LOOKUP) {
+		param->bDestMacLookupDisable = bridge_port[i].
+			bDestMacLookupDisable;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_MC_SRC_MAC_LEARNING) {
+		param->bSrcMacLearningDisable = bridge_port[i].
+			bSrcMacLearningDisable;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_MAC_SPOOFING) {
+		param->bMacSpoofingDetectEnable = bridge_port[i].
+			bMacSpoofingDetectEnable;
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_PORT_LOCK)
+		param->bPortLockEnable = bridge_port[i].bPortLockEnable;
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_INGRESS_VLAN_FILTER) {
+		param->bBypassEgressVlanFilter1 = bridge_port[i].
+			bBypassEgressVlanFilter1;
+		param->bIngressVlanFilterEnable = bridge_port[i].
+			bIngressVlanFilterEnable;
+		param->nIngressVlanFilterBlockId = bridge_port[i].
+			nIngressVlanFilterBlockId;
+		/*param->nIngressVlanFilterBlockSize = bridge_port[i].
+		 *nIngressVlanFilterBlockSize;
+		 */
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_VLAN_FILTER1) {
+		param->bEgressVlanFilter1Enable = bridge_port[i].
+			bEgressVlanFilter1Enable;
+		param->nEgressVlanFilter1BlockId = bridge_port[i].
+			nEgressVlanFilter1BlockId;
+		/*param->nEgressVlanFilter1BlockSize = bridge_port[i].
+		 *nEgressVlanFilter1BlockSize;
+		 */
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_EGRESS_VLAN_FILTER2) {
+		param->bEgressVlanFilter2Enable = bridge_port[i].
+			bEgressVlanFilter2Enable;
+		param->nEgressVlanFilter2BlockId = bridge_port[i].
+			nEgressVlanFilter2BlockId;
+		/*param->nEgressVlanFilter2BlockSize = bridge_port[i].
+		 *nEgressVlanFilter2BlockSize;
+		 */
+	}
+
+	if (param->eMask & GSW_BRIDGE_PORT_CONFIG_MASK_MAC_LEARNING_LIMIT) {
+		param->bMacLearningLimitEnable = bridge_port[i].
+			bMacLearningLimitEnable;
+		param->nMacLearningLimit = bridge_port[i].nMacLearningLimit;
+	}
+	return 0;
+}
+
+int BridgeConfigSet(GSW_BRIDGE_config_t *param)
+{
+	int i = param->nBridgeId;
+
+	if (param->eMask & GSW_BRIDGE_CONFIG_MASK_MAC_LEARNING_LIMIT) {
+		bridge[i].bMacLearningLimitEnable = param->
+			bMacLearningLimitEnable;
+	}
+
+	if (param->eMask & GSW_BRIDGE_CONFIG_MASK_SUB_METER) {
+		memcpy(bridge[i].bSubMeteringEnable, param->bSubMeteringEnable,
+		       sizeof(bridge[i].bSubMeteringEnable));
+		memcpy(bridge[i].nTrafficSubMeterId, param->nTrafficSubMeterId,
+		       sizeof(bridge[i].nTrafficSubMeterId));
+	}
+
+	if (param->eMask & GSW_BRIDGE_CONFIG_MASK_FORWARDING_MODE) {
+		bridge[i].eForwardBroadcast = param->eForwardBroadcast;
+		bridge[i].eForwardUnknownUnicast = param->
+			eForwardUnknownUnicast;
+		bridge[i].eForwardUnknownMulticastNonIp = param->
+			eForwardUnknownMulticastNonIp;
+	}
+	return 0;
+}
+
+int BridgeConfigGet(GSW_BRIDGE_config_t *param)
+{
+	int i = param->nBridgeId;
+
+	if (param->eMask & GSW_BRIDGE_CONFIG_MASK_MAC_LEARNING_LIMIT) {
+		param->bMacLearningLimitEnable = bridge[i].
+			bMacLearningLimitEnable;
+	}
+
+	if (param->eMask & GSW_BRIDGE_CONFIG_MASK_SUB_METER) {
+		memcpy(param->bSubMeteringEnable, bridge[i].bSubMeteringEnable,
+		       sizeof(bridge[i].bSubMeteringEnable));
+		memcpy(param->nTrafficSubMeterId, bridge[i].nTrafficSubMeterId,
+		       sizeof(bridge[i].nTrafficSubMeterId));
+	}
+
+	if (param->eMask & GSW_BRIDGE_CONFIG_MASK_FORWARDING_MODE) {
+		param->eForwardBroadcast = bridge[i].eForwardBroadcast;
+		param->eForwardUnknownUnicast = bridge[i].
+			eForwardUnknownUnicast;
+		param->eForwardUnknownMulticastNonIp = bridge[i].
+			eForwardUnknownMulticastNonIp;
+	}
+	return 0;
+}
+
+#define MAX_EXTENDVLAN  512
+u8 extvlan_f[MAX_EXTENDVLAN] = {0};
+GSW_EXTENDEDVLAN_alloc_t extvlan_alloc[MAX_EXTENDVLAN] = {0};
+GSW_EXTENDEDVLAN_config_t extvlan[MAX_EXTENDVLAN] = {0};
+GSW_return_t ExtendedVlanAlloc(GSW_EXTENDEDVLAN_alloc_t *param)
+{
+	int i, j;
+
+	for (i = 1; i < MAX_EXTENDVLAN; i++) {
+		if (extvlan_f[i])
+			continue;
+		for (j = 0; j < param->nNumberOfEntries; j++)
+			if (extvlan_f[i + j])
+				continue;
+		param->nExtendedVlanBlockId = i;
+		for (j = 0; j < param->nNumberOfEntries; j++) {
+			extvlan_f[i + j] = 1;
+			extvlan_alloc[i + j].nExtendedVlanBlockId = i;
+			extvlan_alloc[i + j].nNumberOfEntries =  param->
+				nNumberOfEntries;
+		}
+		return 0;
+	}
+	return -1;
+}
+
+int ExtendedVlanFree(GSW_EXTENDEDVLAN_alloc_t *param)
+{
+	int i;
+
+	for (i = 0; i < extvlan_alloc[param->nExtendedVlanBlockId].
+	     nNumberOfEntries ; i++)
+		extvlan_f[i + param->nExtendedVlanBlockId] = 0;
+	return 0;
+}
+
+GSW_return_t ExtendedVlanSet(GSW_EXTENDEDVLAN_config_t *param)
+{ /*Note: not support pmapper here */
+	u32 idx = param->nExtendedVlanBlockId + param->nEntryIndex;
+
+	if (idx >= MAX_EXTENDVLAN) {
+		PR_ERR("ERROR : idx %d >= %d\n", idx, MAX_EXTENDVLAN);
+		return -1;
+	}
+
+	memcpy(&extvlan[idx].sFilter, &param->sFilter,
+	       sizeof(extvlan[idx].sFilter));
+	memcpy(&extvlan[idx].sTreatment, &param->sTreatment,
+	       sizeof(extvlan[idx].sTreatment));
+
+	return 0;
+}
+
+GSW_return_t ExtendedVlanGet(GSW_EXTENDEDVLAN_config_t *param)
+{ /*Note: not support pmapper here */
+	u32 idx = param->nExtendedVlanBlockId + param->nEntryIndex;
+
+	if (idx >= MAX_EXTENDVLAN) {
+		PR_ERR("ERROR: idx %d >= %d\n", idx, MAX_EXTENDVLAN);
+		return -1;
+	}
+	memcpy(&param->sFilter, &extvlan[idx].sFilter,
+	       sizeof(extvlan[idx].sFilter));
+	memcpy(&param->sTreatment, &extvlan[idx].sTreatment,
+	       sizeof(extvlan[idx].sTreatment));
+
+	return 0;
+}
+
+//TODO Yet to test below simulation code completely
+//#define MAC_MAX_ENTRY 4096
+#define MAC_MAX_ENTRY 10
+u8 mac_f[MAC_MAX_ENTRY];
+GSW_MAC_tableAdd_t MacAdd[MAC_MAX_ENTRY];
+
+int MacTableAdd(GSW_MAC_tableAdd_t *param)
+{
+	int i;
+
+	for (i = 0; i < MAC_MAX_ENTRY; i++) {
+		if (mac_f[i]) {
+			continue;
+		} else {
+			PR_ERR("MAC add i(%d) value\n", i);
+			mac_f[i] = 1;
+			break;
+		}
+	}
+
+	if (param->nPortId >= MAX_BRIDGE_PORT) {
+		PR_ERR("wrong bridge port for MAC add%d\n", param->nPortId);
+		return GSW_statusErr;
+	}
+
+	if (param->nPortMap) {
+		memcpy(MacAdd[i].nPortMap, param->nPortMap,
+		       sizeof(param->nPortMap));
+	}
+	MacAdd[i].nFId = param->nFId;
+	MacAdd[i].bStaticEntry = param->bStaticEntry;
+	MacAdd[i].nPortId = param->nPortId;
+	MacAdd[i].nSubIfId = param->nSubIfId;
+	memcpy(MacAdd[i].nMAC, param->nMAC, sizeof(param->nMAC));
+	PR_ERR("MAC add for entry:%d %02x:%02x:%02x:%02x:%02x:%02x\n", i,
+	       MacAdd[i].nMAC[0], MacAdd[i].nMAC[1], MacAdd[i].nMAC[2],
+	       MacAdd[i].nMAC[3], MacAdd[i].nMAC[4], MacAdd[i].nMAC[5]);
+	return 0;
+}
+
+int MacTableRemove(GSW_MAC_tableRemove_t *param)
+{
+	int i;
+
+	for (i = 0; i < MAC_MAX_ENTRY; i++) {
+		if (!mac_f[i])
+			continue;
+		if (MacAdd[i].nFId != param->nFId)
+			continue;
+		if (memcmp(MacAdd[i].nMAC, param->nMAC, sizeof(param->nMAC))) {
+			memset(&MacAdd[i], 0, sizeof(MacAdd[i]));
+			PR_ERR("MAC remove for entry:%d\n", i);
+		}
+		mac_f[i] = 0;
+		return 0;
+	}
+	return -1;
+}
+
+int MacTableQuery(GSW_MAC_tableQuery_t *param)
+{
+	int i;
+
+	for (i = 0; i < MAC_MAX_ENTRY; i++) {
+		if (!mac_f[i])
+			continue;
+		if (MacAdd[i].nFId != param->nFId)
+			continue;
+		if (memcmp(MacAdd[i].nMAC, param->nMAC, sizeof(param->nMAC))) {
+			param->nFId = MacAdd[i].nFId;
+			param->bFound = 1;
+			param->nPortId = MacAdd[i].nPortId;
+			param->nSubIfId = MacAdd[i].nSubIfId;
+			param->bStaticEntry = MacAdd[i].bStaticEntry;
+			memcpy(param->nPortMap,
+			       MacAdd[i].nPortMap,
+			       sizeof(MacAdd[i].nPortMap));
+			memcpy(param->nMAC, MacAdd[i].nMAC,
+			       sizeof(MacAdd[i].nMAC));
+
+		PR_ERR("%d    %d    %d	%02x:%02x:%02x:%02x:%02x:%02x\n",
+		       param->nFId, param->nPortId,
+		       param->bStaticEntry,
+		       param->nMAC[0], param->nMAC[1], param->nMAC[2],
+		       param->nMAC[3], param->nMAC[4], param->nMAC[5]);
+			return 0;
+		} else if (MacAdd[i].nPortId == param->nPortId) {
+			param->nFId = MacAdd[i].nFId;
+			param->bFound = 1;
+			param->nPortId = MacAdd[i].nPortId;
+			param->nSubIfId = MacAdd[i].nSubIfId;
+			memcpy(param->nPortMap,
+			       MacAdd[i].nPortMap,
+			       sizeof(MacAdd[i].nPortMap));
+			memcpy(param->nMAC, MacAdd[i].nMAC,
+			       sizeof(MacAdd[i].nMAC));
+			return 0;
+		}
+	}
+	PR_ERR("MAC Tbl query not match\n");
+	return -1;
+}
+
+int MacTableRead(GSW_MAC_tableRead_t *param)
+{
+	int i;
+
+	PR_ERR("MAC Tbl read\n");
+	for (i = 0; i < MAC_MAX_ENTRY; i++) {
+		if (!mac_f[i])
+			return -1;
+		param->nFId = MacAdd[i].nFId;
+		param->nPortId = MacAdd[i].nPortId;
+		param->nSubIfId = MacAdd[i].nSubIfId;
+		param->bStaticEntry = MacAdd[i].bStaticEntry;
+		memcpy(param->nPortMap,
+		       MacAdd[i].nPortMap,
+		       sizeof(MacAdd[i].nPortMap));
+		memcpy(param->nMAC, MacAdd[i].nMAC,
+		       sizeof(MacAdd[i].nMAC));
+	}
+	return 0;
+}
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_gswip_simulate.h b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_gswip_simulate.h
new file mode 100644
index 000000000000..a69fabc46301
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_gswip_simulate.h
@@ -0,0 +1,39 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#ifndef DATAPATH_GSWIP_SIMULATE_H_
+#define DATAPATH_GSWIP_SIMULATE_H_
+
+u32  Pmapper_Alloc(int size);
+int Pmapper_Free(u32 offset, int size);
+int CTP_PortAssignmentAlloc(GSW_CTP_portAssignment_t *param);
+int CTP_PortAssignmentFree(GSW_CTP_portAssignment_t *param);
+int CTP_PortAssignmentSet(GSW_CTP_portAssignment_t *param);
+int CTP_PortAssignmentGet(GSW_CTP_portAssignment_t *param);
+int CtpPortConfigSet(GSW_CTP_portConfig_t *param);
+int CtpPortConfigGet(GSW_CTP_portConfig_t *param);
+int BridgePortAlloc(GSW_BRIDGE_portConfig_t *param);
+int BridgePortFree(GSW_BRIDGE_portConfig_t *param);
+int BridgeAlloc(GSW_BRIDGE_alloc_t *param);
+int BridgeFree(GSW_BRIDGE_alloc_t *param);
+int BridgePortConfigSet(GSW_BRIDGE_portConfig_t *param);
+int BridgePortConfigGet(GSW_BRIDGE_portConfig_t *param);
+int BridgeConfigSet(GSW_BRIDGE_config_t *param);
+int BridgeConfigGet(GSW_BRIDGE_config_t *param);
+int MacTableAdd(GSW_MAC_tableAdd_t *param);
+int MacTableRemove(GSW_MAC_tableRemove_t *param);
+int MacTableQuery(GSW_MAC_tableQuery_t *param);
+int MacTableRead(GSW_MAC_tableAdd_t *param);
+
+GSW_return_t ExtendedVlanAlloc(GSW_EXTENDEDVLAN_alloc_t *param);
+int ExtendedVlanFree(GSW_EXTENDEDVLAN_alloc_t *param);
+GSW_return_t ExtendedVlanSet(GSW_EXTENDEDVLAN_config_t *param);
+GSW_return_t ExtendedVlanGet(GSW_EXTENDEDVLAN_config_t *param);
+#endif
+
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_lookup_proc.c b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_lookup_proc.c
new file mode 100644
index 000000000000..118898f557f9
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_lookup_proc.c
@@ -0,0 +1,644 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/version.h>
+#include <linux/if_ether.h>
+#include <linux/ethtool.h>
+#include <linux/proc_fs.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/clk.h>
+#include <linux/if_ether.h>
+#include <linux/if_vlan.h>
+
+#include <linux/clk.h>
+#include <linux/ip.h>
+#include <net/ip.h>
+
+#include <lantiq.h>
+#include <lantiq_soc.h>
+#include <net/lantiq_cbm_api.h>
+#define DATAPATH_HAL_LAYER   /*must put before include datapath_api.h in
+			      *order to avoid include another platform's
+			      *DMA descriptor and pmac header files
+			      */
+#include <net/lantiq_cbm_api.h>
+#include <net/datapath_api.h>
+#include <net/datapath_api_gswip31.h>
+#include "../datapath.h"
+#include "datapath_proc.h"
+#include "datapath_ppv4.h"
+#include "datapath_misc.h"
+#include <net/datapath_proc_api.h>
+
+#define SEQ_PRINTF seq_printf
+
+#define proc_printf(s, fmt, arg...) \
+	do { \
+		if (!s) \
+			PR_INFO(fmt, ##arg); \
+		else \
+			seq_printf(s, fmt, ##arg); \
+	} while (0)
+
+#define CARE_FLAG      0
+#define CARE_NOT_FLAG  1
+#if 1
+#define LIST_ALL_CASES(t, mask, not_care)  \
+	for (t[0] = 0;  t[0] < ((mask[0] == not_care) ? 2 : 1); t[0]++) \
+	for (t[1] = 0;  t[1] < ((mask[1] == not_care) ? 2 : 1); t[1]++) \
+	for (t[2] = 0;  t[2] < ((mask[2] == not_care) ? 2 : 1); t[2]++) \
+	for (t[3] = 0;  t[3] < ((mask[3] == not_care) ? 2 : 1); t[3]++) \
+	for (t[4] = 0;  t[4] < 1; t[4]++) \
+	for (t[5] = 0;  t[5] < 1; t[5]++) \
+	for (t[6] = 0;  t[6] < 1; t[6]++) \
+	for (t[7] = 0;  t[7] < 1; t[7]++) \
+	for (t[8] = 0;  t[8] < ((mask[8] == not_care) ? 2 : 1); t[8]++) \
+	for (t[9] = 0;  t[9] < ((mask[9] == not_care) ? 2 : 1); t[9]++) \
+	for (t[10] = 0; t[10] < ((mask[10] == not_care) ? 2 : 1); t[10]++) \
+	for (t[11] = 0; t[11] < ((mask[11] == not_care) ? 2 : 1); t[11]++) \
+	for (t[12] = 0; t[12] < ((mask[12] == not_care) ? 2 : 1); t[12]++) \
+	for (t[13] = 0; t[13] < ((mask[13] == not_care) ? 2 : 1); t[13]++)
+#else
+#define LIST_ALL_CASES(t, mask, not_care)  \
+	for (t[13] = 0; t[13] < ((mask[13] == not_care) ? 2 : 1); t[13]++) \
+	for (t[12] = 0; t[12] < ((mask[12] == not_care) ? 2 : 1); t[12]++) \
+	for (t[11] = 0; t[11] < ((mask[11] == not_care) ? 2 : 1); t[11]++) \
+	for (t[10] = 0; t[10] < ((mask[10] == not_care) ? 2 : 1); t[10]++) \
+	for (t[9] = 0;  t[9] < ((mask[9] == not_care) ? 2 : 1); t[9]++) \
+	for (t[8] = 0;  t[8] < ((mask[8] == not_care) ? 2 : 1); t[8]++) \
+	for (t[7] = 0;  t[7] < 1; t[7]++) \
+	for (t[6] = 0;  t[6] < 1; t[6]++) \
+	for (t[5] = 0;  t[5] < 1; t[5]++) \
+	for (t[4] = 0;  t[4] < 1; t[4]++) \
+	for (t[3] = 0;  t[3] < ((mask[3] == not_care) ? 2 : 1); t[3]++) \
+	for (t[2] = 0;  t[2] < ((mask[2] == not_care) ? 2 : 1); t[2]++) \
+	for (t[1] = 0;  t[1] < ((mask[1] == not_care) ? 2 : 1); t[1]++) \
+	for (t[0] = 0;  t[0] < ((mask[0] == not_care) ? 2 : 1); t[0]++)
+#endif
+
+/* The purpose of this file is to find the CBM lookup pattern and
+ * print it in the simple way.
+ * Otherway it may print up to 16K lines in the console to get lookup setting
+ *  Lookup table: flow[1] flow[0] dec end mpe2 mpe1 ep(4) class(4)
+ * Idea: We fixed the EP value during finding lookup setting pattern.
+ * method:
+ *     1st: to find the possible don't care bit from flow[2]/dec/enc/mpe2/mpe1
+ *	 and class(4), excluding ep, ie total 10 bits
+ *	       API: c_not_care_walkthrought
+ *		   Note: from big don't care bit number (ie, maximum don't
+ *		   care case) to 1 (minimal don't care case)
+ *
+ *	  2nd: generate tmp_index based on care bits
+ *	       API: list_care_combination
+ *
+ *    3rd: based on tmp_index, check whether there is really pattern which meet
+ *    don't care, ie, mapping to same qid.
+ *
+ */
+#define LOOKUP_FIELD_BITS 14
+static int lookup_mask_n;
+#define PATTERN_MATCH_INIT  0
+#define PATTERN_MATCH_START 1
+#define PATTERN_MATCH_FAIL  2
+#define PATTERN_MATCH_PASS  3
+
+#define ENTRY_FILLED 0
+#define ENTRY_USED   1
+
+static int pattern_match_flag;	/*1--start matching  2--failed, 3---match 0k */
+static unsigned char lookup_mask1[LOOKUP_FIELD_BITS];
+
+#define C_ARRAY_SIZE  20
+static int c_tmp_data[C_ARRAY_SIZE];
+
+/*store result */
+#define MAX_PATTERN_NUM 1024
+static int lookup_match_num;
+static unsigned short lookup_match_mask[MAX_PATTERN_NUM];
+/*save tmp_index */
+static unsigned short lookup_match_index[MAX_PATTERN_NUM];
+/*save tmp_index */
+static unsigned char lookup_match_qid[MAX_PATTERN_NUM];
+
+static int tmp_pattern_port_id;
+
+static int left_n;
+/*10 bits lookup table except 4 bits EP */
+static unsigned char lookup_tbl_flags[MAX_PATTERN_NUM * 16];
+
+static void combine_util(int *arr, int *data, int start, int end, int index,
+			 int r);
+static int check_pattern(int *data, int r);
+static void lookup_table_via_qid(int qid);
+static void lookup_table_remap(int old_q, int new_q);
+static int find_pattern(int port_id, struct seq_file *s, int qid);
+static int get_dont_care_lookup(char *s);
+static void lookup_table_recursive(int k, int tmp_index, int set_flag, int qid);
+
+/* The main function that prints all combinations of size r*/
+/* in arr[] of size n. This function mainly uses combine_util()*/
+static void c_not_care_walkthrought(int *arr, int n, int r)
+{
+	/* A temporary array data[] to store all combination one by one */
+
+	/* Print all combination using temprary array 'data[]' */
+	combine_util(arr, c_tmp_data, 0, n - 1, 0, r);
+}
+
+/* arr[]  ---> Input Array
+ * data[] ---> Temporary array to store current combination
+ * start & end ---> Staring and Ending indexes in arr[]
+ * index  ---> Current index in data[]
+ * r ---> Size of a combination to be printed
+ *
+ */
+static void combine_util(int *arr, int *data, int start, int end, int index,
+			 int r)
+{
+	int i;
+
+	/* Current combination is ready to be printed, print it */
+	if (left_n <= 0)
+		return;
+	if (index == r) {/*Find one pattern with specified don't care flag */
+
+		check_pattern(data, r);
+		/*find a don't care case and need further check */
+
+		return;
+	}
+	/* replace index with all possible elements. The condition */
+	/* "end-i+1 >= r-index" makes sure that including one element */
+	/* at index will make a combination with remaining elements */
+	/* at remaining positions */
+	for (i = start; i <= end && end - i + 1 >= r - index; i++) {
+		data[index] = arr[i];
+		if (left_n <= 0)
+			break;
+		combine_util(arr, data, i + 1, end, index + 1, r);
+	}
+}
+
+/*Note: when call this API, for those cared bits,
+ * its value already set in tmp_index.
+ */
+static void lookup_pattern_match(int tmp_index)
+{
+	int i;
+	int qid;
+	static int first_qid;
+	int t[LOOKUP_FIELD_BITS] = { 0 };
+	int index;
+
+	DP_DEBUG(DP_DBG_FLAG_LOOKUP,
+		 "trying with tmp_index=0x%x with lookup_match_num=%d\n",
+		 tmp_index, lookup_match_num);
+	pattern_match_flag = PATTERN_MATCH_INIT;
+	lookup_match_index[lookup_match_num] = tmp_index;
+
+	LIST_ALL_CASES(t, lookup_mask1, CARE_NOT_FLAG) {
+		index = tmp_index;
+		for (i = 0; i < LOOKUP_FIELD_BITS; i++)
+			index |= (t[i] << i);
+		DP_DEBUG(DP_DBG_FLAG_LOOKUP, "don't care[14]=");
+		for (i = 0; i < LOOKUP_FIELD_BITS; i++)
+			DP_DEBUG(DP_DBG_FLAG_LOOKUP, "%d ", t[i]);
+		DP_DEBUG(DP_DBG_FLAG_LOOKUP, "\n");
+
+		DP_DEBUG(DP_DBG_FLAG_LOOKUP, "don't care index=%x\n", index);
+
+		if (lookup_tbl_flags[index] == ENTRY_USED) {
+			pattern_match_flag = PATTERN_MATCH_FAIL;
+			goto END;
+		}
+
+		qid = get_lookup_qid_via_index(index);
+
+		if (pattern_match_flag == PATTERN_MATCH_INIT) {
+			pattern_match_flag = PATTERN_MATCH_START;
+			first_qid = qid;
+		} else if (first_qid != qid) {
+			pattern_match_flag = PATTERN_MATCH_FAIL;
+			DP_DEBUG(DP_DBG_FLAG_LOOKUP,
+				 "first_qid(%d) != qid(%d)\n",
+				 first_qid, qid);
+			goto END;
+		}
+	}
+
+END:
+	/*save the result if necessary here */
+	if (pattern_match_flag == PATTERN_MATCH_START) {
+		/*pass since still not fail yet */
+		pattern_match_flag = PATTERN_MATCH_PASS;
+
+		/*mark the entries */
+		LIST_ALL_CASES(t, lookup_mask1, CARE_NOT_FLAG) {
+			index = tmp_index;
+			for (i = 0; i < LOOKUP_FIELD_BITS; i++)
+				index |= (t[i] << i);
+			if (lookup_tbl_flags[index] == ENTRY_USED)
+				PR_ERR("why already used\n");
+			else
+				lookup_tbl_flags[index] = ENTRY_USED;
+		}
+		/*save status */
+		lookup_match_qid[lookup_match_num] = first_qid;
+		lookup_match_mask[lookup_match_num] = 0;
+		for (i = 0; i < LOOKUP_FIELD_BITS; i++)
+			if (lookup_mask1[i])
+				lookup_match_mask[lookup_match_num] |= 1 << i;
+		lookup_match_num++;
+		DP_DEBUG(DP_DBG_FLAG_LOOKUP,
+			 "left_n=%d lookup_mask_n=%d. Need reduce=%d\n",
+			 left_n, lookup_mask_n, (1 << lookup_mask_n));
+		left_n -= (1 << lookup_mask_n);
+	} else {
+		/*failed */
+	}
+}
+
+/*k--number of don't care flags
+ */
+static int list_care_combination(int tmp_index)
+{
+	int i, k, index;
+	int t[14] = { 0 };
+
+	LIST_ALL_CASES(t, lookup_mask1, CARE_FLAG) {
+		index = tmp_index;
+		for (i = 0; i < LOOKUP_FIELD_BITS; i++)
+			index |= (t[i] << i);
+		DP_DEBUG(DP_DBG_FLAG_LOOKUP, "care index=%x\n", index);
+		DP_DEBUG(DP_DBG_FLAG_LOOKUP, "care t[14]=");
+		for (k = 0; k < LOOKUP_FIELD_BITS; k++)
+			DP_DEBUG(DP_DBG_FLAG_LOOKUP, "%d ", t[k]);
+		DP_DEBUG(DP_DBG_FLAG_LOOKUP, "\n");
+		lookup_pattern_match(index);
+	}
+
+	return 0;
+}
+
+/*based on the don't care list, we try to find the all possible pattern:
+ *for example: bit 13 and bit 11 don't care.
+ *data---the flag index list which is don't care
+ *r -- the flag index length
+ */
+static int check_pattern(int *data, int r)
+{
+	int i;
+
+	memset(lookup_mask1, 0, sizeof(lookup_mask1));
+	DP_DEBUG(DP_DBG_FLAG_LOOKUP, "data:");
+	for (i = 0; i < r; i++) {
+		DP_DEBUG(DP_DBG_FLAG_LOOKUP, "%d ", data[i]);
+		lookup_mask1[data[i]] = CARE_NOT_FLAG;
+	}
+	lookup_mask_n = r;
+	pattern_match_flag = 0;
+	DP_DEBUG(DP_DBG_FLAG_LOOKUP, "\n");
+
+	DP_DEBUG(DP_DBG_FLAG_LOOKUP, "Don't care flag: ");
+	for (i = 0; i < LOOKUP_FIELD_BITS; i++)
+		DP_DEBUG(DP_DBG_FLAG_LOOKUP, "%c ",
+			 lookup_mask1[i] ? 'x' : '0');
+	DP_DEBUG(DP_DBG_FLAG_LOOKUP, "\n");
+
+	list_care_combination(tmp_pattern_port_id << 4);
+	return 0;
+}
+
+/*qid: -1: match all queues
+ *      >=0: only match the specified queue
+ */
+int find_pattern(int port_id, struct seq_file *s, int qid)
+{
+	int r, i, j, n;
+	int f = 0;
+	char *flag_s;
+	char flag_buf[40];
+	int deq_port;
+	int arr[] = {13, 12, 11, 10, 9, 8, /*7,6,5,4, */ 3, 2, 1, 0 };
+	int inst = 0;
+	struct hal_priv *priv = (struct hal_priv *)dp_port_prop[inst].priv_hal;
+
+	left_n = 1 << (LOOKUP_FIELD_BITS - 4);	/*maximum lookup entried */
+	lookup_match_num = 0;
+	tmp_pattern_port_id = port_id;
+	memset(lookup_tbl_flags, 0, sizeof(lookup_tbl_flags));
+	n = ARRAY_SIZE(arr);
+	/*list all pattern, ie, don't care numbers from 10 to 1 */
+	for (r = n; r >= 0; r--) {
+		if (left_n <= 0)
+			break;
+		c_not_care_walkthrought(arr, n, r);
+		DP_DEBUG(DP_DBG_FLAG_LOOKUP, "left_n=%d\n", left_n);
+		if (!left_n)
+			break;
+	}
+
+	for (i = 0; i < lookup_match_num; i++) {
+		if ((qid >= 0) && (qid != lookup_match_qid[i]))
+			continue;
+		if (!f) {
+			f = 1;
+			proc_printf(s,
+				    "EP%-2d:%5s%5s%5s%5s%5s%5s%5s%5s%5s%5s%5s%5s%5s%5s%5s%5s\n",
+				    tmp_pattern_port_id, "F2", "F1",
+				    "DEC", "ENC", "MPE2", "MPE1", "EP3",
+				    "EP2", "EP1", "EP0", "C3", "C2", "C1",
+				    "C0", "qid", "id");
+		}
+		deq_port = priv->qos_queue_stat[lookup_match_qid[i]].deq_port;
+		flag_s = get_dma_flags_str31(deq_port, flag_buf,
+					     sizeof(flag_buf));
+
+		proc_printf(s, "    ");
+		for (j = LOOKUP_FIELD_BITS - 1; j >= 0; j--) {
+			if ((lookup_match_mask[i] >> j) & 1)
+				proc_printf(s, "%5c", 'x');
+			else
+				proc_printf(s, "%5d",
+					    (lookup_match_index[i] >> j) &
+					    1);
+		}
+		proc_printf(s, "->%-3d(0x%04x)%s\n", lookup_match_qid[i],
+			    lookup_match_index[i], flag_s);
+	}
+	if (s && seq_has_overflowed(s))
+		return -1;
+
+	return 0;
+}
+
+int lookup_start31(void)
+{
+	return 0;
+}
+
+int lookup_dump31(struct seq_file *s, int pos)
+{
+	if (find_pattern(pos, s, -1) < 0)
+		return pos;
+	pos++;
+	if (pos >= 16)
+		pos = -1;
+	return pos;
+}
+
+ssize_t proc_get_qid_via_index31(struct file *file, const char *buf,
+				 size_t count, loff_t *ppos)
+{
+	int err = 0, len = 0;
+	char data[100];
+	unsigned int lookup_index;
+	unsigned int qid = 0;
+	char *param_list[10];
+	int num;
+
+	len = (count >= sizeof(data)) ? (sizeof(data) - 1) : count;
+	DP_DEBUG(DP_DBG_FLAG_LOOKUP, "len=%d\n", len);
+
+	if (len <= 0) {
+		err = -EFAULT;
+		PR_ERR("Wrong len value (%d)\n", len);
+		return count;
+	}
+
+	if (copy_from_user(data, buf, len)) {
+		err = -EFAULT;
+		PR_ERR("copy_from_user fail");
+		return count;
+	}
+
+	data[len - 1] = 0; /* Make string */
+	num = dp_split_buffer(data, param_list, ARRAY_SIZE(param_list));
+
+	if (num <= 1)
+		goto help;
+	if (!param_list[1])
+		goto help;
+
+	lookup_index = dp_atoi(param_list[1]);
+
+	if ((dp_strncmpi(param_list[0], "set", strlen("set")) == 0) ||
+	    (dp_strncmpi(param_list[0], "write", strlen("write")) == 0)) {
+		if (!param_list[2]) {
+			PR_ERR("wrong command\n");
+			return count;
+		}
+		qid = dp_atoi(param_list[2]);
+		/*workaround for mask support */
+		if (get_dont_care_lookup(param_list[1]) == 0) {
+			lookup_table_recursive(LOOKUP_FIELD_BITS - 1, 0, 1,
+					       qid);
+			return count;
+		}
+		PR_INFO("Set to queue[%u] done\n", qid);
+		set_lookup_qid_via_index(lookup_index, qid);
+		return count;
+	} else if ((dp_strncmpi(param_list[0], "get", strlen("get")) == 0) ||
+		   (dp_strncmpi(param_list[0], "read", strlen("read")) == 0)) {
+		if (get_dont_care_lookup(param_list[1]) == 0) {
+			lookup_table_recursive(LOOKUP_FIELD_BITS - 1, 0, 0,
+					       0);
+			return count;
+		}
+		qid = get_lookup_qid_via_index(lookup_index);
+		PR_INFO("Get lookup[%05u 0x%04x] ->     queue[%u]\n",
+			lookup_index, lookup_index, qid);
+		return count;
+	} else if (dp_strncmpi(param_list[0], "find", strlen("find")) == 0) {
+		/*read out its all flags for specified qid */
+		int i;
+
+		qid = dp_atoi(param_list[1]);
+		for (i = 0; i < 16; i++)
+			find_pattern(i, NULL, qid);
+		return count;
+	} else if (dp_strncmpi(param_list[0], "find2", strlen("find2")) == 0) {
+		/*read out its all flags for specified qid */
+		qid = dp_atoi(param_list[1]);
+		lookup_table_via_qid(qid);
+		return count;
+	} else if (dp_strncmpi(param_list[0], "remap", strlen("remap")) == 0) {
+		int old_q = dp_atoi(param_list[1]);
+		int new_q = dp_atoi(param_list[2]);
+
+		lookup_table_remap(old_q, new_q);
+		PR_INFO("remap queue[%d] to queue[%d] done\n",
+			old_q, new_q);
+		return count;
+	}
+
+	goto help;
+help:
+	PR_INFO("Usage: echo set lookup_flags queue_id > /proc/dp/lookup\n");
+	PR_INFO("     : echo get lookup_flags > /proc/dp/lookup\n");
+	PR_INFO("     : echo find  <x> > /proc/dp/lookup\n");
+	PR_INFO("     : echo find2 <x> > /proc/dp/lookup\n");
+	PR_INFO("     : echo remap <old_q> <new_q> > /proc/dp/lookup\n");
+	PR_INFO("  Hex example: echo set 0x10 10 > /proc/dp/lookup\n");
+	PR_INFO("  Dec:example: echo set 16 10 > /proc/dp/lookup\n");
+	PR_INFO("  Bin:example: echo set b10000 10 > /proc/dp/lookup\n");
+
+	PR_INFO("%s: echo set b1xxxx 10 > /proc/dp/lookup\n",
+		"Special for BIN(Don't care bit)");
+	PR_INFO("Lookup format:\n");
+	PR_INFO("  Bits Index: | %s\n",
+		"13   12 |  11  |  10  |  9   |  8   |7   4 | 3   0 |");
+	PR_INFO("  Fields:     | %s\n",
+		"Flow ID | DEC  | ENC  | MPE2 | MPE1 |  EP  | CLASS |");
+	return count;
+}
+
+void lookup_table_via_qid(int qid)
+{
+	u32 index, tmp, i, j, k, f = 0;
+
+	DP_DEBUG(DP_DBG_FLAG_LOOKUP,
+		 "Try to find all lookup flags mapped to qid %d\n", qid);
+	for (i = 0; i < 16; i++) {	/*ep */
+		for (j = 0; j < 16; j++) {	/*class */
+			for (k = 0; k < 64; k++) {/*flow id/dec/enc/mpe2/mpe1 */
+				index = (k << 8) | (i << 4) | j;
+				tmp = get_lookup_qid_via_index(index);
+				if (tmp != qid)
+					continue;
+				f = 1;
+				PR_INFO("Get lookup[%05u 0x%04x]%s[%d]\n",
+					index, index,
+					" ->     queue", qid);
+			}
+		}
+	}
+	if (!f)
+		PR_ERR("No mapping to queue id %d yet ?\n", qid);
+}
+
+void lookup_table_remap(int old_q, int new_q)
+{
+	u32 index, tmp, i, j, k, f = 0;
+
+	DP_DEBUG(DP_DBG_FLAG_LOOKUP,
+		 "Try to remap lookup flags mapped from old_q %d to new_q %d\n",
+		 old_q, new_q);
+	for (i = 0; i < 16; i++) {	/*ep */
+		for (j = 0; j < 16; j++) {	/*class */
+			for (k = 0; k < 64; k++) {/*flow id/dec/enc/mpe2/mpe1 */
+				index = (k << 8) | (i << 4) | j;
+				tmp = get_lookup_qid_via_index(index);
+				if (tmp != old_q)
+					continue;
+				set_lookup_qid_via_index(index, new_q);
+				f = 1;
+				DP_DEBUG(DP_DBG_FLAG_LOOKUP,
+					 "Remap lookup[%05u 0x%04x] %s[%d]\n",
+					 index, index,
+					 "->     queue", new_q);
+			}
+		}
+	}
+	if (!f)
+		PR_INFO("No mapping to queue id %d yet\n", new_q);
+}
+
+#define LOOKUP_FIELD_BITS 14
+static u8 lookup_flags2[LOOKUP_FIELD_BITS];
+static u8 lookup_mask2[LOOKUP_FIELD_BITS];
+
+/*return 0: get correct bit mask
+ * -1: no
+ */
+int get_dont_care_lookup(char *s)
+{
+	int len, i, j;
+	int flag = 0;
+
+	if (!s)
+		return -1;
+	len = strlen(s);
+	dp_replace_ch(s, strlen(s), ' ', 0);
+	dp_replace_ch(s, strlen(s), '\r', 0);
+	dp_replace_ch(s, strlen(s), '\n', 0);
+	if (s[0] == 0)
+		return -1;
+	memset(lookup_flags2, 0, sizeof(lookup_flags2));
+	memset(lookup_mask2, 0, sizeof(lookup_mask2));
+	if ((s[0] != 'b') && (s[0] != 'B'))
+		return -1;
+
+	if (len >= LOOKUP_FIELD_BITS + 1)
+		len = LOOKUP_FIELD_BITS + 1;
+	for (i = len - 1, j = 0; i >= 1; i--, j++) {
+		if ((s[i] == 'x') || (s[i] == 'X')) {
+			lookup_mask2[j] = 1;
+			flag = 1;
+		} else if (('0' <= s[i]) && (s[i] <= '9')) {
+			lookup_flags2[j] = s[i] - '0';
+		} else if (('A' <= s[i]) && (s[i] <= 'F')) {
+			lookup_flags2[j] = s[i] - 'A' + 10;
+		} else if (('a' <= s[i]) && (s[i] <= 'f')) {
+			lookup_flags2[j] = s[i] - '1' + 10;
+		} else {
+			return -1;
+		}
+	}
+	if (flag) {
+		DP_DEBUG(DP_DBG_FLAG_LOOKUP, "\nGet lookup flag: ");
+		for (i = LOOKUP_FIELD_BITS - 1; i >= 0; i--) {
+			if (lookup_mask2[i])
+				DP_DEBUG(DP_DBG_FLAG_LOOKUP, "x");
+			else
+				DP_DEBUG(DP_DBG_FLAG_LOOKUP, "%d",
+					 lookup_flags2[i]);
+		}
+		DP_DEBUG(DP_DBG_FLAG_LOOKUP, "\n");
+
+		return 0;
+	} else {
+		return -1;
+	}
+}
+
+void lookup_table_recursive(int k, int tmp_index, int set_flag, int qid)
+{
+	int i;
+
+	if (k < 0) {	/*finish recursive and start real read/set action */
+		if (set_flag) {
+			set_lookup_qid_via_index(tmp_index, qid);
+			DP_DEBUG(DP_DBG_FLAG_LOOKUP,
+				 "Set lookup[%05u/0x%04x] ->     queue[%d]\n",
+				 tmp_index, tmp_index, qid);
+		} else {
+			qid = get_lookup_qid_via_index(tmp_index);
+			PR_INFO("Get lookup[%05u/0x%04x] ->     queue[%d]\n",
+				tmp_index, tmp_index, qid);
+		}
+		return;
+	}
+
+	if (lookup_mask2[k]) {
+		for (i = 0; i < 2; i++)
+			lookup_table_recursive(k - 1, tmp_index + (i << k),
+					       set_flag, qid);
+		return;
+	}
+
+	lookup_table_recursive(k - 1, tmp_index + (lookup_flags2[k] << k),
+			       set_flag, qid);
+}
+
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_mib.c b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_mib.c
new file mode 100644
index 000000000000..b09697b493b9
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_mib.c
@@ -0,0 +1,388 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+#include<linux/init.h>
+#include<linux/module.h>
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/version.h>
+#include <linux/timer.h>
+#include <linux/skbuff.h>
+#include <linux/if_ether.h>
+#include <linux/ethtool.h>
+#include <linux/proc_fs.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/clk.h>
+#include <linux/if_ether.h>
+#include <linux/clk.h>
+
+#include <lantiq_soc.h>
+#include <net/lantiq_cbm_api.h>
+#include <net/datapath_api.h>
+#include <net/datapath_proc_api.h>
+#include "../datapath.h"
+#include <net/lantiq_cbm_api.h>
+
+#define WRAPAROUND32   0xFFFFFFFF
+/*timer interval for mib wraparound handling:
+ * Most mib counter is 32 bits, ie, maximu ix 0xFFFFFFFF
+ * one pmac port maximum (cpu port) can support less than 3G, ie,
+ * 1488096 * 3 packets for 64 bytes case. so the time to have 1 wrapround is:
+ * 0xFFFFFFFF / (1488096 * 3) = 962 seconds
+ * If each timer check one port and its subif, then 962/16 = 60 sec.
+ */
+#define POLL_INTERVAL (60 * HZ)
+#define WAN_EP          15	/*WAN Interface's EP value */
+#define MAX_RMON_ITF    256	/*maximum 256 GSW RMON interface supported */
+
+struct mibs_port {
+	u64 rx_good_bytes;
+	u64 rx_bad_bytes;
+	u64 rx_good_pkts;
+	u64 rx_drop_pkts;
+	/*For eth0_x only, for all others, must keep it
+	 * to zero in order to share same algo
+	 */
+	u64 rx_drop_pkts_pae;
+	u64 rx_disc_pkts_redir;	/*for eth1 only */
+	u64 rx_fcs_err_pkts;
+	u64 rx_undersize_good_pkts;
+	u64 rx_oversize_good_pkts;
+	u64 rx_undersize_err_pkts;
+	u64 rx_oversize_err_pkts;
+	u64 rx_align_err_pkts;
+	u64 rx_filter_pkts;
+
+	u64 tx_good_bytes;
+	u64 tx_good_pkts;
+	u64 tx_drop_pkts;
+	/*For eth0_x only, for all others, must keep it
+	 *to zero in order to share same algo
+	 */
+	u64 tx_drop_pkts_pae;
+	u64 tx_acm_drop_pkts;
+	u64 tx_acm_drop_pkts_pae;	/*for eth0_x only */
+	u64 tx_disc_pkts_redir;	/*for eth1 only */
+	u64 tx_coll_pkts;
+	u64 tx_coll_pkts_pae;	/*for eth0_x only */
+	u64 tx_pkts_redir;	/*for eth1 only */
+	u64 tx_bytes_redir;	/*for eth1 only */
+	u64 rx_fcs_err_pkts_pae;
+	u64 rx_undersize_err_pkts_pae;
+	u64 rx_oversize_err_pkts_pae;
+	u64 rx_align_err_pkts_pae;
+
+	/*driver related */
+	u64 rx_drv_drop_pkts;
+	u64 rx_drv_error_pkts;
+	u64 tx_drv_drop_pkts;
+	u64 tx_drv_error_pkts;
+
+	/*for DSL ATM only */
+	u64 tx_drv_pkts;
+	u64 rx_drv_pkts;
+	u64 tx_drv_bytes;
+	u64 rx_drv_bytes;
+};
+
+struct mib_vap {
+	u64 rx_pkts_itf;
+	u64 rx_disc_pkts_itf;
+	u64 rx_disc_pkts_drv;
+	u64 tx_pkts_itf;
+	u64 tx_disc_pkts_itf;
+	u64 tx_disc_pkts_drv;
+};
+
+struct port_mib {
+	struct mibs_port curr;/*tmp variable used for mib counter calculation */
+	struct mib_vap curr_vap[MAX_SUBIF_PER_PORT];	/*for future */
+};
+
+struct mibs_low_lvl_port {
+	GSW_RMON_Port_cnt_t l;          /*only for ethernet LAN ports */
+	GSW_RMON_Port_cnt_t r;
+	GSW_RMON_Redirect_cnt_t redir; /*only for ethernet WAN port */
+	dp_drv_mib_t drv;
+};
+
+struct mibs_low_lvl_vap {
+	GSW_RMON_If_cnt_t gsw_if; /*for pae only since GSW-L not
+				   *support interface mib
+				   */
+	dp_drv_mib_t drv;
+};
+
+static unsigned int proc_mib_vap_start_id = 1;
+static unsigned int proc_mib_vap_end_id = PMAC_MAX_NUM - 1;
+static spinlock_t dp_mib_lock;
+static unsigned long poll_interval = POLL_INTERVAL;
+
+/*save port based lower level last mib counter
+ * for wraparound checking
+ */
+struct mibs_low_lvl_port last[PMAC_MAX_NUM];
+/*save vap/sub interface based lower level last mib counter
+ * for wraparound checking
+ */
+struct mibs_low_lvl_vap last_vap[PMAC_MAX_NUM][MAX_SUBIF_PER_PORT];
+/*Save all necessary aggregated basic MIB */
+static struct port_mib aggregate_mib[PMAC_MAX_NUM];
+/*For PAE CPU port only */
+static struct port_mib aggregate_mib_r[1];
+
+#define THREAD_MODE
+
+#ifdef THREAD_MODE
+#include <linux/kthread.h>
+struct task_struct *thread;
+#else
+static struct timer_list exp_timer;	/*timer setting */
+#endif
+
+/*internal API: update local net mib counters periodically */
+static int update_port_mib_lower_lvl(dp_subif_t *subif, u32 flag);
+static int update_vap_mib_lower_lvl(dp_subif_t *subif, u32 flag);
+
+/* ----- API implementation ------- */
+static u64 wraparound(u64 curr, u64 last, u32 size)
+{
+#define WRAPAROUND_MAX_32 0xFFFFFFFF
+
+	if ((size > 4) || /*for 8 bytes(64bit mib),no need to do wraparound*/
+	    (curr >= last))
+		return curr - last;
+
+	return ((u64)WRAPAROUND_MAX_32) + (u64)curr - last;
+}
+
+static int port_mib_wraparound(u32 ep, struct mibs_low_lvl_port *curr,
+			       struct mibs_low_lvl_port *last)
+{
+	return 0;
+}
+
+static int vap_mib_wraparound(dp_subif_t *subif,
+			      struct mibs_low_lvl_vap *curr,
+			      struct mibs_low_lvl_vap *last)
+{
+	return 0;
+}
+
+static int get_gsw_port_rmon(u32 ep, char *gsw_drv_name,
+			     GSW_RMON_Port_cnt_t *mib)
+{
+	return 0;
+}
+
+static int get_gsw_redirect_rmon(u32 ep, int index,
+				 GSW_RMON_Redirect_cnt_t *mib)
+{
+	return 0;
+}
+
+static int get_gsw_itf_rmon(u32 index, int index,
+			    GSW_RMON_If_cnt_t *mib)
+{
+	return 0;
+}
+
+int get_gsw_interface_base(int port_id)
+{
+	return 0;
+}
+
+/* if ethernet WAN redirect is enabled, return 1,
+ * else return 0
+ */
+int gsw_eth_wan_redirect_status(void)
+{
+	return 0;
+}
+
+/*Note:
+ * Update mib counter for physical port only
+ * flag so far no much use only
+ */
+static int update_port_mib_lower_lvl(dp_subif_t *subif, u32 flag)
+{
+	return 0;
+}
+
+static void mib_wraparound_timer_poll(unsigned long data)
+{
+}
+
+static int update_vap_mib_lower_lvl(dp_subif_t *subif, u32 flag)
+{
+	return 0;
+}
+
+int dp_reset_sys_mib(u32 flag)
+{
+	return 0;
+}
+
+void proc_mib_timer_read(struct seq_file *s)
+{
+	seq_printf(s, "\nMib timer interval is %u sec\n",
+		   (unsigned int)poll_interval / HZ);
+}
+
+ssize_t proc_mib_timer_write(struct file *file, const char *buf, size_t count,
+			     loff_t *ppos)
+{
+	int len, num;
+	char str[64];
+	char *param_list[2];
+#define MIN_POLL_TIME 2
+	len = (sizeof(str) > count) ? count : sizeof(str) - 1;
+	len -= copy_from_user(str, buf, len);
+	str[len] = 0;
+	num = dp_split_buffer(str, param_list, ARRAY_SIZE(param_list));
+	poll_interval = dp_atoi(param_list[0]);
+
+	if (poll_interval < MIN_POLL_TIME)
+		poll_interval = MIN_POLL_TIME;
+
+	poll_interval *= HZ;
+#ifndef THREAD_MODE
+	mod_timer(&exp_timer, jiffies + poll_interval);
+#endif
+	PR_INFO("new poll_interval=%u sec\n",
+		(unsigned int)poll_interval / HZ);
+	return count;
+}
+
+static unsigned int proc_mib_port_start_id = 1;
+static unsigned int proc_mib_port_end_id = PMAC_MAX_NUM - 1;
+int proc_mib_inside_dump(struct seq_file *s, int pos)
+{
+	return -1;
+}
+
+int proc_mib_inside_start(void)
+{
+	return proc_mib_port_start_id;
+}
+
+ssize_t proc_mib_inside_write(struct file *file, const char *buf,
+			      size_t count, loff_t *ppos)
+{
+	return count;
+}
+
+/*Note:
+ *if (flag & DP_F_STATS_SUBIF), get sub-interface/vap mib only
+ *otherwise, get physical port's mib
+ */
+int dp_get_port_vap_mib_31(dp_subif_t *subif, void *priv,
+			   struct rtnl_link_stats64 *net_mib, u32 flag)
+{
+	return -1;
+}
+
+/*Clear GSW Interface MIB: only for sub interface/vap only  */
+int clear_gsw_itf_mib(dp_subif_t *subif, u32 flag)
+{
+	return 0;
+}
+
+int dp_clear_netif_mib_31(dp_subif_t *subif, void *priv, u32 flag)
+{
+	return 0;
+}
+
+int proc_mib_port_start(void)
+{
+	return 0;
+}
+
+int proc_mib_port_dump(struct seq_file *s, int pos)
+{
+	return -1;
+}
+
+ssize_t proc_mib_port_write(struct file *file, const char *buf, size_t count,
+			    loff_t *ppos)
+{
+	return count;
+}
+
+int proc_mib_vap_dump(struct seq_file *s, int pos)
+{
+	return -1;
+}
+
+int proc_mib_vap_start(void)
+{
+	return proc_mib_vap_start_id;
+}
+
+ssize_t proc_mib_vap_write(struct file *file, const char *buf, size_t count,
+			   loff_t *ppos)
+{
+	return count;
+}
+
+#ifdef THREAD_MODE
+int mib_wraparound_thread(void *data)
+{
+	while (1) {
+		mib_wraparound_timer_poll(0);
+		msleep(poll_interval / HZ * 1000 / PMAC_MAX_NUM / 2);
+		DP_DEBUG(DP_DBG_FLAG_MIB, "mib_wraparound_thread\n");
+	}
+}
+#endif
+
+int adjust_itf(void)
+{
+	return 0;
+}
+
+int set_gsw_itf(u8 ep, u8 ena, int start)
+{
+	return 0;
+}
+
+int reset_gsw_itf(u8 ep)
+{
+}
+
+int dp_mib_init(u32 flag)
+{
+	spin_lock_init(&dp_mib_lock);
+	memset(&aggregate_mib, 0, sizeof(aggregate_mib));
+	memset(&last, 0, sizeof(last));
+	memset(&last_vap, 0, sizeof(last_vap));
+	adjust_itf();
+
+#ifdef THREAD_MODE
+	thread = kthread_run(mib_wraparound_thread, 0, "dp_mib");
+#else
+	init_timer_on_stack(&exp_timer);
+	exp_timer.expires = jiffies + poll_interval;
+	exp_timer.data = 0;
+	exp_timer.function = mib_wraparound_timer_poll;
+	add_timer(&exp_timer);
+	PR_INFO("dp_mib_init done\n");
+#endif
+	return 0;
+}
+
+void dp_mib_exit(void)
+{
+#ifdef THREAD_MODE
+	if (thread)
+		kthread_stop(thread);
+#else
+	del_timer(&exp_timer);
+#endif
+}
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_mib.h b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_mib.h
new file mode 100644
index 000000000000..ad14817c7895
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_mib.h
@@ -0,0 +1,19 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#ifndef DATAPATH_MIB_H
+#define DATAPATH_MIB_H
+
+int dp_reset_mib(u32 flag);
+int set_gsw_itf(u8 ep, u8 ena, int start);
+int reset_gsw_itf(u8 ep);
+int dp_get_port_vap_mib_31(dp_subif_t *subif, void *priv,
+			   struct rtnl_link_stats64 *net_mib, u32 flag);
+
+#endif
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_misc.c b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_misc.c
new file mode 100644
index 000000000000..fecd8e92047c
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_misc.c
@@ -0,0 +1,1390 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/version.h>
+#include <linux/if_ether.h>
+#include <linux/ethtool.h>
+#include <linux/proc_fs.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/clk.h>
+#include <linux/if_ether.h>
+#include <linux/if_vlan.h>
+
+#include <linux/clk.h>
+#include <linux/ip.h>
+#include <net/ip.h>
+
+#include <lantiq.h>
+#include <lantiq_soc.h>
+#include <net/lantiq_cbm_api.h>
+#define DATAPATH_HAL_LAYER   /*must put before include datapath_api.h in
+			      *order to avoid include another platform's
+			      *DMA descriptor and pmac header files
+			      */
+#include <net/lantiq_cbm_api.h>
+#include <net/datapath_api.h>
+#include <net/datapath_api_gswip31.h>
+#include "../datapath.h"
+#include "datapath_proc.h"
+#include "datapath_ppv4.h"
+#include "datapath_misc.h"
+
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_SWITCHDEV)
+#include "datapath_switchdev.h"
+#endif
+
+static void init_dma_desc_mask(void)
+{
+	/*mask 0: to remove the bit, 1 -- keep the bit */
+	dma_rx_desc_mask1.all = 0xFFFFFFFF;
+	dma_rx_desc_mask3.all = 0xFFFFFFFF;
+	dma_rx_desc_mask3.field.own = 0; /*remove owner bit */
+	dma_rx_desc_mask3.field.c = 0;
+	dma_rx_desc_mask3.field.sop = 0;
+	dma_rx_desc_mask3.field.eop = 0;
+	dma_rx_desc_mask3.field.dic = 0;
+	dma_rx_desc_mask3.field.byte_offset = 0;
+	dma_rx_desc_mask1.field.dec = 0;
+	dma_rx_desc_mask1.field.enc = 0;
+	dma_rx_desc_mask1.field.mpe2 = 0;
+	dma_rx_desc_mask1.field.mpe1 = 0;
+	/*mask to keep some value via 1 set by
+	 * top application all others set to 0
+	 */
+	dma_tx_desc_mask0.all = 0;
+	dma_tx_desc_mask1.all = 0;
+	dma_tx_desc_mask0.field.flow_id = 0xFF;
+	dma_tx_desc_mask0.field.dest_sub_if_id = 0x7FFF;
+	dma_tx_desc_mask1.field.mpe1 = 0x1;
+	dma_tx_desc_mask1.field.color = 0x3;
+	dma_tx_desc_mask1.field.ep = 0xF;
+}
+
+static void init_dma_pmac_template(int portid, u32 flags)
+{
+	int i;
+	struct pmac_port_info2 *dp_info = &dp_port_info2[0][portid];
+
+	/*Note:
+	 * final tx_dma0 = (tx_dma0 & dma0_mask_template) | dma0_template
+	 * final tx_dma1 = (tx_dma1 & dma1_mask_template) | dma1_template
+	 * final tx_pmac = pmac_template
+	 */
+	memset(dp_info->pmac_template, 0, sizeof(dp_info->pmac_template));
+	memset(dp_info->dma0_template, 0, sizeof(dp_info->dma0_template));
+	memset(dp_info->dma1_template, 0, sizeof(dp_info->dma1_template));
+	for (i = 0; i < MAX_TEMPLATE; i++) {
+		dp_info->dma0_mask_template[i].all = 0xFFFFFFFF;
+		dp_info->dma1_mask_template[i].all = 0xFFFFFFFF;
+	}
+	if ((flags & DP_F_FAST_ETH_LAN) || (flags & DP_F_FAST_ETH_WAN) ||
+	    (flags & DP_F_GPON) || (flags & DP_F_EPON)) {
+		/*always with pmac */
+		for (i = 0; i < MAX_TEMPLATE; i++) {
+			dp_info->pmac_template[i].class_en = 1;
+			SET_PMAC_IGP_EGP(&dp_info->pmac_template[i], portid);
+			//TODO changed redir to '0' for HAPS Local testing
+			dp_info->dma0_template[i].field.redir = 1;
+			dp_info->dma0_mask_template[i].field.redir = 0;
+		}
+	} else if (flags & DP_F_FAST_WLAN) {/*someties with pmac*/
+		/*normal fast_wlan without pmac.*/
+	} else if (flags & DP_F_DIRECTLINK) { /*always with pmac*/
+		/*normal dirctpath without checksum support
+		 *but with pmac to Switch for accelerate
+		 */
+		dp_info->pmac_template[TEMPL_NORMAL].igp_msb = portid;
+		dp_info->pmac_template[TEMPL_NORMAL].class_en = 1;
+		SET_PMAC_IGP_EGP(&dp_info->pmac_template[TEMPL_NORMAL], portid);
+
+		dp_info->dma1_template[TEMPL_CHECKSUM].field.enc = 1;
+		dp_info->dma1_template[TEMPL_CHECKSUM].field.dec = 1;
+		dp_info->dma1_template[TEMPL_CHECKSUM].field.mpe2 = 0;
+		dp_info->dma1_mask_template[TEMPL_CHECKSUM].field.enc = 0;
+		dp_info->dma1_mask_template[TEMPL_CHECKSUM].field.dec = 0;
+		dp_info->dma1_mask_template[TEMPL_CHECKSUM].field.mpe2 = 0;
+		/*dirctpath with checksum support */
+		dp_info->pmac_template[TEMPL_CHECKSUM].igp_msb = portid;
+		dp_info->dma0_template[TEMPL_CHECKSUM].field.redir = 1;
+		dp_info->dma0_mask_template[TEMPL_CHECKSUM].field.redir = 0;
+		dp_info->pmac_template[TEMPL_CHECKSUM].tcp_chksum = 1;
+		dp_info->pmac_template[TEMPL_CHECKSUM].class_en = 1;
+		SET_PMAC_IGP_EGP(&dp_info->pmac_template[TEMPL_CHECKSUM],
+				 portid);
+		dp_info->dma1_template[TEMPL_CHECKSUM].field.enc = 1;
+		dp_info->dma1_template[TEMPL_CHECKSUM].field.dec = 1;
+		dp_info->dma1_template[TEMPL_CHECKSUM].field.mpe2 = 1;
+		dp_info->dma1_mask_template[TEMPL_CHECKSUM].field.enc = 0;
+		dp_info->dma1_mask_template[TEMPL_CHECKSUM].field.dec = 0;
+		dp_info->dma1_mask_template[TEMPL_CHECKSUM].field.mpe2 = 0;
+
+		/*dirctpath without checksum support but send packet to MPE
+		 * DL FW
+		 */
+		dp_info->pmac_template[TEMPL_OTHERS].igp_msb = portid;
+		dp_info->dma0_template[TEMPL_OTHERS].field.redir = 1;
+		dp_info->dma0_mask_template[TEMPL_OTHERS].field.redir = 0;
+		dp_info->pmac_template[TEMPL_OTHERS].class_en = 1;
+		SET_PMAC_IGP_EGP(&dp_info->pmac_template[TEMPL_OTHERS], portid);
+#if defined(CONFIG_ACCL_11AC_CS2) || defined(CONFIG_ACCL_11AC_CS2_MODULE)
+			/* CPU traffic to PAE via cbm to apply PCE rule */
+		dp_info->dma1_template[TEMPL_OTHERS].field.enc = 1;
+		dp_info->dma1_template[TEMPL_OTHERS].field.dec = 1;
+		dp_info->dma1_template[TEMPL_OTHERS].field.mpe2 = 0;
+		dp_info->dma1_mask_template[TEMPL_OTHERS].field.enc = 0;
+		dp_info->dma1_mask_template[TEMPL_OTHERS].field.dec = 0;
+		dp_info->dma1_mask_template[TEMPL_OTHERS].field.mpe2 = 0;
+#else
+		/* No need since already set to zero by default
+		 *dp_info->dma1_template[TEMPL_OTHERS].field.enc = 0;
+		 *dp_info->dma1_template[TEMPL_OTHERS].field.dec = 0;
+		 *dp_info->dma1_template[TEMPL_OTHERS].field.mpe2 = 0;
+		 *dp_info->dma1_mask_template[TEMPL_OTHERS].field.enc = 0;
+		 *dp_info->dma1_mask_template[TEMPL_OTHERS].field.dec = 0;
+		 *dp_info->dma1_mask_template[TEMPL_OTHERS].field.mpe2 = 0;
+		 */
+#endif
+	} else if (flags & DP_F_FAST_DSL) { /*sometimes with pmac*/
+		/* For normal single DSL upstream, there is no pmac at all*/
+		dp_info->dma1_template[TEMPL_NORMAL].field.dec = 1;
+		dp_info->dma1_template[TEMPL_NORMAL].field.mpe2 = 1;
+		dp_info->dma1_mask_template[TEMPL_NORMAL].field.enc = 0;
+		dp_info->dma1_mask_template[TEMPL_NORMAL].field.dec = 0;
+		dp_info->dma1_mask_template[TEMPL_NORMAL].field.mpe2 = 0;
+
+		/*DSL  with checksum support */
+		dp_info->pmac_template[TEMPL_CHECKSUM].igp_msb = portid;
+		dp_info->dma0_template[TEMPL_CHECKSUM].field.redir = 1;
+		dp_info->dma0_mask_template[TEMPL_CHECKSUM].field.redir = 0;
+		/*checksum*/
+		dp_info->pmac_template[TEMPL_CHECKSUM].tcp_chksum = 1;
+		dp_info->pmac_template[TEMPL_CHECKSUM].class_en = 1;
+		SET_PMAC_IGP_EGP(&dp_info->pmac_template[TEMPL_CHECKSUM],
+				 portid);
+		dp_info->dma1_template[TEMPL_CHECKSUM].field.enc = 1;
+		dp_info->dma1_template[TEMPL_CHECKSUM].field.dec = 1;
+		dp_info->dma1_template[TEMPL_CHECKSUM].field.mpe2 = 1;
+		dp_info->dma1_mask_template[TEMPL_CHECKSUM].field.enc = 0;
+		dp_info->dma1_mask_template[TEMPL_CHECKSUM].field.dec = 0;
+		dp_info->dma1_mask_template[TEMPL_CHECKSUM].field.mpe2 = 0;
+
+		/*Bonding DSL  FCS Support via GSWIP */
+		dp_info->pmac_template[TEMPL_OTHERS].igp_msb = portid;
+		dp_info->dma0_template[TEMPL_OTHERS].field.redir = 1;
+		dp_info->dma0_mask_template[TEMPL_OTHERS].field.redir = 0;
+		/*dp_info->pmac_template[TEMPL_OTHERS].tcp_chksum = 1; */
+		dp_info->pmac_template[TEMPL_OTHERS].class_en = 1;
+		SET_PMAC_IGP_EGP(&dp_info->pmac_template[TEMPL_CHECKSUM],
+				 portid);
+		dp_info->dma1_template[TEMPL_OTHERS].field.enc = 1;
+		dp_info->dma1_template[TEMPL_OTHERS].field.dec = 1;
+		dp_info->dma1_template[TEMPL_OTHERS].field.mpe2 = 1;
+		dp_info->dma1_mask_template[TEMPL_OTHERS].field.enc = 0;
+		dp_info->dma1_mask_template[TEMPL_OTHERS].field.dec = 0;
+		dp_info->dma1_mask_template[TEMPL_OTHERS].field.mpe2 = 0;
+	} else /*if(flags & DP_F_DIRECT ) */{/*always with pmac*/
+		/*normal dirctpath without checksum support */
+		dp_info->pmac_template[TEMPL_NORMAL].igp_msb = portid;
+		dp_info->pmac_template[TEMPL_NORMAL].class_en = 1;
+		SET_PMAC_IGP_EGP(&dp_info->pmac_template[TEMPL_NORMAL], portid);
+		dp_info->dma1_template[TEMPL_CHECKSUM].field.enc = 1;
+		dp_info->dma1_template[TEMPL_CHECKSUM].field.dec = 1;
+		dp_info->dma1_template[TEMPL_CHECKSUM].field.mpe2 = 0;
+		dp_info->dma1_mask_template[TEMPL_CHECKSUM].field.enc = 0;
+		dp_info->dma1_mask_template[TEMPL_CHECKSUM].field.dec = 0;
+		dp_info->dma1_mask_template[TEMPL_CHECKSUM].field.mpe2 = 0;
+
+		/*dirctpath with checksum support */
+		dp_info->pmac_template[TEMPL_CHECKSUM].igp_msb = PMAC_CPU_ID;
+		dp_info->dma0_template[TEMPL_CHECKSUM].field.redir = 1;
+		dp_info->dma0_mask_template[TEMPL_CHECKSUM].field.redir = 0;
+		dp_info->pmac_template[TEMPL_CHECKSUM].tcp_chksum = 1;
+		dp_info->pmac_template[TEMPL_CHECKSUM].class_en = 1;
+		SET_PMAC_IGP_EGP(&dp_info->pmac_template[TEMPL_CHECKSUM],
+				 portid);
+		dp_info->dma1_template[TEMPL_CHECKSUM].field.enc = 1;
+		dp_info->dma1_template[TEMPL_CHECKSUM].field.dec = 1;
+		dp_info->dma1_template[TEMPL_CHECKSUM].field.mpe2 = 1;
+		dp_info->dma1_mask_template[TEMPL_CHECKSUM].field.enc = 0;
+		dp_info->dma1_mask_template[TEMPL_CHECKSUM].field.dec = 0;
+		dp_info->dma1_mask_template[TEMPL_CHECKSUM].field.mpe2 = 0;
+	}
+}
+
+void dump_rx_dma_desc(struct dma_rx_desc_0 *desc_0,
+		      struct dma_rx_desc_1 *desc_1,
+		      struct dma_rx_desc_2 *desc_2,
+		      struct dma_rx_desc_3 *desc_3)
+{
+	if (!desc_0 || !desc_1 || !desc_2 || !desc_3) {
+		PR_ERR("rx desc_0/1/2/3 NULL\n");
+		return;
+	}
+
+	PR_INFO(" DMA Descripotr:D0=0x%08x D1=0x%08x D2=0x%08x D3=0x%08x\n",
+		*(u32 *)desc_0, *(u32 *)desc_1,
+		*(u32 *)desc_2, *(u32 *)desc_3);
+	PR_INFO("  DW0:redir=%d res=%d tunl=%d flow=%d ether=%d %s=0x%04x\n",
+		desc_0->field.redir,
+		desc_0->field.resv, desc_0->field.tunnel_id,
+		desc_0->field.flow_id, desc_0->field.eth_type,
+		"subif", desc_0->field.dest_sub_if_id);
+	PR_INFO("  DW1:%s=%d tcp_err=%d nat=%d dec=%d enc=%d mpe2/1=%d/%d\n",
+		"sess/src_subif", desc_1->field.session_id,
+		desc_1->field.tcp_err,
+		desc_1->field.nat, desc_1->field.dec, desc_1->field.enc,
+		desc_1->field.mpe2, desc_1->field.mpe1);
+	PR_INFO("      color=%02d ep=%02d ip=%02d classid=%02d\n",
+		desc_1->field.color, desc_1->field.ep, desc_1->field.ip,
+		desc_1->field.classid);
+	PR_INFO("  DW2:data_ptr=0x%08x\n", desc_2->field.data_ptr);
+	PR_INFO("  DW3:own=%d c=%d sop=%d eop=%d dic=%d pdu_type=%d\n",
+		desc_3->field.own, desc_3->field.c, desc_3->field.sop,
+		desc_3->field.eop, desc_3->field.dic, desc_3->field.pdu_type);
+	PR_INFO("      offset=%d policy=%d res=%d pool=%d len=%d\n",
+		desc_3->field.byte_offset, desc_3->field.policy,
+		desc_3->field.res, desc_3->field.pool,
+		desc_3->field.data_len);
+}
+
+void dump_tx_dma_desc(struct dma_tx_desc_0 *desc_0,
+		      struct dma_tx_desc_1 *desc_1,
+		      struct dma_tx_desc_2 *desc_2,
+		      struct dma_tx_desc_3 *desc_3)
+{
+	int lookup;
+	int inst = 0;
+	int dp_port;
+
+	if (!desc_0 || !desc_1 || !desc_2 || !desc_3) {
+		PR_ERR("tx desc_0/1/2/3 NULL\n");
+		return;
+	}
+	PR_INFO(" DMA Descripotr:D0=0x%08x D1=0x%08x D2=0x%08x D3=0x%08x\n",
+		*(u32 *)desc_0, *(u32 *)desc_1,
+		*(u32 *)desc_2, *(u32 *)desc_3);
+	PR_INFO("  DW0:redir=%d res=%d tunl=%d flow=%d ether=%d subif=0x%04x\n",
+		desc_0->field.redir,
+		desc_0->field.resv, desc_0->field.tunnel_id,
+		desc_0->field.flow_id, desc_0->field.eth_type,
+		desc_0->field.dest_sub_if_id);
+	PR_INFO("  DW1:sess=%d tcp_err=%d nat=%d dec=%d enc=%d mpe2/1=%d/%d\n",
+		desc_1->field.session_id, desc_1->field.tcp_err,
+		desc_1->field.nat, desc_1->field.dec, desc_1->field.enc,
+		desc_1->field.mpe2, desc_1->field.mpe1);
+	PR_INFO("  color=%02d ep=%02d ip=%02d class=%d\n",
+		desc_1->field.color, desc_1->field.ep,
+		desc_1->field.ip, desc_1->field.classid);
+	PR_INFO("  DW2:data_ptr=0x%08x\n", desc_2->field.data_ptr);
+	PR_INFO("  DW3:own=%d c=%d sop=%d eop=%d dic=%d pdu_type=%d\n",
+		desc_3->field.own, desc_3->field.c, desc_3->field.sop,
+		desc_3->field.eop, desc_3->field.dic, desc_3->field.pdu_type);
+	PR_INFO("  offset=%d policy=%d res=%d pool=%d data_len=%d\n",
+		desc_3->field.byte_offset,
+		desc_3->field.policy, desc_3->field.res,
+		desc_3->field.pool, desc_3->field.data_len);
+	dp_port = desc_1->field.ep;
+	if (dp_port_info[inst][dp_port].cqe_lu_mode == CQE_LU_MODE0)
+		/*Flow[7:6] DEC ENC MPE2 MPE1 EP Class */
+		lookup = ((desc_0->field.flow_id >> 6) << 12) |
+			 ((desc_1->field.dec) << 11) |
+			 ((desc_1->field.enc) << 10) |
+			 ((desc_1->field.mpe2) << 9) |
+			 ((desc_1->field.mpe1) << 8) |
+			 ((desc_1->field.ep) << 4) |
+			 desc_1->field.classid;
+	else if (dp_port_info[inst][dp_port].cqe_lu_mode == CQE_LU_MODE1)
+		/*Subif[7:4] MPE2 MPE1 EP Subif[3:0] */
+		lookup = ((desc_0->field.dest_sub_if_id >> 4) << 10) |
+			 ((desc_1->field.mpe2) << 9) |
+			 ((desc_1->field.mpe1) << 8) |
+			 ((desc_1->field.ep) << 4) |
+			 (desc_0->field.dest_sub_if_id & 0xf);
+	else if (dp_port_info[inst][dp_port].cqe_lu_mode == CQE_LU_MODE2)
+		/*Subif[7:4] MPE2 MPE1 EP Class */
+		lookup = ((desc_0->field.dest_sub_if_id >> 4) << 10) |
+			 ((desc_1->field.mpe2) << 9) |
+			 ((desc_1->field.mpe1) << 8) |
+			 ((desc_1->field.ep) << 4) |
+			 desc_1->field.classid;
+	else /*mode3*/
+		/*Subif[4:1] MPE2 MPE1 EP Subif[0:0] Class[2:0] */
+		lookup = (((desc_0->field.dest_sub_if_id >> 1) & 0xf) << 10) |
+			 ((desc_1->field.mpe2) << 9) |
+			 ((desc_1->field.mpe1) << 8) |
+			 ((desc_1->field.ep) << 4) |
+			 ((desc_0->field.dest_sub_if_id & 0x1) << 3) |
+			 (desc_1->field.classid & 7); /*lower 3 bits*/
+	PR_INFO("  lookup index=0x%x qid=%d\n", lookup,
+		get_lookup_qid_via_index(lookup));
+}
+
+static void dump_rx_pmac(struct pmac_rx_hdr *pmac)
+{
+	int i, l;
+	unsigned char *p = (char *)pmac;
+	unsigned char buf[100];
+
+	if (!pmac) {
+		PR_ERR(" pmac NULL ??\n");
+		return;
+	}
+
+	l = sprintf(buf, "PMAC at 0x%p: ", p);
+	for (i = 0; i < 8; i++)
+		l += sprintf(buf + l, "0x%02x ", p[i]);
+	l += sprintf(buf + l, "\n");
+	PR_INFO("%s", buf);
+
+	/*byte 0 */
+	PR_INFO("  byte 0:res=%d ver_done=%d ip_offset=%d\n", pmac->res0,
+		pmac->ver_done, pmac->ip_offset);
+	/*byte 1 */
+	PR_INFO("  byte 1:tcp_h_offset=%d tcp_type=%d\n", pmac->tcp_h_offset,
+		pmac->tcp_type);
+	/*byte 2 */
+	PR_INFO("  byte 2:class=%d res=%d\n", pmac->class, pmac->res2);
+	/*byte 3 */
+	PR_INFO("  byte 3:pkt_type=%d ext=%d ins=%d res31=%d oam=%d res32=%d\n",
+		pmac->pkt_type, pmac->ext, pmac->ins, pmac->res31,
+		pmac->oam, pmac->res32);
+	/*byte 4 */
+	PR_INFO("  byte 4:res=%d ptp=%d one_step=%d src_dst_subif_id_msb=%d\n",
+		pmac->res4, pmac->ptp, pmac->one_step,
+		pmac->src_dst_subif_id_msb);
+	/*byte 5 */
+	PR_INFO("  byte 5:src_sub_inf_id2=%d\n", pmac->src_dst_subif_id_lsb);
+	/*byte 6 */
+	PR_INFO("  byte 6:record_id_msb=%d\n", pmac->record_id_msb);
+	/*byte 7 */
+	PR_INFO("  byte 7:record_id_lsb=%d igp_egp=%d\n",
+		pmac->record_id_lsb, pmac->igp_egp);
+}
+
+static void dump_tx_pmac(struct pmac_tx_hdr *pmac)
+{
+	int i, l;
+	unsigned char *p = (char *)pmac;
+	unsigned char buf[100];
+
+	if (!pmac) {
+		PR_ERR("dump_tx_pmac pmac NULL ??\n");
+		return;
+	}
+
+	l = sprintf(buf, "PMAC at 0x%p: ", p);
+	for (i = 0; i < 8; i++)
+		l += sprintf(buf + l, "0x%02x ", p[i]);
+	sprintf(buf + l, "\n");
+	PR_INFO("%s", buf);
+
+	/*byte 0 */
+	PR_INFO("  byte 0:res=%d tcp_chksum=%d ip_offset=%d\n", pmac->res1,
+		pmac->tcp_chksum, pmac->ip_offset);
+	/*byte 1 */
+	PR_INFO("  byte 1:tcp_h_offset=%d tcp_type=%d\n", pmac->tcp_h_offset,
+		pmac->tcp_type);
+	/*byte 2 */
+	PR_INFO("  byte 2:igp_msb=%d res=%d\n", pmac->igp_msb, pmac->res2);
+	/*byte 3 */
+	PR_INFO("  byte 3:%s=%d %s=%d %s=%d %s=%d %s=%d %s=%d %s=%d\n",
+		"pkt_type", pmac->pkt_type,
+		"ext", pmac->ext,
+		"ins", pmac->ins,
+		"res3", pmac->res3,
+		"oam", pmac->oam,
+		"lrnmd", pmac->lrnmd,
+		"class_en", pmac->class_en);
+	/*byte 4 */
+	PR_INFO("  byte 4:%s=%d ptp=%d one_step=%d src_dst_subif_id_msb=%d\n",
+		"fcs_ins_dis", pmac->fcs_ins_dis,
+		pmac->ptp, pmac->one_step,
+		pmac->src_dst_subif_id_msb);
+	/*byte 5 */
+	PR_INFO("  byte 5:src_dst_subif_id_lsb=%d\n",
+		pmac->src_dst_subif_id_lsb);
+	/*byte 6 */
+	PR_INFO("  byte 6:record_id_msb=%d\n", pmac->record_id_msb);
+	/*byte 7 */
+	PR_INFO("  byte 7:record_id_lsb=%d igp_egp=%d\n", pmac->record_id_lsb,
+		pmac->igp_egp);
+}
+
+static void mib_init(u32 flag)
+{
+#ifdef CONFIG_LTQ_DATAPATH_MIB
+	dp_mib_init(0);
+#endif
+	gsw_mib_reset_31(0, 0); /* GSW O */
+}
+
+void dp_sys_mib_reset_31(u32 flag)
+{
+#ifdef CONFIG_LTQ_DATAPATH_MIB
+	dp_reset_sys_mib(0);
+#else
+	gsw_mib_reset_31(0, 0); /* GSW L */
+	gsw_mib_reset_31(1, 0); /* GSW R */
+	dp_clear_all_mib_inside(0);
+#endif
+}
+
+#ifndef CONFIG_LTQ_DATAPATH_QOS_HAL
+int alloc_q_to_port(struct ppv4_q_sch_port *info, u32 flag)
+{
+	struct ppv4_queue q;
+	struct ppv4_port port;
+	int inst = info->inst;
+	struct hal_priv *priv = HAL(inst);
+
+	if (!priv) {
+		PR_ERR("why priv NULL ???\n");
+		return -1;
+	}
+
+	if (priv->deq_port_stat[info->cqe_deq].flag == PP_NODE_FREE) {
+		port.cqm_deq_port = info->cqe_deq;
+		port.tx_pkt_credit = info->tx_pkt_credit;
+		port.tx_ring_addr = info->tx_ring_addr;
+		port.tx_ring_size = info->tx_ring_size;
+		port.inst = inst;
+		port.dp_port = info->dp_port;
+		if (dp_pp_alloc_port(&port)) {
+			PR_ERR("%s fail for deq_port=%d qos_deq_port=%d\n",
+			       "dp_pp_alloc_port",
+			       port.cqm_deq_port, port.qos_deq_port);
+			return -1;
+		}
+		//priv->deq_port_stat[info->cqe_deq].node_id = port.node_id;
+		//priv->deq_port_stat[info->cqe_deq].flag = PP_NODE_ALLOC;
+		info->port_node = port.node_id;
+		info->f_deq_port_en = 1;
+	} else {
+		port.node_id = priv->deq_port_stat[info->cqe_deq].node_id;
+	}
+	q.qid = -1;
+	q.parent = port.node_id;
+	q.inst = inst;
+#ifdef CONFIG_LTQ_DATAPATH_DUMMY_QOS
+	q.dq_port = info->cqe_deq; /*for qos slim driver only */
+#endif
+	if (dp_pp_alloc_queue(&q)) {
+		PR_ERR("%s fail\n",
+		       "dp_pp_alloc_queue");
+		return -1;
+	}
+	PORT_VAP(info->inst, info->dp_port, info->ctp, qid) = q.qid;
+	PORT_VAP(info->inst, info->dp_port, info->ctp, q_node) = q.node_id;
+	info->qid = q.qid;
+	info->q_node = q.node_id;
+	priv->qos_queue_stat[q.qid].deq_port = info->cqe_deq;
+	priv->qos_queue_stat[q.qid].node_id = q.node_id;
+	priv->qos_queue_stat[q.qid].flag = PP_NODE_ALLOC;
+	DP_DEBUG(DP_DBG_FLAG_QOS, "alloc_q_to_port done\n");
+	return 0;
+}
+#else
+int alloc_q_to_port(struct ppv4_q_sch_port *info, u32 flag)
+{
+	struct dp_node_link link = {0};
+
+	link.cqm_deq_port.cqm_deq_port = info->cqe_deq;
+	link.dp_port = info->dp_port; /*in case for qos node alloc resv pool*/
+	link.inst = info->inst;
+	link.node_id.q_id = DP_NODE_AUTO_ID;
+	link.node_type = DP_NODE_QUEUE;
+	link.p_node_id.cqm_deq_port = info->cqe_deq;
+	link.p_node_type = DP_NODE_PORT;
+	link.arbi = ARBITRATION_WRR;
+	link.prio_wfq = 0;
+
+	if (dp_node_link_add(&link, 0)) {
+		PR_ERR("dp_node_link_add_31 fail: cqm_deq_port=%d\n",
+		       info->cqe_deq);
+		return -1;
+	}
+	//info->f_deq_port_en = 1;
+	info->qid = link.node_id.q_id;
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "qid=%d p_node_id=%d for cqm port=%d\n",
+		 link.node_id.q_id,
+		 link.p_node_id.cqm_deq_port, info->cqe_deq);
+	return 0;
+}
+#endif /*CONFIG_LTQ_DATAPATH_QOS_HAL*/
+
+int dp_platform_queue_set(int inst, u32 flag)
+{
+	int ret, i;
+	struct ppv4_queue q = {0};
+	cbm_queue_map_entry_t lookup = {0};
+	struct cbm_cpu_port_data cpu_data = {0};
+	struct ppv4_q_sch_port q_port = {0};
+	u8 f_cpu_q = 0;
+	struct cbm_dp_en_data en_data = {0};
+	struct hal_priv *priv = (struct hal_priv *)dp_port_prop[inst].priv_hal;
+	struct pmac_port_info *port_info;
+
+	port_info = &dp_port_info[inst][0]; /*CPU*/
+	if ((flag & DP_PLATFORM_DE_INIT) == DP_PLATFORM_DE_INIT) {
+		PR_ERR("Need to free resoruce in the future\n");
+		return 0;
+	}
+
+	/*Allocate a drop queue*/
+	if (priv->ppv4_drop_q < 0) {
+		q.parent = 0;
+		q.inst = inst;
+		if (dp_pp_alloc_queue(&q)) {
+			PR_ERR("%s fail to alloc a drop queue ??\n",
+			       "dp_pp_alloc_queue");
+			return -1;
+		}
+		priv->ppv4_drop_q = q.qid;
+	} else {
+		PR_INFO("drop queue: %d\n", priv->ppv4_drop_q);
+	}
+	/*Map all lookup entry to drop queue at the beginning*/
+	lookup.mode = CQE_LU_MODE0;
+	cbm_queue_map_set(dp_port_prop[inst].cbm_inst, priv->ppv4_drop_q,
+			  &lookup,
+			  CBM_QUEUE_MAP_F_FLOWID_L_DONTCARE |
+			  CBM_QUEUE_MAP_F_FLOWID_H_DONTCARE |
+			  CBM_QUEUE_MAP_F_SUBIF_DONTCARE |
+			  CBM_QUEUE_MAP_F_EN_DONTCARE |
+			  CBM_QUEUE_MAP_F_DE_DONTCARE |
+			  CBM_QUEUE_MAP_F_MPE1_DONTCARE |
+			  CBM_QUEUE_MAP_F_MPE2_DONTCARE |
+			  CBM_QUEUE_MAP_F_EP_DONTCARE |
+			  CBM_QUEUE_MAP_F_TC_DONTCARE);
+
+	/*Set CPU port to Mode0 only*/
+	dp_port_info[inst][0].cqe_lu_mode = CQE_LU_MODE0;
+	lookup.mode = CQE_LU_MODE0;
+	lookup.ep = PMAC_CPU_ID;
+	cqm_mode_table_set(dp_port_prop[inst].cbm_inst, &lookup,
+			   CBM_QUEUE_MAP_F_MPE1_DONTCARE |
+			   CBM_QUEUE_MAP_F_MPE2_DONTCARE);
+
+	/*Alloc queue/scheduler/port per CPU port */
+	cpu_data.dp_inst = inst;
+	cpu_data.cbm_inst = dp_port_prop[inst].cbm_inst;
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_DDR_SIMULATE_GSWIP31)
+	cpu_data.dq_tx_push_info[0].deq_port = 0;
+	cpu_data.dq_tx_push_info[1].deq_port = -1;
+	cpu_data.dq_tx_push_info[2].deq_port = -1;
+	cpu_data.dq_tx_push_info[3].deq_port = -1;
+#else
+	ret = cbm_cpu_port_get(&cpu_data, 0);
+#endif
+	if (ret == -1) {
+		PR_ERR("%s fail for CPU Port. Why ???\n",
+		       "cbm_cpu_port_get");
+		return -1;
+	}
+	port_info->deq_port_base = 0;
+	port_info->deq_port_num = 4;  /*need improve later*/
+	for (i = 0; i < ARRAY_SIZE(cpu_data.dq_tx_push_info); i++) {
+		if (cpu_data.dq_tx_push_info[i].deq_port == (u32)-1)
+			continue;
+		DP_DEBUG(DP_DBG_FLAG_QOS, "cpu(%d) deq_port=%d",
+			 i, cpu_data.dq_tx_push_info[i].deq_port);
+		q_port.cqe_deq = cpu_data.dq_tx_push_info[i].deq_port;
+		q_port.tx_pkt_credit = cpu_data.dq_tx_push_info[i].
+							tx_pkt_credit;
+		q_port.tx_ring_addr = cpu_data.dq_tx_push_info[i].tx_ring_addr;
+		q_port.tx_ring_size = cpu_data.dq_tx_push_info[i].tx_ring_size;
+
+		/*Sotre Ring Info */
+		dp_deq_port_tbl[inst][q_port.cqe_deq].tx_pkt_credit =
+			cpu_data.dq_tx_push_info[i].tx_pkt_credit;
+		dp_deq_port_tbl[inst][q_port.cqe_deq].tx_ring_addr =
+			cpu_data.dq_tx_push_info[i].tx_ring_addr;
+		dp_deq_port_tbl[inst][q_port.cqe_deq].tx_ring_size =
+			cpu_data.dq_tx_push_info[i].tx_ring_size;
+		dp_deq_port_tbl[inst][q_port.cqe_deq].dp_port = 0;/* CPU */
+		DP_DEBUG(DP_DBG_FLAG_QOS, "Store CPU ring info\n");
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  ring_address[%d]=0x%x\n",
+			 q_port.cqe_deq,
+			 dp_deq_port_tbl[inst][q_port.cqe_deq].tx_ring_addr);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  ring_size[%d]=%d\n",
+			 q_port.cqe_deq,
+			 dp_deq_port_tbl[inst][q_port.cqe_deq].tx_ring_size);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  credit[%d]=%d\n",
+			 q_port.cqe_deq,
+			 dp_deq_port_tbl[inst][q_port.cqe_deq].tx_pkt_credit);
+		q_port.inst = inst;
+		q_port.dp_port = PMAC_CPU_ID;
+		DP_DEBUG(DP_DBG_FLAG_QOS, "CPU[%d] ring addr=%x\n", i,
+			 cpu_data.dq_tx_push_info[i].tx_ring_addr);
+		q_port.ctp = i; /*fake CTP for CPU port to store its qid*/
+		DP_DEBUG(DP_DBG_FLAG_QOS, "alloc_q_to_port...\n");
+		if (alloc_q_to_port(&q_port, 0)) {
+			PR_ERR("alloc_q_to_port fail for dp_port=%d\n",
+			       q_port.dp_port);
+			return -1;
+		}
+		port_info->deq_port_num++;
+		port_info->subif_info[i].qid = q_port.qid;
+		port_info->subif_info[i].q_node = q_port.q_node;
+		port_info->subif_info[i].qos_deq_port = q_port.port_node;
+		port_info->subif_info[i].cqm_deq_port = q_port.cqe_deq;
+		if (!f_cpu_q) {
+			f_cpu_q = 1;
+			/*Map all CPU port's lookup to its 1st queue only */
+			lookup.mode = CQE_LU_MODE0;
+			lookup.ep = PMAC_CPU_ID;
+			cbm_queue_map_set(dp_port_prop[inst].cbm_inst,
+					  q_port.qid,
+					  &lookup,
+					  CBM_QUEUE_MAP_F_FLOWID_L_DONTCARE |
+					  CBM_QUEUE_MAP_F_FLOWID_H_DONTCARE |
+					  CBM_QUEUE_MAP_F_SUBIF_DONTCARE |
+					  CBM_QUEUE_MAP_F_EN_DONTCARE |
+					  CBM_QUEUE_MAP_F_DE_DONTCARE |
+					  CBM_QUEUE_MAP_F_MPE1_DONTCARE |
+					  CBM_QUEUE_MAP_F_MPE2_DONTCARE |
+					  CBM_QUEUE_MAP_F_TC_DONTCARE);
+		}
+		/*Note:
+		 *    CPU port no DMA and don't set en_data.dma_chnl_init to 1
+		 */
+		en_data.cbm_inst = dp_port_prop[inst].cbm_inst;
+		en_data.dp_inst = inst;
+		en_data.deq_port = cpu_data.dq_tx_push_info[i].deq_port;
+		if (cbm_dp_enable(NULL, PMAC_CPU_ID, &en_data, 0, 0)) {
+			PR_ERR("Fail to enable CPU[%d]\n", en_data.deq_port);
+			return -1;
+		}
+	}
+	return 0;
+}
+
+static int dp_platform_set(int inst, u32 flag)
+{
+	GSW_QoS_portRemarkingCfg_t port_remark;
+	struct core_ops *gsw_handle;
+	struct hal_priv *priv;
+
+	/* For initialize */
+	if ((flag & DP_PLATFORM_INIT) == DP_PLATFORM_INIT) {
+		dp_port_prop[inst].priv_hal =
+			kzalloc(sizeof(*priv), GFP_KERNEL);
+		if (!dp_port_prop[inst].priv_hal) {
+			PR_ERR("kmalloc failed: %d bytes\n",
+			       sizeof(struct hal_priv));
+			return -1;
+		}
+		priv = (struct hal_priv *)dp_port_prop[inst].priv_hal;
+		priv->ppv4_drop_q = MAX_QUEUE - 1; /*Need change later */
+		gsw_handle = dp_port_prop[inst].ops[0];
+		if (!inst)/*only inst zero need DMA descriptor */
+			init_dma_desc_mask();
+		if (!dp_port_prop[inst].ops[0] ||
+		    !dp_port_prop[inst].ops[1]) {
+			PR_ERR("Why GSWIP handle Zero\n");
+			return -1;
+		}
+		if (!inst)
+			dp_sub_proc_install_31();
+		init_qos_fn();
+		/*just for debugging purpose */
+		dp_port_info[inst][0].subif_info[0].bp = CPU_BP;
+		INIT_LIST_HEAD(&dp_port_info[inst][0].subif_info[0].logic_dev);
+
+		priv->bp_def = alloc_bridge_port(inst, CPU_PORT, CPU_SUBIF,
+						 CPU_FID, CPU_BP);
+		DP_DEBUG(DP_DBG_FLAG_DBG, "bp_def[%d]=%d\n",
+			 inst, priv->bp_def);
+
+		if (!inst) /*only inst zero will support mib feature */
+			mib_init(0);
+		dp_get_gsw_parser_31(NULL, NULL, NULL, NULL);
+#ifdef CONFIG_LTQ_DATAPATH_CPUFREQ
+	if (!inst)
+		dp_coc_cpufreq_init();
+#endif
+		/*disable egress VLAN modification for CPU port*/
+		port_remark.nPortId = 0;
+		if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
+				 .QoS_PortRemarkingCfgGet,
+				 gsw_handle, &port_remark)) {
+			PR_ERR("GSW_QOS_PORT_REMARKING_CFG_GET failed\n");
+			return -1;
+		}
+		port_remark.bPCP_EgressRemarkingEnable = 0;
+		if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_qos_ops
+				 .QoS_PortRemarkingCfgGet,
+				 gsw_handle, &port_remark)) {
+			PR_ERR("GSW_QOS_PORT_REMARKING_CFG_GET failed\n");
+			return -1;
+		}
+
+		if (init_ppv4_qos(inst, flag)) {
+			PR_ERR("init_ppv4_qos fail\n");
+			return -1;
+		}
+		if (dp_platform_queue_set(inst, flag)) {
+			PR_ERR("dp_platform_queue_set fail\n");
+
+			return -1;
+		}
+		if (cpu_vlan_mod_dis(inst)) {
+			PR_ERR("cpu_vlan_mod_dis fail\n");
+			return -1;
+		}
+		return 0;
+	}
+
+	/* De-initialize */
+	priv = (struct hal_priv *)dp_port_prop[inst].priv_hal;
+	if (priv->bp_def)
+		free_bridge_port(inst, priv->bp_def);
+	init_ppv4_qos(inst, flag); /*de-initialize qos */
+	kfree(priv);
+	dp_port_prop[inst].priv_hal = NULL;
+	return 0;
+}
+
+static int port_platform_set(int inst, u8 ep, struct dp_port_data *data,
+			     u32 flags)
+{
+	int idx, i;
+	cbm_queue_map_entry_t lookup = {0};
+	struct hal_priv *priv = (struct hal_priv *)dp_port_prop[inst].priv_hal;
+	struct gsw_itf *itf;
+	struct pmac_port_info *port_info = &dp_port_info[inst][ep];
+
+	if (!priv) {
+		PR_ERR("priv is NULL\n");
+		return DP_FAILURE;
+	}
+	itf = ctp_port_assign(inst, ep, priv->bp_def, flags);
+	/*reset_gsw_itf(ep); */
+	dp_port_info[inst][ep].itf_info = itf;
+	if (flags & DP_F_DEREGISTER) {
+		dp_node_reserve(inst, ep, NULL, flags);
+		return 0;
+	}
+
+	DP_DEBUG(DP_DBG_FLAG_QOS, "priv=%p deq_port_stat=%p qdev=%p\n",
+		 priv,
+		 priv ? priv->deq_port_stat : NULL,
+		 priv ? priv->qdev : NULL);
+	idx = port_info->deq_port_base;
+
+	for (i = 0; i < port_info->deq_port_num; i++) {
+		dp_deq_port_tbl[inst][i + idx].tx_ring_addr =
+			port_info->tx_ring_addr +
+			(port_info->tx_ring_offset * i);
+		dp_deq_port_tbl[inst][i + idx].tx_ring_size =
+			port_info->tx_ring_size;
+		dp_deq_port_tbl[inst][i + idx].tx_pkt_credit =
+			port_info->tx_pkt_credit;
+		dp_deq_port_tbl[inst][i + idx].dp_port = ep;
+	}
+	lookup.mode = dp_port_info[inst][ep].cqe_lu_mode;
+	lookup.ep = ep;
+	/*Set all mode based on MPE1/2 to same single mode as specified */
+	cqm_mode_table_set(dp_port_prop[inst].cbm_inst, &lookup,
+			   CBM_QUEUE_MAP_F_MPE1_DONTCARE |
+			   CBM_QUEUE_MAP_F_MPE2_DONTCARE);
+	dp_node_reserve(inst, ep, data, flags);
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_DBG)
+	if (DP_DBG_FLAG_QOS & dp_dbg_flag) {
+		for (i = 0; i < port_info->deq_port_num; i++) {
+			PR_INFO("cqm[%d]: addr=%x credit=%d size==%d\n",
+				i + idx,
+				dp_deq_port_tbl[inst][i + idx].tx_ring_addr,
+				dp_deq_port_tbl[inst][i + idx].tx_pkt_credit,
+				dp_deq_port_tbl[inst][i + idx].tx_ring_size);
+		}
+	}
+#endif
+	return 0;
+}
+
+static int set_ctp_bp(int inst, int ctp, int portid, int bp)
+{
+	GSW_CTP_portConfig_t tmp;
+	struct core_ops *gsw_handle;
+
+	gsw_handle = dp_port_prop[inst].ops[GSWIP_L];
+	memset(&tmp, 0, sizeof(tmp));
+	tmp.nLogicalPortId = portid;
+	tmp.nSubIfIdGroup = ctp;
+	tmp.eMask = GSW_CTP_PORT_CONFIG_MASK_BRIDGE_PORT_ID;
+	tmp.nBridgePortId = bp;
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_ctp_ops.CTP_PortConfigSet,
+			 gsw_handle, &tmp) != 0) {
+		PR_ERR("Failed to CTP(%d)'s bridge port=%d for ep=%d\n",
+		       ctp, bp, portid);
+		return -1;
+	}
+
+	return 0;
+}
+
+static int reset_ctp_bp(int inst, int ctp, int portid, int bp)
+{
+	GSW_CTP_portConfig_t tmp;
+	struct core_ops *gsw_handle;
+
+	gsw_handle = dp_port_prop[inst].ops[GSWIP_L];
+	memset(&tmp, 0, sizeof(tmp));
+	tmp.nLogicalPortId = portid;
+	tmp.nSubIfIdGroup = ctp;
+	tmp.nBridgePortId = bp;
+	if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_ctp_ops.CTP_PortConfigReset,
+			 gsw_handle, &tmp) != 0) {
+		PR_ERR("Failed to reset CTP(%d)'s bridge port=%d for ep=%d\n",
+		       ctp, bp, portid);
+		return -1;
+	}
+
+	return 0;
+}
+
+static char *q_flag_str(int q_flag)
+{
+	if (q_flag == DP_SUBIF_AUTO_NEW_Q)
+		return "auto_new_queue";
+	if (q_flag == DP_SUBIF_SPECIFIC_Q)
+		return "specified_queue";
+	return "sharing_queue";
+}
+
+static int subif_hw_set(int inst, int portid, int subif_ix,
+			struct subif_platform_data *data, u32 flags)
+{
+	struct ppv4_q_sch_port q_port = {0};
+	static cbm_queue_map_entry_t lookup = {0};
+	u32 lookup_f = CBM_QUEUE_MAP_F_FLOWID_L_DONTCARE |
+		CBM_QUEUE_MAP_F_FLOWID_H_DONTCARE |
+		CBM_QUEUE_MAP_F_EN_DONTCARE |
+		CBM_QUEUE_MAP_F_DE_DONTCARE |
+		CBM_QUEUE_MAP_F_MPE1_DONTCARE |
+		CBM_QUEUE_MAP_F_MPE2_DONTCARE |
+		CBM_QUEUE_MAP_F_TC_DONTCARE;
+	int subif, deq_port_idx = 0, bp = -1;
+	struct pmac_port_info *port_info;
+	struct hal_priv *priv = HAL(inst);
+	int q_flag = 0;
+
+	if (!data || !data->subif_data) {
+		PR_ERR("data NULL or subif_data NULL\n");
+		return -1;
+	}
+	port_info = &dp_port_info[inst][portid];
+	subif = SET_VAP(subif_ix, port_info->vap_offset,
+			port_info->vap_mask);
+
+	if (data->subif_data->ctp_dev) /* for pmapper later */
+		bp = bp_pmapper_dev_get(inst, data->dev);
+	if (bp >= 0) {
+		DP_DEBUG(DP_DBG_FLAG_DBG, "%s Reuse BP(%d) ctp_dev=%s\n",
+			 data->dev ? data->dev->name : "NULL",
+			 bp,
+			 data->subif_data->ctp_dev->name);
+	} else {
+		DP_DEBUG(DP_DBG_FLAG_DBG, "need alloc BP for %s ctp_dev=%s\n",
+			 data->dev ? data->dev->name : "NULL",
+			 data->subif_data->ctp_dev ?
+				data->subif_data->ctp_dev->name : "NULL");
+		bp = alloc_bridge_port(inst, portid,
+				       subif_ix, CPU_FID, CPU_BP);
+		if (bp < 0) {
+			PR_ERR("Fail to alloc bridge port\n");
+			return -1;
+		}
+	}
+	port_info->subif_info[subif_ix].bp = bp;
+	set_ctp_bp(inst, subif_ix, portid,
+		   port_info->subif_info[subif_ix].bp);
+	data->act = 0;
+	if (flags & DP_F_SUBIF_LOGICAL) {
+		PR_ERR("need more for logical dev??\n");
+		return 0;
+	}
+	if (data->subif_data->ctp_dev) {
+		DP_DEBUG(DP_DBG_FLAG_DBG,
+			 "dp_bp_dev_tbl[%d][%d]=%s current reg_cnt=%d\n",
+			 inst, bp, data->dev->name,
+			 dp_bp_dev_tbl[inst][bp].ref_cnt);
+		dp_bp_dev_tbl[inst][bp].dev = data->dev;
+		dp_bp_dev_tbl[inst][bp].ref_cnt++;
+		dp_bp_dev_tbl[inst][bp].flag = 1;
+		port_info->subif_info[subif_ix].ctp_dev =
+			data->subif_data->ctp_dev;
+	}
+	DP_DEBUG(DP_DBG_FLAG_DBG,
+		 "inst=%d portid=%d dp numsubif=%d subif_ix=%d pmapper.cnt=%d\n",
+		 inst, portid,
+		 port_info->num_subif, subif_ix,
+		 dp_bp_dev_tbl[inst][bp].ref_cnt);
+	if (data->subif_data)
+		deq_port_idx = data->subif_data->deq_port_idx;
+	if (port_info->deq_port_num < deq_port_idx + 1) {
+		PR_ERR("Wrong deq_port_idx(%d), should < %d\n",
+		       deq_port_idx, port_info->deq_port_num);
+		return -1;
+	}
+	/*QUEUE_CFG if needed */
+	q_port.cqe_deq = port_info->deq_port_base + deq_port_idx;
+	if (!priv) {
+		PR_ERR("priv NULL\n");
+		return -1;
+	}
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_DBG)
+	if (unlikely(dp_dbg_flag & DP_DBG_FLAG_QOS)) {
+		DP_DEBUG(DP_DBG_FLAG_QOS, "cqe_deq=%d\n", q_port.cqe_deq);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "priv=%p deq_port_stat=%p qdev=%p\n",
+			 priv,
+			 priv ? priv->deq_port_stat : NULL,
+			 priv ? priv->qdev : NULL);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "cqe_deq=%d inst=%d\n",
+			 q_port.cqe_deq, inst);
+	}
+#endif
+	q_port.tx_pkt_credit =
+		dp_deq_port_tbl[inst][q_port.cqe_deq].tx_pkt_credit;
+	q_port.tx_ring_addr =
+		dp_deq_port_tbl[inst][q_port.cqe_deq].tx_ring_addr;
+	q_port.tx_ring_size =
+		dp_deq_port_tbl[inst][q_port.cqe_deq].tx_ring_size;
+	q_port.inst = inst;
+	q_port.dp_port = portid;
+	q_port.ctp = subif_ix;
+
+	if (data->subif_data->flag_ops & DP_SUBIF_SPECIFIC_Q) {
+		q_flag = DP_SUBIF_SPECIFIC_Q;
+	} else if (data->subif_data->flag_ops & DP_SUBIF_AUTO_NEW_Q) {
+		q_flag = DP_SUBIF_AUTO_NEW_Q;
+	}  else { /*sharing mode (default)*/
+		if (!dp_deq_port_tbl[inst][q_port.cqe_deq].f_first_qid)
+			q_flag = DP_SUBIF_AUTO_NEW_Q; /*no queue created yet*/
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS, "Queue decision:%s\n", q_flag_str(q_flag));
+	if (q_flag == DP_SUBIF_AUTO_NEW_Q) {
+		int cqe_deq;
+
+		if (alloc_q_to_port(&q_port, 0)) {
+			PR_ERR("alloc_q_to_port fail for dp_port=%d\n",
+			       q_port.dp_port);
+			return -1;
+		}
+		if (dp_q_tbl[inst][q_port.qid].flag) {
+			PR_ERR("Why dp_q_tbl[%d][%d].flag =%d:expect 0?\n",
+			       inst, q_port.qid,
+			       dp_q_tbl[inst][q_port.qid].flag);
+			return -1;
+		}
+		if (dp_q_tbl[inst][q_port.qid].ref_cnt) {
+			PR_ERR("Why dp_q_tbl[%d][%d].ref_cnt =%d:expect 0?\n",
+			       inst, q_port.qid,
+			       dp_q_tbl[inst][q_port.qid].ref_cnt);
+			return -1;
+		}
+		/*update queue table */
+		dp_q_tbl[inst][q_port.qid].flag = 1;
+		dp_q_tbl[inst][q_port.qid].need_free = 1;
+		dp_q_tbl[inst][q_port.qid].ref_cnt = 1;
+		dp_q_tbl[inst][q_port.qid].q_node_id = q_port.q_node;
+		dp_q_tbl[inst][q_port.qid].cqm_dequeue_port = q_port.cqe_deq;
+
+		/*update port table */
+		cqe_deq = q_port.cqe_deq;
+		dp_deq_port_tbl[inst][cqe_deq].ref_cnt++;
+		dp_deq_port_tbl[inst][cqe_deq].qos_port = q_port.port_node;
+		if (!dp_deq_port_tbl[inst][cqe_deq].f_first_qid) {
+			dp_deq_port_tbl[inst][cqe_deq].first_qid = q_port.qid;
+			dp_deq_port_tbl[inst][cqe_deq].f_first_qid = 1;
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "dp_deq_port_tbl[%d][%d].first_qid=%d\n",
+				 inst, q_port.cqe_deq,
+				 dp_deq_port_tbl[inst][cqe_deq].first_qid);
+		}
+		/*update scheduler table later */
+
+	} else if (q_flag == DP_SUBIF_SPECIFIC_Q) { /*specified queue */
+		if (!dp_q_tbl[inst][q_port.qid].flag) {
+			/*1st time to use it
+			 *In this case, normally this queue is created by caller
+			 */
+			dp_q_tbl[inst][q_port.qid].flag = 1;
+			dp_q_tbl[inst][q_port.qid].need_free = 0; /*caller q*/
+			dp_q_tbl[inst][q_port.qid].ref_cnt = 1;
+
+			/*update port table
+			 *Note: since this queue is created by caller itself
+			 *      we need find way to get cqm_dequeue_port
+			 *      and qos_port later
+			 */
+			PR_INFO("need set cqm_dequeue_port/qos_port... ?\n");
+			dp_q_tbl[inst][q_port.qid].cqm_dequeue_port =
+				q_port.cqe_deq;
+			dp_deq_port_tbl[inst][q_port.cqe_deq].qos_port = -1;
+			dp_deq_port_tbl[inst][q_port.cqe_deq].ref_cnt++;
+		} else {
+			/*note: don't change need_free in this case */
+			dp_q_tbl[inst][q_port.cqe_deq].ref_cnt++;
+			dp_deq_port_tbl[inst][q_port.cqe_deq].ref_cnt++;
+		}
+
+		/*get already stored q_node_id/qos_port id to q_port
+		 */
+		q_port.q_node = dp_q_tbl[inst][q_port.qid].q_node_id;
+		q_port.port_node =
+			dp_deq_port_tbl[inst][q_port.cqe_deq].qos_port;
+
+		PR_INFO("need to further set q_port.q_node/port_node\n");
+		PR_INFO("via special internal QOS HAL API to get it\n");
+		PR_INFO("since it is created by caller itself\n");
+
+	} else { /*auto sharing queue: if go to here, it means sharing queue
+		  *is ready and it is created by previous dp_register_subif_ext
+		  */
+
+		/*get already stored q_node_id/qos_port id to q_port
+		 */
+		q_port.qid = dp_deq_port_tbl[inst][q_port.cqe_deq].first_qid;
+		q_port.q_node = dp_deq_port_tbl[inst][q_port.cqe_deq].q_node;
+		q_port.port_node =
+			dp_deq_port_tbl[inst][q_port.cqe_deq].qos_port;
+		dp_q_tbl[inst][q_port.qid].ref_cnt++;
+		dp_deq_port_tbl[inst][q_port.cqe_deq].ref_cnt++;
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "%s:%s=%d %s=%d q[%d].cnt=%d cqm_p[%d].cnt=%d\n",
+		 "subif_hw_set",
+		 "dp_port", portid,
+		 "vap", subif_ix,
+		 q_port.qid, dp_q_tbl[inst][q_port.qid].ref_cnt,
+		 q_port.cqe_deq, dp_deq_port_tbl[inst][q_port.cqe_deq].ref_cnt);
+#ifdef CONFIG_LTQ_DATAPATH_QOS_HAL
+	if (dp_deq_port_tbl[inst][q_port.cqe_deq].ref_cnt == 1) /*first CTP*/
+		data->act = TRIGGER_CQE_DP_ENABLE;
+#else
+	if (q_port.f_deq_port_en)
+		data->act = TRIGGER_CQE_DP_ENABLE;
+#endif
+	/*update subif table */
+	port_info->subif_info[subif_ix].qid = q_port.qid;
+	port_info->subif_info[subif_ix].q_node = q_port.q_node;
+	port_info->subif_info[subif_ix].qos_deq_port = q_port.port_node;
+	port_info->subif_info[subif_ix].cqm_deq_port = q_port.cqe_deq;
+	port_info->subif_info[subif_ix].cqm_port_idx = deq_port_idx;
+
+	/* Map this port's lookup to its 1st queue only */
+	//lookup.mode = dp_port_info[inst][portid].cqe_lu_mode; /*no need */
+	lookup.ep = portid;
+	lookup.sub_if_id = subif; /* Note:CQM API need full subif(15bits) */
+	/* For 1st subif and mode 0, use CBM_QUEUE_MAP_F_SUBIF_DONTCARE,
+	 * otherwise, don't use this flag
+	 */
+	if (!dp_port_info[inst][portid].num_subif &&
+	    (dp_port_info[inst][portid].cqe_lu_mode == CQE_LU_MODE0))
+		lookup_f |= CBM_QUEUE_MAP_F_SUBIF_DONTCARE;
+	cbm_queue_map_set(dp_port_prop[inst].cbm_inst,
+			  q_port.qid,
+			  &lookup,
+			  lookup_f);
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "%s %s=%d %s=%d %s=%d %s=%d %s=0x%x %s=0x%x(%d)\n",
+		 "cbm_queue_map_set",
+		 "qid", q_port.qid,
+		 "for dp_port", lookup.ep,
+		 "num_subif", dp_port_info[inst][portid].num_subif,
+		 "lu_mode", dp_port_info[inst][portid].cqe_lu_mode,
+		 "flag", lookup_f,
+		 "subif", subif, subif_ix);
+	return 0;
+}
+
+static int subif_hw_reset(int inst, int portid, int subif_ix,
+			  struct subif_platform_data *data, u32 flags)
+{
+	int qid;
+	int cqm_deq_port;
+	struct pmac_port_info *port_info = &dp_port_info[inst][portid];
+	struct dp_node_alloc node;
+	int bp = port_info->subif_info[subif_ix].bp;
+
+	qid = port_info->subif_info[subif_ix].qid;
+	cqm_deq_port = port_info->subif_info[subif_ix].cqm_deq_port;
+	bp = port_info->subif_info[subif_ix].bp;
+	/* santity check table */
+	if (!dp_q_tbl[inst][qid].ref_cnt) {
+		PR_ERR("Why dp_q_tbl[%d][%d].ref_cnt Zero: expect > 0\n",
+		       inst, qid);
+		return DP_FAILURE;
+	}
+	if (!dp_deq_port_tbl[inst][cqm_deq_port].ref_cnt) {
+		PR_ERR("Why dp_deq_port_tbl[%d][%d].ref_cnt Zero\n",
+		       inst, cqm_deq_port);
+		return DP_FAILURE;
+	}
+	if ((port_info->subif_info[subif_ix].ctp_dev) &&
+	    !dp_bp_dev_tbl[inst][bp].ref_cnt) {
+		PR_ERR("Why dp_bp_dev_tbl[%d][%d].ref_cnt =%d\n",
+		       inst, bp, dp_bp_dev_tbl[inst][bp].ref_cnt);
+		return DP_FAILURE;
+	}
+	/* update queue/port/sched/bp_pmapper table's ref_cnt */
+	dp_q_tbl[inst][qid].ref_cnt--;
+	dp_deq_port_tbl[inst][cqm_deq_port].ref_cnt--;
+	if (port_info->subif_info[subif_ix].ctp_dev) { /* pmapper */
+		dp_bp_dev_tbl[inst][bp].ref_cnt--;
+		if (!dp_bp_dev_tbl[inst][bp].ref_cnt) {
+			port_info->subif_info[subif_ix].ctp_dev = NULL;
+			dp_bp_dev_tbl[inst][bp].dev = NULL;
+			dp_bp_dev_tbl[inst][bp].flag = 0;
+			DP_DEBUG(DP_DBG_FLAG_REG,
+				 "ctp ref_cnt becomes zero:%s\n",
+				 port_info->subif_info[subif_ix].device_name);
+		}
+	}
+
+	reset_ctp_bp(inst, subif_ix, portid, bp);
+	if (!dp_bp_dev_tbl[inst][bp].dev) /*NULL already, then free it */ {
+		DP_DEBUG(DP_DBG_FLAG_PAE, "Free BP[%d] vap=%d\n",
+			 bp, subif_ix);
+		free_bridge_port(inst, bp);
+	}
+#ifdef CONFIG_LTQ_DATAPATH_QOS_HAL
+	qid = port_info->subif_info[subif_ix].qid;
+	cqm_deq_port = dp_q_tbl[inst][qid].cqm_dequeue_port;
+
+	if (dp_q_tbl[inst][qid].flag &&
+	    !dp_q_tbl[inst][qid].ref_cnt &&/*no one is using */
+	    dp_q_tbl[inst][qid].need_free) {
+		DP_DEBUG(DP_DBG_FLAG_QOS, "Free qid %d\n", qid);
+		node.id.q_id = qid;
+		/*if no subif using this queue, need to delete it*/
+		node.inst = inst;
+		node.dp_port = portid;
+		node.type = DP_NODE_QUEUE;
+		dp_node_free(&node, 0);
+
+		/*update dp_q_tbl*/
+		dp_q_tbl[inst][qid].flag = 0;
+		dp_q_tbl[inst][qid].need_free = 0;
+		if (dp_deq_port_tbl[inst][cqm_deq_port].f_first_qid &&
+		    (dp_deq_port_tbl[inst][cqm_deq_port].first_qid
+		     == qid)) {
+			dp_deq_port_tbl[inst][cqm_deq_port].f_first_qid = 0;
+			dp_deq_port_tbl[inst][cqm_deq_port].first_qid = 0;
+			DP_DEBUG(DP_DBG_FLAG_QOS, "q_id[%d] is freed\n", qid);
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "dp_deq_port_tbl[%d][%d].f_first_qid reset\n",
+				 inst, cqm_deq_port);
+		}
+	} else {
+		DP_DEBUG(DP_DBG_FLAG_QOS, "q_id[%d] dont need freed\n", qid);
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "%s:%s=%d %s=%d q[%d].cnt=%d cqm_p[%d].cnt=%d\n",
+		 "subif_hw_reset",
+		 "dp_port", portid,
+		 "vap", subif_ix,
+		 qid, dp_q_tbl[inst][qid].ref_cnt,
+		 cqm_deq_port, dp_deq_port_tbl[inst][cqm_deq_port].ref_cnt);
+#else
+	qos_queue_flush(priv->qdev, port_info->subif_info[subif_ix].q_node);
+	qos_queue_remove(priv->qdev, port_info->subif_info[subif_ix].q_node);
+	qos_port_remove(priv->qdev,
+			port_info->subif_info[subif_ix].qos_deq_port);
+	priv->deq_port_stat[port_info->subif_info[subif_ix].cqm_deq_port].flag =
+		PP_NODE_FREE;
+#endif /* CONFIG_LTQ_DATAPATH_QOS_HAL */
+
+	if (!port_info->num_subif &&
+	    dp_deq_port_tbl[inst][cqm_deq_port].ref_cnt) {
+		PR_ERR("num_subif(%d) not match dp_deq_port[%d][%d].ref_cnt\n",
+		       port_info->num_subif,
+		       inst, cqm_deq_port);
+		return DP_FAILURE;
+	}
+
+	return DP_SUCCESS;
+}
+
+/*Set basic BP/CTP */
+static int subif_platform_set(int inst, int portid, int subif_ix,
+			      struct subif_platform_data *data, u32 flags)
+{
+	if (flags & DP_F_DEREGISTER)
+		return subif_hw_reset(inst, portid, subif_ix, data, flags);
+	return subif_hw_set(inst, portid, subif_ix, data, flags);
+}
+
+static int supported_logic_dev(int inst, struct net_device *dev,
+			       char *subif_name)
+{
+	return is_vlan_dev(dev);
+}
+
+static int subif_platform_set_unexplicit(int inst, int port_id,
+					 struct logic_dev *logic_dev,
+					 u32 flags)
+{
+	if (flags & DP_F_DEREGISTER) {
+		free_bridge_port(inst, logic_dev->bp);
+	} else {
+		logic_dev->bp =
+			alloc_bridge_port(inst, port_id, logic_dev->ctp,
+					  CPU_FID, CPU_BP);
+	}
+
+	return 0;
+}
+
+static int not_valid_rx_ep(int ep)
+{
+	return (((ep >= 3) && (ep <= 6)) || (ep == 2) || (ep > 15));
+}
+
+static void set_pmac_subif(struct pmac_tx_hdr *pmac, int32_t subif)
+{
+	pmac->src_dst_subif_id_lsb = subif & 0xff;
+	pmac->src_dst_subif_id_msb =  (subif >> 8) & 0x1f;
+}
+
+static void update_port_vap(int inst, u32 *ep, int *vap,
+			    struct sk_buff *skb,
+			    struct pmac_rx_hdr *pmac, char *decryp)
+{
+	//*ep = pmac->igp_egp; /*get the port_id from pmac's sppid */
+	*ep = (skb->DW1 >> 4) & 0xF; /*get the port_id from pmac's sppid */
+	if (dp_port_info[inst][*ep].alloc_flags & DP_F_LOOPBACK) {
+		/*get the real source port from VAP for ipsec */
+		/* related tunnel decap case */
+		*ep = GET_VAP((u32)pmac->src_dst_subif_id_lsb +
+			(u32)(pmac->src_dst_subif_id_msb << 8),
+			PORT_INFO(inst, *ep, vap_offset),
+			PORT_INFO(inst, *ep, vap_mask));
+		*vap = 0;
+		*decryp = 1;
+	} else {
+		struct dma_rx_desc_1 *desc_1;
+
+		desc_1 = (struct dma_rx_desc_1 *)&skb->DW1;
+		*vap = desc_1->field.session_id;
+	}
+}
+
+static void get_dma_pmac_templ(int index, struct pmac_tx_hdr *pmac,
+			       struct dma_tx_desc_0 *desc_0,
+			       struct dma_tx_desc_1 *desc_1,
+			       struct pmac_port_info2 *dp_info)
+{
+	if (likely(pmac))
+		memcpy(pmac, &dp_info->pmac_template[index], sizeof(*pmac));
+	desc_0->all = (desc_0->all & dp_info->dma0_mask_template[index].all) |
+				dp_info->dma0_template[index].all;
+	desc_1->all = (desc_1->all & dp_info->dma1_mask_template[index].all) |
+				dp_info->dma1_template[index].all;
+}
+
+static int check_csum_cap(void)
+{
+	return 0;
+}
+
+static int get_itf_start_end(struct gsw_itf *itf_info, u16 *start, u16 *end)
+{
+	if (!itf_info)
+		return -1;
+	if (start)
+		*start = itf_info->start;
+	if (end)
+		*end = itf_info->end;
+
+	return 0;
+}
+
+int register_dp_cap_gswip31(int flag)
+{
+	struct dp_hw_cap cap;
+
+	memset(&cap, 0, sizeof(cap));
+	cap.info.type = GSWIP31_TYPE;
+	cap.info.ver = GSWIP31_VER;
+
+	cap.info.dp_platform_set = dp_platform_set;
+	cap.info.port_platform_set = port_platform_set;
+	cap.info.subif_platform_set_unexplicit = subif_platform_set_unexplicit;
+	cap.info.proc_print_ctp_bp_info = proc_print_ctp_bp_info;
+	cap.info.init_dma_pmac_template = init_dma_pmac_template;
+	//dp.info.port_platform_set_unexplicit = port_platform_set_unexplicit;
+	cap.info.subif_platform_set = subif_platform_set;
+	cap.info.init_dma_pmac_template = init_dma_pmac_template;
+	cap.info.not_valid_rx_ep = not_valid_rx_ep;
+	cap.info.set_pmac_subif = set_pmac_subif;
+	cap.info.update_port_vap = update_port_vap;
+	cap.info.check_csum_cap = check_csum_cap;
+	cap.info.get_dma_pmac_templ = get_dma_pmac_templ;
+	cap.info.get_itf_start_end = get_itf_start_end;
+	cap.info.dump_rx_dma_desc = dump_rx_dma_desc;
+	cap.info.dump_tx_dma_desc = dump_tx_dma_desc;
+	cap.info.dump_rx_pmac = dump_rx_pmac;
+	cap.info.dump_tx_pmac = dump_tx_pmac;
+	cap.info.supported_logic_dev = supported_logic_dev;
+	cap.info.dp_pmac_set = dp_pmac_set_31;
+	cap.info.dp_set_gsw_parser = dp_set_gsw_parser_31;
+	cap.info.dp_get_gsw_parser = dp_get_gsw_parser_31;
+	cap.info.dp_qos_platform_set = qos_platform_set;
+	cap.info.dp_set_gsw_pmapper = dp_set_gsw_pmapper_31;
+	cap.info.dp_get_gsw_pmapper = dp_get_gsw_pmapper_31;
+#ifdef CONFIG_LTQ_DATAPATH_HAL_GSWIP31_MIB
+	cap.info.dp_get_port_vap_mib = dp_get_port_vap_mib_31;
+	cap.info.dp_clear_netif_mib = dp_clear_netif_mib_31;
+#endif
+
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_SWITCHDEV)
+	cap.info.swdev_flag = 1;
+	cap.info.swdev_alloc_bridge_id = dp_swdev_alloc_bridge_id;
+	cap.info.swdev_free_brcfg = dp_swdev_free_brcfg;
+	cap.info.swdev_bridge_cfg_set = dp_swdev_bridge_cfg_set;
+	cap.info.swdev_bridge_port_cfg_reset = dp_swdev_bridge_port_cfg_reset;
+	cap.info.swdev_bridge_port_cfg_set = dp_swdev_bridge_port_cfg_set;
+	cap.info.dp_mac_set = dp_gswip_mac_entry_add;
+	cap.info.dp_mac_reset = dp_gswip_mac_entry_del;
+	cap.info.dp_cfg_vlan = dp_gswip_ext_vlan; /*for symmetric VLAN */
+	cap.info.dp_tc_vlan_set = tc_vlan_set_31;
+#endif
+	cap.info.cap.tx_hw_chksum = 0;
+	cap.info.cap.rx_hw_chksum = 0;
+	cap.info.cap.hw_tso = 0;
+	cap.info.cap.hw_gso = 0;
+	strncpy(cap.info.cap.qos_eng_name, "ppv4",
+		sizeof(cap.info.cap.qos_eng_name));
+	strncpy(cap.info.cap.pkt_eng_name, "mpe",
+		sizeof(cap.info.cap.pkt_eng_name));
+	cap.info.cap.max_num_queues = MAX_QUEUE;
+	cap.info.cap.max_num_scheds = MAX_SCHD;
+	cap.info.cap.max_num_deq_ports = MAX_CQM_DEQ;
+	cap.info.cap.max_num_qos_ports = MAX_PPV4_PORT;
+	cap.info.cap.max_num_dp_ports = PMAC_MAX_NUM;
+	cap.info.cap.max_num_subif_per_port = MAX_SUBIF_PER_PORT;
+	cap.info.cap.max_num_subif = 288;
+	cap.info.cap.max_num_bridge_port = 128;
+
+	if (register_dp_hw_cap(&cap, flag)) {
+		PR_ERR("Why register_dp_hw_cap fail\n");
+		return -1;
+	}
+
+	return 0;
+}
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_misc.h b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_misc.h
new file mode 100644
index 000000000000..a44d4cb7f0cb
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_misc.h
@@ -0,0 +1,219 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#ifndef DATAPATH_MISC31_H
+#define DATAPATH_MISC31_H
+
+#include <linux/notifier.h>
+#include <linux/netdevice.h>
+#include "datapath_ppv4.h"
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+
+#define PMAC_MAX_NUM  16
+#define PAMC_LAN_MAX_NUM 7
+#define VAP_OFFSET 8
+#define VAP_MASK  0xF
+#define VAP_DSL_OFFSET 3
+#define NEW_CBM_API 1
+#define PMAPPER_DISC_CTP 255
+
+#define GSWIP_O_DEV_NAME 1
+#define GSWIP_L GSWIP_O_DEV_NAME
+#define GSWIP_R GSWIP_O_DEV_NAME
+#define MAX_SUBIF_PER_PORT 256
+#define MAX_CTP 288
+#define MAX_BP_NUM 128
+#define CPU_PORT 0
+#define CPU_SUBIF 0 /*cpu default subif ID*/
+#define CPU_BP 0 /*cpu default bridge port ID */
+#define CPU_FID 0 /*cpu default bridge ID */
+#define SET_BP_MAP(x, ix) (x[(ix) / 16] |= 1 << ((ix) % 16))
+#define GET_BP_MAP(x, ix) ((x[(ix) / 16] >> ((ix) % 16)) & 1)
+#define UNSET_BP_MAP(x, ix) (x[(ix) / 16] &= ~(1 << ((ix) % 16)))
+
+enum CQE_LOOKUP_MODE {
+	CQE_LU_MODE0 = 0,
+	CQE_LU_MODE1,
+	CQE_LU_MODE2,
+	CQE_LU_MODE3,
+	CQE_LU_MODE_NOT_VALID,
+};
+
+#define PMAC_SIZE 8
+
+struct gsw_itf {
+	u8 ep; /*-1 means no assigned yet for dynamic case */
+	u8 fixed; /*fixed (1) or dynamically allocate (0)*/
+	u16 start;
+	u16 end;
+	u16 n;
+	u8 mode;
+	u8 cqe_mode; /*CQE look up mode */
+};
+
+struct cqm_deq_stat;
+struct pp_queue_stat;
+
+struct resv_q {
+	int flag;
+	int id;
+	int physical_id;
+};
+
+struct resv_sch {
+	int flag;
+	int id;
+};
+
+struct resv_info {
+	int num_resv_q; /*!< input:reserve the required number of queues*/
+	int num_resv_sched; /*!< input:reserve required number of schedulers*/
+	struct resv_q  *resv_q; /*!< reserved queues info*/
+	struct resv_sch *resv_sched; /*!< reserved schedulers info */
+};
+
+struct pp_qos_dev;
+struct hal_priv {
+	struct cqm_deq_stat deq_port_stat[MAX_CQM_DEQ];
+	struct pp_queue_stat qos_queue_stat[MAX_QUEUE];
+	struct pp_sch_stat qos_sch_stat[QOS_MAX_NODES];
+	struct resv_info resv[MAX_DP_PORTS];
+	int bp_def;
+	struct pp_qos_dev *qdev; /* ppv4 qos dev */
+	s32 ppv4_drop_q;  /* drop queue: physical id */
+	int cqm_drop_p; /* cqm drop/flush port id*/
+	u32 ppv4_drop_p;  /* drop qos port(logical node_id):workaround for
+			   * PPV4 API issue to get physical queue id
+			   * before pp_qos_queue_set
+			   */
+	u32 ppv4_tmp_p; /* workaround for ppv4 queue allocate to
+			 * to get physical queue id
+			 */
+};
+
+struct datapath_ctrl {
+	struct dentry *debugfs;
+	const char *name;
+};
+
+#define SET_PMAC_IGP_EGP(pmac, port_id) ((pmac)->igp_egp = (port_id) & 0xF)
+
+#define SET_PMAC_SUBIF(pmac, subif) do { \
+	(pmac)->src_dst_subif_id_lsb = (subif) & 0xff; \
+	(pmac)->src_dst_subif_id_msb =  ((subif) >> 8) & 0x1f; \
+} while (0)
+
+int alloc_bridge_port(int inst, int portid, int subif, int fid, int bp_member);
+int free_bridge_port(int inst, int bp);
+struct gsw_itf *ctp_port_assign(int inst, u8 ep, int bp_default,
+				u32 flags);
+void dp_sys_mib_reset_31(u32 flag);
+int dp_pmac_set_31(int inst, u32 port, dp_pmac_cfg_t *pmac_cfg);
+int dp_set_gsw_parser_31(u8 flag, u8 cpu, u8 mpe1, u8 mpe2, u8 mpe3);
+int dp_get_gsw_parser_31(u8 *cpu, u8 *mpe1, u8 *mpe2, u8 *mpe3);
+int gsw_mib_reset_31(int dev, u32 flag);
+int proc_print_ctp_bp_info(struct seq_file *s, int inst,
+			   struct pmac_port_info *port,
+			   int subif_index, u32 flag);
+
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_SWITCHDEV)
+int dp_gswip_mac_entry_add(int bport, int fid, int inst, u8 *addr);
+int dp_gswip_mac_entry_del(int bport, int fid, int inst, u8 *addr);
+int set_gswip_ext_vlan(struct core_ops *ops, struct ext_vlan_info *vlan,
+		       int flag);
+#endif
+int qos_platform_set(int cmd_id, void *node, int flag);
+int dp_node_alloc_31(struct dp_node_alloc *node, int flag);
+int dp_node_free_31(struct dp_node_alloc *node, int flag);
+int dp_deq_port_res_get_31(struct dp_dequeue_res *res, int flag);
+int dp_node_link_en_get_31(struct dp_node_link_enable *en, int flag);
+int dp_node_link_en_set_31(struct dp_node_link_enable *en, int flag);
+int dp_qos_link_prio_set_31(struct dp_node_prio *info, int flag);
+int dp_qos_link_prio_get_31(struct dp_node_prio *info, int flag);
+int dp_node_link_add_31(struct dp_node_link *info, int flag);
+int dp_link_add_31(struct dp_qos_link *cfg, int flag);
+int dp_link_get_31(struct dp_qos_link *cfg, int flag);
+int dp_node_unlink_31(struct dp_node_link *info, int flag);
+int dp_node_link_get_31(struct dp_node_link *info, int flag);
+int dp_queue_conf_set_31(struct dp_queue_conf *cfg, int flag);
+int dp_queue_conf_get_31(struct dp_queue_conf *cfg, int flag);
+int dp_shaper_conf_set_31(struct dp_shaper_conf *cfg, int flag);
+int dp_shaper_conf_get_31(struct dp_shaper_conf *cfg, int flag);
+int dp_queue_map_get_31(struct dp_queue_map_get *cfg, int flag);
+int dp_queue_map_set_31(struct dp_queue_map_set *cfg, int flag);
+int dp_counter_mode_set_31(struct dp_counter_conf *cfg, int flag);
+int dp_counter_mode_get_31(struct dp_counter_conf *cfg, int flag);
+int dp_set_gsw_pmapper_31(int inst, int bport, int lport,
+			  struct dp_pmapper *mapper, u32 flag);
+int dp_get_gsw_pmapper_31(int inst, int bport, int lport,
+			  struct dp_pmapper *mapper, u32 flag);
+int dp_children_get_31(struct dp_node_child *cfg, int flag);
+int dp_free_children_via_parent_31(struct dp_node_alloc *node, int flag);
+int dp_node_reserve(int inst, int ep, struct dp_port_data *data, int flags);
+int dp_qos_level_get_31(struct dp_qos_level *dp, int flag);
+
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_DDR_SIMULATE_GSWIP31)
+GSW_return_t gsw_core_api_ddr_simu31(dp_gsw_cb func, void *ops, void *param);
+#define GSW_SIMUTE_DDR_NOT_MATCH  0x1234
+#endif
+
+static inline GSW_return_t gsw_core_api(dp_gsw_cb func, void *ops, void *param)
+{
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_DBG)
+	if (dp_dbg_flag & DP_DBG_FLAG_GSWIP_API)
+		print_symbol_name((unsigned long)func);
+#endif /*CONFIG_LTQ_DATAPATH_DBG*/
+
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_DDR_SIMULATE_GSWIP31)
+	{
+		GSW_return_t res;
+
+		res = gsw_core_api_ddr_simu31(func, ops, param);
+		if (res != GSW_SIMUTE_DDR_NOT_MATCH)
+			return res;
+	}
+#endif /*CONFIG_LTQ_DATAPATH_DDR_SIMULATE_GSWIP31*/
+	return func(ops, param);
+}
+
+static inline char *parser_flag_str(u8 f)
+{
+	if (f == DP_PARSER_F_DISABLE)
+		return "No Parser";
+	else if (f == DP_PARSER_F_HDR_ENABLE)
+		return "Parser Flag only";
+	else if (f == DP_PARSER_F_HDR_OFFSETS_ENABLE)
+		return "Parser Full";
+	else
+		return "Reserved";
+}
+
+int dp_sub_proc_install_31(void);
+char *get_dma_flags_str31(u32 epn, char *buf, int buf_len);
+int lookup_dump31(struct seq_file *s, int pos);
+int lookup_start31(void);
+ssize_t proc_get_qid_via_index31(struct file *file, const char *buf,
+				 size_t count, loff_t *ppos);
+ssize_t proc_get_qid_via_index(struct file *file, const char *buf,
+			       size_t count, loff_t *ppos);
+int datapath_debugfs_init(struct datapath_ctrl *pctrl);
+int get_q_qocc(int inst, int qid, u32 *c);
+int get_q_mib(int inst, int qid,
+	      u32 *total_accept,
+	      u32 *total_drop,
+	      u32 *red_drop);
+int get_p_mib(int inst, int pid,
+	      u32 *green /* bytes*/,
+	      u32 *yellow /*bytes*/);
+int cpu_vlan_mod_dis(int inst);
+int tc_vlan_set_31(struct core_ops *ops, struct dp_tc_vlan *vlan,
+		   struct dp_tc_vlan_info *info,
+		   int flag);
+#endif
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_ppv4.c b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_ppv4.c
new file mode 100644
index 000000000000..b313bf3c2a67
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_ppv4.c
@@ -0,0 +1,822 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include <net/datapath_api.h>
+#include "../datapath.h"
+#include "datapath_misc.h"
+
+int (*qos_queue_remove)(struct pp_qos_dev *qos_dev, unsigned int id);
+int (*qos_queue_allocate)(struct pp_qos_dev *qos_dev, unsigned int *id);
+int (*qos_queue_info_get)(struct pp_qos_dev *qos_dev, unsigned int id,
+			  struct pp_qos_queue_info *info);
+int (*qos_port_remove)(struct pp_qos_dev *qos_dev, unsigned int id);
+int (*qos_sched_allocate)(struct pp_qos_dev *qos_dev, unsigned int *id);
+int (*qos_sched_remove)(struct pp_qos_dev *qos_dev, unsigned int id);
+int (*qos_port_allocate)(struct pp_qos_dev *qos_dev,
+			 unsigned int physical_id,
+			 unsigned int *id);
+int (*qos_port_set)(struct pp_qos_dev *qos_dev, unsigned int id,
+		    const struct pp_qos_port_conf *conf);
+void (*qos_port_conf_set_default)(struct pp_qos_port_conf *conf);
+void (*qos_queue_conf_set_default)(struct pp_qos_queue_conf *conf);
+int (*qos_queue_set)(struct pp_qos_dev *qos_dev, unsigned int id,
+		     const struct pp_qos_queue_conf *conf);
+void (*qos_sched_conf_set_default)(struct pp_qos_sched_conf *conf);
+int (*qos_sched_set)(struct pp_qos_dev *qos_dev, unsigned int id,
+		     const struct pp_qos_sched_conf *conf);
+int (*qos_queue_conf_get)(struct pp_qos_dev *qos_dev, unsigned int id,
+			  struct pp_qos_queue_conf *conf);
+int (*qos_queue_flush)(struct pp_qos_dev *qos_dev, unsigned int id);
+int (*qos_sched_conf_get)(struct pp_qos_dev *qos_dev, unsigned int id,
+			  struct pp_qos_sched_conf *conf);
+int (*qos_sched_get_queues)(struct pp_qos_dev *qos_dev, unsigned int id,
+			    u16 *queue_ids, unsigned int size,
+			    unsigned int *queues_num);
+int (*qos_port_get_queues)(struct pp_qos_dev *qos_dev, unsigned int id,
+			   u16 *queue_ids, unsigned int size,
+				  unsigned int *queues_num);
+int (*qos_port_conf_get)(struct pp_qos_dev *qdev, unsigned int id,
+			 struct pp_qos_port_conf *conf);
+int (*qos_port_info_get)(struct pp_qos_dev *qdev, unsigned int id,
+			 struct pp_qos_port_info *info);
+struct pp_qos_dev *(*qos_dev_open)(unsigned int id);
+int (*qos_dev_init)(struct pp_qos_dev *qos_dev,
+		    struct pp_qos_init_param *conf);
+
+#ifdef CONFIG_LTQ_DATAPATH_DUMMY_QOS
+struct fixed_q_port {
+	int deq_port; /*cqm dequeue port */
+	int queue_id; /*queue physical id*/
+	int port_node; /*qos dequeue port node id */
+	int queue_node; /*queue node id */
+	int q_used;    /*flag to indicate used or free*/
+};
+
+struct fixed_q_port q_port[] = {
+	{0, 14, 0, 2, 0},
+	{0, 74, 0, 4, 0},
+	{0, 30, 0, 6, 0},
+	{0, 87, 0, 8, 0},
+	{7, 235, 7, 10, 0},
+	{7, 42, 7, 12, 0},
+	{7, 242, 7, 14, 0},
+	{26, 190, 26, 16, 0},
+	{26, 119, 26, 18, 0},
+	{26, 103, 26, 20, 0}
+};
+
+static struct pp_qos_dev qdev;
+
+int test_qos_queue_remove(struct pp_qos_dev *qos_dev, unsigned int id)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(q_port); i++) {
+		if (q_port[i].queue_id == id) {
+			q_port[i].q_used = PP_NODE_FREE;
+			DP_DEBUG(DP_DBG_FLAG_DBG, "to free qid=%d\n", id);
+			return 0;
+		}
+	}
+	return -1;
+}
+
+int test_qos_queue_allocate(struct pp_qos_dev *qos_dev, unsigned int *id)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(q_port); i++) {
+		if ((q_port[i].deq_port == qos_dev->dq_port) &&
+		    (q_port[i].q_used == PP_NODE_FREE)) {
+			q_port[i].q_used = PP_NODE_ALLOC;
+			DP_DEBUG(DP_DBG_FLAG_DBG, "allocate qnode_id=%d\n",
+				 q_port[i].queue_node);
+			*id = q_port[i].queue_node;
+			return 0;
+		}
+	}
+	return -1;
+}
+
+int test_qos_queue_info_get(struct pp_qos_dev *qos_dev, unsigned int id,
+			    struct pp_qos_queue_info *info)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(q_port); i++) {
+		if (q_port[i].queue_node == id) {
+			DP_DEBUG(DP_DBG_FLAG_DBG, "q[%d]'s qid=%d\n",
+				 id, q_port[i].queue_id);
+			info->physical_id = q_port[i].queue_id;
+			return 0;
+		}
+	}
+	return -1;
+}
+
+int test_qos_port_remove(struct pp_qos_dev *qos_dev, unsigned int id)
+{
+	return 0;
+}
+
+int test_qos_sched_allocate(struct pp_qos_dev *qos_dev, unsigned int *id)
+{
+	return 0;
+}
+
+int test_qos_sched_remove(struct pp_qos_dev *qos_dev, unsigned int id)
+{
+	return 0;
+}
+
+int test_qos_port_allocate(struct pp_qos_dev *qos_dev, unsigned int physical_id,
+			   unsigned int *id)
+{
+	int i;
+
+	if (!id)
+		return -1;
+
+	for (i = 0; i < ARRAY_SIZE(q_port); i++) {
+		if (physical_id == q_port[i].deq_port) {
+			*id = q_port[i].port_node;
+			DP_DEBUG(DP_DBG_FLAG_DBG,
+				 "Ok: Deq_port/Node_id=%d/%d\n",
+				 physical_id, *id);
+			return 0;
+		}
+	}
+	return -1;
+}
+
+int test_qos_port_set(struct pp_qos_dev *qos_dev, unsigned int id,
+		      const struct pp_qos_port_conf *conf)
+{
+	return 0;
+}
+
+void test_qos_port_conf_set_default(struct pp_qos_port_conf *conf)
+{
+}
+
+void test_qos_queue_conf_set_default(struct pp_qos_queue_conf *conf)
+{
+}
+
+int test_qos_queue_set(struct pp_qos_dev *qos_dev, unsigned int id,
+		       const struct pp_qos_queue_conf *conf)
+{
+	return 0;
+}
+
+void test_qos_sched_conf_set_default(struct pp_qos_sched_conf *conf)
+{
+}
+
+int test_qos_sched_set(struct pp_qos_dev *qos_dev, unsigned int id,
+		       const struct pp_qos_sched_conf *conf)
+{
+	return 0;
+}
+
+int test_qos_queue_conf_get(struct pp_qos_dev *qos_dev, unsigned int id,
+			    struct pp_qos_queue_conf *conf)
+{
+	int i;
+
+	if (!conf)
+		return -1;
+	memset(conf, 0, sizeof(*conf));
+	for (i = 0; i < ARRAY_SIZE(q_port); i++) {
+		if (id == q_port[i].queue_node) {
+			conf->queue_child_prop.parent = q_port[i].port_node;
+			conf->common_prop.bandwidth_limit = 0;
+			conf->blocked = 0;
+			return 0;
+		}
+	}
+	return -1;
+}
+
+int test_qos_queue_flush(struct pp_qos_dev *qos_dev, unsigned int id)
+{
+	return 0;
+}
+
+int test_qos_sched_conf_get(struct pp_qos_dev *qos_dev, unsigned int id,
+			    struct pp_qos_sched_conf *conf)
+{
+	return -1;
+}
+
+int test_qos_sched_get_queues(struct pp_qos_dev *qos_dev, unsigned int id,
+			      u16 *queue_ids, unsigned int size,
+			    unsigned int *queues_num)
+{
+	return 0;
+}
+
+int test_qos_port_get_queues(struct pp_qos_dev *qos_dev, unsigned int id,
+			     u16 *queue_ids, unsigned int size,
+			   unsigned int *queues_num)
+{
+	int i, num = 0;
+
+	for (i = 0; i < ARRAY_SIZE(q_port); i++) {
+		if (q_port[i].port_node != id)
+			continue;
+		if (queue_ids && (num < size)) {
+			queue_ids[num] = q_port[i].queue_node;
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "saved[%d] qid[%d/%d] for cqm[%d/%d]\n",
+				 num,
+				 q_port[i].queue_id,
+				 q_port[i].queue_node,
+				 q_port[i].deq_port,
+				 q_port[i].port_node);
+		} else {
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "unsaved[%d]: qid[%d/%d] for cqm[%d/%d]\n",
+				 num,
+				 q_port[i].queue_id,
+				 q_port[i].queue_node,
+				 q_port[i].deq_port,
+				 q_port[i].port_node);
+		}
+		num++;
+	}
+	if (queues_num)
+		*queues_num = num;
+	return 0;
+}
+
+int test_qos_port_conf_get(struct pp_qos_dev *qdev, unsigned int id,
+			   struct pp_qos_port_conf *conf)
+{
+	return 0;
+}
+
+int test_qos_port_info_get(struct pp_qos_dev *qdev, unsigned int id,
+			   struct pp_qos_port_info *info)
+{
+	return 0;
+}
+
+/*this test code is only support one instance */
+struct pp_qos_dev *test_qos_dev_open(unsigned int id)
+{
+	return &qdev;
+}
+
+int test_qos_dev_init(struct pp_qos_dev *qos_dev,
+		      struct pp_qos_init_param *conf)
+{
+	return 0;
+}
+
+#ifdef DUMMY_PPV4_QOS_API_OLD
+s32 qos_node_config(struct qos_node_api_param *param)
+{
+	int i;
+
+	if (param->op_type != QOS_OP_ADD)  {/*only support ADD op */
+		PR_ERR("Wrong op_type=%d\n", param->op_type);
+		return -1;
+	}
+	if (param->node_conf.node_type == (enum qos_node_type_e)DP_NODE_PORT) {
+		for (i = 0; i < ARRAY_SIZE(q_port); i++) {
+			if (param->deq_port == q_port[i].deq_port) {
+				param->out_param.node_id = q_port[i].port_node;
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "Ok: Deq_port/Node_id=%d/%d\n",
+					 param->deq_port,
+					 param->out_param.node_id);
+				return 0;
+			}
+		}
+		PR_ERR("QoS Port Add failed for deq_port=%d\n",
+		       param->deq_port);
+		return -1;
+	}
+
+	if (param->node_conf.node_type == (enum qos_node_type_e)DP_NODE_QUEUE) {
+		for (i = 0; i < ARRAY_SIZE(q_port); i++) {
+			if (q_port[i].q_used)
+				continue;
+			if (param->node_conf.parent_node_id ==
+						q_port[i].port_node) {
+				param->out_param.node_id = q_port[i].queue_node;
+				param->out_param.queue_id = q_port[i].queue_id;
+				q_port[i].q_used = 1;
+				DP_DEBUG(DP_DBG_FLAG_DBG,
+					 "Ok:Deq_port/Node_id=%d/%d %s=%d/%d\n",
+					 q_port[i].deq_port,
+					 param->node_conf.parent_node_id,
+					 "Queue_id/Node",
+					 param->out_param.queue_id,
+					 param->out_param.node_id);
+				return 0;
+			}
+		}
+	}
+	PR_ERR("QoS Queue Add failed for parent node=%d\n",
+	       param->node_conf.parent_node_id);
+	return -1;
+}
+#endif /*DUMMY_PPV4_QOS_API_OLD*/
+
+#endif /*CONFIG_LTQ_DATAPATH_DUMMY_QOS*/
+
+void init_qos_fn(void)
+{
+#ifdef CONFIG_LTQ_DATAPATH_DUMMY_QOS
+	qos_queue_remove = test_qos_queue_remove;
+	qos_queue_allocate = test_qos_queue_allocate;
+	qos_queue_info_get = test_qos_queue_info_get;
+	qos_port_remove = test_qos_port_remove;
+	qos_sched_allocate = test_qos_sched_allocate;
+	qos_sched_remove = test_qos_sched_remove;
+	qos_port_allocate = test_qos_port_allocate;
+	qos_port_set = test_qos_port_set;
+	qos_port_conf_set_default = test_qos_port_conf_set_default;
+	qos_queue_conf_set_default = test_qos_queue_conf_set_default;
+	qos_queue_set = test_qos_queue_set;
+	qos_sched_conf_set_default = test_qos_sched_conf_set_default;
+	qos_sched_set = test_qos_sched_set;
+	qos_queue_conf_get = test_qos_queue_conf_get;
+	qos_queue_flush = test_qos_queue_flush;
+	qos_sched_conf_get = test_qos_sched_conf_get;
+	qos_sched_get_queues = test_qos_sched_get_queues;
+	qos_port_get_queues = test_qos_port_get_queues;
+	qos_port_conf_get = test_qos_port_conf_get;
+	qos_dev_open = test_qos_dev_open;
+	qos_dev_init = test_qos_dev_init;
+#elif CONFIG_LTQ_PPV4_QOS
+	qos_queue_remove = pp_qos_queue_remove;
+	qos_queue_allocate = pp_qos_queue_allocate;
+	qos_queue_info_get = pp_qos_queue_info_get;
+	qos_port_remove = pp_qos_port_remove;
+	qos_sched_allocate = pp_qos_sched_allocate;
+	qos_sched_remove = pp_qos_sched_remove;
+	qos_port_allocate = pp_qos_port_allocate;
+	qos_port_set = pp_qos_port_set;
+	qos_port_conf_set_default = pp_qos_port_conf_set_default;
+	qos_queue_conf_set_default = pp_qos_queue_conf_set_default;
+	qos_queue_set = pp_qos_queue_set;
+	qos_sched_conf_set_default = pp_qos_sched_conf_set_default;
+	qos_sched_set = pp_qos_sched_set;
+	qos_queue_conf_get = pp_qos_queue_conf_get;
+	qos_queue_flush = pp_qos_queue_flush;
+	qos_sched_conf_get = pp_qos_sched_conf_get;
+	qos_sched_get_queues = pp_qos_sched_get_queues;
+	qos_port_get_queues = pp_qos_port_get_queues;
+	qos_port_conf_get = pp_qos_port_conf_get;
+	qos_dev_open = pp_qos_dev_open;
+	qos_dev_init = pp_qos_dev_init;
+#else
+	/*all NULL function pointer */
+	PR_INFO("call QOS function pointer set to NULL\n");
+#endif /*CONFIG_LTQ_DATAPATH_DUMMY_QOS*/
+}
+
+#ifdef DUMMY_PPV4_QOS_API_OLD
+/*Port Add
+ * Input
+ *    Operation Type: Add
+ *    Operation Flags
+ *       BW Limit param set
+ *       Extra Port Configuration param set
+ *    Node information
+ *       Node Type: Port
+ *       Scheduling Type:WSP(Weighted Strict Priority)+WRR(W Round Robin),WFQ
+ *       BW Limit in Mbps (0 = No BW Limit applied)
+ *    Extra Port Configuration:
+ *       TX manager configuration:
+ *          TX manager configuration flags
+ *            Packet credit set
+ *            Byte credit attributes set
+ *            Ring address set
+ *            Ring size set
+ *          TX Port Packet credit
+ *          Port Byte Credit EN/DIS
+ *          If Port Byte Credit EN ' TX Port Byte credit
+ *          TX Port Ring address
+ *          TX Port Ring size
+ * Output
+ *    Logical Node ID
+ */
+int dp_pp_alloc_port(struct ppv4_port *info)
+{
+	struct qos_node_api_param param = {0};
+	struct hal_priv *priv = HAL(info->inst);
+
+	param.op_type = QOS_OP_ADD;
+	param.node_conf_flags = NODE_PARAM_FLAG_BW_LIMIT_SET |
+		NODE_PARAM_FLAG_EXTRA_PORT_CONF_SET;
+
+	param.node_conf.node_type = DP_NODE_PORT;
+	param.node_conf.sch_type = QOS_SCH_WSP_WRR;
+	/*NODE_PARAM_FLAG_BW_LIMIT_SET */
+	param.node_conf.bw_limit_Mbps = 0; /*No limit*/
+	//param.node_conf.bw_allocation_weight = 0; /* ??? not valid for port */
+	//param.node_conf.priority = 0; /* ??? not valid for port */
+
+	/*NODE_PARAM_FLAG_EXTRA_PORT_CONF_SET*/
+	param.port_conf.port_conf_flags =
+			PORT_PARAM_FLAG_DISABLE_BYTES_CREDIT; /* ??*/
+	param.port_conf.port_tx_packets_credit = info->tx_pkt_credit;
+	param.port_conf.port_tx_ring_address = info->tx_ring_addr;
+	param.port_conf.port_tx_ring_size = info->tx_ring_size;
+	param.deq_port = info->cqm_port;
+
+	if (qos_node_config(&param)) {
+		PR_ERR("Failed to alloc QoS for deq_port=%d\n", param.deq_port);
+		return -1;
+	}
+	priv->deq_port_stat[info->cqm_port].flag = PP_NODE_ALLOC;
+	priv->deq_port_stat[info->cqm_port].node_id = param.out_param.node_id;
+	info->node_id = param.out_param.node_id;
+	return 0;
+}
+
+/*Scheduler Add
+ * Input
+ *    Operation Type: Add
+ *    Operation Flags
+ *      BW Allocation param set
+ *      Priority param set
+ *      Parent Node ID param set
+ *    Node information
+ *      Node Type: Scheduler
+ *      Scheduling Type: WSP+WRR, WFQ
+ *      BW Allocation weight (0 = Best Effort [1 in credit])
+ *      Priority (0-15, 0xFF = WRR, not relevant for WFQ)
+ *      Parent Node ID
+ *    Output
+ *      Logical Node ID
+ */
+int dp_pp_alloc_sched(struct ppv4_scheduler *info)
+{
+	struct qos_node_api_param param = {0};
+
+	param.op_type = QOS_OP_ADD;
+	param.node_conf_flags = NODE_PARAM_FLAG_BW_ALLOCATION_SET |
+		NODE_PARAM_FLAG_PARENT_NODE_SET |
+		NODE_PARAM_FLAG_PRIORITY_SET;
+
+	param.node_conf.node_type = QOS_NODE_SCH;
+	param.node_conf.sch_type = QOS_SCH_WSP_WRR;
+	/*NODE_PARAM_FLAG_BW_ALLOCATION_SET */
+	param.node_conf.bw_allocation_weight = 0; /*best effort*/
+
+	/* NODE_PARAM_FLAG_PRIORITY_SET */
+	param.node_conf.priority = 0xFF; /* ??? */
+
+	/*NODE_PARAM_FLAG_PARENT_NODE_SET */
+	param.node_conf.parent_node_id = info->parent;
+
+	if (qos_node_config(&param)) {
+		PR_ERR("Failed to alloc ppv4 scheduler\n");
+		return -1;
+	}
+	info->node_id = param.out_param.node_id;
+	return 0;
+}
+
+/* Queue Add
+ * Input
+ *   Operation Type: Add
+ *     Operation Flags
+ *       BW Allocation param set/Priority param set/Parent Node ID param set
+ *       Extra Queue Configuration param set
+ *   Node information
+ *      Node Type: Queue
+ *      BW Allocation weight (0 = Best Effort [1 in credit])
+ *      Priority (0-15, 0xFF = WRR, not relevant for WFQ)
+ *      Parent Node ID
+ *  Extra Queue Configuration:
+ *    WRED(Weighted Random Early Drop) configuration
+ *     WRED configuration flags
+ *      WRED DIS
+ *      Following flags are relevant only if this flag is not set
+ *        Min average green set
+ *        Max average green set
+ *        Slope green set
+ *        Min average yellow set
+ *        Max average yellow set
+ *        Slope yellow set
+ *      Min guaranteed DIS
+ *      Min guaranteed set (Relevant only if Min guaranteed DIS is clear)
+ *      Max allowed DIS
+ *      Max allowed set (Relevant only if Max allowed DIS is clear)
+ *     Min average green
+ *     Max average green
+ *     Slope green
+ *     Min average yellow
+ *     Max average yellow
+ *     Slope yellow
+ *     Min guaranteed
+ *     Max allowed
+ */
+int dp_pp_alloc_queue(struct ppv4_queue *info)
+{
+	struct qos_node_api_param param = {0};
+
+	param.op_type = QOS_OP_ADD;
+	param.node_conf_flags = NODE_PARAM_FLAG_BW_ALLOCATION_SET |
+		NODE_PARAM_FLAG_PRIORITY_SET |
+		NODE_PARAM_FLAG_PARENT_NODE_SET |
+		NODE_PARAM_FLAG_EXTRA_QUEUE_CONF_SET;
+
+	param.node_conf.node_type = DP_NODE_QUEUE;
+
+	/*NODE_PARAM_FLAG_BW_ALLOCATION_SET */
+	param.node_conf.bw_allocation_weight = 0; /*best effort*/
+
+	/* NODE_PARAM_FLAG_PRIORITY_SET */
+	param.node_conf.priority = 0; /* ??? */
+
+	/*NODE_PARAM_FLAG_PARENT_NODE_SET */
+	param.node_conf.parent_node_id = info->parent;
+
+	/* NODE_PARAM_FLAG_EXTRA_QUEUE_CONF_SET */
+	param.queue_conf.queue_conf_flags = QUEUE_PARAM_FLAG_DISABLE_WRED |
+		QUEUE_PARAM_FLAG_DISABLE_MIN_GUARANTEED |
+		QUEUE_PARAM_FLAG_DISABLE_MAX_ALLOWED;
+
+	if (qos_node_config(&param)) {
+		PR_ERR("Failed to alloc ppv4 queue\n");
+		return -1;
+	}
+	info->qid = param.out_param.queue_id;
+	info->node_id = param.out_param.node_id;
+	return 0;
+}
+
+#else /*DUMMY_PPV4_QOS_API_OLD*/
+
+int dp_pp_alloc_port(struct ppv4_port *info)
+{
+	int qos_p_id = 0;
+	struct pp_qos_port_conf conf;
+	struct hal_priv *priv = HAL(info->inst);
+	struct pp_qos_dev *qos_dev = priv->qdev;
+
+	if (qos_port_allocate(qos_dev,
+			      info->cqm_deq_port,
+			      &qos_p_id)) {
+		PR_ERR("Failed to alloc QoS for deq_port=%d\n",
+		       info->cqm_deq_port);
+		return -1;
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "qos_port_allocate ok with port(cqm/qos)=%d/%d\n",
+		 info->cqm_deq_port, qos_p_id);
+
+	qos_port_conf_set_default(&conf);
+	conf.port_parent_prop.arbitration = PP_QOS_ARBITRATION_WRR;
+	conf.ring_address = (void *)info->tx_ring_addr;
+	conf.ring_size = info->tx_ring_size;
+	conf.packet_credit_enable = 1;
+	conf.credit = info->tx_pkt_credit;
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_DBG)
+	if (dp_dbg_flag & DP_DBG_FLAG_QOS) {
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "qos_port_set info for p[%d/%d] dp_port=%d:\n",
+			 info->cqm_deq_port, qos_p_id,
+			 info->dp_port);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  arbitration=%d\n",
+			 conf.port_parent_prop.arbitration);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  ring_address=0x%x\n",
+			 (unsigned int)conf.ring_address);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  ring_size=%d\n",
+			 conf.ring_size);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  packet_credit_enable=%d\n",
+			 conf.packet_credit_enable);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  credit=%d\n",
+			 conf.credit);
+	}
+#endif
+	if (qos_port_set(qos_dev, qos_p_id, &conf)) {
+		PR_ERR("qos_port_set fail for port(cqm/qos) %d/%d\n",
+		       info->cqm_deq_port, qos_p_id);
+		qos_port_remove(qos_dev, qos_p_id);
+		return -1;
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "qos_port_set ok for port(cqm/qos) %d/%d\n",
+		       info->cqm_deq_port, qos_p_id);
+	info->node_id = qos_p_id;
+	priv->deq_port_stat[info->cqm_deq_port].flag = PP_NODE_ALLOC;
+	priv->deq_port_stat[info->cqm_deq_port].node_id = qos_p_id;
+	info->node_id = qos_p_id;
+	return 0;
+}
+
+int dp_pp_alloc_queue(struct ppv4_queue *info)
+{
+	struct pp_qos_queue_conf conf;
+	int q_node_id;
+	struct pp_qos_queue_info q_info;
+	struct hal_priv *priv = HAL(info->inst);
+	struct pp_qos_dev *qos_dev = priv->qdev;
+
+#ifdef CONFIG_LTQ_DATAPATH_DUMMY_QOS
+	qos_dev->dq_port = info->dq_port;
+#endif
+	if (qos_queue_allocate(qos_dev, &q_node_id)) {
+#ifdef CONFIG_LTQ_DATAPATH_DUMMY_QOS
+		PR_ERR("qos_queue_allocate fail for dq_port %d\n",
+		       info->dq_port);
+#else
+		PR_ERR("qos_queue_allocate fail\n");
+#endif
+		return -1;
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS, "qos_queue_allocate ok q_node=%d\n",
+		 q_node_id);
+
+	qos_queue_conf_set_default(&conf);
+	conf.wred_enable = 0;
+	conf.queue_wred_max_allowed = 0x400; /*max qocc in pkt */
+	conf.queue_child_prop.parent = info->parent;
+	if (qos_queue_set(qos_dev, q_node_id, &conf)) {
+		PR_ERR("qos_queue_set fail for node_id=%d to parent=%d\n",
+		       q_node_id, info->parent);
+		return -1;
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS, "To attach q_node=%d to parent_node=%d\n",
+		 q_node_id, conf.queue_child_prop.parent);
+	if (qos_queue_info_get(qos_dev, q_node_id, &q_info)) {
+		PR_ERR("qos_queue_info_get fail for queue node_id=%d\n",
+		       q_node_id);
+		return -1;
+	}
+	info->qid = q_info.physical_id;
+	info->node_id = q_node_id;
+	DP_DEBUG(DP_DBG_FLAG_QOS, "Attached q[%d/%d] to parent_node=%d\n",
+		 q_info.physical_id, q_node_id,
+		 info->parent);
+	return 0;
+}
+#endif /*DUMMY_PPV4_QOS_API_OLD*/
+
+int init_ppv4_qos(int inst, int flag)
+{
+	union qos_init {
+		struct pp_qos_port_conf p_conf;
+		struct pp_qos_queue_conf q_conf;
+		struct pp_qos_queue_info q_info;
+		struct pp_qos_init_param param;
+	};
+	union qos_init *t = NULL;
+	int res = DP_FAILURE, i;
+	struct hal_priv *priv = HAL(inst);
+#ifdef CONFIG_LTQ_DATAPATH_QOS_HAL
+	unsigned int q, idx;
+	struct cbm_tx_push *flush_port;
+	struct cbm_cpu_port_data cpu_data = {0};
+#endif
+	if (!priv) {
+		PR_ERR("priv is NULL\n");
+		return DP_FAILURE;
+	}
+	if (!(flag & DP_PLATFORM_INIT)) {
+		PR_INFO("need to implement de-initialization for qos later\n");
+		priv->qdev = NULL;
+		return DP_SUCCESS;
+	}
+	priv->qdev = qos_dev_open(dp_port_prop[inst].qos_inst);
+	if (!priv->qdev) {
+		PR_ERR("Could not open qos instance %d\n",
+		       dp_port_prop[inst].qos_inst);
+		return DP_FAILURE;
+	}
+	PR_INFO("qos_dev_open qdev=%p\n", priv->qdev);
+	t = kzalloc(sizeof(*t), GFP_ATOMIC);
+	if (!t) {
+		PR_ERR("kzalloc fail: %d bytes\n", sizeof(*t));
+		return DP_FAILURE;
+	}
+	t->param.wred_total_avail_resources = 0x10000;
+	t->param.wred_p_const = 512;
+	t->param.wred_max_q_size = 10000;
+	/*reserve all ppv4 port to 1:1 sequnce link cqm port */
+	for (i = 0; i < ARRAY_SIZE(t->param.reserved_ports); i++)
+		t->param.reserved_ports[i] = 1;
+	res = qos_dev_init(priv->qdev, &t->param);
+	if (res) {
+		PR_ERR("qos_dev_init fail for qos inst %d\n",
+		       dp_port_prop[inst].qos_inst);
+		goto EXIT;
+	}
+	PR_INFO("qos_dev_init done\n");
+	if (cbm_cpu_port_get(&cpu_data, 0)) {
+		PR_ERR("cbm_cpu_port_get for CPU port?\n");
+		goto EXIT;
+	}
+	/* Sotre drop/flush port's info */
+	flush_port = &cpu_data.dq_tx_flush_info;
+	idx = flush_port->deq_port;
+	if ((idx == 0) || (idx >= ARRAY_SIZE(dp_deq_port_tbl[inst]))) {
+		PR_ERR("Wrog DP Flush port[%d]\n", idx);
+		goto EXIT;
+	}
+	priv->cqm_drop_p = idx;
+	dp_deq_port_tbl[inst][idx].tx_pkt_credit = flush_port->tx_pkt_credit;
+	dp_deq_port_tbl[inst][idx].tx_ring_addr = flush_port->tx_ring_addr;
+	dp_deq_port_tbl[inst][idx].tx_ring_size = flush_port->tx_ring_size;
+	dp_deq_port_tbl[inst][idx].dp_port = 0;/* dummy one */
+	PR_INFO("DP Flush port[%d]: ring addr=0x%x size=%d pkt_credit=%d\n",
+		priv->cqm_drop_p, dp_deq_port_tbl[inst][idx].tx_ring_addr,
+		dp_deq_port_tbl[inst][idx].tx_ring_size,
+		dp_deq_port_tbl[inst][idx].tx_pkt_credit);
+#ifdef CONFIG_LTQ_DATAPATH_QOS_HAL
+	DP_DEBUG(DP_DBG_FLAG_DBG, "priv=%p deq_port_stat=%p q_dev=%p\n",
+		 priv, priv ? priv->deq_port_stat : NULL,
+		 priv ? priv->qdev : NULL);
+	if (qos_port_allocate(priv->qdev,
+			      priv->cqm_drop_p,
+			      &priv->ppv4_drop_p)) {
+		PR_ERR("Failed to alloc  qos drop port=%d\n",
+		       priv->cqm_drop_p);
+		goto EXIT;
+	}
+	qos_port_conf_set_default(&t->p_conf);
+	t->p_conf.port_parent_prop.arbitration = PP_QOS_ARBITRATION_WRR;
+	t->p_conf.ring_address = (void *)dp_deq_port_tbl[inst][idx].tx_ring_addr;
+	t->p_conf.ring_size = dp_deq_port_tbl[inst][idx].tx_ring_size;
+	t->p_conf.packet_credit_enable = 1;
+	t->p_conf.credit = dp_deq_port_tbl[inst][idx].tx_pkt_credit;
+	t->p_conf.disable = 1; /*not allowed for dequeue*/
+
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_DBG)
+	if (dp_dbg_flag & DP_DBG_FLAG_QOS) {
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "qos_port_set param: %d/%d for drop pot:\n",
+			 priv->cqm_drop_p, priv->ppv4_drop_p);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  arbitration=%d\n",
+			 t->p_conf.port_parent_prop.arbitration);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  ring_address=0x%x\n",
+			 (unsigned int)t->p_conf.ring_address);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  ring_size=%d\n",
+			 t->p_conf.ring_size);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  packet_credit_enable=%d\n",
+			 t->p_conf.packet_credit_enable);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  credit=%d\n",
+			 t->p_conf.credit);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  disabled=%d\n",
+			 t->p_conf.disable);
+	}
+#endif
+	if (qos_port_set(priv->qdev, priv->ppv4_drop_p, &t->p_conf)) {
+		PR_ERR("qos_port_set fail for port(cqm/qos) %d/%d\n",
+		       priv->cqm_drop_p, priv->ppv4_drop_p);
+		qos_port_remove(priv->qdev, priv->ppv4_drop_p);
+		goto EXIT;
+	}
+
+	if (qos_queue_allocate(priv->qdev, &q)) {
+		PR_ERR("qos_queue_allocate fail\n");
+		qos_port_remove(priv->qdev, q);
+		goto EXIT;
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS, "ppv4_drop_q alloc ok q_node=%d\n", q);
+
+	qos_queue_conf_set_default(&t->q_conf);
+	t->q_conf.blocked = 1; /*drop mode */
+	t->q_conf.wred_enable = 0;
+	t->q_conf.queue_wred_max_allowed = 0; /*max qocc in pkt */
+	t->q_conf.queue_child_prop.parent = priv->ppv4_drop_p;
+	if (qos_queue_set(priv->qdev, q, &t->q_conf)) {
+		PR_ERR("qos_queue_set fail for node_id=%d to parent=%d\n",
+		       q, t->q_conf.queue_child_prop.parent);
+		goto EXIT;
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS, "To attach q_node=%d to parent_node=%d\n",
+		 q, priv->ppv4_drop_p);
+	if (qos_queue_info_get(priv->qdev, q, &t->q_info)) {
+		PR_ERR("qos_queue_info_get fail for queue node_id=%d\n",
+		       q);
+		goto EXIT;
+	}
+	priv->ppv4_drop_q = t->q_info.physical_id;
+	DP_DEBUG(DP_DBG_FLAG_QOS, "Drop queue q[%d/%d] to parent=%d/%d\n",
+		 priv->ppv4_drop_q, q,
+		 priv->cqm_drop_p,
+		 priv->ppv4_drop_p);
+#endif /* end of CONFIG_LTQ_DATAPATH_QOS_HAL */
+	DP_DEBUG(DP_DBG_FLAG_DBG, "init_ppv4_qos done\n");
+	res = DP_SUCCESS;
+
+EXIT:
+	kfree(t);
+	t = NULL;
+	return res;
+}
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_ppv4.h b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_ppv4.h
new file mode 100644
index 000000000000..f78dac48cd83
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_ppv4.h
@@ -0,0 +1,207 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#ifndef DATAPATH_PPV4_H
+#define DATAPATH_PPV4_H
+
+#ifdef CONFIG_LTQ_DATAPATH_DUMMY_QOS
+struct pp_qos_dev {
+	int dq_port;
+};
+#endif
+
+#define MAX_PPV4_PORT 128
+#define MAX_CQM_DEQ   90
+#define MAX_QUEUE   256 /* Need further check */
+#define MAX_PP_CHILD_PER_NODE  8 /* Maximum queue per scheduler */
+#define MAX_Q_PER_PORT 32 /* Maximum queue per port */
+#define QOS_MAX_NODES  2048 /* Maximum PPV4 nodes */
+#define MAX_SCHD 128 /* Need further check */
+#define INV_RESV_IDX 0xFFFF  /* Invalid reserved resource index */
+#define MAX_LOOKUP_TBL_SIZE (16 * 1024) /* lookup table size */
+#define DEF_QRED_MAX_ALLOW 0x400  /* max qocc in queue */
+
+#define HAL(inst) ((struct hal_priv *)dp_port_prop[inst].priv_hal)
+#define PARENT(x) (x.queue_child_prop.parent)
+#define PARENT_S(x) (x.sched_child_prop.parent)
+#define CHILD(x, idx) (priv->qos_sch_stat[x].child[idx])
+#define DP_PORT(p) (dp_deq_port_tbl[p->inst][p->cqm_deq_port.cqm_deq_port])
+
+enum flag {
+	DP_NODE_DEC = BIT(0), /* flag to reduce node counter */
+	DP_NODE_INC = BIT(1), /* flag to increase node counter */
+	DP_NODE_RST = BIT(2), /* flag to reset node counter */
+	C_FLAG = BIT(8), /* scheduler flag linked to node */
+	P_FLAG = BIT(9) /* scheduler flag linked to parent */
+};
+
+struct ppv4_queue {
+	int inst;  /* dp instance */
+	u16 qid;  /* -1 means dynammic, otherwise already configured */
+	u16 node_id; /*output */
+	u16 sch;  /* -1 means dynammic, otherwise already configured */
+	u16 parent; /* -1 means no parent.
+		     * it is used for shared dropping queueu purpose
+		     */
+#ifdef CONFIG_LTQ_DATAPATH_DUMMY_QOS
+	int dq_port; /* cqm dequeue port for qos slim driver queue alloc */
+#endif
+};
+
+struct ppv4_scheduler {
+	u16 sch;  /* -1 means dynammic, otherwise already configured */
+	u16 parent; /* input */
+	u16 node_id; /* output */
+};
+
+struct ppv4_port {
+	int inst;
+	u16 dp_port;
+	u16 qos_deq_port; /* -1 means dynammic, otherwise already specified.
+			   * Remove in new datapath lib
+			   */
+	u16 cqm_deq_port;  /* rename in new datapath lib */
+	u16 node_id; /* output */
+
+	u32 tx_pkt_credit;  /* PP port tx bytes credit */
+	u32 tx_ring_addr;  /* PP port ring address */
+	u32 tx_ring_size; /* PP ring size */
+};
+
+struct ppv4_q_sch_port {
+	/* input */
+	int inst;
+	int dp_port; /* for storing q/scheduler */
+	int ctp; /* for storing q/scheduler: masked subifid. */
+	u32 cqe_deq; /* CQE dequeue port */
+	u32 tx_pkt_credit;  /* PP port tx bytes credit */
+	u32 tx_ring_addr;  /* PP port ring address. */
+	u32 tx_ring_size; /* PP ring size */
+
+	/* output of PP */
+	u32 qid;
+	u32 q_node;
+	u32 schd_node;
+	u32 port_node; /* qos port node id */
+	u32 f_deq_port_en:1; /* flag to trigger cbm_dp_enable */
+};
+
+struct pp_sch_list {
+	u32 flag:1; /* 0: valid 1-used 2-reserved */
+	u16 node;
+	u16 parent_type;  /* scheduler/port */
+	u16 parent;
+};
+
+enum PP_NODE_STAT {
+	PP_NODE_FREE = 0, /* Free and not allocated yet */
+	PP_NODE_ALLOC = BIT(0), /* allocated */
+	PP_NODE_ACTIVE = BIT(1), /* linked */
+	PP_NODE_RESERVE = BIT(2) /* reserved */
+};
+
+struct pp_node {
+	enum PP_NODE_STAT flag; /* 0: FREE 1-used/alloc */
+	u16 type; /* node type */
+	u16 node_id;  /* node id */
+};
+
+struct pp_queue_stat {
+	enum PP_NODE_STAT flag; /* 0: valid 1-used 2-reserved */
+	u16 deq_port; /* cqm dequeue port id */
+	u16 node_id;  /* queue node id */
+	u16 resv_idx; /* index of reserve table */
+	u16 dp_port; /* datapath port id */
+};
+
+struct pp_sch_stat {
+	enum PP_NODE_STAT c_flag; /* sch flag linked to child */
+	enum PP_NODE_STAT p_flag; /* sch flag linked to parent */
+	u16 resv_idx; /* index of reserve table */
+	struct pp_node child[MAX_PP_CHILD_PER_NODE];
+	u16 child_num; /* Number of child */
+	int type; /* Node type for queue/sch/port:
+		   * sched table is not just for scheduler, also for queue/port
+		   * It is table index is based on logical node id,
+		   * not just scheduler id
+		   */
+	struct pp_node parent; /* valid for node type queue/sch */
+	u16 dp_port; /* datapath port id */
+};
+
+struct cqm_deq_stat {
+	enum PP_NODE_STAT flag; /* 0: valid 1-used 2-reserved */
+	u16 deq_id; /* qos dequeue port physical id. Maybe no need */
+	u16 node_id; /* qos dequeue port's node id */
+	u16 child_num; /* Number of child */
+};
+
+struct limit_map {
+	int pp_limit; /* pp shaper limit */
+	int dp_limit; /* dp shaper limit */
+};
+
+struct arbi_map {
+	int pp_arbi; /* pp arbitrate */
+	int dp_arbi; /* dp arbitrate */
+};
+
+struct dp_lookup_entry {
+	int entry[MAX_LOOKUP_TBL_SIZE];
+	int num; /* num of valid lookup entries save in entry[] array */
+};
+
+void init_qos_fn(void);
+extern int (*qos_queue_remove)(struct pp_qos_dev *qos_dev, unsigned int id);
+extern int (*qos_queue_allocate)(struct pp_qos_dev *qos_dev, unsigned int *id);
+extern int (*qos_queue_info_get)(struct pp_qos_dev *qos_dev, unsigned int id,
+				 struct pp_qos_queue_info *info);
+extern int (*qos_port_remove)(struct pp_qos_dev *qos_dev, unsigned int id);
+extern int (*qos_sched_allocate)(struct pp_qos_dev *qos_dev, unsigned int *id);
+extern int (*qos_sched_remove)(struct pp_qos_dev *qos_dev, unsigned int id);
+extern int (*qos_port_allocate)(struct pp_qos_dev *qos_dev,
+				unsigned int physical_id,
+				unsigned int *id);
+extern int (*qos_port_set)(struct pp_qos_dev *qos_dev, unsigned int id,
+			   const struct pp_qos_port_conf *conf);
+extern void (*qos_port_conf_set_default)(struct pp_qos_port_conf *conf);
+extern void (*qos_queue_conf_set_default)(struct pp_qos_queue_conf *conf);
+extern int (*qos_queue_set)(struct pp_qos_dev *qos_dev, unsigned int id,
+			    const struct pp_qos_queue_conf *conf);
+extern void (*qos_sched_conf_set_default)(struct pp_qos_sched_conf *conf);
+extern int (*qos_sched_set)(struct pp_qos_dev *qos_dev, unsigned int id,
+			    const struct pp_qos_sched_conf *conf);
+extern int (*qos_queue_conf_get)(struct pp_qos_dev *qos_dev, unsigned int id,
+				 struct pp_qos_queue_conf *conf);
+extern int (*qos_queue_flush)(struct pp_qos_dev *qos_dev, unsigned int id);
+extern int (*qos_sched_conf_get)(struct pp_qos_dev *qos_dev, unsigned int id,
+				 struct pp_qos_sched_conf *conf);
+extern int (*qos_sched_get_queues)(struct pp_qos_dev *qos_dev, unsigned int id,
+				   u16 *queue_ids, unsigned int size,
+				   unsigned int *queues_num);
+extern int (*qos_port_get_queues)(struct pp_qos_dev *qos_dev, unsigned int id,
+				  u16 *queue_ids, unsigned int size,
+				  unsigned int *queues_num);
+extern int (*qos_port_conf_get)(struct pp_qos_dev *qdev, unsigned int id,
+				struct pp_qos_port_conf *conf);
+extern int (*qos_port_info_get)(struct pp_qos_dev *qdev, unsigned int id,
+				struct pp_qos_port_info *info);
+extern struct pp_qos_dev *(*qos_dev_open)(unsigned int id);
+extern int (*qos_dev_init)(struct pp_qos_dev *qos_dev,
+			   struct pp_qos_init_param *conf);
+int dp_map_to_drop_q(int inst, int q_id, struct dp_lookup_entry *lookup);
+int dp_pp_alloc_port(struct ppv4_port *info);
+int dp_pp_alloc_sched(struct ppv4_scheduler *info);
+int dp_pp_alloc_queue(struct ppv4_queue *info);
+int alloc_q_to_port(struct ppv4_q_sch_port *info, u32 flag);
+extern struct cqm_deq_stat deq_port_stat[MAX_CQM_DEQ];
+extern struct pp_queue_stat qos_queue_stat[MAX_QUEUE];
+int init_ppv4_qos(int inst, int flag);
+#endif /* DATAPATH_PPV4_H */
+
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_ppv4_api.c b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_ppv4_api.c
new file mode 100644
index 000000000000..79d1179aa3d2
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_ppv4_api.c
@@ -0,0 +1,4561 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include <net/datapath_api.h>
+#include <net/datapath_api_qos.h>
+#include <net/pp_qos_drv.h>
+#include "../datapath.h"
+#include "datapath_misc.h"
+#include <net/lantiq_cbm_api.h>
+
+#define FLUSH_RESTORE_LOOKUP BIT(0)
+
+static struct limit_map limit_maps[] = {
+	{QOS_NO_BANDWIDTH_LIMIT, DP_NO_SHAPER_LIMIT},
+	{QOS_MAX_BANDWIDTH_LIMIT, DP_MAX_SHAPER_LIMIT}
+};
+
+static struct arbi_map arbi_maps[] = {
+	{PP_QOS_ARBITRATION_WSP, ARBITRATION_WSP},
+	{PP_QOS_ARBITRATION_WRR, ARBITRATION_WRR},
+	{PP_QOS_ARBITRATION_WFQ, ARBITRATION_WFQ},
+	{PP_QOS_ARBITRATION_WRR, ARBITRATION_NULL}
+};
+
+int qos_platform_set(int cmd_id, void *node, int flag)
+{
+	struct dp_node_link *node_link = (struct dp_node_link *)node;
+	int inst;
+	struct hal_priv *priv;
+	int res = DP_FAILURE;
+
+	if (!node)
+		return DP_FAILURE;
+	inst = node_link->inst;
+	priv = HAL(inst);
+	if (!priv->qdev) {
+		PR_ERR("qdev NULL with inst=%d\n", inst);
+		return DP_FAILURE;
+	}
+
+	switch (cmd_id) {
+	case NODE_LINK_ADD:
+		res = dp_node_link_add_31((struct dp_node_link *)node, flag);
+		break;
+	case NODE_LINK_GET:
+		res = dp_node_link_get_31((struct dp_node_link *)node, flag);
+		break;
+	case NODE_LINK_EN_GET:
+		res = dp_node_link_en_get_31((struct dp_node_link_enable *)node,
+					     flag);
+		break;
+	case NODE_LINK_EN_SET:
+		res = dp_node_link_en_set_31((struct dp_node_link_enable *)node,
+					     flag);
+		break;
+	case NODE_UNLINK:
+		res = dp_node_unlink_31((struct dp_node_link *)node, flag);
+		break;
+	case LINK_ADD:
+		res = dp_link_add_31((struct dp_qos_link *)node, flag);
+		break;
+	case LINK_GET:
+		res = dp_link_get_31((struct dp_qos_link *)node, flag);
+		break;
+	case LINK_PRIO_SET:
+		res = dp_qos_link_prio_set_31((struct dp_node_prio *)node,
+					      flag);
+		break;
+	case LINK_PRIO_GET:
+		res = dp_qos_link_prio_get_31((struct dp_node_prio *)node,
+					      flag);
+		break;
+	case QUEUE_CFG_SET:
+		res = dp_queue_conf_set_31((struct dp_queue_conf *)node, flag);
+		break;
+	case QUEUE_CFG_GET:
+		res = dp_queue_conf_get_31((struct dp_queue_conf *)node, flag);
+		break;
+	case SHAPER_SET:
+		res = dp_shaper_conf_set_31((struct dp_shaper_conf *)node,
+					    flag);
+		break;
+	case SHAPER_GET:
+		res = dp_shaper_conf_get_31((struct dp_shaper_conf *)node,
+					    flag);
+		break;
+	case NODE_ALLOC:
+		res = dp_node_alloc_31((struct dp_node_alloc *)node, flag);
+		break;
+	case NODE_FREE:
+		res = dp_node_free_31((struct dp_node_alloc *)node, flag);
+		break;
+	case NODE_CHILDREN_FREE:
+		res =
+		dp_free_children_via_parent_31((struct dp_node_alloc *)node,
+					       flag);
+		break;
+	case DEQ_PORT_RES_GET:
+		res = dp_deq_port_res_get_31((struct dp_dequeue_res *)node,
+					     flag);
+		break;
+	case COUNTER_MODE_SET:
+		res = dp_counter_mode_set_31((struct dp_counter_conf *)node,
+					     flag);
+		break;
+	case COUNTER_MODE_GET:
+		res = dp_counter_mode_set_31((struct dp_counter_conf *)node,
+					     flag);
+		break;
+	case QUEUE_MAP_GET:
+		res = dp_queue_map_get_31((struct dp_queue_map_get *)node,
+					  flag);
+		break;
+	case QUEUE_MAP_SET:
+		res = dp_queue_map_set_31((struct dp_queue_map_set *)node,
+					  flag);
+		break;
+	case NODE_CHILDREN_GET:
+		res = dp_children_get_31((struct dp_node_child *)node, flag);
+		break;
+	case QOS_LEVEL_GET:
+		res = dp_qos_level_get_31((struct dp_qos_level *)node, flag);
+		break;
+	default:
+		PR_ERR("no support yet cmd_id %d\n", cmd_id);
+		break;
+	}
+	return res;
+}
+
+#define MBPS_2_KBPS 1000
+/* convert pp shaper limit to dp shaper limit */
+static int limit_pp2dp(u32 pp_limit, u32 *dp_limit)
+{
+	int i;
+
+	if (!dp_limit) {
+		PR_ERR("dp_limit is NULL!\n");
+		return DP_FAILURE;
+	}
+
+	if (pp_limit > QOS_MAX_BANDWIDTH_LIMIT) {
+		PR_ERR("Wrong pp shaper limit: %u\n", pp_limit);
+		return DP_FAILURE;
+	}
+
+	for (i = 0; i < ARRAY_SIZE(limit_maps); i++) {
+		if (limit_maps[i].pp_limit == pp_limit) {
+			*dp_limit = limit_maps[i].dp_limit;
+			return DP_SUCCESS;
+		}
+	}
+	*dp_limit = pp_limit * MBPS_2_KBPS;/* mbps to kbps */
+
+	if ((*dp_limit <= 0) || (*dp_limit > DP_MAX_SHAPER_LIMIT)) {
+		PR_ERR("Wrong dp shaper limit: %u\n", *dp_limit);
+		return DP_FAILURE;
+	}
+	return DP_SUCCESS;
+}
+
+/* convert dp shaper limit to pp shaper limit */
+static int limit_dp2pp(u32 dp_limit, u32 *pp_limit)
+{
+	int i;
+
+	if (!pp_limit) {
+		PR_ERR("pp_limit is NULL!\n");
+		return DP_FAILURE;
+	}
+
+	if ((dp_limit > DP_MAX_SHAPER_LIMIT) || (dp_limit == 0)) {
+		PR_ERR("Wrong dp shaper limit: %u\n", dp_limit);
+		return DP_FAILURE;
+	}
+
+	for (i = 0; i < ARRAY_SIZE(limit_maps); i++) {
+		if (limit_maps[i].dp_limit == dp_limit) {
+			*pp_limit = limit_maps[i].pp_limit;
+			return DP_SUCCESS;
+		}
+	}
+
+	if (dp_limit % MBPS_2_KBPS)
+		*pp_limit = dp_limit / MBPS_2_KBPS + 1;/* kbps to mbps */
+	else
+		*pp_limit = dp_limit / MBPS_2_KBPS;
+
+	if (*pp_limit > QOS_MAX_BANDWIDTH_LIMIT) {
+		PR_ERR("Wrong dp shaper limit: %u\n", *pp_limit);
+		return DP_FAILURE;
+	}
+	return DP_SUCCESS;
+}
+
+/* convert PP arbitrate to DP arbitrate */
+int arbi_pp2dp(int pp_arbi)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(arbi_maps); i++) {
+		if (arbi_maps[i].pp_arbi == pp_arbi)
+			return arbi_maps[i].dp_arbi;
+	}
+	PR_ERR("Wrong pp_arbitrate: %d\n", pp_arbi);
+	return DP_FAILURE;
+}
+
+/* convert DP arbitrate to PP arbitrate */
+int arbi_dp2pp(int dp_arbi)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(arbi_maps); i++) {
+		if (arbi_maps[i].dp_arbi == dp_arbi)
+			return arbi_maps[i].pp_arbi;
+	}
+	PR_ERR("Wrong dp_arbitrate: %d\n", dp_arbi);
+	return DP_FAILURE;
+}
+
+/* get_qid_by_node API
+ * checks for queue node id
+ * upon Success
+ *    return physical id of queue
+ *    else return DP_FAILURE
+ */
+static int get_qid_by_node(int inst, int node_id, int flag)
+{
+	int i;
+	struct hal_priv *priv = HAL(inst);
+
+	for (i = 0; i < MAX_QUEUE; i++) {
+		if (node_id == priv->qos_queue_stat[i].node_id)
+			return i;
+	}
+	return DP_FAILURE;
+}
+
+/* get_cqm_deq_port_by_node API
+ * checks for qos deque port
+ * upon Success
+ *    return physical cqm_deq_port id
+ *    else return DP_FAILURE
+ */
+static int get_cqm_deq_port_by_node(int inst, int node_id, int flag)
+{
+	int i;
+	struct hal_priv *priv = HAL(inst);
+
+	for (i = 0; i < MAX_CQM_DEQ; i++) {
+		if (node_id == priv->deq_port_stat[i].node_id)
+			return i;
+	}
+	return DP_FAILURE;
+}
+
+#ifndef DP_FLUSH_VIA_AUTO
+static int cqm_queue_flush_31(int cqm_inst, int cqm_drop_port, int qid)
+{
+/* Before call this API, the queue is already unmapped in lookup table,
+ * For the queue itself, it is blocked and resume.
+ * Also attached to drop port
+ */
+	/* need call low level CQM API */
+
+	cqm_qos_queue_flush(cqm_inst, cqm_drop_port, qid);
+
+	DP_DEBUG(DP_DBG_FLAG_QOS, "cqm_queue_flush_31 done\n");
+	return DP_SUCCESS;
+}
+#endif
+
+/* Note: When this API is returned,make sure the queue is in suspend/block
+ *      since the queue may need to move to other scheduler/port after flush.
+ *      node_id is logical node it here
+ */
+static int queue_flush_31(int inst, int node_id, int flag)
+{
+	struct hal_priv *priv = HAL(inst);
+	struct pp_qos_queue_conf queue_cfg = {0};
+#ifdef DP_FLUSH_VIA_AUTO
+	u32 qocc;
+	int times = 10000;
+#else
+	struct pp_qos_queue_conf tmp_q_cfg = {0};
+#endif
+	int qid = get_qid_by_node(inst, node_id, 0);
+	int res = DP_SUCCESS;
+	struct dp_lookup_entry *lookup = NULL;
+
+	if (qid < 0) {
+		PR_ERR("no physical qid for q_node=%d\n", node_id);
+		res = DP_FAILURE;
+		goto EXIT;
+	}
+	if (qos_queue_conf_get(priv->qdev, node_id, &queue_cfg)) {
+		PR_ERR("qos_queue_conf_get fail: q[%d/%d]\n",
+		       qid, node_id);
+		res = DP_FAILURE;
+		goto EXIT;
+	}
+	lookup = kzalloc(sizeof(*lookup), GFP_ATOMIC);
+	if (!lookup) {
+		res = DP_FAILURE;
+		PR_ERR("kmalloc fail to alloc %d bytes\n", sizeof(*lookup));
+		goto EXIT;
+	}
+	/* map to drop queue and save the changed lookup entries for recover */
+	if (dp_map_to_drop_q(inst, qid, lookup)) {
+		PR_ERR("failed to dp_map_to_drop_q for Q:%d\n", qid);
+		res = DP_FAILURE;
+		goto EXIT;
+	}
+	/* block/disable: ensure to drop all coming enqueue packet */
+	if (queue_cfg.blocked == 0) { /* to block */
+		if (pp_qos_queue_block(priv->qdev, node_id)) {
+			PR_ERR("pp_qos_queue_block fail: q[%d/%d]\n",
+			       qid, node_id);
+			res = DP_FAILURE;
+			goto EXIT;
+		}
+	}
+#ifdef DP_FLUSH_VIA_AUTO
+	while (times--) {
+		get_q_qocc(inst, qid, &qocc);
+		if (qocc == 0) /* no packet in the queue */
+			break;
+		udelay(10);
+	};
+	if (qocc)
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Auto Q[%d] Flushing failed:qocc=%u ???\n",
+			 qid, qocc);
+	else
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Auto Q[%d] Flushing ok:qocc=%u\n", qid, qocc);
+#else /* flush queue via CQM API */
+	if (queue_cfg.queue_child_prop.parent == priv->ppv4_drop_p) {
+		/* already attached to drop queue and can directly flush */
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Flush:Q[%d] already under drop port[/%d]\n",
+			 qid, priv->ppv4_drop_p);
+
+		cqm_queue_flush_31(dp_port_prop[inst].cbm_inst,
+				   priv->cqm_drop_p, qid);
+	} else {
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Queue movement before flush");
+
+		/*move to drop port and set block and resume the queue */
+		qos_queue_conf_set_default(&tmp_q_cfg); /*use new variable */
+		tmp_q_cfg.wred_enable = 0;
+		tmp_q_cfg.queue_wred_max_allowed = DEF_QRED_MAX_ALLOW;
+		tmp_q_cfg.queue_child_prop.parent = priv->ppv4_drop_p;
+		if (qos_queue_set(priv->qdev, node_id, &tmp_q_cfg)) {
+			PR_ERR("qos_queue_set fail for queue=%d to parent=%d\n",
+			       qid, tmp_q_cfg.queue_child_prop.parent);
+			goto EXIT;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Flush:Move Q[%d] to drop port[/%d]\n",
+			 qid,
+			 tmp_q_cfg.queue_child_prop.parent);
+
+		cqm_queue_flush_31(dp_port_prop[inst].cbm_inst,
+				   priv->cqm_drop_p, qid);
+#ifdef DP_FLUSH_SUSPEND_Q
+		/* set to suspend again before move back to original parent */
+		if (pp_qos_queue_suspend(priv->qdev, node_id)) {
+			PR_ERR("pp_qos_queue_suspend fail q[%d] to parent=%d\n",
+			       qid, tmp_q_cfg.queue_child_prop.parent);
+			goto EXIT;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "suspend queue[%d/%d]\n", qid, node_id);
+#endif
+		/* move back the queue to original parent
+		 * with orignal variable queue_cfg
+		 */
+		if (qos_queue_set(priv->qdev, node_id, &queue_cfg)) {
+			PR_ERR("qos_queue_conf_get fail: q[%d/%d]\n",
+			       qid, node_id);
+			res = DP_FAILURE;
+			goto EXIT;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Flush:Move Q[%d] back to port[/%d]\n",
+			 qid, queue_cfg.queue_child_prop.parent);
+	}
+#endif /* end of DP_FLUSH_VIA_AUTO */
+
+#ifdef DP_FLUSH_SUSPEND_Q
+	if (pp_qos_queue_suspend(priv->qdev, node_id)) {
+		PR_ERR("qos_queue_set fail\n");
+		goto EXIT;
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "suspend queue[%d/%d]\n", qid, node_id);
+#endif
+	/* restore lookup entry mapping for this qid if needed */
+	if (flag & FLUSH_RESTORE_LOOKUP) {
+		int i;
+
+		if (lookup->num) {
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "Try to restore qid[%d] lookup entry: %d\n",
+				 qid, lookup->num);
+			for (i = 0; i < lookup->num; i++)
+				set_lookup_qid_via_index(lookup->entry[i], qid);
+		}
+	}
+EXIT:
+	kfree(lookup);
+	lookup = NULL;
+	return res;
+}
+
+/* get_node_type_by_node_id API
+ * get node_type node_id in sch global table
+ * upon Success
+ *    return node_type of node_id
+ */
+static int get_node_type_by_node_id(int inst, int node_id, int flag)
+{
+	struct hal_priv *priv = HAL(inst);
+
+	return priv->qos_sch_stat[node_id].type;
+}
+
+/* get_free_child_idx API
+ * check free flag for child in parent's table and return index
+ * else return DP_FAILURE
+ */
+static int get_free_child_idx(int inst, int node_id, int flag)
+{
+	int i;
+	struct hal_priv *priv = HAL(inst);
+
+	for (i = 0; i < DP_MAX_CHILD_PER_NODE; i++) {
+		if (priv->qos_sch_stat[node_id].child[i].flag == PP_NODE_FREE)
+			return i;
+	}
+	return DP_FAILURE;
+}
+
+/* get_parent_node API
+ * check parent flag in node global table if active retrun parent id
+ * else return DP_FAILURE
+ */
+static int get_parent_node(int inst, int node_id, int flag)
+{
+	struct hal_priv *priv = HAL(inst);
+	int type = get_node_type_by_node_id(inst, node_id, 0);
+
+	if ((priv->qos_sch_stat[node_id].parent.flag) &&
+	    (type != DP_NODE_PORT))
+		return priv->qos_sch_stat[node_id].parent.node_id;
+	return DP_FAILURE;
+}
+
+/* get_child_idx_node_id API
+ * check free flag in parent's global table and return index
+ * else return DP_FAILURE
+ */
+static int get_child_idx_node_id(int inst, int node_id, int flag)
+{
+	struct hal_priv *priv = HAL(inst);
+	int i, p_id;
+
+	p_id = priv->qos_sch_stat[node_id].parent.node_id;
+
+	for (i = 0; i < DP_MAX_CHILD_PER_NODE; i++) {
+		if (node_id == priv->qos_sch_stat[p_id].child[i].node_id)
+			return i;
+	}
+	return DP_FAILURE;
+}
+
+/* node_queue_dec API
+ * for queue id = node_id, flag = DP_NODE_DEC
+ * Set Queue flag from PP_NODE_ACTIVE to PP_NODE_ALLOC
+ * else return DP_FAILURE
+ */
+static int node_queue_dec(int inst, int node_id, int flag)
+{
+	struct hal_priv *priv;
+	int phy_id, pid, idx;
+
+	priv = HAL(inst);
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	phy_id = get_qid_by_node(inst, node_id, flag);
+	if (phy_id == DP_FAILURE) {
+		PR_ERR("get_qid_by_node failed\n");
+		return DP_FAILURE;
+	}
+
+	pid = get_parent_node(inst, node_id, flag);
+	if (pid == DP_FAILURE) {
+		PR_ERR("get_parent_node failed for Q:%d\n", phy_id);
+		return DP_FAILURE;
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS, "parent:%d of Q:%d\n", pid, phy_id);
+
+	idx = get_child_idx_node_id(inst, node_id, 0);
+	if (idx == DP_FAILURE) {
+		PR_ERR("get_child_idx_node_id failed for Q:%d\n", phy_id);
+		return DP_FAILURE;
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "node_queue_dec: parent:%d of Q:[%d/%d] child idx:%d\n",
+		 pid, phy_id, node_id, idx);
+
+	if (!(priv->qos_queue_stat[phy_id].flag & PP_NODE_ACTIVE)) {
+		PR_ERR("Wrong Q[%d] Stat(%d):Expect ACTIVE\n", phy_id,
+		       priv->qos_queue_stat[phy_id].flag);
+		return DP_FAILURE;
+	}
+	if (!priv->qos_sch_stat[node_id].parent.flag) {
+		PR_ERR("Wrong Q[%d]'s Parent Stat(%d):Expect ACTIVE\n",
+		       node_id, priv->qos_sch_stat[node_id].parent.flag);
+		return DP_FAILURE;
+	}
+	if (pid == priv->qos_sch_stat[node_id].parent.node_id) {
+		priv->qos_sch_stat[pid].child[idx].flag = PP_NODE_FREE;
+		priv->qos_sch_stat[pid].child[idx].node_id = 0;
+		priv->qos_sch_stat[pid].child[idx].type = 0;
+	}
+	priv->qos_sch_stat[node_id].parent.node_id = 0;
+	priv->qos_sch_stat[node_id].parent.type = 0;
+	priv->qos_sch_stat[node_id].parent.flag = 0;
+	priv->qos_queue_stat[phy_id].flag |= PP_NODE_ALLOC;
+	priv->qos_sch_stat[node_id].p_flag |= PP_NODE_ALLOC;
+	return DP_SUCCESS;
+}
+
+/* node_queue_inc API
+ * for queue id = node_id, flag = DP_NODE_INC
+ * Set Queue flag from PP_NODE_ALLOC to PP_NODE_ACTIVE
+ * else return DP_FAILURE
+ */
+static int node_queue_inc(int inst, int node_id, int flag)
+{
+	struct hal_priv *priv;
+	int phy_id, pid, idx = 0;
+
+	priv = HAL(inst);
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	phy_id = get_qid_by_node(inst, node_id, flag);
+	if (phy_id == DP_FAILURE) {
+		PR_ERR("get_qid_by_node failed\n");
+		return DP_FAILURE;
+	}
+
+	pid = get_parent_node(inst, node_id, flag);
+	if (pid == DP_FAILURE) {
+		PR_ERR("get_parent_node failed for Q:%d\n", phy_id);
+		return DP_FAILURE;
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS, "parent:%d of Q:%d\n", pid, phy_id);
+
+	idx = get_free_child_idx(inst, pid, 0);
+	if (idx == DP_FAILURE) {
+		PR_ERR("get_free_child_idx failed for Q:%d\n", phy_id);
+		return DP_FAILURE;
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "node_queue_inc: parent:%d of Q:[%d/%d] child idx:%d\n",
+		 pid, phy_id, node_id, idx);
+
+	if (!(priv->qos_queue_stat[phy_id].flag & PP_NODE_ALLOC)) {
+		PR_ERR("Wrong Q[%d] Stat(%d):Expect ALLOC\n", phy_id,
+		       priv->qos_queue_stat[phy_id].flag);
+		return DP_FAILURE;
+	}
+	if (pid == priv->qos_sch_stat[node_id].parent.node_id) {
+		priv->qos_sch_stat[pid].child[idx].flag = PP_NODE_ACTIVE;
+		priv->qos_sch_stat[pid].child[idx].node_id = node_id;
+		priv->qos_sch_stat[pid].child[idx].type = DP_NODE_QUEUE;
+	}
+	priv->qos_queue_stat[phy_id].flag |= PP_NODE_ACTIVE;
+	priv->qos_sch_stat[node_id].p_flag |= PP_NODE_ACTIVE;
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "Q:[%d] type:%d idx:%d attach to parent:%d\n",
+		 CHILD(pid, idx).node_id,
+		 CHILD(pid, idx).type, idx, pid);
+	return DP_SUCCESS;
+}
+
+/* node_queue_rst API
+ * for queue id = node_id, flag = DP_NODE_RST
+ * Set Queue flag from PP_NODE_ALLOC to PP_NODE_FREE
+ * Set allocated memory free
+ * else return DP_FAILURE
+ */
+static int node_queue_rst(int inst, int node_id, int flag)
+{
+	struct hal_priv *priv;
+	int phy_id = get_qid_by_node(inst, node_id, flag);
+	int dp_port, resv_idx;
+
+	priv = HAL(inst);
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (phy_id == DP_FAILURE) {
+		PR_ERR("get_qid_by_node failed\n");
+		return DP_FAILURE;
+	}
+	dp_port = priv->qos_queue_stat[phy_id].dp_port;
+	resv_idx = priv->qos_queue_stat[phy_id].resv_idx;
+
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "node_queue_rst: Q:[%d/%d] resv_idx:%d\n",
+		 phy_id, node_id, resv_idx);
+
+	if (!(priv->qos_queue_stat[phy_id].flag & PP_NODE_ALLOC)) {
+		PR_ERR("Wrong Q[%d] Stat(%d):Expect ALLOC\n", phy_id,
+		       priv->qos_queue_stat[phy_id].flag);
+		return DP_FAILURE;
+	}
+
+	/* Check for Reserve resource */
+	if (priv->qos_queue_stat[phy_id].flag & PP_NODE_RESERVE) {
+		priv->resv[dp_port].resv_q[resv_idx].flag = PP_NODE_FREE;
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "node_queue_rst:Q:[%d/%d] resv_idx:%d of EP:%d free\n",
+			 phy_id, node_id, resv_idx, dp_port);
+	}
+	/* Remove resource from global table */
+	memset(&priv->qos_queue_stat[phy_id], 0,
+	       sizeof(priv->qos_queue_stat[phy_id]));
+	memset(&priv->qos_sch_stat[node_id], 0,
+	       sizeof(priv->qos_sch_stat[node_id]));
+	priv->qos_queue_stat[phy_id].resv_idx = INV_RESV_IDX;
+	priv->qos_sch_stat[node_id].resv_idx = INV_RESV_IDX;
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "node_queue_rst:%s Q:[%d/%d] resv_idx:%d dp_port=%d\n",
+		 "After mem free", phy_id, node_id, resv_idx, dp_port);
+	return DP_SUCCESS;
+}
+
+/* node_sched_dec API
+ * for scheduler id = node_id, flag = DP_NODE_DEC
+ * Set Sched flag from PP_NODE_ACTIVE to PP_NODE_ALLOC
+ * else return DP_FAILURE
+ */
+static int node_sched_dec(int inst, int node_id, int flag)
+{
+	struct hal_priv *priv;
+	int pid, idx;
+
+	priv = HAL(inst);
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (flag & C_FLAG) {
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "node_sched_dec: parentSCH:[%d]\n", node_id);
+
+		if (!priv->qos_sch_stat[node_id].child_num ||
+		    !(priv->qos_sch_stat[node_id].c_flag & PP_NODE_ACTIVE)) {
+			PR_ERR("Wrong Sch[%d] Stat(%d)/child_num(%d):%s\n",
+			       node_id, priv->qos_sch_stat[node_id].c_flag,
+			       priv->qos_sch_stat[node_id].child_num,
+			       "Expect ACTIVE Or non-zero child_num");
+			return DP_FAILURE;
+		}
+		priv->qos_sch_stat[node_id].child_num--;
+		if (!priv->qos_sch_stat[node_id].child_num)
+			priv->qos_sch_stat[node_id].c_flag |= PP_NODE_ALLOC;
+		return DP_SUCCESS;
+	} else if (flag & P_FLAG) {
+		pid = get_parent_node(inst, node_id, flag);
+		if (pid == DP_FAILURE) {
+			PR_ERR("get_parent_node failed for sched:%d\n",
+			       node_id);
+			return DP_FAILURE;
+		}
+
+		idx = get_child_idx_node_id(inst, node_id, flag);
+		if (idx == DP_FAILURE) {
+			PR_ERR("get_child_idx_node_id failed for sched:%d\n",
+			       node_id);
+			return DP_FAILURE;
+		}
+
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "node_sched_dec: parent:%d of SCH:[%d] child idx:%d\n",
+			 pid, node_id, idx);
+
+		if (!(priv->qos_sch_stat[node_id].p_flag & PP_NODE_ACTIVE)) {
+			PR_ERR("Wrong Sch[%d] Stat(%d):Expect ACTIVE\n",
+			       node_id, priv->qos_sch_stat[node_id].p_flag);
+			return DP_FAILURE;
+		}
+		if (!priv->qos_sch_stat[node_id].parent.flag) {
+			PR_ERR("Wrong SCH[%d] Parent Stat(%d):Expect ACTIV\n",
+			       node_id,
+			       priv->qos_sch_stat[node_id].parent.flag);
+			return DP_FAILURE;
+		}
+		if (pid == priv->qos_sch_stat[node_id].parent.node_id) {
+			priv->qos_sch_stat[pid].child[idx].flag = PP_NODE_FREE;
+			priv->qos_sch_stat[pid].child[idx].node_id = 0;
+			priv->qos_sch_stat[pid].child[idx].type = 0;
+		}
+		priv->qos_sch_stat[node_id].parent.node_id = 0;
+		priv->qos_sch_stat[node_id].parent.type = 0;
+		priv->qos_sch_stat[node_id].parent.flag = 0;
+		priv->qos_sch_stat[node_id].p_flag |= PP_NODE_ALLOC;
+		return DP_SUCCESS;
+	}
+	return DP_FAILURE;
+}
+
+/* node_sched_inc API
+ * for scheduler id = node_id, flag = DP_NODE_INC
+ * Set Sched flag from PP_NODE_ALLOC to PP_NODE_ACTIVE
+ * else return DP_FAILURE
+ */
+static int node_sched_inc(int inst, int node_id, int flag)
+{
+	struct hal_priv *priv;
+	int pid, idx;
+
+	priv = HAL(inst);
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (flag & C_FLAG) {
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "node_sched_inc: parent SCH:[%d]\n", node_id);
+
+		if (priv->qos_sch_stat[node_id].child_num &&
+		    !(priv->qos_sch_stat[node_id].c_flag & PP_NODE_ACTIVE)) {
+			PR_ERR("Wrong Sch[%d] Stat(%d)/child_num(%d):%s\n",
+			       node_id, priv->qos_sch_stat[node_id].c_flag,
+			       priv->qos_sch_stat[node_id].child_num,
+			       "Expect ACTIVE And Non-Zero child_num");
+			return DP_FAILURE;
+		}
+		if (!priv->qos_sch_stat[node_id].child_num &&
+		    !(priv->qos_sch_stat[node_id].c_flag & PP_NODE_ALLOC)) {
+			PR_ERR("Wrong Sch[%d] Stat(%d)/child_num(%d):%s\n",
+			       node_id, priv->qos_sch_stat[node_id].c_flag,
+			       priv->qos_sch_stat[node_id].child_num,
+			       "Expect ALLOC And zero child_num");
+			return DP_FAILURE;
+		}
+		priv->qos_sch_stat[node_id].child_num++;
+		priv->qos_sch_stat[node_id].c_flag |= PP_NODE_ACTIVE;
+		return DP_SUCCESS;
+	} else if (flag & P_FLAG) {
+		pid = get_parent_node(inst, node_id, flag);
+		if (pid == DP_FAILURE) {
+			PR_ERR("get_parent_node failed for sched:%d\n",
+			       node_id);
+			return DP_FAILURE;
+		}
+
+		idx = get_free_child_idx(inst, pid, 0);
+		if (idx == DP_FAILURE) {
+			PR_ERR("get_free_child_idx failed for sched:%d\n",
+			       node_id);
+			return DP_FAILURE;
+		}
+
+		if (!(priv->qos_sch_stat[node_id].p_flag & PP_NODE_ALLOC)) {
+			PR_ERR("Wrong Sch[%d] Stat(%d):Expect ALLOC\n",
+			       node_id, priv->qos_sch_stat[node_id].p_flag);
+			return DP_FAILURE;
+		}
+
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "node_sched_inc: parent:%d of SCH:[%d] child idx:%d\n",
+			 pid, node_id, idx);
+
+		if (pid == priv->qos_sch_stat[node_id].parent.node_id) {
+			CHILD(pid, idx).flag = PP_NODE_ACTIVE;
+			CHILD(pid, idx).node_id = node_id;
+			CHILD(pid, idx).type = DP_NODE_SCH;
+		}
+		priv->qos_sch_stat[node_id].p_flag |= PP_NODE_ACTIVE;
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "SCH:[%d] type:%d idx:%d attach to parent:%d\n",
+			 CHILD(pid, idx).node_id,
+			 CHILD(pid, idx).type, idx, pid);
+		return DP_SUCCESS;
+	}
+	return DP_FAILURE;
+}
+
+/* node_sched_rst API
+ * for scheduler id = node_id, flag = DP_NODE_RST
+ * sanity check for child_num and both c and p_flag in alloc state
+ * then reset whole sched
+ * Set Sched flag from PP_NODE_ALLOC to PP_NODE_FREE
+ * else return DP_FAILURE
+ */
+static int node_sched_rst(int inst, int node_id, int flag)
+{
+	struct hal_priv *priv;
+	int dp_port, resv_idx;
+
+	priv = HAL(inst);
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	dp_port = priv->qos_sch_stat[node_id].dp_port;
+	resv_idx = priv->qos_sch_stat[node_id].resv_idx;
+
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "node_sched_rst:dp_port=%d SCH:%d resv_idx:%d\n",
+		 dp_port, node_id, resv_idx);
+	/* sanity check for child_num and both c and p_flag in alloc state
+	 * then reset whole sched
+	 */
+	if (priv->qos_sch_stat[node_id].child_num ||
+	    !(priv->qos_sch_stat[node_id].c_flag & PP_NODE_ALLOC) ||
+	    !(priv->qos_sch_stat[node_id].p_flag & PP_NODE_ALLOC)) {
+		PR_ERR("Wrong Sch[%d] c_flag(%d)/p_flag(%d)/child_num(%d):%s\n",
+		       node_id, priv->qos_sch_stat[node_id].c_flag,
+		       priv->qos_sch_stat[node_id].p_flag,
+		       priv->qos_sch_stat[node_id].child_num,
+		       "Expect c_flag OR p_flag ALLOC OR Non-zero child_num");
+		return DP_FAILURE;
+	}
+	/* Free Reserved Resource */
+	if (priv->qos_sch_stat[node_id].p_flag & PP_NODE_RESERVE) {
+		priv->resv[dp_port].resv_sched[resv_idx].flag = PP_NODE_FREE;
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "node_sch_rst:Sch:[%d] resv_idx:%d of EP:%d free\n",
+			 node_id, resv_idx, dp_port);
+	}
+	/* Free Global Resource */
+	memset(&priv->qos_sch_stat[node_id], 0,
+	       sizeof(priv->qos_sch_stat[node_id]));
+	priv->qos_sch_stat[node_id].resv_idx = INV_RESV_IDX;
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "node_queue_rst:%s SCH:[/%d] resv_idx:%d dp_port=%d\n",
+		 "After mem free", node_id, resv_idx, dp_port);
+	return DP_SUCCESS;
+}
+
+/* node_port_dec API
+ * Check for child_num and active flag
+ * for port logical node_id, flag = DP_NODE_DEC
+ * Set Port flag from PP_NODE_ACTIVE to PP_NODE_ALLOC
+ * else return DP_FAILURE
+ */
+static int node_port_dec(int inst, int node_id, int flag)
+{
+	struct hal_priv *priv;
+	int phy_id;
+
+	priv = HAL(inst);
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	phy_id = get_cqm_deq_port_by_node(inst, node_id, flag);
+	if (phy_id == DP_FAILURE) {
+		PR_ERR("get_cqm_deq_port_by_node failed\n");
+		return DP_FAILURE;
+	}
+
+	if (!priv->deq_port_stat[phy_id].child_num ||
+	    !(priv->deq_port_stat[phy_id].flag & PP_NODE_ACTIVE)) {
+		PR_ERR("Wrong P[%d] Stat(%d)/child_num(%d):%s\n",
+		       phy_id, priv->deq_port_stat[phy_id].flag,
+		       priv->deq_port_stat[phy_id].child_num,
+		       "Expect ACTIVE Or non-zero child_num");
+		return DP_FAILURE;
+	}
+	priv->qos_sch_stat[node_id].child_num--;
+	priv->deq_port_stat[phy_id].child_num--;
+	if (!priv->deq_port_stat[phy_id].child_num)
+		priv->deq_port_stat[phy_id].flag = PP_NODE_ALLOC;
+	return DP_SUCCESS;
+}
+
+/* node_port_inc API
+ * for port logical node_id, flag = DP_NODE_INC
+ * Set Port flag from PP_NODE_ALLOC to PP_NODE_ACTIVE
+ * else return DP_FAILURE
+ */
+static int node_port_inc(int inst, int node_id, int flag)
+{
+	struct hal_priv *priv;
+	int phy_id;
+
+	priv = HAL(inst);
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	phy_id = get_cqm_deq_port_by_node(inst, node_id, flag);
+	if (phy_id == DP_FAILURE) {
+		PR_ERR("get_cqm_deq_port_by_node failed\n");
+		return DP_FAILURE;
+	}
+
+	if (priv->deq_port_stat[phy_id].child_num &&
+	    !(priv->deq_port_stat[phy_id].flag & PP_NODE_ACTIVE)) {
+		PR_ERR("Wrong P[%d] Stat(%d)/child_num(%d):%s\n", phy_id,
+		       priv->deq_port_stat[phy_id].flag,
+		       priv->deq_port_stat[phy_id].child_num,
+		       "Expect ACTIVE And Non-Zero child_num");
+		return DP_FAILURE;
+	}
+	if (!priv->deq_port_stat[phy_id].child_num &&
+	    !(priv->deq_port_stat[phy_id].flag & PP_NODE_ALLOC)) {
+		PR_ERR("Wrong P[%d] Stat(%d)/child_num(%d):%s\n", phy_id,
+		       priv->deq_port_stat[phy_id].flag,
+		       priv->deq_port_stat[phy_id].child_num,
+		       "Expect ALLOC And Zero child_num");
+		return DP_FAILURE;
+	}
+	priv->qos_sch_stat[node_id].child_num++;
+	priv->deq_port_stat[phy_id].child_num++;
+	priv->deq_port_stat[phy_id].flag = PP_NODE_ACTIVE;
+	return DP_SUCCESS;
+}
+
+/* node_port_rst API
+ * Check for child_num and alloc flag
+ * for port logical node_id, flag = DP_NODE_RST
+ * Set Port flag from PP_NODE_ALLOC to PP_NODE_FREE
+ * else return DP_FAILURE
+ */
+static int node_port_rst(int inst, int node_id, int flag)
+{
+	struct hal_priv *priv;
+	int phy_id;
+
+	priv = HAL(inst);
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	phy_id = get_cqm_deq_port_by_node(inst, node_id, flag);
+	if (phy_id == DP_FAILURE) {
+		PR_ERR("get_cqm_deq_port_by_node failed\n");
+		return DP_FAILURE;
+	}
+
+	if (priv->deq_port_stat[phy_id].child_num ||
+	    !(priv->deq_port_stat[phy_id].flag & PP_NODE_ALLOC)) {
+		PR_ERR("Wrong P[%d] Stat(%d)/child_num(%d):%s\n", phy_id,
+		       priv->deq_port_stat[phy_id].flag,
+		       priv->deq_port_stat[phy_id].child_num,
+		       "Expect ALLOC Or non-zero child_num");
+		return DP_FAILURE;
+	}
+	memset(&priv->deq_port_stat[phy_id], 0,
+	       sizeof(priv->deq_port_stat[phy_id]));
+	memset(&priv->qos_sch_stat[node_id], 0,
+	       sizeof(priv->qos_sch_stat[node_id]));
+	return DP_SUCCESS;
+}
+
+/* node_stat_update API
+ * node_id is logical node id
+ * if flag = DP_NODE_DEC
+ *           update flag PP_NODE_ACTIVE to PP_NODE_ALLOC if needed
+ *           update child info
+ * else if flag = DP_NODE_INC
+ *           update flag PP_NODE_ALLOC to PP_NODE_ACTIVE
+ * else if flag = DP_NODE_RST
+ *           update flag PP_NODE_ALLOC to PP_NODE_FREE
+ *           reset table info
+ * else return DP_FAILURE
+ */
+static int node_stat_update(int inst, int node_id, int flag)
+{
+	int node_type = get_node_type_by_node_id(inst, node_id, 0);
+
+	if (flag & DP_NODE_DEC) {
+		if (node_type == DP_NODE_QUEUE)
+			return node_queue_dec(inst, node_id, flag);
+		else if (node_type == DP_NODE_SCH)
+			return node_sched_dec(inst, node_id, flag);
+		else if (node_type == DP_NODE_PORT)
+			return node_port_dec(inst, node_id, flag);
+		return DP_FAILURE;
+	} else if (flag & DP_NODE_INC) {
+		if (node_type == DP_NODE_QUEUE)
+			return node_queue_inc(inst, node_id, flag);
+		else if (node_type == DP_NODE_SCH)
+			return node_sched_inc(inst, node_id, flag);
+		else if (node_type == DP_NODE_PORT)
+			return node_port_inc(inst, node_id, flag);
+		return DP_FAILURE;
+	} else if (flag & DP_NODE_RST) {
+		if (node_type == DP_NODE_QUEUE)
+			return node_queue_rst(inst, node_id, flag);
+		else if (node_type == DP_NODE_SCH)
+			return node_sched_rst(inst, node_id, flag);
+		else if (node_type == DP_NODE_PORT)
+			return node_port_rst(inst, node_id, flag);
+		return DP_FAILURE;
+	}
+	return DP_FAILURE;
+}
+
+#ifdef DP_TMP_DEL
+/* dp_link_unset API
+ * call node_stat_update with DP_NODE_DEC flag
+ * upon success unlinks node to parent and returns DP_SUCCESS
+ * else return DP_FAILURE
+ */
+static int dp_link_unset(struct dp_node_link *info, int flag)
+{
+	int node_id, p_id;
+	struct pp_qos_queue_conf queue_cfg = {0};
+	struct pp_qos_sched_conf sched_cfg = {0};
+	struct hal_priv *priv;
+
+	if (!info) {
+		PR_ERR("info cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(info->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (info->node_type == DP_NODE_QUEUE) {
+		node_id = priv->qos_queue_stat[info->node_id.q_id].node_id;
+		if (qos_queue_conf_get(priv->qdev, node_id, &queue_cfg)) {
+			PR_ERR("failed to qos_queue_conf_get\n");
+			return DP_FAILURE;
+		}
+		if (queue_cfg.queue_child_prop.parent ==
+						*(int *)&info->p_node_id) {
+			if (qos_queue_remove(priv->qdev, node_id)) {
+				PR_ERR("failed to qos_queue_remove\n");
+				return DP_FAILURE;
+			}
+		}
+		goto EXIT;
+	} else if (info->node_type == DP_NODE_SCH) {
+		if (qos_sched_conf_get(priv->qdev, info->node_id.sch_id,
+				       &sched_cfg)) {
+			PR_ERR("failed to qos_queue_conf_get\n");
+			return DP_FAILURE;
+		}
+		if (sched_cfg.sched_child_prop.parent ==
+						*(int *)&info->p_node_id) {
+			if (qos_sched_remove(priv->qdev,
+					     info->node_id.sch_id)) {
+				PR_ERR("failed to qos_sched_remove\n");
+				return DP_FAILURE;
+			}
+		}
+		goto EXIT;
+	}
+	return DP_FAILURE;
+
+EXIT:
+	/* Child flag update after unlink */
+	node_stat_update(info->inst, *(int *)&info->node_id,
+			 DP_NODE_DEC | P_FLAG);
+	/* reduce child_num in parent's global table */
+	if (info->p_node_type == DP_NODE_SCH)
+		node_stat_update(info->inst, PARENT(queue_cfg),
+				 DP_NODE_DEC | C_FLAG);
+	else /* convert parent to cqm_deq_port */
+		p_id = get_cqm_deq_port_by_node(info->inst,
+						*(int *)&info->p_node_id,
+						flag);
+		node_stat_update(info->inst, p_id, DP_NODE_DEC);
+	return DP_SUCCESS;
+}
+#endif
+
+/* dp_node_alloc_resv_pool API
+ * Checks for flag and input node
+ * upon success allocate resource from reserve table
+ * otherwise return failure
+ */
+static int dp_node_alloc_resv_pool(struct dp_node_alloc *node, int flag)
+{
+	int i, cnt, phy_id, node_id;
+	struct hal_priv *priv;
+	struct resv_q *resv_q;
+
+	if (!node) {
+		PR_ERR("node is  NULL\n");
+		return DP_FAILURE;
+	}
+	priv = node ? HAL(node->inst) : NULL;
+	if (!priv) {
+		PR_ERR("priv is NULL\n");
+		return DP_FAILURE;
+	}
+	resv_q = priv->resv[node->dp_port].resv_q;
+
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "inst=%d dp_port=%d num_resv_q=%d num_resv_sched=%d\n",
+		 node->inst,
+		 node->dp_port,
+		 priv->resv[node->dp_port].num_resv_q,
+		 priv->resv[node->dp_port].num_resv_sched);
+
+	if (node->type == DP_NODE_QUEUE) {
+		cnt = priv->resv[node->dp_port].num_resv_q;
+		if (!cnt)
+			return DP_FAILURE;
+		DP_DEBUG(DP_DBG_FLAG_QOS, "try to look for resv queue...\n");
+		for (i = 0; i < cnt; i++) {
+			if (resv_q[i].flag != PP_NODE_FREE)
+				continue;
+			phy_id = resv_q[i].physical_id;
+			node_id = resv_q[i].id;
+			resv_q[i].flag = PP_NODE_ALLOC;
+			priv->qos_queue_stat[phy_id].flag = PP_NODE_RESERVE |
+								PP_NODE_ALLOC;
+			priv->qos_queue_stat[phy_id].node_id = node_id;
+			priv->qos_queue_stat[phy_id].resv_idx = i;
+			priv->qos_queue_stat[phy_id].dp_port = node->dp_port;
+			priv->qos_sch_stat[node_id].dp_port = node->dp_port;
+			priv->qos_sch_stat[node_id].resv_idx = i;
+			priv->qos_sch_stat[node_id].type = DP_NODE_QUEUE;
+			priv->qos_sch_stat[node_id].p_flag = PP_NODE_RESERVE |
+								PP_NODE_ALLOC;
+			node->id.q_id = phy_id;
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "queue[%d/%d]:Resv_idx=%d\n",
+				 phy_id, node_id,
+				 priv->qos_queue_stat[phy_id].resv_idx);
+			return DP_SUCCESS;
+		}
+	} else if (node->type == DP_NODE_SCH) {
+		struct resv_sch *resv_sched;
+
+		cnt = priv->resv[node->dp_port].num_resv_sched;
+		if (!cnt)
+			return DP_FAILURE;
+		resv_sched = priv->resv[node->dp_port].resv_sched;
+		for (i = 0; i < cnt; i++) {
+			if (resv_sched[i].flag != PP_NODE_FREE)
+				continue;
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "try to look for resv sche...\n");
+			node_id = resv_sched[i].id;
+			resv_sched[i].flag = PP_NODE_ALLOC;
+			priv->qos_sch_stat[node_id].c_flag = PP_NODE_RESERVE |
+								PP_NODE_ALLOC;
+			priv->qos_sch_stat[node_id].p_flag = PP_NODE_RESERVE |
+								PP_NODE_ALLOC;
+			priv->qos_sch_stat[node_id].resv_idx = i;
+			priv->qos_sch_stat[node_id].child_num = 0;
+			priv->qos_sch_stat[node_id].dp_port = node->dp_port;
+			priv->qos_sch_stat[node_id].type = DP_NODE_SCH;
+			node->id.sch_id = node_id;
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "Sched[/%d]: Resv_idx=%d\n",
+				 resv_sched[i].id,
+				 priv->qos_sch_stat[node_id].resv_idx);
+			return DP_SUCCESS;
+		}
+	}
+	return DP_FAILURE;
+}
+
+/* dp_node_alloc_global_pool API
+ * Checks for flag and input node
+ * upon success allocate resource from global table
+ * otherwise return failure
+ */
+static int dp_node_alloc_global_pool(struct dp_node_alloc *node, int flag)
+{
+	int id, phy_id;
+	struct pp_qos_queue_info *qos_queue_info = NULL;
+	struct pp_qos_queue_conf *q_conf = NULL;
+	struct hal_priv *priv;
+	int res = DP_FAILURE;
+
+	if (!node) {
+		PR_ERR("node is  NULL\n");
+		goto EXIT;
+	}
+
+	priv = HAL(node->inst);
+	if (!priv) {
+		PR_ERR("priv is NULL\n");
+		goto EXIT;
+	}
+
+	q_conf = kzalloc(sizeof(*q_conf), GFP_KERNEL);
+	if (!q_conf) {
+		PR_ERR("fail to alloc %d bytes\n", sizeof(*q_conf));
+		goto EXIT;
+	}
+
+	qos_queue_info = kzalloc(sizeof(*qos_queue_info), GFP_KERNEL);
+	if (!qos_queue_info) {
+		PR_ERR("fail to alloc %d bytes\n", sizeof(*qos_queue_info));
+		goto EXIT;
+	}
+
+	if (node->type == DP_NODE_QUEUE) {
+		if (qos_queue_allocate(priv->qdev, &id)) {
+			PR_ERR("qos_queue_allocate failed\n");
+			goto EXIT;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS, "qos_queue_allocate: %d\n", id);
+#define WORKAROUND_PORT_SET
+#define WORKAROUND_DROP_PORT
+#ifdef WORKAROUND_PORT_SET /* workaround to get physcal queue id */
+		/* workarpound: in order to get queue's physical queue id,
+		 * it is a must call pp_qos_queue_set first,
+		 * then call qos_queue_info_get to get physical queue id
+		 */
+		qos_queue_conf_set_default(q_conf);
+		q_conf->wred_enable = 0;
+		q_conf->queue_wred_max_allowed = DEF_QRED_MAX_ALLOW;
+#ifdef WORKAROUND_DROP_PORT /* use drop port */
+		q_conf->queue_child_prop.parent = priv->ppv4_drop_p;
+#else
+		q_conf->queue_child_prop.parent = priv->ppv4_tmp_p;
+#endif /* WORKAROUND_DROP_PORT */
+		if (qos_queue_set(priv->qdev, id, q_conf)) {
+			PR_ERR("qos_queue_set fail for queue=%d to parent=%d\n",
+			       id, q_conf->queue_child_prop.parent);
+			goto EXIT;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Workaround queue(/%d)-> tmp parent(/%d)\n",
+			 id, q_conf->queue_child_prop.parent);
+		/* workarpound end ---- */
+#endif /* WORKAROUND_PORT_SET */
+		if (qos_queue_info_get(priv->qdev, id, qos_queue_info)) {
+			qos_queue_remove(priv->qdev, id);
+			PR_ERR("qos_queue_info_get: %d\n", id);
+			goto EXIT;
+		}
+
+		phy_id = qos_queue_info->physical_id;
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "queue info: %d/%d to parent(/%d) dp_port=%d\n",
+			 phy_id, id,
+			 q_conf->queue_child_prop.parent, node->dp_port);
+		priv->qos_queue_stat[phy_id].flag = PP_NODE_ALLOC;
+		priv->qos_queue_stat[phy_id].node_id = id;
+		priv->qos_queue_stat[phy_id].resv_idx = INV_RESV_IDX;
+		priv->qos_queue_stat[phy_id].dp_port = node->dp_port;
+		priv->qos_sch_stat[id].dp_port = node->dp_port;
+		priv->qos_sch_stat[id].resv_idx = INV_RESV_IDX;
+		priv->qos_sch_stat[id].type = DP_NODE_QUEUE;
+		priv->qos_sch_stat[id].p_flag = PP_NODE_ALLOC;
+		node->id.q_id = phy_id;
+		res = DP_SUCCESS;
+		goto EXIT;
+	} else if (node->type == DP_NODE_SCH) {
+		if (qos_sched_allocate(priv->qdev, &id)) {
+			PR_ERR("failed to qos_sched_allocate\n");
+			qos_sched_remove(priv->qdev, id);
+			goto EXIT;
+		}
+		priv->qos_sch_stat[id].c_flag = PP_NODE_ALLOC;
+		priv->qos_sch_stat[id].p_flag = PP_NODE_ALLOC;
+		priv->qos_sch_stat[id].resv_idx = INV_RESV_IDX;
+		priv->qos_sch_stat[id].dp_port = node->dp_port;
+		priv->qos_sch_stat[id].child_num = 0;
+		priv->qos_sch_stat[id].type = DP_NODE_SCH;
+		node->id.sch_id = id;
+		res = DP_SUCCESS;
+		goto EXIT;
+	} else {
+		PR_ERR("Unknown node type %d\n", node->type);
+	}
+
+EXIT:
+	kfree(q_conf);
+	kfree(qos_queue_info);
+	return res;
+}
+
+/* dp_alloc_qos_port API
+ * upon success returns qos_deq_port
+ * otherwise return failure
+ */
+static int dp_alloc_qos_port(struct dp_node_alloc *node, int flag)
+{
+	unsigned int qos_port;
+	int cqm_deq_port;
+	int inst;
+	struct pp_qos_port_conf port_cfg;
+	struct hal_priv *priv;
+
+	if (!node) {
+		PR_ERR("node NULL\n");
+		goto EXIT;
+	}
+	inst = node->inst;
+	priv = HAL(node->inst);
+	if (!priv) {
+		PR_ERR("priv NULL\n");
+		goto EXIT;
+	}
+	cqm_deq_port = node->id.cqm_deq_port;
+	DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+		 "inst=%d dp_port=%d cqm_deq_port=%d\n",
+		 node->inst, node->dp_port, cqm_deq_port);
+	if (cqm_deq_port == DP_NODE_AUTO_ID) {
+		PR_ERR("cqm_deq_port wrong: %d\n", cqm_deq_port);
+		goto EXIT;
+	}
+	if (priv->deq_port_stat[cqm_deq_port].flag != PP_NODE_FREE) {
+		DP_DEBUG(DP_DBG_FLAG_QOS, "cqm_deq_port[%d] already init:\n",
+			 cqm_deq_port);
+		return priv->deq_port_stat[cqm_deq_port].node_id;
+	}
+	if (qos_port_allocate(priv->qdev, cqm_deq_port, &qos_port)) {
+		PR_ERR("failed to qos_port_allocate:%d\n", cqm_deq_port);
+		goto EXIT;
+	}
+	/* PR_INFO("qos_port_alloc succeed: %d/%d\n",
+	 *	   cqm_deq_port, qos_port);
+	 */
+	/* Configure QOS dequeue port */
+	qos_port_conf_set_default(&port_cfg);
+	port_cfg.ring_address =
+		(void *)dp_deq_port_tbl[inst][cqm_deq_port].tx_ring_addr;
+	port_cfg.ring_size = dp_deq_port_tbl[inst][cqm_deq_port].tx_ring_size;
+	port_cfg.credit = dp_deq_port_tbl[inst][cqm_deq_port].tx_pkt_credit;
+	if (port_cfg.credit)
+		port_cfg.packet_credit_enable = 1;
+	port_cfg.port_parent_prop.arbitration = PP_QOS_ARBITRATION_WRR;
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_DBG)
+	if (unlikely(dp_dbg_flag & DP_DBG_FLAG_QOS)) {
+		DP_DEBUG(DP_DBG_FLAG_QOS, "qos_port_set parameter: %d/%d\n",
+			 cqm_deq_port, qos_port);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  ring_address=0x%p\n",
+			 port_cfg.ring_address);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  ring_size=%d\n",
+			 port_cfg.ring_size);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  credit=%d\n",
+			 port_cfg.credit);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  packet_credit_enable=%d\n",
+			 port_cfg.packet_credit_enable);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "  Arbitration=%d\n",
+			 port_cfg.port_parent_prop.arbitration);
+		DP_DEBUG(DP_DBG_FLAG_QOS, "priv->qdev=0x%p\n",
+			 priv->qdev);
+	}
+#endif
+	if (qos_port_set(priv->qdev, qos_port, &port_cfg)) {
+		PR_ERR("qos_port_set fail for port %d/%d\n",
+		       cqm_deq_port, qos_port);
+		qos_port_remove(priv->qdev, qos_port);
+		goto EXIT;
+	}
+	priv->deq_port_stat[cqm_deq_port].flag = PP_NODE_ALLOC;
+	priv->qos_sch_stat[qos_port].type = DP_NODE_PORT;
+	priv->qos_sch_stat[qos_port].child_num = 0;
+	priv->deq_port_stat[cqm_deq_port].node_id = qos_port;
+	DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+		 "dp_alloc_qos_port ok: port=%d/%d for dp_port=%d\n",
+		 cqm_deq_port, qos_port, node->dp_port);
+	return qos_port;
+EXIT:
+	return DP_FAILURE;
+}
+
+/* dp_node_alloc_31 API
+ * Checks for flag and input node type
+ * upon success allocate node from global/reserve resource
+ * otherwise return failure
+ */
+int dp_node_alloc_31(struct dp_node_alloc *node, int flag)
+{
+	struct hal_priv *priv;
+
+	if (!node) {
+		PR_ERR("node NULL\n");
+		return DP_FAILURE;
+	}
+
+	priv = HAL(node->inst);
+	if (!priv) {
+		PR_ERR("priv is NULL cannot proceed!!\n");
+		return DP_FAILURE;
+	}
+
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "allocate flag %d\n", flag);
+	if (flag & DP_ALLOC_RESV_ONLY) {
+	/* if can get from its reserved resource and return DP_SUCCESS.
+	 *	Otherwise return DP_FAIL;
+	 */
+		return dp_node_alloc_resv_pool(node, flag);
+	}
+	if (flag & DP_ALLOC_GLOBAL_ONLY) {
+	/* if can get from global pool and return DP_SUCCESS.
+	 *	Otherwise return DP_FAILURE;
+	 */
+		return dp_node_alloc_global_pool(node, flag);
+	}
+	if (flag & DP_ALLOC_GLOBAL_FIRST) {
+	/* if can get from the global pool, return DP_SUCCESS;
+	 * if can get from its reserved resource and return DP_SUCCESS;
+	 * return DP_FAILURE;
+	 */
+		if (dp_node_alloc_global_pool(node, flag) == DP_SUCCESS)
+			return DP_SUCCESS;
+		return dp_node_alloc_resv_pool(node, flag);
+	}
+	/* default order: reserved pool, */
+	/* if can get from its reserved resource and return DP_SUCCESS
+	 * if can get from the global pool, return DP_SUCCESS
+	 * return DP_FAILURE
+	 */
+	if (dp_node_alloc_resv_pool(node, flag) == DP_SUCCESS)
+		return DP_SUCCESS;
+	return dp_node_alloc_global_pool(node, flag);
+}
+
+/* dp_map_to_drop_q API
+ * check index in lookup table for q_id
+ * param q_buf: to save the lookup entries which mapped to this q_id
+ * param num: q_buf size
+ * set drop_q and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_map_to_drop_q(int inst, int q_id, struct dp_lookup_entry *lookup)
+{
+	u32 i, k = 0;
+	struct hal_priv *priv = HAL(inst);
+
+	for (i = 0; i < MAX_LOOKUP_TBL_SIZE; i++) {
+		if (q_id == get_lookup_qid_via_index(i)) {
+			if (lookup) {
+				lookup->entry[k] = i;
+				k++;
+			}
+			set_lookup_qid_via_index(i, priv->ppv4_drop_q);
+		}
+	}
+	if (lookup)
+		lookup->num = k;
+	return DP_SUCCESS;
+}
+
+/* dp_smart_free_from_child_31 API
+ * flush and unlink queue from its parent
+ * check parent's child list if empty free parent recursively
+ * else return DP_FAILURE
+ */
+static int dp_smart_free_from_child_31(struct dp_node_alloc *node, int flag)
+{
+	int id, res, f_free;
+	struct dp_node_link info = {0};
+	struct dp_node_alloc temp = {0};
+	struct hal_priv *priv;
+
+	if (!node) {
+		PR_ERR("node is NULL cannot proceed!!\n");
+		return DP_FAILURE;
+	}
+
+	priv = HAL(node->inst);
+	if (!priv) {
+		PR_ERR("priv is NULL cannot proceed!!\n");
+		return DP_FAILURE;
+	}
+
+	DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+		 "dp_smart_free_from_child_31:type=%d q_id=%d\n",
+		 node->type,
+		 node->id.q_id);
+	if (node->type == DP_NODE_QUEUE) {
+		if ((node->id.q_id < 0) || (node->id.q_id >= MAX_QUEUE)) {
+			PR_ERR("Wrong Parameter: QID[%d]Out Of Range\n",
+			       node->id.q_id);
+			return DP_FAILURE;
+		}
+
+		info.node_id.q_id = node->id.q_id;
+		info.node_type = node->type;
+		id = priv->qos_queue_stat[node->id.q_id].node_id;
+
+		info.p_node_id.q_id = priv->qos_sch_stat[id].parent.node_id;
+		info.p_node_type = priv->qos_sch_stat[id].parent.type;
+
+		if (!info.p_node_id.q_id) {
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "current node doesnot have parent\n");
+		} else if (dp_node_unlink_31(&info, 0)) {
+			PR_ERR("dp_node_unlink_31 failed\n");
+			return DP_FAILURE;
+		}
+
+		if (dp_node_free_31(node, 0)) {
+			PR_ERR("failed to free Queue:[%d]\n", node->id.q_id);
+			return DP_FAILURE;
+		}
+	} else if (node->type == DP_NODE_SCH) {
+		if ((node->id.sch_id < 0) ||
+		    (node->id.sch_id >= QOS_MAX_NODES)) {
+			PR_ERR("Wrong Parameter: Sched[%d]Out Of Range\n",
+			       node->id.sch_id);
+			return DP_FAILURE;
+		}
+
+		info.node_id.sch_id = node->id.sch_id;
+		info.node_type = node->type;
+		id = node->id.sch_id;
+		info.p_node_id.q_id = priv->qos_sch_stat[id].parent.node_id;
+		info.p_node_type = priv->qos_sch_stat[id].parent.type;
+
+		if (!info.p_node_id.sch_id) {
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "current node doesnot have parent\n");
+			return DP_FAILURE;
+		}
+
+		if (dp_node_free_31(node, 0)) {
+			PR_ERR("failed to free Sched:[%d]\n", node->id.sch_id);
+			return DP_FAILURE;
+		}
+	} else if (node->type == DP_NODE_PORT) {
+		if ((node->id.cqm_deq_port < 0) ||
+		    (node->id.cqm_deq_port >= MAX_CQM_DEQ)) {
+			PR_ERR("Wrong Parameter: Port[%d]Out Of Range\n",
+			       node->id.cqm_deq_port);
+			return DP_FAILURE;
+		}
+
+		if (dp_node_free_31(node, 0)) {
+			PR_ERR("failed to free Port:[%d]\n",
+			       node->id.cqm_deq_port);
+			return DP_FAILURE;
+		}
+		return DP_SUCCESS;
+	}
+
+	while (1) {
+		info.node_id = info.p_node_id;
+		info.node_type = info.p_node_type;
+		temp.id = info.p_node_id;
+		temp.type = info.p_node_type;
+
+		if (temp.type == DP_NODE_PORT) {
+			temp.id.cqm_deq_port =
+				get_cqm_deq_port_by_node(node->inst,
+							 temp.id.cqm_deq_port,
+							 flag);
+			id = temp.id.cqm_deq_port;
+			if ((id < 0) || (id >= MAX_CQM_DEQ)) {
+				PR_ERR("Wrong Parameter: Port[%d]%s\n",
+				       id, "Out Of Range");
+				return DP_FAILURE;
+			}
+		} else {
+			id = temp.id.sch_id;
+			if ((id < 0) || (id >= QOS_MAX_NODES)) {
+				PR_ERR("Wrong Parameter: Sched[%d]%s\n",
+				       id, "Out Of Range");
+				return DP_FAILURE;
+			}
+		}
+
+		DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+			 "dp_smart_free_from_child_31:type=%d q_id=%d\n",
+			 temp.type,
+			 id);
+		if ((temp.type == DP_NODE_SCH &&
+		     priv->qos_sch_stat[id].child_num) ||
+		    (temp.type == DP_NODE_PORT &&
+		     priv->deq_port_stat[id].child_num)) {
+			DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+				 "parent node(%d) still have child\n", id);
+			break;
+		}
+		f_free = (dp_node_link_get_31(&info, 0));
+		res = dp_node_free_31(&temp, 0);
+		if (res) {
+			PR_ERR("failed to free node:%d res %d\n",
+			       temp.id.q_id, res);
+			return DP_FAILURE;
+		}
+		if (f_free) {
+			DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+				 "current node doesnot have parent\n");
+			break;
+		}
+	}
+	return DP_SUCCESS;
+}
+
+/* dp_free_children_via_parent_31 API
+ * reset parent to free state
+ * check parent's child list and free all resources recursively
+ * else return DP_FAILURE
+ */
+int dp_free_children_via_parent_31(struct dp_node_alloc *node, int flag)
+{
+	int idx, id, pid, child_id;
+	struct dp_node_alloc temp = {0};
+	struct dp_node_link info = {0};
+	struct hal_priv *priv;
+
+	if (!node) {
+		PR_ERR("node is NULL cannot proceed!!\n");
+		return DP_FAILURE;
+	}
+
+	priv = HAL(node->inst);
+	if (!priv) {
+		PR_ERR("priv is NULL cannot proceed!!\n");
+		return DP_FAILURE;
+	}
+
+	if (node->type == DP_NODE_PORT) {
+		if ((node->id.cqm_deq_port < 0) ||
+		    (node->id.cqm_deq_port >= MAX_CQM_DEQ)) {
+			PR_ERR("Wrong Parameter: Port[%d]Out Of Range\n",
+			       node->id.cqm_deq_port);
+			return DP_FAILURE;
+		}
+		id = priv->deq_port_stat[node->id.cqm_deq_port].node_id;
+
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "parent Port(%d) have child num:%d\n",
+			 node->id.cqm_deq_port,
+			 priv->qos_sch_stat[id].child_num);
+
+		for (idx = 0; idx < DP_MAX_CHILD_PER_NODE; idx++) {
+			if (priv->qos_sch_stat[id].child[idx].flag &
+			    PP_NODE_ACTIVE) {
+				temp.type = CHILD(id, idx).type;
+				child_id =
+					get_qid_by_node(node->inst,
+							CHILD(id, idx).node_id,
+							0);
+				if (CHILD(id, idx).type == DP_NODE_SCH)
+					temp.id.q_id = CHILD(id, idx).node_id;
+				else
+					temp.id.q_id = child_id;
+				if (dp_free_children_via_parent_31(&temp, 0)) {
+					PR_ERR("fail %s=%d child:%d type=%d\n",
+					       "to free Port's",
+					       node->id.cqm_deq_port,
+					       CHILD(id, idx).node_id,
+					       CHILD(id, idx).type);
+					return DP_FAILURE;
+				}
+			}
+		}
+
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Port(%d)'s all children:%d freed!\n",
+			 node->id.cqm_deq_port,
+			 priv->qos_sch_stat[id].child_num);
+
+		if (!priv->qos_sch_stat[id].child_num) {
+			if (dp_node_free_31(node, 0)) {
+				PR_ERR("failed to free Port:[%d]\n",
+				       node->id.cqm_deq_port);
+				return DP_FAILURE;
+			}
+		}
+	} else if (node->type == DP_NODE_SCH) {
+		if ((node->id.sch_id < 0) ||
+		    (node->id.sch_id >= QOS_MAX_NODES)) {
+			PR_ERR("Wrong Parameter: Sched[%d]Out Of Range\n",
+			       node->id.sch_id);
+			return DP_FAILURE;
+		}
+		id = node->id.sch_id;
+
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "parent SCH(%d) have child num:%d\n",
+			 node->id.sch_id,
+			 priv->qos_sch_stat[id].child_num);
+
+		for (idx = 0; idx < DP_MAX_CHILD_PER_NODE; idx++) {
+			if (priv->qos_sch_stat[id].child[idx].flag &
+			    PP_NODE_ACTIVE) {
+				temp.type = CHILD(id, idx).type;
+				child_id =
+					get_qid_by_node(node->inst,
+							CHILD(id, idx).node_id,
+							0);
+				if (CHILD(id, idx).type == DP_NODE_SCH)
+					temp.id.q_id = CHILD(id, idx).node_id;
+				else
+					temp.id.q_id = child_id;
+				if (dp_free_children_via_parent_31(&temp, 0)) {
+					PR_ERR("fail %s=%d child:%d type=%d\n",
+					       "to free Sched's",
+					       node->id.sch_id,
+					       CHILD(id, idx).node_id,
+					       CHILD(id, idx).type);
+					return DP_FAILURE;
+				}
+				DP_DEBUG(DP_DBG_FLAG_QOS,
+					 "Free SCH(%d)'s child:%d done!\n",
+					 node->id.sch_id,
+					 CHILD(id, idx).node_id);
+			}
+		}
+
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "SCH(%d)'s all children:%d freed!\n",
+			 node->id.sch_id,
+			 priv->qos_sch_stat[id].child_num);
+
+		if (!priv->qos_sch_stat[id].child_num) {
+			if (dp_node_free_31(node, 0)) {
+				PR_ERR("failed to free Sched:[%d]\n",
+				       node->id.sch_id);
+				return DP_FAILURE;
+			}
+		}
+	} else if (node->type == DP_NODE_QUEUE) {
+		if ((node->id.q_id < 0) || (node->id.q_id >= MAX_QUEUE)) {
+			PR_ERR("Wrong Parameter: QID[%d]Out Of Range\n",
+			       node->id.q_id);
+			return DP_FAILURE;
+		}
+
+		/* get parent node from global table and fill info */
+		info.node_id.q_id = node->id.q_id;
+		info.node_type = node->type;
+		pid = priv->qos_queue_stat[node->id.q_id].node_id;
+		info.p_node_id.q_id = priv->qos_sch_stat[pid].parent.node_id;
+		info.p_node_type = priv->qos_sch_stat[pid].parent.type;
+
+		if (priv->qos_queue_stat[node->id.q_id].flag != PP_NODE_FREE) {
+			if (dp_node_unlink_31(&info, 0)) {
+				PR_ERR("dp_node_unlink_31 failed\n");
+				return DP_FAILURE;
+			}
+
+			if (dp_node_free_31(node, 0)) {
+				PR_ERR("failed to free Queue:[%d]\n",
+				       node->id.q_id);
+				return DP_FAILURE;
+			}
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "%s:Q[%d] Parent:%d type:%d\n",
+				 "free_children_via_parent",
+				 node->id.q_id,
+				 info.p_node_id.q_id,
+				 info.p_node_type);
+		}
+	} else {
+		PR_ERR("Incorrect Parameter:%d\n", node->type);
+		return DP_FAILURE;
+	}
+	return DP_SUCCESS;
+}
+
+struct link_free_var {
+	struct pp_qos_queue_conf queue_cfg;
+	struct pp_qos_queue_conf tmp_q;
+	struct pp_qos_sched_conf sched_cfg;
+	struct pp_qos_sched_conf tmp_sch;
+	struct dp_node_link info;
+	int sch_id, phy_id, node_id, parent_id, p_type;
+	int dp_port, resv_flag;
+};
+
+/* dp_node_free_31 API
+ * if (this node is not unlinked yet)
+ * unlink it
+ * If (this node is a reserved node)
+ * return this node to this device's reserved node table
+ * mark this node as Free in this device's reserved node table
+ * else
+ * return this node to the system global table
+ * mark this node free in system global table
+ */
+int dp_node_free_31(struct dp_node_alloc *node, int flag)
+{
+	struct hal_priv *priv;
+	struct link_free_var *t = NULL;
+	int res = DP_FAILURE;
+
+	if (!node) {
+		PR_ERR("node is NULL cannot proceed!!\n");
+		return DP_FAILURE;
+	}
+
+	priv = HAL(node->inst);
+	if (!priv) {
+		PR_ERR("priv is NULL cannot proceed!!\n");
+		return DP_FAILURE;
+	}
+	t = kzalloc(sizeof(*t), GFP_KERNEL);
+	if (!t) {
+		PR_ERR("fail to alloc %d bytes\n", sizeof(*t));
+		return DP_FAILURE;
+	}
+
+	if (flag == DP_NODE_SMART_FREE) {/* dont pass flag */
+		res = dp_smart_free_from_child_31(node, 0);
+		if (res == DP_FAILURE) {
+			PR_ERR("dp_smart_free_from_child_31 failed\n");
+			goto EXIT;
+		}
+	}
+
+	if (node->type == DP_NODE_QUEUE) {
+		t->node_id = priv->qos_queue_stat[node->id.q_id].node_id;
+		t->dp_port = priv->qos_queue_stat[node->id.q_id].dp_port;
+		t->resv_flag = priv->qos_queue_stat[node->id.q_id].flag;
+
+		if (priv->qos_queue_stat[node->id.q_id].flag == PP_NODE_FREE) {
+			PR_ERR("Node Q[%d] is already Free Stat\n",
+			       node->id.q_id);
+			goto EXIT;
+		}
+
+		if (qos_queue_conf_get(priv->qdev, t->node_id, &t->queue_cfg)) {
+			res = node_stat_update(node->inst, t->node_id,
+					       DP_NODE_RST);
+			if (res == DP_FAILURE) {
+				PR_ERR("node_stat_update failed\n");
+				goto EXIT;
+			}
+			if (qos_queue_remove(priv->qdev, t->node_id)) {
+				PR_ERR("failed to qos_queue_remove\n");
+				goto EXIT;
+			}
+			res = DP_SUCCESS;
+			goto EXIT;
+		}
+		/* call node unlink api and set drop queue */
+		t->info.inst = node->inst;
+		t->info.node_id = node->id;
+		t->info.node_type = node->type;
+		if (dp_node_unlink_31(&t->info, 0)) {
+			PR_ERR("failed to dp_node_unlink_31 for Q:%d\n",
+			       node->id.q_id);
+			goto EXIT;
+		}
+
+		t->parent_id = t->queue_cfg.queue_child_prop.parent;
+		t->p_type = get_node_type_by_node_id(node->inst,
+						     t->parent_id, flag);
+		/* Remove Queue link only for global resource */
+		if (!(t->resv_flag & PP_NODE_RESERVE)) {
+			if (qos_queue_remove(priv->qdev, t->node_id)) {
+				PR_ERR("failed to qos_queue_remove\n");
+				goto EXIT;
+			}
+		}
+		if (node_stat_update(node->inst, t->node_id, DP_NODE_DEC)) {
+			PR_ERR("node_stat_update failed\n");
+			goto EXIT;
+		}
+		/* call node_stat_update to update parent status */
+		if (node_stat_update(node->inst, t->parent_id,
+				     DP_NODE_DEC | C_FLAG)) {
+			PR_ERR("stat update fail Q:[%d/%d]'s parent:%d\n",
+			       node->id.q_id, t->node_id, t->parent_id);
+			goto EXIT;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Q[%d] removed parent:[%d] stat updated\n",
+			 node->id.q_id, t->parent_id);
+		/* call node_Stat_update to free the node */
+		if (node_stat_update(node->inst, t->node_id,
+				     DP_NODE_RST)) {
+			PR_ERR("node_stat_update failed\n");
+			goto EXIT;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Queue[%d/%d] removed and stat updated\n",
+			 node->id.q_id, t->node_id);
+		/* Reserve Q temp attach to drop port */
+		if (t->resv_flag & PP_NODE_RESERVE) {
+			qos_queue_conf_set_default(&t->tmp_q);
+			t->tmp_q.queue_child_prop.parent = priv->ppv4_drop_p;
+			if (qos_queue_set(priv->qdev, t->node_id, &t->tmp_q)) {
+				PR_ERR("qos_queue_set %s=%d to parent=%d\n",
+				       "fail to reserve queue", t->node_id,
+				       t->tmp_q.queue_child_prop.parent);
+				goto EXIT;
+			}
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "Q:%d/%d attached temp to Drop_port:%d\n",
+				 node->id.q_id,
+				 t->node_id, t->tmp_q.queue_child_prop.parent);
+		}
+		res = DP_SUCCESS;
+		goto EXIT;
+	} else if (node->type == DP_NODE_SCH) {
+		t->sch_id = node->id.sch_id;
+		t->dp_port = priv->qos_sch_stat[t->sch_id].dp_port;
+		t->resv_flag = priv->qos_sch_stat[t->sch_id].p_flag;
+
+		if (priv->qos_sch_stat[t->sch_id].child_num) {
+			PR_ERR("Node Sch[%d] still have child num %d\n",
+			       t->sch_id,
+			       priv->qos_sch_stat[t->sch_id].child_num);
+			goto EXIT;
+		}
+
+		if (priv->qos_sch_stat[t->sch_id].p_flag == PP_NODE_FREE) {
+			PR_ERR("Node Sch[%d] is already Free Stat\n",
+			       t->sch_id);
+			goto EXIT;
+		}
+
+		if (qos_sched_conf_get(priv->qdev, t->sch_id, &t->sched_cfg)) {
+			if (node_stat_update(node->inst, t->sch_id,
+					     DP_NODE_RST | P_FLAG)) {
+				PR_ERR("node_stat_update failed\n");
+				goto EXIT;
+			}
+			if (qos_sched_remove(priv->qdev, t->sch_id)) {
+				PR_ERR("failed to qos_sched_remove\n");
+				goto EXIT;
+			}
+			res = DP_SUCCESS;
+			goto EXIT;
+		}
+		t->parent_id = t->sched_cfg.sched_child_prop.parent;
+		t->p_type = get_node_type_by_node_id(node->inst,
+						     t->parent_id, flag);
+		/* Remove Sched link only if global resource */
+		if (!(t->resv_flag & PP_NODE_RESERVE)) {
+			if (qos_sched_remove(priv->qdev, t->sch_id)) {
+				PR_ERR("failed to qos_sched_remove\n");
+				goto EXIT;
+			}
+		}
+		if (node_stat_update(node->inst, t->sch_id,
+				     DP_NODE_DEC | P_FLAG)) {
+			PR_ERR("node_stat_update failed\n");
+			goto EXIT;
+		}
+		/* call node_stat_update to update parent status */
+		if (node_stat_update(node->inst, t->parent_id,
+				     DP_NODE_DEC | C_FLAG)) {
+			PR_ERR("stat update fail Sch:[%d]'s parent:%d\n",
+			       node->id.sch_id, t->parent_id);
+			goto EXIT;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "SCH[%d] removed and parent:[%d] stat updated\n",
+			node->id.sch_id, t->parent_id);
+		/* call node_stat_update to free the node */
+		if (node_stat_update(node->inst, t->sch_id,
+				     DP_NODE_RST | P_FLAG)) {
+			PR_ERR("Node Reset failed Sched[/%d]\n",
+			       node->id.sch_id);
+			goto EXIT;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Sched[%d] removed and stat updated\n",
+			 node->id.sch_id);
+		/* Reserve Sched temp attach to drop port */
+		if (t->resv_flag & PP_NODE_RESERVE) {
+			qos_sched_conf_set_default(&t->tmp_sch);
+			t->tmp_sch.sched_child_prop.parent = priv->ppv4_drop_p;
+			if (qos_sched_set(priv->qdev, t->sch_id, &t->tmp_sch)) {
+				PR_ERR("qos_sched_set %s=%d to parent=%d\n",
+				       "fail to reserve SCH", t->sch_id,
+				       t->tmp_sch.sched_child_prop.parent);
+				goto EXIT;
+			}
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "SCH:%d attached temp to Drop_port:%d\n",
+				 t->sch_id,
+				 t->tmp_sch.sched_child_prop.parent);
+		}
+		res = DP_SUCCESS;
+		goto EXIT;
+	} else if (node->type == DP_NODE_PORT) {
+		t->phy_id = node->id.cqm_deq_port;
+		t->node_id = priv->deq_port_stat[t->phy_id].node_id;
+
+		if (priv->deq_port_stat[t->phy_id].child_num) {
+			PR_ERR("Node port[%d] still have child num %d\n",
+			       t->phy_id,
+			       priv->deq_port_stat[t->phy_id].child_num);
+			goto EXIT;
+		}
+		if (priv->deq_port_stat[t->phy_id].flag & PP_NODE_ACTIVE) {
+			res = node_stat_update(node->inst, t->node_id,
+					       DP_NODE_DEC);
+			if (res == DP_FAILURE) {
+				PR_ERR("Wrong Port %d flag:0x%x\n", t->phy_id,
+				       priv->deq_port_stat[t->phy_id].flag);
+				goto EXIT;
+			}
+			goto EXIT;
+		}
+		/* No reset API call for Port should freed by child's call */
+		if (priv->deq_port_stat[t->phy_id].flag & PP_NODE_ALLOC) {
+			res = DP_SUCCESS;
+			goto EXIT;
+		}
+		PR_ERR("Unexpect port %d flag %d\n",
+		       t->phy_id, priv->deq_port_stat[t->phy_id].flag);
+		goto EXIT;
+	}
+	PR_ERR("Unexpect node->type %d\n", node->type);
+
+EXIT:
+	if (res == DP_FAILURE)
+		PR_ERR("failed to free node:%d res %d\n",
+		       node->id.q_id, res);
+	kfree(t);
+	t = NULL;
+	return res;
+}
+
+/* dp_qos_parent_chk API
+ * checks for parent type
+ * upon Success
+ *    return parent node id
+ * else return DP_FAILURE
+ */
+static int dp_qos_parent_chk(struct dp_node_link *info, int flag)
+{
+	struct dp_node_alloc node;
+
+	if (info->p_node_type == DP_NODE_SCH) {
+		if (info->p_node_id.sch_id == DP_NODE_AUTO_ID) {
+			node.inst = info->inst;
+			node.type = info->p_node_type;
+			node.dp_port = info->dp_port;
+
+			if ((dp_node_alloc_31(&node, flag)) == DP_FAILURE) {
+				PR_ERR("dp_node_alloc_31 queue alloc fail\n");
+				return DP_FAILURE;
+			}
+			info->p_node_id = node.id;
+		}
+		return info->p_node_id.sch_id;
+	} else if (info->p_node_type == DP_NODE_PORT) {
+		node.inst = info->inst;
+		node.id = info->cqm_deq_port;
+		node.type = info->p_node_type;
+		node.dp_port = info->dp_port;
+		return dp_alloc_qos_port(&node, flag);
+	}
+	return DP_FAILURE;
+}
+
+/* dp_node_link_get_31 API
+ * upon success check node link info and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_node_link_get_31(struct dp_node_link *info, int flag)
+{
+	struct pp_qos_queue_conf queue_cfg = {0};
+	struct pp_qos_sched_conf sched_cfg = {0};
+	struct hal_priv *priv;
+	int node_id;
+
+	if (!info) {
+		PR_ERR("info cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(info->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (info->node_type == DP_NODE_QUEUE) {
+		node_id = priv->qos_queue_stat[info->node_id.q_id].node_id;
+		if (qos_queue_conf_get(priv->qdev, node_id, &queue_cfg)) {
+			PR_ERR("failed to qos_queue_conf_get\n");
+			return DP_FAILURE;
+		}
+		if (!queue_cfg.queue_child_prop.parent)
+			return DP_FAILURE;
+		if (!(priv->qos_queue_stat[info->node_id.q_id].flag &
+		      PP_NODE_ACTIVE)) {
+			return DP_FAILURE;
+		}
+		info->p_node_id.sch_id =
+			queue_cfg.queue_child_prop.parent;
+		info->p_node_type = get_node_type_by_node_id(info->inst,
+				queue_cfg.queue_child_prop.parent, flag);
+		info->prio_wfq = queue_cfg.queue_child_prop.priority;
+		info->arbi = 0;
+		info->leaf = 0;
+		return DP_SUCCESS;
+	} else if (info->node_type == DP_NODE_SCH) {
+		if (qos_sched_conf_get(priv->qdev, info->node_id.sch_id,
+				       &sched_cfg)) {
+			PR_ERR("failed to qos_sched_conf_get\n");
+			return DP_FAILURE;
+		}
+		if (!sched_cfg.sched_child_prop.parent) {
+			PR_ERR("sched child do not have parent\n");
+			return DP_FAILURE;
+		}
+		if (!(priv->qos_sch_stat[info->node_id.sch_id].p_flag &
+		      PP_NODE_ACTIVE)) {
+			PR_ERR("sched id %d flag not active, flag %d\n",
+			       info->node_id.sch_id,
+			       priv->qos_sch_stat[info->node_id.sch_id].p_flag);
+			return DP_FAILURE;
+		}
+		info->p_node_id.sch_id = sched_cfg.sched_child_prop.parent;
+		info->p_node_type = get_node_type_by_node_id(info->inst,
+				sched_cfg.sched_child_prop.parent, flag);
+		info->prio_wfq = sched_cfg.sched_child_prop.priority;
+		info->arbi =
+			arbi_pp2dp(sched_cfg.sched_parent_prop.arbitration);
+		info->leaf = 0;
+		return DP_SUCCESS;
+	}
+	return DP_FAILURE;
+}
+
+/* dp_link_set API
+ * upon success links node to parent and returns DP_SUCCESS
+ * else return DP_FAILURE
+ */
+static int dp_link_set(struct dp_node_link *info, int parent_node, int flag)
+{
+	int node_id;
+	int res = DP_FAILURE;
+	int node_flag = DP_NODE_INC;
+	struct hal_priv *priv = HAL(info->inst);
+	struct pp_qos_queue_conf *queue_cfg;
+	struct pp_qos_sched_conf *sched_cfg;
+
+	queue_cfg = kzalloc(sizeof(*queue_cfg), GFP_KERNEL);
+	sched_cfg = kzalloc(sizeof(*sched_cfg), GFP_KERNEL);
+	if (!queue_cfg || !sched_cfg)
+		goto ERROR_EXIT;
+
+	if (info->node_type == DP_NODE_QUEUE) {
+		qos_queue_conf_set_default(queue_cfg);
+		queue_cfg->queue_child_prop.parent = parent_node;
+		queue_cfg->wred_enable = 0;
+		queue_cfg->queue_wred_max_allowed = DEF_QRED_MAX_ALLOW;
+		queue_cfg->queue_child_prop.priority = info->prio_wfq;
+		/* convert q_id to logical node id and pass it to
+		 * low level api
+		 */
+		node_id = priv->qos_queue_stat[info->node_id.q_id].node_id;
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Try to link Q[%d/%d] to parent[%d/%d] port[%d]\n",
+			 info->node_id.q_id,
+			 node_id,
+			 info->p_node_id.cqm_deq_port,
+			 queue_cfg->queue_child_prop.parent,
+			 info->cqm_deq_port.cqm_deq_port);
+		if (qos_queue_set(priv->qdev, node_id, queue_cfg)) {
+			PR_ERR("failed to qos_queue_set\n");
+			qos_queue_remove(priv->qdev, node_id);
+			goto ERROR_EXIT;
+		}
+		goto EXIT;
+	} else if (info->node_type == DP_NODE_SCH) {
+		qos_sched_conf_set_default(sched_cfg);
+		sched_cfg->sched_child_prop.parent = parent_node;
+		sched_cfg->sched_child_prop.priority = info->prio_wfq;
+		sched_cfg->sched_parent_prop.arbitration = info->arbi;
+		node_id = info->node_id.sch_id;
+
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Try to link SCH[/%d] to parent[%d/%d] port[%d]\n",
+			 node_id,
+			 info->p_node_id.cqm_deq_port,
+			 sched_cfg->sched_child_prop.parent,
+			 info->cqm_deq_port.cqm_deq_port);
+
+		if (qos_sched_set(priv->qdev, node_id, sched_cfg)) {
+			PR_ERR("failed to %s %d parent_node %d\n",
+			       "qos_sched_set node_id",
+			       node_id, parent_node);
+			qos_sched_remove(priv->qdev, node_id);
+			goto ERROR_EXIT;
+		}
+		node_flag |= P_FLAG;
+		goto EXIT;
+	}
+	goto ERROR_EXIT;
+EXIT:
+	res = DP_SUCCESS;
+	/* fill parent info in child's global table */
+	priv->qos_sch_stat[node_id].parent.node_id = parent_node;
+	priv->qos_sch_stat[node_id].parent.type = info->p_node_type;
+	priv->qos_sch_stat[node_id].parent.flag = PP_NODE_ACTIVE;
+	/* increase child_num in parent's global table and status */
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "node_stat_update after dp_link_set start\n");
+	node_stat_update(info->inst, node_id, node_flag);
+	node_stat_update(info->inst, parent_node, DP_NODE_INC | C_FLAG);
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "node_stat_update after dp_link_set end\n");
+ERROR_EXIT:
+
+	kfree(queue_cfg);
+
+	kfree(sched_cfg);
+	return res;
+}
+
+/* get_parent_arbi API
+ * return parent's arbi of given node
+ * else return DP_FAILURE
+ */
+static int get_parent_arbi(int inst, int node_id, int flag)
+{
+	int pid, arbi;
+	struct hal_priv *priv = HAL(inst);
+	struct pp_qos_sched_conf sched_cfg = {0};
+	struct pp_qos_port_conf port_cfg = {0};
+
+	if (priv->qos_sch_stat[node_id].parent.flag == PP_NODE_FREE) {
+		PR_ERR("Parent is not set for node\n");
+		return DP_FAILURE;
+	}
+	pid = priv->qos_sch_stat[node_id].parent.node_id;
+
+	if (priv->qos_sch_stat[node_id].parent.type == DP_NODE_SCH) {
+		if (qos_sched_conf_get(priv->qdev, pid, &sched_cfg)) {
+			PR_ERR("fail to get sched config\n");
+			return DP_FAILURE;
+		}
+		arbi = arbi_pp2dp(sched_cfg.sched_parent_prop.arbitration);
+	} else if (priv->qos_sch_stat[node_id].parent.type == DP_NODE_PORT) {
+		if (qos_port_conf_get(priv->qdev, pid, &port_cfg)) {
+			PR_ERR("fail to get port config\n");
+			return DP_FAILURE;
+		}
+		arbi = arbi_pp2dp(port_cfg.port_parent_prop.arbitration);
+	} else {
+		PR_ERR("incorrect parent type:0x%x for node:%d.\n",
+		       priv->qos_sch_stat[node_id].parent.type,
+		       node_id);
+		return DP_FAILURE;
+	}
+	return arbi;
+}
+
+/* set_parent_arbi API
+ * set arbitration of node_id's all children and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+static int set_parent_arbi(int inst, int node_id, int arbi, int flag)
+{
+	int pid;
+	struct hal_priv *priv = HAL(inst);
+	struct pp_qos_sched_conf sched_cfg = {0};
+	struct pp_qos_port_conf port_cfg = {0};
+
+	if (priv->qos_sch_stat[node_id].parent.flag == PP_NODE_FREE) {
+		PR_ERR("Parent is not set for node\n");
+		return DP_FAILURE;
+	}
+	pid = priv->qos_sch_stat[node_id].parent.node_id;
+
+	arbi = arbi_dp2pp(arbi);
+
+	if (arbi == DP_FAILURE) {
+		PR_ERR("Incorrect arbitration value provided:%d!\n", arbi);
+		return DP_FAILURE;
+	}
+
+	if (priv->qos_sch_stat[node_id].parent.type == DP_NODE_SCH) {
+		if (qos_sched_conf_get(priv->qdev, pid, &sched_cfg)) {
+			PR_ERR("fail to get sched config\n");
+			return DP_FAILURE;
+		}
+		sched_cfg.sched_parent_prop.arbitration = arbi;
+		if (qos_sched_set(priv->qdev, pid, &sched_cfg)) {
+			PR_ERR("fail to set arbi sched:%d parent of node:%d\n",
+			       pid, node_id);
+			return DP_FAILURE;
+		}
+	} else if (priv->qos_sch_stat[node_id].parent.type == DP_NODE_PORT) {
+		if (qos_port_conf_get(priv->qdev, pid, &port_cfg)) {
+			PR_ERR("fail to get port config\n");
+			return DP_FAILURE;
+		}
+		port_cfg.port_parent_prop.arbitration = arbi;
+		if (qos_port_set(priv->qdev, pid, &port_cfg)) {
+			PR_ERR("fail to set arbi port:%d parent of node:%d\n",
+			       pid, node_id);
+			return DP_FAILURE;
+		}
+	} else {
+		PR_ERR("incorrect parent type:0x%x for node:%d.\n",
+		       priv->qos_sch_stat[node_id].parent.type,
+		       node_id);
+		return DP_FAILURE;
+	}
+	return DP_SUCCESS;
+}
+
+/* dp_qos_link_prio_set_31 API
+ * set node_prio struct for link and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_qos_link_prio_set_31(struct dp_node_prio *info, int flag)
+{
+	struct pp_qos_queue_conf queue_cfg = {0};
+	struct pp_qos_sched_conf sched_cfg = {0};
+	struct hal_priv *priv;
+	int node_id;
+
+	if (!info) {
+		PR_ERR("info cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(info->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (info->type == DP_NODE_QUEUE) {
+		if ((info->id.q_id < 0) || (info->id.q_id >= MAX_QUEUE)) {
+			PR_ERR("Invalid Queue ID:%d\n", info->id.q_id);
+			return DP_FAILURE;
+		}
+		if (priv->qos_queue_stat[info->id.q_id].flag == PP_NODE_FREE) {
+			PR_ERR("Invalid Queue flag:0x%x\n",
+			       priv->qos_queue_stat[info->id.q_id].flag);
+			return DP_FAILURE;
+		}
+		node_id = priv->qos_queue_stat[info->id.q_id].node_id;
+		if (qos_queue_conf_get(priv->qdev, node_id, &queue_cfg)) {
+			PR_ERR("fail to get queue prio and parent\n");
+			return DP_FAILURE;
+		}
+		queue_cfg.queue_child_prop.priority = info->prio_wfq;
+		DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+			 "Prio:%d paased to low level for queue[%d]\n",
+			 info->prio_wfq, info->id.q_id);
+		if (qos_queue_set(priv->qdev, node_id, &queue_cfg)) {
+			PR_ERR("failed to qos_queue_set\n");
+			return DP_FAILURE;
+		}
+		/* get parent conf and set arbi in parent */
+		if (set_parent_arbi(info->inst, node_id, info->arbi, flag)) {
+			PR_ERR("fail to set arbi:%d in Parent of Q:%d\n",
+			       info->arbi, node_id);
+			return DP_FAILURE;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+			 "dp_qos_link_prio_set_31:Q=%d arbi=%d prio=%d\n",
+			 info->id.q_id, info->arbi, info->prio_wfq);
+		return DP_SUCCESS;
+	} else if (info->type == DP_NODE_SCH) {
+		if ((info->id.sch_id < 0) ||
+		    (info->id.sch_id >= QOS_MAX_NODES)) {
+			PR_ERR("Invalid Sched ID:%d\n", info->id.sch_id);
+			return DP_FAILURE;
+		}
+		if (priv->qos_sch_stat[info->id.sch_id].p_flag ==
+		    PP_NODE_FREE) {
+			PR_ERR("Invalid Sched flag:0x%x\n",
+			       priv->qos_sch_stat[info->id.sch_id].p_flag);
+			return DP_FAILURE;
+		}
+		if (qos_sched_conf_get(priv->qdev, info->id.sch_id,
+				       &sched_cfg)) {
+			PR_ERR("fail to get sched prio and parent\n");
+			return DP_FAILURE;
+		}
+		sched_cfg.sched_child_prop.priority = info->prio_wfq;
+		DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+			 "Prio:%d paased to low level for Sched[%d]\n",
+			 info->prio_wfq, info->id.sch_id);
+		if (qos_sched_set(priv->qdev, info->id.sch_id, &sched_cfg)) {
+			PR_ERR("failed to qos_sched_set\n");
+			return DP_FAILURE;
+		}
+		/* get parent conf and set arbi in parent */
+		if (set_parent_arbi(info->inst, info->id.sch_id,
+				    info->arbi, 0)) {
+			PR_ERR("fail to set arbi:%d Parent of Sched:%d\n",
+			       info->arbi, info->id.sch_id);
+			return DP_FAILURE;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+			 "dp_qos_link_prio_set_31:Sched=%d arbi=%d prio=%d\n",
+			 info->id.sch_id, info->arbi, info->prio_wfq);
+		return DP_SUCCESS;
+	}
+	PR_ERR("incorrect info type provided:0x%x\n", info->type);
+	return DP_FAILURE;
+}
+
+/* dp_qos_link_prio_get_31 API
+ * get node_prio struct for link and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_qos_link_prio_get_31(struct dp_node_prio *info, int flag)
+{
+	struct pp_qos_queue_conf queue_cfg = {0};
+	struct pp_qos_sched_conf sched_cfg = {0};
+	struct hal_priv *priv;
+	int node_id, arbi;
+
+	if (!info) {
+		PR_ERR("info cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(info->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (info->type == DP_NODE_QUEUE) {
+		if ((info->id.q_id < 0) || (info->id.q_id >= MAX_QUEUE)) {
+			PR_ERR("Invalid Queue ID:%d\n", info->id.q_id);
+			return DP_FAILURE;
+		}
+		if (priv->qos_queue_stat[info->id.q_id].flag == PP_NODE_FREE) {
+			PR_ERR("Invalid Queue flag:%d\n",
+			       priv->qos_queue_stat[info->id.q_id].flag);
+			return DP_FAILURE;
+		}
+		node_id = priv->qos_queue_stat[info->id.q_id].node_id;
+		if (qos_queue_conf_get(priv->qdev, node_id, &queue_cfg)) {
+			PR_ERR("fail to get queue prio_wfq value!\n");
+			return DP_FAILURE;
+		}
+
+		arbi = get_parent_arbi(info->inst, node_id, flag);
+
+		if (arbi == DP_FAILURE) {
+			PR_ERR("get_parent_arbi fail for Q:%d!\n", node_id);
+			return DP_FAILURE;
+		}
+		info->arbi = arbi;
+		info->prio_wfq = queue_cfg.queue_child_prop.priority;
+		return DP_SUCCESS;
+	} else if (info->type == DP_NODE_SCH) {
+		if ((info->id.sch_id < 0) ||
+		    (info->id.sch_id >= QOS_MAX_NODES)) {
+			PR_ERR("Invalid Sched ID:%d\n", info->id.sch_id);
+			return DP_FAILURE;
+		}
+		if (priv->qos_sch_stat[info->id.sch_id].p_flag ==
+		    PP_NODE_FREE) {
+			PR_ERR("Invalid Sched flag:%d\n",
+			       priv->qos_sch_stat[info->id.sch_id].p_flag);
+			return DP_FAILURE;
+		}
+		if (qos_sched_conf_get(priv->qdev, info->id.sch_id,
+				       &sched_cfg)) {
+			PR_ERR("fail to get sched arbi and prio values!\n");
+			return DP_FAILURE;
+		}
+
+		arbi = get_parent_arbi(info->inst, info->id.sch_id, flag);
+
+		if (arbi == DP_FAILURE) {
+			PR_ERR("get_parent_arbi fail for Sched:%d!\n",
+			       info->id.sch_id);
+			return DP_FAILURE;
+		}
+		info->arbi = arbi;
+		info->prio_wfq = sched_cfg.sched_child_prop.priority;
+		return DP_SUCCESS;
+	}
+	PR_ERR("incorrect info type provided:0x%x\n", info->type);
+	return DP_FAILURE;
+}
+
+/* dp_deq_port_res_get_31 API
+ * Remove link of attached nodes and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+union local_t {
+	struct pp_qos_port_info p_info;
+	struct pp_qos_queue_info q_info;
+	struct pp_qos_queue_conf q_conf;
+	struct pp_qos_sched_conf sched_conf;
+};
+
+static union local_t t;
+
+int dp_deq_port_res_get_31(struct dp_dequeue_res *res, int flag)
+{
+	struct hal_priv *priv;
+	u16 q_ids[MAX_Q_PER_PORT] = {0};
+	unsigned int q_num;
+	unsigned int q_size = MAX_Q_PER_PORT;
+	int i, j, k;
+	int port_num = 1;
+	int p_id, idx;
+	struct pmac_port_info *port_info;
+
+	if (!res) {
+		PR_ERR("res cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(res->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	port_info = &dp_port_info[res->inst][res->dp_port];
+	if (!port_info->deq_port_num)
+		return DP_FAILURE;
+	if (res->cqm_deq_idx == DEQ_PORT_OFFSET_ALL) {
+		port_num = port_info->deq_port_num;
+		res->cqm_deq_port = port_info->deq_port_base;
+		res->num_deq_ports = port_info->deq_port_num;
+	} else {
+		res->cqm_deq_port = port_info->deq_port_base + res->cqm_deq_idx;
+		res->num_deq_ports = 1;
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+		 "dp_deq_port_res_get_31:dp_port=%d cqm_deq_port=(%d ~%d) %d\n",
+		 res->dp_port,
+		 res->cqm_deq_port, res->cqm_deq_port + port_num - 1,
+		 port_info->deq_port_num);
+	res->num_q = 0;
+	idx = 0;
+	for (k = res->cqm_deq_port;
+	     k < (res->cqm_deq_port + port_num);
+	     k++) {
+		if (priv->deq_port_stat[k].flag == PP_NODE_FREE)
+			continue;
+		q_num = 0;
+		if (qos_port_get_queues(priv->qdev,
+					priv->deq_port_stat[k].node_id,
+					q_ids, q_size, &q_num)) {
+			PR_ERR("qos_port_get_queues: port[%d/%d]\n",
+			       k,
+			       priv->deq_port_stat[k].node_id);
+			return DP_FAILURE;
+		}
+		res->num_q += q_num;
+		if (!res->q_res)
+			continue;
+		DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL, "port[%d/%d] queue list\n",
+			 k, priv->deq_port_stat[k].node_id);
+		for (i = 0; i < q_num; i++)
+			DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+				 "  q[/%d]\n", q_ids[i]);
+		for (i = 0; (i < q_num) && (idx < res->q_res_size); i++) {
+			memset(&t.q_info, 0, sizeof(t.q_info));
+			if (qos_queue_info_get(priv->qdev,
+					       q_ids[i], &t.q_info)) {
+				PR_ERR("qos_port_info_get fail:q[/%d]\n",
+				       q_ids[i]);
+				continue;
+			}
+			res->q_res[idx].q_id = t.q_info.physical_id;
+			res->q_res[idx].q_node = q_ids[i];
+			DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL, "q[%d/%d]\n",
+				 t.q_info.physical_id, q_ids[i]);
+			res->q_res[idx].sch_lvl = 0;
+			memset(&t.q_conf, 0, sizeof(t.q_conf));
+			if (qos_queue_conf_get(priv->qdev,
+					       q_ids[i], &t.q_conf)) {
+				PR_ERR("qos_port_conf_get fail:q[/%d]\n",
+				       q_ids[i]);
+				continue;
+			}
+			p_id = t.q_conf.queue_child_prop.parent;
+			j = 0;
+			do {
+				if (priv->qos_sch_stat[p_id].type ==
+				    DP_NODE_PORT) {/* port */
+					res->q_res[idx].qos_deq_port = p_id;
+					res->q_res[idx].cqm_deq_port = k;
+					break;
+				} else if (priv->qos_sch_stat[p_id].type !=
+					   DP_NODE_SCH) {
+					PR_ERR("wrong p[/%d] type:%d\n",
+					       p_id,
+					       priv->qos_sch_stat[p_id].type);
+					break;
+				}
+				/* for sched as parent */
+				res->q_res[idx].sch_id[j] = p_id;
+				j++;
+				res->q_res[idx].sch_lvl = j;
+				/* get next parent */
+				if (qos_sched_conf_get(priv->qdev,
+						       p_id, &t.sched_conf)) {
+					PR_ERR("qos_sched_conf_get %s[/%d]\n",
+					       "fail:sch", p_id);
+					break;
+				}
+				p_id = t.sched_conf.sched_child_prop.parent;
+			} while (1);
+			idx++;
+		}
+		return DP_SUCCESS;
+	}
+	return DP_SUCCESS;
+}
+
+/* dp_node_unlink_31 API
+ * check child node keep queue in blocked state
+ * flush queues and return DP_SUCCESS
+ * Else return DP_FAILURE
+ */
+int dp_node_unlink_31(struct dp_node_link *info, int flag)
+{
+	struct pp_qos_queue_conf queue_cfg = {0};
+	struct pp_qos_sched_conf sched_cfg = {0};
+	struct hal_priv *priv;
+	u16 queue_buf[MAX_Q_PER_PORT] = {0};
+	int queue_size = MAX_Q_PER_PORT;
+	int queue_num = 0;
+	int i, node_id;
+
+	if (!info) {
+		PR_ERR("info cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(info->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (info->node_type == DP_NODE_QUEUE) {
+		node_id = priv->qos_queue_stat[info->node_id.q_id].node_id;
+		/* Need to check ACTIVE Flag */
+		if (!(priv->qos_queue_stat[info->node_id.q_id].flag &
+		    PP_NODE_ACTIVE)) {
+			PR_INFO("Wrong Queue[%d] Stat(%d):Expect ACTIVE\n",
+				info->node_id.q_id,
+				priv->qos_queue_stat[info->node_id.q_id].flag);
+		}
+		if (qos_queue_conf_get(priv->qdev, node_id, &queue_cfg) == 0)
+			queue_flush_31(info->inst, node_id, 0);
+	} else if (info->node_type == DP_NODE_SCH) {
+		if (!(priv->qos_sch_stat[info->node_id.sch_id].c_flag &
+								PP_NODE_ACTIVE))
+			PR_INFO("Wrong Sched FLAG Expect ACTIVE\n");
+		if (qos_sched_conf_get(priv->qdev, info->node_id.sch_id,
+				       &sched_cfg))
+			return DP_FAILURE;
+		if (qos_sched_get_queues(priv->qdev, info->node_id.sch_id,
+					 queue_buf, queue_size, &queue_num))
+			return DP_FAILURE;
+		for (i = 0; i < queue_num; i++) {
+			if (qos_queue_conf_get(priv->qdev, queue_buf[i],
+					       &queue_cfg))
+				continue;
+			queue_flush_31(info->inst, queue_buf[i], 0);
+		}
+	}
+	return DP_SUCCESS;
+}
+
+struct link_add_var {
+	struct pp_qos_queue_conf queue_cfg;
+	struct pp_qos_sched_conf sched_cfg;
+	struct dp_node_alloc node;
+	u16 queue_buf[MAX_Q_PER_PORT];
+	int q_orig_block[MAX_Q_PER_PORT];
+	int q_orig_suspend[MAX_Q_PER_PORT];
+	int queue_size;
+	int queue_num;
+	int node_id;
+	struct pp_node parent;
+	int f_child_free;
+	int f_parent_free;
+	int f_sch_auto_id;
+	int f_restore;
+};
+
+/* dp_node_link_add_31 API
+ * check for parent type and allocate parent node
+ * then check for child type and allocate child node
+ * then call dp_link_set api to link child to parent
+ * upon success links node to given parent and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_node_link_add_31(struct dp_node_link *info, int flag)
+{
+	#define DP_SUSPEND(t) ((t)->queue_cfg.common_prop.suspended)
+	int i;
+	int res = DP_SUCCESS;
+	struct hal_priv *priv;
+	struct link_add_var *t = NULL;
+
+	if (!info) {
+		PR_ERR("info cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(info->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if ((!info->dp_port) && (info->dp_port != DP_PORT(info).dp_port)) {
+		PR_INFO("Fix wrong dp_port from %d to %d\n",
+			info->dp_port, DP_PORT(info).dp_port);
+		info->dp_port = DP_PORT(info).dp_port;
+	}
+	t = kzalloc(sizeof(*t), GFP_KERNEL);
+	if (!t) {
+		PR_ERR("fail to alloc %d bytes\n", sizeof(*t));
+		return DP_FAILURE;
+	}
+	for (i = 0; i < ARRAY_SIZE(t->q_orig_block); i++) {
+		t->q_orig_block[i] = -1;
+		t->q_orig_suspend[i] = -1;
+	}
+	t->queue_size = MAX_Q_PER_PORT;
+
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "inst=%d dp_port=%d\n",
+		 info->inst, info->dp_port);
+	/* Get Parent node_id after sanity check */
+	if (info->p_node_type == DP_NODE_SCH &&
+	    info->p_node_id.sch_id == DP_NODE_AUTO_ID)
+		t->f_sch_auto_id = 1;
+	i = dp_qos_parent_chk(info, flag);
+	if (i == DP_FAILURE) {
+		PR_ERR("dp_qos_parent_chk fail\n");
+		goto EXIT_ERR;
+	}
+	t->parent.node_id = i;
+	t->parent.type = info->p_node_type;
+	t->parent.flag = 1;
+
+	/* Check parent's children limit not exceeded */
+	if (priv->qos_sch_stat[t->parent.node_id].child_num >=
+	    DP_MAX_CHILD_PER_NODE) {
+		PR_ERR("Child Num:%d is exceeding limit for Node:[%d]\n",
+		       priv->qos_sch_stat[t->parent.node_id].child_num,
+		       t->parent.node_id);
+		goto EXIT_ERR;
+	}
+
+	DP_DEBUG(DP_DBG_FLAG_QOS,
+		 "dp_qos_parent_chk succeed: parent node %d\n",
+		 t->parent.node_id);
+	/* workaround to pass parrent to queue allcoate api */
+	priv->ppv4_tmp_p = t->parent.node_id;
+	if (t->f_sch_auto_id)
+		t->f_parent_free = 1;
+
+	/* Get Child node after sanity check */
+	if (info->node_type == DP_NODE_QUEUE) {
+		if (info->node_id.q_id == DP_NODE_AUTO_ID) {
+			t->node.inst = info->inst;
+			t->node.dp_port = info->dp_port;
+			t->node.type = info->node_type;
+			if ((dp_node_alloc_31(&t->node, flag)) == DP_FAILURE) {
+				PR_ERR("dp_node_alloc_31 queue alloc fail\n");
+				goto EXIT_ERR;
+			}
+			info->node_id = t->node.id;
+			t->f_child_free = 1;
+		}
+		/* add check for free flag and error */
+		if (priv->qos_queue_stat[info->node_id.q_id].flag ==
+		    PP_NODE_FREE) {
+			PR_ERR("Queue ID:%d is in Free state:0x%x\n",
+			       info->node_id.q_id,
+			       priv->qos_queue_stat[info->node_id.q_id].flag);
+			goto EXIT_ERR;
+		}
+		/* convert q_id to logical node id and pass it to
+		 * low level api
+		 */
+
+		t->node_id = priv->qos_queue_stat[info->node_id.q_id].node_id;
+		if (qos_queue_conf_get(priv->qdev, t->node_id,
+				       &t->queue_cfg) == 0) {
+			t->queue_num = 1;
+			t->queue_buf[0] = t->node_id;
+			/* save original block/suspend status */
+			if (t->queue_cfg.blocked == 0)
+				t->q_orig_block[0] = t->queue_cfg.blocked;
+
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "qos_queue_flush queue[%d]\n", t->node_id);
+			queue_flush_31(info->inst, t->node_id,
+				       FLUSH_RESTORE_LOOKUP);
+			if (t->queue_cfg.queue_child_prop.parent !=
+					 priv->ppv4_drop_p) {/* decrease stat */
+				/* Child flag update before link */
+				DP_DEBUG(DP_DBG_FLAG_QOS,
+					 "node_stat_update for queue[%d]\n",
+					 t->node_id);
+
+				if (node_stat_update(info->inst, t->node_id,
+						     DP_NODE_DEC)) {
+					PR_ERR("node_stat_update fail\n");
+					goto EXIT_ERR;
+				}
+				/* reduce child_num in parent's global table */
+				DP_DEBUG(DP_DBG_FLAG_QOS,
+					 "node_stat_update parent %d for q[%d]\n",
+					 PARENT(t->queue_cfg), t->node_id);
+				if (node_stat_update(info->inst,
+						     PARENT(t->queue_cfg),
+						     DP_NODE_DEC | C_FLAG)) {
+					PR_ERR("node_stat_update fail\n");
+					goto EXIT_ERR;
+				}
+			}
+		}
+		/* link set */
+		/* if parent is same, but need to fill in other parameters for
+		 * parents hence commenting below code
+		 */
+		/* if (info->p_node_id.sch_id == parent.node_id ||
+		 *    info->p_node_id.cqm_deq_port == parent.node_id)
+		 *	goto EXIT_ERR;
+		 */
+		if (dp_link_set(info, t->parent.node_id, flag)) {
+			PR_ERR("dp_link_set fail to link to parent\n");
+			goto EXIT_ERR;
+		}
+	} else if (info->node_type == DP_NODE_SCH) {
+		if (info->node_id.sch_id == DP_NODE_AUTO_ID) {
+			t->node.inst = info->inst;
+			t->node.dp_port = info->dp_port;
+			t->node.type = info->node_type;
+
+			if ((dp_node_alloc_31(&t->node, flag)) == DP_FAILURE) {
+				PR_ERR("dp_node_alloc_31 sched alloc fail\n");
+				goto EXIT_ERR;
+			}
+			info->node_id = t->node.id;
+			t->f_child_free = 1;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+			 "inst=%d dp_port=%d type=%d info->node_id %d\n",
+			 t->node.inst, t->node.dp_port, t->node.type,
+			 info->node_id.q_id);
+		/* add check for free flag and error */
+		if (priv->qos_sch_stat[info->node_id.sch_id].p_flag ==
+		    PP_NODE_FREE) {
+			PR_ERR("Sched:%d is in Free state:0x%x\n",
+			       info->node_id.sch_id,
+			       priv->qos_sch_stat[info->node_id.sch_id].p_flag);
+			goto EXIT_ERR;
+		}
+		if ((t->f_child_free == 0) &&
+		    qos_sched_conf_get(priv->qdev, info->node_id.sch_id,
+				       &t->sched_cfg) == 0) {
+			DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+				 "info->node_id.sch_id %d\n",
+				 info->node_id.sch_id);
+			if (qos_sched_get_queues(priv->qdev,
+						 info->node_id.sch_id,
+						 t->queue_buf, t->queue_size,
+						 &t->queue_num)) {
+				PR_ERR("Can not get queues:%d\n",
+				       info->node_id.sch_id);
+				goto EXIT_ERR;
+			}
+			for (i = 0; i < t->queue_num; i++) {
+				if (qos_queue_conf_get(priv->qdev,
+						       t->queue_buf[i],
+						       &t->queue_cfg))
+					continue;
+				if (t->queue_cfg.blocked == 0)
+					t->q_orig_block[i] =
+						t->queue_cfg.blocked;
+
+				queue_flush_31(info->inst, t->queue_buf[i],
+					       FLUSH_RESTORE_LOOKUP);
+			}
+			/* update flag for sch node */
+			if (node_stat_update(info->inst, info->node_id.sch_id,
+					     DP_NODE_DEC | P_FLAG)) {
+				PR_ERR("node_stat_update fail\n");
+				goto EXIT_ERR;
+			}
+			/* reduce child_num in parent's global table */
+			if (node_stat_update(info->inst, PARENT_S(t->sched_cfg),
+					     DP_NODE_DEC | C_FLAG)) {
+				PR_ERR("node_stat_update fail\n");
+				goto EXIT_ERR;
+			}
+		}
+		/* if parent is same, but need to fill in other parameters for
+		 * parents hence commenting below code
+		 */
+		/* if (info->p_node_id.sch_id == parent.node_id ||
+		 *    info->p_node_id.cqm_deq_port == parent.node_id)
+		 *	goto EXIT_ERR;
+		 */
+		if (dp_link_set(info, t->parent.node_id, flag)) {
+			PR_ERR("dp_link_set failed to link to parent\n");
+			goto EXIT_ERR;
+		}
+	}
+
+	for (i = 0; i <= t->queue_num; i++) {
+		if ((t->q_orig_block[i] < 0) &&/* non-valid block stat */
+		    (t->q_orig_suspend[i] < 0))/* non-valid suspend stat */
+			continue;
+		if (qos_queue_conf_get(priv->qdev, t->queue_buf[i],
+				       &t->queue_cfg))
+			continue;
+		t->f_restore = 0;
+		if (t->q_orig_block[i] >= 0) {
+			t->f_restore = 1;
+			t->queue_cfg.blocked = t->q_orig_block[i];/* restore */
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "to unblock queue[%d/%d]:block=%d\n",
+				 get_qid_by_node(info->inst,
+						 t->queue_buf[i], 0),
+				 t->queue_buf[i],
+				 t->queue_cfg.blocked);
+		}
+
+		if (!t->f_restore)
+			continue;
+		if (qos_queue_set(priv->qdev, t->queue_buf[i], &t->queue_cfg)) {
+			PR_ERR("qos_queue_set fail for q[/%d]\n",
+			       t->queue_buf[i]);
+			res = DP_FAILURE;
+		}
+	}
+	kfree(t);
+	t = NULL;
+	return res;
+
+EXIT_ERR:
+	res = DP_FAILURE;
+	if (t->f_child_free) {
+		if (t->node.type == DP_NODE_QUEUE) {
+			DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+				 "queue remove id %d\n", t->node_id);
+			qos_queue_remove(priv->qdev, t->node_id);
+		} else if (t->node.type == DP_NODE_SCH) {
+			DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+				 "sched remove id %d\n", t->node.id.sch_id);
+			qos_sched_remove(priv->qdev, t->node.id.sch_id);
+		} else {
+			PR_ERR("Unexpect node type %d\n", t->node.type);
+		}
+	}
+	if (t->f_parent_free) {
+		if (info->p_node_type == DP_NODE_PORT) {
+			DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+				 "port remove id %d\n", t->parent.node_id);
+			qos_port_remove(priv->qdev, t->parent.node_id);
+		} else if (info->p_node_type == DP_NODE_SCH) {
+			DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+				 "sched remove id %d\n", t->parent.node_id);
+			qos_sched_remove(priv->qdev, t->parent.node_id);
+		} else {
+			PR_ERR("Unexpect node type %d\n", t->node.type);
+		}
+	}
+
+	kfree(t);
+	t = NULL;
+	return res;
+}
+
+/* dp_queue_conf_set_31 API
+ * Set Current Queue config and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_queue_conf_set_31(struct dp_queue_conf *cfg, int flag)
+{
+	struct pp_qos_queue_conf *conf;
+	struct hal_priv *priv;
+	int node_id, res = DP_FAILURE;
+
+	if (!cfg) {
+		PR_ERR("cfg cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(cfg->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	conf = kzalloc(sizeof(*conf), GFP_KERNEL);
+	if (!conf) {
+		PR_ERR("fail to alloc %d bytes\n", sizeof(*conf));
+		return DP_FAILURE;
+	}
+
+	if ((cfg->q_id < 0) || (cfg->q_id >= MAX_QUEUE)) {
+		PR_ERR("Invalid Queue ID:%d\n", cfg->q_id);
+		goto EXIT;
+	}
+	if (priv->qos_queue_stat[cfg->q_id].flag == PP_NODE_FREE) {
+		PR_ERR("Invalid Queue flag:%d\n",
+		       priv->qos_queue_stat[cfg->q_id].flag);
+		goto EXIT;
+	}
+	node_id = priv->qos_queue_stat[cfg->q_id].node_id;
+
+	if (qos_queue_conf_get(priv->qdev, node_id, conf)) {
+		PR_ERR("qos_queue_conf_get fail:%d\n", cfg->q_id);
+		goto EXIT;
+	}
+	if (flag & (cfg->act & DP_NODE_DIS))
+		conf->blocked = 1;
+	else if (flag & (cfg->act & DP_NODE_EN))
+		conf->blocked = 0;
+
+	if (flag & (cfg->drop == DP_QUEUE_DROP_WRED)) {
+		conf->wred_enable = 1;
+		conf->queue_wred_max_avg_green = cfg->max_size[0];
+		conf->queue_wred_max_avg_yellow = cfg->max_size[1];
+		conf->queue_wred_min_avg_green = cfg->min_size[0];
+		conf->queue_wred_min_avg_yellow = cfg->min_size[1];
+		conf->queue_wred_slope_green = cfg->wred_slope[0];
+		conf->queue_wred_slope_yellow = cfg->wred_slope[1];
+		conf->queue_wred_min_guaranteed = cfg->wred_min_guaranteed;
+		conf->queue_wred_max_allowed = cfg->wred_max_allowed;
+	} else if (flag & (cfg->drop == DP_QUEUE_DROP_TAIL)) {
+		PR_ERR("Further check PPv4 Tail Drop Capability.\n");
+		conf->wred_enable = 0;
+		conf->queue_wred_min_avg_green = cfg->min_size[0];
+		conf->queue_wred_min_avg_yellow = cfg->min_size[1];
+		conf->queue_wred_min_guaranteed = cfg->wred_min_guaranteed;
+		conf->queue_wred_max_allowed = cfg->wred_max_allowed;
+	}
+	if (qos_queue_set(priv->qdev, node_id, conf)) {
+		PR_ERR("failed to qos_queue_set:%d\n", cfg->q_id);
+		goto EXIT;
+	}
+	res = DP_SUCCESS;
+
+EXIT:
+	kfree(conf);
+	conf = NULL;
+	return res;
+}
+
+/* dp_queue_conf_get_31 API
+ * Get Current Queue config and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_queue_conf_get_31(struct dp_queue_conf *cfg, int flag)
+{
+	int node_id, res = DP_FAILURE;
+	struct pp_qos_queue_conf *conf;
+	struct hal_priv *priv;
+
+	if (!cfg) {
+		PR_ERR("cfg cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(cfg->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	conf = kzalloc(sizeof(*conf), GFP_KERNEL);
+	if (!conf) {
+		PR_ERR("fail to alloc %d bytes\n", sizeof(*conf));
+		return DP_FAILURE;
+	}
+
+	if ((cfg->q_id < 0) || (cfg->q_id >= MAX_QUEUE)) {
+		PR_ERR("Invalid Queue ID:%d\n", cfg->q_id);
+		goto EXIT;
+	}
+	if (priv->qos_queue_stat[cfg->q_id].flag == PP_NODE_FREE) {
+		PR_ERR("Invalid Queue flag:%d\n",
+		       priv->qos_queue_stat[cfg->q_id].flag);
+		goto EXIT;
+	}
+	node_id = priv->qos_queue_stat[cfg->q_id].node_id;
+
+	if (qos_queue_conf_get(priv->qdev, node_id, conf)) {
+		PR_ERR("qos_queue_conf_get fail\n");
+		goto EXIT;
+	}
+
+	if (conf->blocked)
+		cfg->act = DP_NODE_DIS;
+	else
+		cfg->act = DP_NODE_EN;
+
+	if (conf->wred_enable) {
+		cfg->drop = DP_QUEUE_DROP_WRED;
+		cfg->wred_slope[0] = conf->queue_wred_slope_green;
+		cfg->wred_slope[1] = conf->queue_wred_slope_yellow;
+		cfg->wred_slope[2] = 0;
+		cfg->wred_max_allowed = conf->queue_wred_max_allowed;
+		cfg->wred_min_guaranteed = conf->queue_wred_min_guaranteed;
+		cfg->min_size[0] = conf->queue_wred_min_avg_green;
+		cfg->min_size[1] = conf->queue_wred_min_avg_yellow;
+		cfg->min_size[2] = 0;
+		cfg->max_size[0] = conf->queue_wred_max_avg_green;
+		cfg->max_size[1] = conf->queue_wred_max_avg_yellow;
+		cfg->max_size[2] = 0;
+		//cfg->unit = conf->max_burst;
+		res = DP_SUCCESS;
+		goto EXIT;
+	}
+	cfg->drop = DP_QUEUE_DROP_TAIL;
+	cfg->min_size[0] = conf->queue_wred_min_avg_green;
+	cfg->min_size[1] = conf->queue_wred_min_avg_yellow;
+	cfg->max_size[0] = conf->queue_wred_max_avg_green;
+	cfg->max_size[1] = conf->queue_wred_max_avg_yellow;
+	//cfg->unit = conf->max_burst;
+	res = DP_SUCCESS;
+
+EXIT:
+	kfree(conf);
+	conf = NULL;
+	return res;
+}
+
+/* dp_node_link_en_set_31 API
+ * Enable current link node and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_node_link_en_set_31(struct dp_node_link_enable *en, int flag)
+{
+	struct pp_qos_queue_conf queue_cfg = {0};
+	struct pp_qos_sched_conf sched_cfg = {0};
+	struct pp_qos_port_conf port_cfg = {0};
+	struct hal_priv *priv;
+	int node_id;
+
+	if (!en) {
+		PR_ERR("en info cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(en->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if ((en->en & DP_NODE_EN) && (en->en & DP_NODE_DIS)) {
+		PR_ERR("enable & disable cannot be set together!\n");
+		return DP_FAILURE;
+	}
+	if ((en->en & DP_NODE_SUSPEND) && (en->en & DP_NODE_RESUME)) {
+		PR_ERR("suspend & resume cannot be set together!\n");
+		return DP_FAILURE;
+	}
+
+	if (en->type == DP_NODE_QUEUE) {
+		if (!(en->en & (DP_NODE_EN | DP_NODE_DIS | DP_NODE_SUSPEND |
+				    DP_NODE_RESUME))) {
+			PR_ERR("Incorrect commands provided!\n");
+			return DP_FAILURE;
+		}
+		if ((en->id.q_id < 0) || (en->id.q_id >= MAX_QUEUE)) {
+			PR_ERR("Wrong Parameter: QID[%d]Out Of Range\n",
+			       en->id.q_id);
+			return DP_FAILURE;
+		}
+		node_id = priv->qos_queue_stat[en->id.q_id].node_id;
+
+		if (priv->qos_queue_stat[en->id.q_id].flag == PP_NODE_FREE) {
+			PR_ERR("Node Q[%d] is not allcoated\n", en->id.q_id);
+			return DP_FAILURE;
+		}
+		if (en->en & DP_NODE_EN) {
+			if (pp_qos_queue_unblock(priv->qdev, node_id)) {
+				PR_ERR("pp_qos_queue_unblock fail Queue[%d]\n",
+				       en->id.q_id);
+				return DP_FAILURE;
+			}
+		}
+		if (en->en & DP_NODE_DIS) {
+			if (pp_qos_queue_block(priv->qdev, node_id)) {
+				PR_ERR("pp_qos_queue_block fail Queue[%d]\n",
+				       en->id.q_id);
+				return DP_FAILURE;
+			}
+		}
+
+		if (qos_queue_conf_get(priv->qdev, node_id, &queue_cfg)) {
+			PR_ERR("qos_queue_conf_get fail: q[%d]\n", en->id.q_id);
+			return DP_FAILURE;
+		}
+		if (en->en & DP_NODE_EN) {
+			if (queue_cfg.blocked) {
+				PR_ERR("Incorrect value set for Queue[%d]:%d\n",
+				       en->id.q_id,
+				       queue_cfg.blocked);
+				return DP_FAILURE;
+			}
+		}
+		if (en->en & DP_NODE_DIS) {
+			if (!queue_cfg.blocked) {
+				PR_ERR("Incorrect value set for Queue[%d]:%d\n",
+				       en->id.q_id,
+				       queue_cfg.blocked);
+				return DP_FAILURE;
+			}
+		}
+
+	} else if (en->type == DP_NODE_SCH) {
+		if (!(en->en & (DP_NODE_SUSPEND | DP_NODE_RESUME))) {
+			PR_ERR("Incorrect commands provided!\n");
+			return DP_FAILURE;
+		}
+		if ((en->id.sch_id < 0) || (en->id.sch_id >= QOS_MAX_NODES)) {
+			PR_ERR("Wrong Parameter: Sched[%d]Out Of Range\n",
+			       en->id.sch_id);
+			return DP_FAILURE;
+		}
+		if (priv->qos_sch_stat[en->id.sch_id].p_flag == PP_NODE_FREE) {
+			PR_ERR("Node Sched[%d] is not allcoated\n",
+			       en->id.sch_id);
+			return DP_FAILURE;
+		}
+
+		if (qos_sched_conf_get(priv->qdev, en->id.sch_id, &sched_cfg)) {
+			PR_ERR("qos_sched_conf_get fail: sch[%d]\n",
+			       en->id.sch_id);
+			return DP_FAILURE;
+		}
+
+	} else if (en->type == DP_NODE_PORT) {
+		if (!(en->en & (DP_NODE_EN | DP_NODE_DIS | DP_NODE_SUSPEND |
+				    DP_NODE_RESUME))) {
+			PR_ERR("Incorrect commands provided!\n");
+			return DP_FAILURE;
+		}
+		if ((en->id.cqm_deq_port < 0) ||
+		    (en->id.cqm_deq_port >= MAX_CQM_DEQ)) {
+			PR_ERR("Wrong Parameter: Port[%d]Out Of Range\n",
+			       en->id.cqm_deq_port);
+			return DP_FAILURE;
+		}
+		if (priv->deq_port_stat[en->id.cqm_deq_port].flag ==
+		    PP_NODE_FREE) {
+			PR_ERR("Node Port[%d] is not allcoated\n",
+			       en->id.cqm_deq_port);
+			return DP_FAILURE;
+		}
+		node_id = priv->deq_port_stat[en->id.cqm_deq_port].node_id;
+		if (en->en & DP_NODE_EN) {
+			if (pp_qos_port_unblock(priv->qdev, node_id)) {
+				PR_ERR("pp_qos_port_unblock fail Port[%d]\n",
+				       en->id.cqm_deq_port);
+				return DP_FAILURE;
+			}
+		}
+		if (en->en & DP_NODE_DIS) {
+			if (pp_qos_port_block(priv->qdev, node_id)) {
+				PR_ERR("pp_qos_port_block fail Port[%d]\n",
+				       en->id.cqm_deq_port);
+				return DP_FAILURE;
+			}
+		}
+		if (en->en & DP_NODE_SUSPEND) {
+			if (pp_qos_port_disable(priv->qdev, node_id)) {
+				PR_ERR("pp_qos_port_disable fail Port[%d]\n",
+				       en->id.cqm_deq_port);
+				return DP_FAILURE;
+			}
+		}
+		if (en->en & DP_NODE_RESUME) {
+			if (pp_qos_port_enable(priv->qdev, node_id)) {
+				PR_ERR("pp_qos_port_enable fail Port[%d]\n",
+				       en->id.cqm_deq_port);
+				return DP_FAILURE;
+			}
+		}
+		if (qos_port_conf_get(priv->qdev, node_id, &port_cfg)) {
+			PR_ERR("qos_port_conf_get fail: port[%d]\n",
+			       en->id.cqm_deq_port);
+			return DP_FAILURE;
+		}
+		if (en->en & DP_NODE_SUSPEND) {
+			if (!port_cfg.disable) {
+				PR_ERR("Incorrect value set for Port[%d]:%d\n",
+				       en->id.cqm_deq_port,
+				       port_cfg.disable);
+				return DP_FAILURE;
+			}
+		}
+		if (en->en & DP_NODE_RESUME) {
+			if (port_cfg.disable) {
+				PR_ERR("Incorrect value set for Port[%d]:%d\n",
+				       en->id.cqm_deq_port,
+				       port_cfg.disable);
+				return DP_FAILURE;
+			}
+		}
+	}
+	return DP_SUCCESS;
+}
+
+/* dp_node_link_en_get_31 API
+ * Get status of link node and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_node_link_en_get_31(struct dp_node_link_enable *en, int flag)
+{
+	int node_id;
+	struct hal_priv *priv = HAL(en->inst);
+
+	if (!priv || !priv->qdev) {
+		PR_ERR("priv or priv->qdev NULL\n");
+		return DP_FAILURE;
+	}
+	if (!en) {
+		PR_ERR("en info NULL\n");
+		return DP_FAILURE;
+	}
+	if (en->type == DP_NODE_QUEUE) {
+		struct pp_qos_queue_conf q_conf = {0};
+
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "en->id.q_id=%d\n", en->id.q_id);
+		node_id = priv->qos_queue_stat[en->id.q_id].node_id;
+		if (qos_queue_conf_get(priv->qdev, node_id, &q_conf)) {
+			PR_ERR("qos_queue_conf_get fail: q[%d]\n",
+			       en->id.q_id);
+			return DP_FAILURE;
+		}
+		if (q_conf.blocked)
+			en->en |= DP_NODE_DIS;
+		else
+			en->en |= DP_NODE_EN;
+	} else if (en->type == DP_NODE_SCH) {
+		struct pp_qos_sched_conf sched_conf = {0};
+
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "en->id.sch_id=%d\n", en->id.sch_id);
+		if (qos_sched_conf_get(priv->qdev, en->id.sch_id,
+				       &sched_conf)) {
+			PR_ERR("qos_sched_conf_get fail: sched[/%d]\n",
+			       en->id.sch_id);
+			return DP_FAILURE;
+		}
+		en->en |= DP_NODE_EN;
+	} else if (en->type == DP_NODE_PORT) {
+		struct pp_qos_port_conf p_conf = {0};
+
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "en->id.cqm_deq_port=%d\n", en->id.cqm_deq_port);
+		node_id = priv->deq_port_stat[en->id.cqm_deq_port].node_id;
+		if (qos_port_conf_get(priv->qdev, node_id, &p_conf)) {
+			PR_ERR("qos_queue_conf_get fail: port[%d]\n",
+			       en->id.cqm_deq_port);
+			return DP_FAILURE;
+		}
+		if (p_conf.disable)
+			en->en |= DP_NODE_DIS;
+		else
+			en->en |= DP_NODE_EN;
+	}
+	return DP_SUCCESS;
+}
+
+/* dp_link_get_31 API
+ * get full link based on queue and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_link_get_31(struct dp_qos_link *cfg, int flag)
+{
+	struct pp_qos_queue_conf queue_cfg = {0};
+	struct pp_qos_sched_conf sched_cfg = {0};
+	struct hal_priv *priv;
+	int i, node_id;
+
+	if (!cfg) {
+		PR_ERR("cfg cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(cfg->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	node_id = priv->qos_queue_stat[cfg->q_id].node_id;
+
+	if (!(priv->qos_queue_stat[cfg->q_id].flag & PP_NODE_ACTIVE)) {
+		PR_ERR("Incorrect queue:%d state:expect ACTIV\n", cfg->q_id);
+		return DP_FAILURE;
+	}
+
+	if (qos_queue_conf_get(priv->qdev, node_id, &queue_cfg)) {
+		PR_ERR("failed to qos_queue_conf_get\n");
+		return DP_FAILURE;
+	}
+	cfg->q_arbi = get_parent_arbi(cfg->inst, node_id, 0);
+	cfg->q_leaf = 0;
+	cfg->n_sch_lvl = 0;
+	cfg->q_prio_wfq = queue_cfg.queue_child_prop.priority;
+
+	if (priv->qos_sch_stat[node_id].parent.type == DP_NODE_PORT) {
+		cfg->cqm_deq_port = priv->qos_sch_stat[node_id].parent.node_id;
+		return DP_SUCCESS;
+	} else if (priv->qos_sch_stat[node_id].parent.type == DP_NODE_SCH) {
+		for (i = 0; i < DP_MAX_SCH_LVL - 1; i++) {
+			cfg->sch[i].id =
+				priv->qos_sch_stat[node_id].parent.node_id;
+			node_id = cfg->sch[i].id;
+			cfg->n_sch_lvl = i + 1;
+			cfg->sch[i].leaf = 0;
+			cfg->sch[i].arbi = get_parent_arbi(cfg->inst,
+							   cfg->sch[i].id, 0);
+			cfg->sch[i + 1].id =
+				priv->qos_sch_stat[node_id].parent.node_id;
+			if (qos_sched_conf_get(priv->qdev, cfg->sch[i].id,
+					       &sched_cfg)) {
+				PR_ERR("dp_link_get:sched[/%d] conf get fail\n",
+				       cfg->sch[i].id);
+				return DP_FAILURE;
+			}
+			cfg->sch[i].prio_wfq =
+					sched_cfg.sched_child_prop.priority;
+			if (priv->qos_sch_stat[cfg->sch[i].id].parent.type ==
+			    DP_NODE_PORT)
+				break;
+		}
+		cfg->cqm_deq_port =
+			priv->qos_sch_stat[cfg->sch[i].id].parent.node_id;
+		return DP_SUCCESS;
+	}
+	return DP_FAILURE;
+}
+
+/* dp_link_add_31 API
+ * configure end to end link and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_link_add_31(struct dp_qos_link *cfg, int flag)
+{
+	struct dp_node_link info = {0};
+	int i;
+	int f_q_free = 0;
+	int f_q_auto = 0; /* flag if node is DP_NODE_AUTO_ID */
+	struct f {
+		u16 flag;
+		u16 sch_id;
+		u16 f_auto; /* flag if node is DP_NODE_AUTO_ID */
+	};
+	struct f f_sch_free[DP_MAX_SCH_LVL] = {0};
+
+	if (!cfg) {
+		PR_ERR("cfg cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (cfg->n_sch_lvl > DP_MAX_SCH_LVL) {
+		PR_ERR("Incorrect sched_lvl:%s(%d) > %s(%d)\n",
+		       "cfg->n_sch_lvl", cfg->n_sch_lvl,
+		       "DP_MAX_SCH_LVL", DP_MAX_SCH_LVL);
+		return DP_FAILURE;
+	}
+
+	info.inst = cfg->inst;
+	info.dp_port = cfg->dp_port;
+	info.node_id.q_id = cfg->q_id;
+	info.cqm_deq_port.cqm_deq_port = cfg->cqm_deq_port;
+
+	if (cfg->q_id == DP_NODE_AUTO_ID)
+		f_q_auto = 1;
+
+	DP_DEBUG(DP_DBG_FLAG_QOS_DETAIL,
+		 "inst=%d dp_port=%d q_id=%d cqm_deq_port=%d n_sch_lvl=%d\n",
+		 info.inst, info.dp_port, info.node_id.q_id,
+		 info.cqm_deq_port.cqm_deq_port, cfg->n_sch_lvl);
+	if (cfg->n_sch_lvl) {
+		info.node_id.sch_id = cfg->sch[cfg->n_sch_lvl - 1].id;
+		info.node_type = DP_NODE_SCH;
+		info.p_node_id.cqm_deq_port = cfg->cqm_deq_port;
+		info.p_node_type = DP_NODE_PORT;
+		info.arbi = cfg->sch[cfg->n_sch_lvl - 1].arbi;
+		info.leaf = cfg->sch[cfg->n_sch_lvl - 1].leaf;
+		info.prio_wfq = cfg->sch[cfg->n_sch_lvl - 1].prio_wfq;
+		f_sch_free[cfg->n_sch_lvl - 1].flag = 1;
+
+		if (dp_node_link_add_31(&info, flag)) {
+			PR_ERR("Failed to link Sch:%d to Port:%d\n",
+			       cfg->sch[cfg->n_sch_lvl - 1].id,
+			       cfg->cqm_deq_port);
+			goto EXIT;
+		}
+		/* link sched to port */
+		if (cfg->sch[cfg->n_sch_lvl - 1].id == DP_NODE_AUTO_ID)
+			f_sch_free[cfg->n_sch_lvl - 1].f_auto = 1;
+
+		f_sch_free[cfg->n_sch_lvl - 1].sch_id = info.node_id.sch_id;
+
+		/* link sched to sched */
+		for (i = (cfg->n_sch_lvl - 2); i >= 0; i--) {
+			info.node_id.sch_id = cfg->sch[i].id;
+			info.node_type = DP_NODE_SCH;
+			info.p_node_id.sch_id = f_sch_free[i + 1].sch_id;
+			info.p_node_type = DP_NODE_SCH;
+			info.arbi = cfg->sch[i].arbi;
+			info.leaf = cfg->sch[i].leaf;
+			info.prio_wfq = cfg->sch[i].prio_wfq;
+			f_sch_free[i].flag = 1;
+
+			if (dp_node_link_add_31(&info, flag)) {
+				PR_ERR("Failed to link Sch:%d to Sch:%d\n",
+				       cfg->sch[i].id, cfg->sch[i + 1].id);
+				goto EXIT;
+			}
+			if (cfg->sch[i].id == DP_NODE_AUTO_ID)
+				f_sch_free[i].f_auto = 1;
+
+			f_sch_free[i].sch_id = info.node_id.sch_id;
+		}
+		/* link Queue to sched */
+		info.node_type = DP_NODE_QUEUE;
+		info.node_id.q_id = cfg->q_id;
+		info.p_node_id.sch_id = f_sch_free[0].sch_id;
+		info.p_node_type = DP_NODE_SCH;
+		info.arbi = cfg->sch[0].arbi;
+		info.leaf = cfg->sch[0].leaf;
+		info.prio_wfq = cfg->sch[0].prio_wfq;
+
+		if (dp_node_link_add_31(&info, flag)) {
+			PR_ERR("Failed to link Q:%d to Sch:%d\n",
+			       cfg->q_id, cfg->sch[0].id);
+			f_q_free = 1;
+			goto EXIT;
+		}
+	} else {
+		/* link Queue to Port */
+		info.node_type = DP_NODE_QUEUE;
+		info.p_node_id.cqm_deq_port = cfg->cqm_deq_port;
+		info.p_node_type = DP_NODE_PORT;
+		info.arbi = cfg->q_arbi;
+		info.leaf = cfg->q_leaf;
+		info.prio_wfq = cfg->q_prio_wfq;
+
+		if (dp_node_link_add_31(&info, flag)) {
+			PR_ERR("Failed to link Q:%d to Port:%d\n",
+			       cfg->q_id, cfg->cqm_deq_port);
+			f_q_free = 1;
+			goto EXIT;
+		}
+	}
+	return DP_SUCCESS;
+EXIT:
+	for (i = (cfg->n_sch_lvl - 1); i >= 0; i--) {
+		if (!f_sch_free[i].flag)
+			continue;
+		/* sch is auto_alloc move it to FREE */
+		if (f_sch_free[i].f_auto) {
+			struct dp_node_alloc node = {0};
+
+			node.id.sch_id = f_sch_free[i].sch_id;
+			node.type = DP_NODE_SCH;
+			node.dp_port = cfg->dp_port;
+			node.inst = cfg->inst;
+			dp_node_free_31(&node,
+					flag);
+		}
+		/* sch provided by caller move it to ALLOC */
+		if (node_stat_update(info.inst, f_sch_free[i].sch_id,
+				     DP_NODE_DEC)) {
+			PR_ERR("Failed to %s sched:%d DP_NODE_DEC\n",
+			       "node_stat_update",
+			       f_sch_free[i].sch_id);
+			continue;
+		}
+	}
+	if (f_q_free) {
+		/* queue is auto_alloc move it to FREE */
+		if (f_q_auto) {
+			struct dp_node_alloc node = {0};
+
+			node.id.q_id = cfg->q_id;
+			node.type = DP_NODE_QUEUE;
+			node.dp_port = cfg->dp_port;
+			node.inst = cfg->inst;
+			dp_node_free_31(&node, flag);
+		}
+		/* queue provided by caller move it to ALLOC */
+		if (node_stat_update(info.inst, cfg->q_id, DP_NODE_DEC)) {
+			PR_ERR("Failed to update stat qid %d DP_NODE_DEC\n",
+			       cfg->q_id);
+			return DP_FAILURE;
+		}
+	}
+	return DP_FAILURE;
+}
+
+/* dp_shaper_conf_set_31 API
+ * DP_NO_SHAPER_LIMIT no limit for shaper
+ * DP_MAX_SHAPER_LIMIT max limit for shaper
+ * configure shaper limit for node and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_shaper_conf_set_31(struct dp_shaper_conf *cfg, int flag)
+{
+	struct pp_qos_queue_conf queue_cfg = {0};
+	struct pp_qos_sched_conf sched_cfg = {0};
+	struct pp_qos_port_conf port_cfg = {0};
+	struct hal_priv *priv;
+	int node_id, res;
+	u32 bw_limit;
+
+	if (!cfg) {
+		PR_ERR("cfg cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(cfg->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (cfg->type == DP_NODE_QUEUE) {
+		if ((cfg->id.q_id < 0) || (cfg->id.q_id >= MAX_QUEUE)) {
+			PR_ERR("Invalid Queue ID:%d\n", cfg->id.q_id);
+			return DP_FAILURE;
+		}
+		if (priv->qos_queue_stat[cfg->id.q_id].flag == PP_NODE_FREE) {
+			PR_ERR("Invalid Queue flag:%d\n",
+			       priv->qos_queue_stat[cfg->id.q_id].flag);
+			return DP_FAILURE;
+		}
+		node_id = priv->qos_queue_stat[cfg->id.q_id].node_id;
+
+		if (qos_queue_conf_get(priv->qdev, node_id, &queue_cfg)) {
+			PR_ERR("qos_queue_conf_get fail:%d\n", cfg->id.q_id);
+			return DP_FAILURE;
+		}
+
+		if ((cfg->cmd == DP_SHAPER_CMD_ADD) ||
+		    (cfg->cmd == DP_SHAPER_CMD_ENABLE)) {
+			res = limit_dp2pp(cfg->cir, &bw_limit);
+
+			if (res == DP_FAILURE) {
+				PR_ERR("Wrong dp shaper limit:%u\n", cfg->cir);
+				return DP_FAILURE;
+			}
+			queue_cfg.common_prop.bandwidth_limit = bw_limit;
+		} else if ((cfg->cmd == DP_SHAPER_CMD_REMOVE) ||
+			   (cfg->cmd == DP_SHAPER_CMD_DISABLE)) {
+			queue_cfg.common_prop.bandwidth_limit = 0;
+		} else {
+			PR_ERR("Incorrect command provided:%d\n", cfg->cmd);
+			return DP_FAILURE;
+		}
+
+		if (qos_queue_set(priv->qdev, node_id, &queue_cfg)) {
+			PR_ERR("qos_queue_set fail:%d\n", cfg->id.q_id);
+			return DP_FAILURE;
+		}
+		return DP_SUCCESS;
+	} else if (cfg->type == DP_NODE_SCH) {
+		if ((cfg->id.sch_id < 0) || (cfg->id.sch_id >= QOS_MAX_NODES)) {
+			PR_ERR("Invalid Sched ID:%d\n", cfg->id.sch_id);
+			return DP_FAILURE;
+		}
+		if (priv->qos_sch_stat[cfg->id.sch_id].p_flag ==
+		    PP_NODE_FREE) {
+			PR_ERR("Invalid Sched flag:%d\n",
+			       priv->qos_sch_stat[cfg->id.sch_id].p_flag);
+			return DP_FAILURE;
+		}
+
+		if (qos_sched_conf_get(priv->qdev, cfg->id.sch_id,
+				       &sched_cfg)) {
+			PR_ERR("qos_sched_conf_get fail:%d\n", cfg->id.sch_id);
+			return DP_FAILURE;
+		}
+
+		if ((cfg->cmd == DP_SHAPER_CMD_ADD) ||
+		    (cfg->cmd == DP_SHAPER_CMD_ENABLE)) {
+			res = limit_dp2pp(cfg->cir, &bw_limit);
+
+			if (res == DP_FAILURE) {
+				PR_ERR("Wrong dp shaper limit:%u\n", cfg->cir);
+				return DP_FAILURE;
+			}
+			sched_cfg.common_prop.bandwidth_limit = bw_limit;
+		} else if ((cfg->cmd == DP_SHAPER_CMD_REMOVE) ||
+			   (cfg->cmd == DP_SHAPER_CMD_DISABLE)) {
+			sched_cfg.common_prop.bandwidth_limit = 0;
+		} else {
+			PR_ERR("Incorrect command provided:%d\n", cfg->cmd);
+			return DP_FAILURE;
+		}
+
+		if (qos_sched_set(priv->qdev, cfg->id.sch_id, &sched_cfg)) {
+			PR_ERR("qos_sched_set fail:%d\n", cfg->id.sch_id);
+			return DP_FAILURE;
+		}
+		return DP_SUCCESS;
+	} else if (cfg->type == DP_NODE_PORT) {
+		if ((cfg->id.cqm_deq_port < 0) ||
+		    (cfg->id.cqm_deq_port >= MAX_CQM_DEQ)) {
+			PR_ERR("Invalid Port ID:%d\n", cfg->id.cqm_deq_port);
+			return DP_FAILURE;
+		}
+		if (priv->deq_port_stat[cfg->id.cqm_deq_port].flag ==
+		    PP_NODE_FREE) {
+			PR_ERR("Invalid Port flag:%d\n",
+			       priv->deq_port_stat[cfg->id.cqm_deq_port].flag);
+			return DP_FAILURE;
+		}
+		node_id = priv->deq_port_stat[cfg->id.cqm_deq_port].node_id;
+
+		if (qos_port_conf_get(priv->qdev, node_id, &port_cfg)) {
+			PR_ERR("qos_port_conf_get fail:%d\n",
+			       cfg->id.cqm_deq_port);
+			return DP_FAILURE;
+		}
+
+		if ((cfg->cmd == DP_SHAPER_CMD_ADD) ||
+		    (cfg->cmd == DP_SHAPER_CMD_ENABLE)) {
+			res = limit_dp2pp(cfg->cir, &bw_limit);
+
+			if (res == DP_FAILURE) {
+				PR_ERR("Wrong dp shaper limit:%u\n", cfg->cir);
+				return DP_FAILURE;
+			}
+			port_cfg.common_prop.bandwidth_limit = bw_limit;
+		} else if ((cfg->cmd == DP_SHAPER_CMD_REMOVE) ||
+			   (cfg->cmd == DP_SHAPER_CMD_DISABLE)) {
+			port_cfg.common_prop.bandwidth_limit = 0;
+		} else {
+			PR_ERR("Incorrect command provided:%d\n", cfg->cmd);
+			return DP_FAILURE;
+		}
+
+		if (qos_port_set(priv->qdev, node_id, &port_cfg)) {
+			PR_ERR("qos_port_set fail:%d\n", cfg->id.cqm_deq_port);
+			return DP_FAILURE;
+		}
+		return DP_SUCCESS;
+	}
+	PR_ERR("Unkonwn type provided:0x%x\n", cfg->type);
+	return DP_FAILURE;
+}
+
+/* dp_shaper_conf_get_31 API
+ * DP_NO_SHAPER_LIMIT no limit for shaper
+ * DP_MAX_SHAPER_LIMIT max limit for shaper
+ * get shaper limit for node fill struct and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_shaper_conf_get_31(struct dp_shaper_conf *cfg, int flag)
+{
+	struct pp_qos_queue_conf queue_cfg = {0};
+	struct pp_qos_sched_conf sched_cfg = {0};
+	struct pp_qos_port_conf port_cfg = {0};
+	struct hal_priv *priv;
+	int node_id, res;
+	u32 bw_limit;
+
+	if (!cfg) {
+		PR_ERR("cfg cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(cfg->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (cfg->type == DP_NODE_QUEUE) {
+		if ((cfg->id.q_id < 0) || (cfg->id.q_id >= MAX_QUEUE)) {
+			PR_ERR("Invalid Queue ID:%d\n", cfg->id.q_id);
+			return DP_FAILURE;
+		}
+		if (priv->qos_queue_stat[cfg->id.q_id].flag == PP_NODE_FREE) {
+			PR_ERR("Invalid Queue flag:%d\n",
+			       priv->qos_queue_stat[cfg->id.q_id].flag);
+			return DP_FAILURE;
+		}
+		node_id = priv->qos_queue_stat[cfg->id.q_id].node_id;
+
+		if (qos_queue_conf_get(priv->qdev, node_id, &queue_cfg)) {
+			PR_ERR("qos_queue_conf_get fail:%d\n", cfg->id.q_id);
+			return DP_FAILURE;
+		}
+		res = limit_pp2dp(queue_cfg.common_prop.bandwidth_limit,
+				  &bw_limit);
+
+		if (res == DP_FAILURE) {
+			PR_ERR("Wrong pp shaper limit:%u\n",
+			       queue_cfg.common_prop.bandwidth_limit);
+			return DP_FAILURE;
+		}
+	} else if (cfg->type == DP_NODE_SCH) {
+		if ((cfg->id.sch_id < 0) || (cfg->id.sch_id >= QOS_MAX_NODES)) {
+			PR_ERR("Invalid Sched ID:%d\n", cfg->id.sch_id);
+			return DP_FAILURE;
+		}
+		if (priv->qos_sch_stat[cfg->id.sch_id].p_flag ==
+		    PP_NODE_FREE) {
+			PR_ERR("Invalid Sched flag:%d\n",
+			       priv->qos_sch_stat[cfg->id.sch_id].p_flag);
+			return DP_FAILURE;
+		}
+
+		if (qos_sched_conf_get(priv->qdev, cfg->id.sch_id,
+				       &sched_cfg)) {
+			PR_ERR("qos_sched_conf_get fail:%d\n", cfg->id.sch_id);
+			return DP_FAILURE;
+		}
+
+		res = limit_pp2dp(sched_cfg.common_prop.bandwidth_limit,
+				  &bw_limit);
+
+		if (res == DP_FAILURE) {
+			PR_ERR("Wrong pp shaper limit:%u\n",
+			       sched_cfg.common_prop.bandwidth_limit);
+			return DP_FAILURE;
+		}
+	} else if (cfg->type == DP_NODE_PORT) {
+		if ((cfg->id.cqm_deq_port < 0) ||
+		    (cfg->id.cqm_deq_port >= MAX_CQM_DEQ)) {
+			PR_ERR("Invalid Port ID:%d\n", cfg->id.cqm_deq_port);
+			return DP_FAILURE;
+		}
+		if (priv->deq_port_stat[cfg->id.cqm_deq_port].flag ==
+		    PP_NODE_FREE) {
+			PR_ERR("Invalid Port flag:%d\n",
+			       priv->deq_port_stat[cfg->id.cqm_deq_port].flag);
+			return DP_FAILURE;
+		}
+		node_id = priv->deq_port_stat[cfg->id.cqm_deq_port].node_id;
+		if (qos_port_conf_get(priv->qdev, node_id, &port_cfg)) {
+			PR_ERR("qos_port_conf_get fail:%d\n",
+			       cfg->id.cqm_deq_port);
+			return DP_FAILURE;
+		}
+		res = limit_pp2dp(port_cfg.common_prop.bandwidth_limit,
+				  &bw_limit);
+
+		if (res == DP_FAILURE) {
+			PR_ERR("Wrong pp shaper limit:%u\n",
+			       port_cfg.common_prop.bandwidth_limit);
+			return DP_FAILURE;
+		}
+	} else {
+		PR_ERR("Unkonwn type provided:0x%x\n", cfg->type);
+		return DP_FAILURE;
+	}
+
+	cfg->cir = bw_limit;
+	cfg->pir = 0;
+	cfg->cbs = 0;
+	cfg->pbs = 0;
+	return DP_SUCCESS;
+}
+
+int dp_queue_map_get_31(struct dp_queue_map_get *cfg, int flag)
+{
+	struct hal_priv *priv;
+	cbm_queue_map_entry_t *qmap_entry = NULL;
+	s32 num_entry;
+	int i;
+	int res = DP_SUCCESS;
+
+	if (!cfg) {
+		PR_ERR("cfg cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(cfg->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	if ((cfg->q_id < 0) || (cfg->q_id >= MAX_QUEUE)) {
+		PR_ERR("Invalid Queue ID:%d\n", cfg->q_id);
+		return DP_FAILURE;
+	}
+	if (priv->qos_queue_stat[cfg->q_id].flag == PP_NODE_FREE) {
+		PR_ERR("Invalid Queue flag:%d\n",
+		       priv->qos_queue_stat[cfg->q_id].flag);
+		return DP_FAILURE;
+	}
+
+	if (cbm_queue_map_get(cfg->inst, cfg->q_id, &num_entry,
+			      &qmap_entry, 0)) {
+		PR_ERR("cbm_queue_map_get fail:%d\n", cfg->q_id);
+		return DP_FAILURE;
+	}
+
+	cfg->num_entry = num_entry;
+
+	if (!qmap_entry) {
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "queue map entry returned null value\n");
+		if (num_entry) {
+			PR_ERR("num_entry is not null:%d\n", num_entry);
+			res = DP_FAILURE;
+		}
+		goto EXIT;
+	}
+
+	if (!cfg->qmap_entry)
+		goto EXIT;
+
+	if (num_entry > cfg->qmap_size) {
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "num_entry is greater than qmap_size:%d\n",
+			 num_entry);
+		goto EXIT;
+	}
+
+	for (i = 0; i < num_entry; i++) {
+		if (cfg->qmap_entry[i].qm_mode == DP_Q_MAP_MODE0) {
+			cfg->qmap_entry[i].qmap.map0.flowid =
+							qmap_entry[i].flowid;
+			cfg->qmap_entry[i].qmap.map0.dec = qmap_entry[i].dec;
+			cfg->qmap_entry[i].qmap.map0.enc = qmap_entry[i].enc;
+			cfg->qmap_entry[i].qmap.map0.mpe1 = qmap_entry[i].mpe1;
+			cfg->qmap_entry[i].qmap.map0.mpe2 = qmap_entry[i].mpe2;
+			cfg->qmap_entry[i].qmap.map0.dp_port = qmap_entry[i].ep;
+			cfg->qmap_entry[i].qmap.map0.class = qmap_entry[i].tc;
+		} else if (cfg->qmap_entry[i].qm_mode == DP_Q_MAP_MODE1) {
+			cfg->qmap_entry[i].qm_mode = qmap_entry[i].mode;
+			cfg->qmap_entry[i].qmap.map1.subif =
+							qmap_entry[i].sub_if_id;
+			cfg->qmap_entry[i].qmap.map1.mpe1 = qmap_entry[i].mpe1;
+			cfg->qmap_entry[i].qmap.map1.mpe2 = qmap_entry[i].mpe2;
+			cfg->qmap_entry[i].qmap.map1.dp_port = qmap_entry[i].ep;
+		} else if (cfg->qmap_entry[i].qm_mode == DP_Q_MAP_MODE2) {
+			cfg->qmap_entry[i].qm_mode = qmap_entry[i].mode;
+			cfg->qmap_entry[i].qmap.map2.subif =
+							qmap_entry[i].sub_if_id;
+			cfg->qmap_entry[i].qmap.map2.mpe1 = qmap_entry[i].mpe1;
+			cfg->qmap_entry[i].qmap.map2.mpe2 = qmap_entry[i].mpe2;
+			cfg->qmap_entry[i].qmap.map2.dp_port = qmap_entry[i].ep;
+			cfg->qmap_entry[i].qmap.map2.class = qmap_entry[i].tc;
+		} else if (cfg->qmap_entry[i].qm_mode == DP_Q_MAP_MODE3) {
+			cfg->qmap_entry[i].qm_mode = qmap_entry[i].mode;
+			cfg->qmap_entry[i].qmap.map3.subif =
+							qmap_entry[i].sub_if_id;
+			cfg->qmap_entry[i].qmap.map3.mpe1 = qmap_entry[i].mpe1;
+			cfg->qmap_entry[i].qmap.map3.mpe2 = qmap_entry[i].mpe2;
+			cfg->qmap_entry[i].qmap.map3.dp_port = qmap_entry[i].ep;
+			cfg->qmap_entry[i].qmap.map3.class = qmap_entry[i].tc;
+		} else {
+			PR_ERR("queue map not found for q:%d\n", cfg->q_id);
+			res = DP_FAILURE;
+		}
+	}
+
+EXIT:
+	if (!qmap_entry)
+		kfree(qmap_entry);
+	qmap_entry = NULL;
+	return res;
+}
+
+int dp_queue_map_set_31(struct dp_queue_map_set *cfg, int flag)
+{
+	struct hal_priv *priv;
+	cbm_queue_map_entry_t qmap_cfg = {0};
+	u32 cqm_flags = 0;
+
+	if (!cfg) {
+		PR_ERR("cfg cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(cfg->inst);
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	if ((cfg->q_id < 0) || (cfg->q_id >= MAX_QUEUE)) {
+		PR_ERR("Invalid Queue ID:%d\n", cfg->q_id);
+		return DP_FAILURE;
+	}
+	if (priv->qos_queue_stat[cfg->q_id].flag == PP_NODE_FREE) {
+		PR_ERR("Invalid Queue flag:%d\n",
+		       priv->qos_queue_stat[cfg->q_id].flag);
+		return DP_FAILURE;
+	}
+
+	if (cfg->qm_mode == DP_Q_MAP_MODE0) {
+		qmap_cfg.mode = cfg->qm_mode;
+		qmap_cfg.mpe1 = cfg->map.map0.mpe1;
+		qmap_cfg.mpe2 = cfg->map.map0.mpe2;
+		qmap_cfg.ep = cfg->map.map0.dp_port;
+		qmap_cfg.flowid = cfg->map.map0.flowid;
+		qmap_cfg.dec = cfg->map.map0.dec;
+		qmap_cfg.enc = cfg->map.map0.enc;
+		qmap_cfg.tc = cfg->map.map0.class;
+		if (cfg->mask.mask0.mpe1)
+			cqm_flags |= CBM_QUEUE_MAP_F_MPE1_DONTCARE;
+		if (cfg->mask.mask0.mpe2)
+			cqm_flags |= CBM_QUEUE_MAP_F_MPE2_DONTCARE;
+		if (cfg->mask.mask0.dp_port)
+			cqm_flags |= CBM_QUEUE_MAP_F_EP_DONTCARE;
+		if (cfg->mask.mask0.flowid) {
+			cqm_flags |= CBM_QUEUE_MAP_F_FLOWID_L_DONTCARE;
+			cqm_flags |= CBM_QUEUE_MAP_F_FLOWID_H_DONTCARE;
+		}
+		if (cfg->mask.mask0.dec)
+			cqm_flags |= CBM_QUEUE_MAP_F_DE_DONTCARE;
+		if (cfg->mask.mask0.enc)
+			cqm_flags |= CBM_QUEUE_MAP_F_EN_DONTCARE;
+		if (cfg->mask.mask0.class)
+			cqm_flags |= CBM_QUEUE_MAP_F_TC_DONTCARE;
+	} else if (cfg->qm_mode == DP_Q_MAP_MODE1) {
+		qmap_cfg.mode = cfg->qm_mode;
+		qmap_cfg.mpe1 = cfg->map.map1.mpe1;
+		qmap_cfg.mpe2 = cfg->map.map1.mpe2;
+		qmap_cfg.ep = cfg->map.map1.dp_port;
+		qmap_cfg.sub_if_id = cfg->map.map1.subif;
+		if (cfg->mask.mask1.mpe1)
+			cqm_flags |= CBM_QUEUE_MAP_F_MPE1_DONTCARE;
+		if (cfg->mask.mask1.mpe2)
+			cqm_flags |= CBM_QUEUE_MAP_F_MPE2_DONTCARE;
+		if (cfg->mask.mask1.dp_port)
+			cqm_flags |= CBM_QUEUE_MAP_F_EP_DONTCARE;
+		if (cfg->mask.mask1.subif)
+			cqm_flags |= CBM_QUEUE_MAP_F_SUBIF_DONTCARE;
+	} else if (cfg->qm_mode == DP_Q_MAP_MODE2) {
+		qmap_cfg.mode = cfg->qm_mode;
+		qmap_cfg.mpe1 = cfg->map.map2.mpe1;
+		qmap_cfg.mpe2 = cfg->map.map2.mpe2;
+		qmap_cfg.ep = cfg->map.map2.dp_port;
+		qmap_cfg.sub_if_id = cfg->map.map2.subif;
+		qmap_cfg.tc = cfg->map.map2.class;
+		if (cfg->mask.mask2.mpe1)
+			cqm_flags |= CBM_QUEUE_MAP_F_MPE1_DONTCARE;
+		if (cfg->mask.mask2.mpe2)
+			cqm_flags |= CBM_QUEUE_MAP_F_MPE2_DONTCARE;
+		if (cfg->mask.mask2.dp_port)
+			cqm_flags |= CBM_QUEUE_MAP_F_EP_DONTCARE;
+		if (cfg->mask.mask2.subif)
+			cqm_flags |= CBM_QUEUE_MAP_F_SUBIF_DONTCARE;
+		if (cfg->mask.mask2.class)
+			cqm_flags |= CBM_QUEUE_MAP_F_TC_DONTCARE;
+	} else if (cfg->qm_mode == DP_Q_MAP_MODE3) {
+		qmap_cfg.mode = cfg->qm_mode;
+		qmap_cfg.mpe1 = cfg->map.map3.mpe1;
+		qmap_cfg.mpe2 = cfg->map.map3.mpe2;
+		qmap_cfg.ep = cfg->map.map3.dp_port;
+		qmap_cfg.sub_if_id = cfg->map.map3.subif;
+		qmap_cfg.tc = cfg->map.map3.class;
+		if (cfg->mask.mask3.mpe1)
+			cqm_flags |= CBM_QUEUE_MAP_F_MPE1_DONTCARE;
+		if (cfg->mask.mask3.mpe2)
+			cqm_flags |= CBM_QUEUE_MAP_F_MPE2_DONTCARE;
+		if (cfg->mask.mask3.dp_port)
+			cqm_flags |= CBM_QUEUE_MAP_F_EP_DONTCARE;
+		if (cfg->mask.mask3.subif)
+			cqm_flags |= CBM_QUEUE_MAP_F_SUBIF_DONTCARE;
+		if (cfg->mask.mask3.class)
+			cqm_flags |= CBM_QUEUE_MAP_F_TC_DONTCARE;
+	} else {
+		PR_ERR("Incorrect qm_mode provided:%d\n", cfg->qm_mode);
+		return DP_FAILURE;
+	}
+
+	if (cbm_queue_map_set(cfg->inst, cfg->q_id, &qmap_cfg, cqm_flags)) {
+		PR_ERR("cbm_queue_map_set fail for Q:%d\n", cfg->q_id);
+		return DP_FAILURE;
+	}
+	return DP_SUCCESS;
+}
+
+int dp_counter_mode_set_31(struct dp_counter_conf *cfg, int flag)
+{
+	return DP_FAILURE;
+}
+
+int dp_counter_mode_get_31(struct dp_counter_conf *cfg, int flag)
+{
+	return DP_FAILURE;
+}
+
+int get_sch_level(int inst, int pid, int flag)
+{
+	struct hal_priv *priv;
+	int level;
+
+	priv = HAL(inst);
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	for (level = 0; level < DP_MAX_SCH_LVL; level++) {
+		if (priv->qos_sch_stat[pid].parent.type == DP_NODE_PORT) {
+			level = level + 1;
+			break;
+		}
+		pid = priv->qos_sch_stat[pid].parent.node_id;
+	}
+	return level;
+}
+
+/* dp_qos_level_get_31 API
+ * get max scheduler level and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_qos_level_get_31(struct dp_qos_level *dp, int flag)
+{
+	struct hal_priv *priv;
+	u16 i, id, pid, lvl_x = 0;
+
+	if (!dp) {
+		PR_ERR("dp cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	dp->max_sch_lvl = 0;
+	priv = HAL(dp->inst);
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	for (i = 0; i < MAX_QUEUE; i++) {
+		if (priv->qos_queue_stat[i].flag & PP_NODE_FREE)
+			continue;
+		id = priv->qos_queue_stat[i].node_id;
+
+		if (priv->qos_sch_stat[id].parent.type == DP_NODE_PORT) {
+			continue;
+		} else if (priv->qos_sch_stat[id].parent.type == DP_NODE_SCH) {
+			pid = priv->qos_sch_stat[id].parent.node_id;
+			lvl_x = get_sch_level(dp->inst, pid, 0);
+		}
+		if (lvl_x > dp->max_sch_lvl)
+			dp->max_sch_lvl = lvl_x;
+	}
+	if (dp->max_sch_lvl >= 0)
+		return DP_SUCCESS;
+	else
+		return DP_FAILURE;
+}
+
+#ifdef PP_QOS_LINK_EXAMPLE
+/* Example: queue -> QOS/CQM dequeue port
+ * inst: dp instance
+ * dp_port: dp port id
+ * t_conf: for pon, it is used to get cqm dequeue port via t-cont index
+ *         for others, its value should be 0
+ * q_node: queueu node id
+ */
+
+int ppv4_queue_port_example(int inst, int dp_port, int t_cont, int q_node)
+{
+	struct pp_qos_port_conf  port_cfg;
+	struct pp_qos_queue_conf queue_cfg;
+	int qos_port_node;
+	int rc;
+	int cqm_deq_port;
+	struct pmac_port_info *port_info;
+
+	port_info = &dp_port_info[inst][dp_port];
+	cqm_deq_port = port_info->deq_port_base + t_cont;
+
+	/* Allocate qos dequeue port's node id via cqm_deq_port */
+	rc = qos_port_allocate(priv->qdev, cqm_deq_port, &qos_port_node);
+	if (rc) {
+		PR_ERR("failed to qos_port_allocate\n");
+		return DP_FAILURE;
+	}
+
+	/* Configure QOS dequeue port */
+	qos_port_conf_set_default(&port_cfg);
+	port_cfg.ring_address = (void *)(port_info->tx_ring_addr +
+		port_info->tx_ring_offset * t_cont);
+	port_cfg.ring_size = port_info->tx_ring_size;
+	if (port_info->tx_pkt_credit) {
+		port_cfg.packet_credit_enable = 1;
+		port_cfg.credit = port_info->tx_pkt_credit;
+	}
+#ifdef TX_BYTE_CREDIT
+	if (port_info->tx_b_credit) {
+		port_cfg.byte_credit_enable = 1;
+		port_cfg.byte_credit = port_info->tx_pkt_credit;
+	}
+#endif
+#ifdef EXT_BW
+	port_cfg.parent.arbitration = ARBITRATION_WRR;
+	port_cfg.common.bandwidth_limit = 1000;
+#endif
+	rc = qos_port_set(priv->qdev, qos_port_node, &port_cfg);
+	if (rc) {
+		PR_ERR("failed to qos_port_set\n");
+		qos_port_remove(priv->qdev, qos_port_node);
+		return DP_FAILURE;
+	}
+
+	/* Attach queue to QoS port */
+	qos_queue_conf_set_default(&queue_cfg);
+	queue_cfg.queue_child_prop.parent = qos_port_node;
+#ifdef EXT_BW
+	queue_cfg.max_burst  = 64;
+	queue_cfg.child.bandwidth_share = 50;
+	queue_cfg.queue_wred_min_guaranteed = 1;
+	queue_cfg.queue_wred_max_allowed = 10;
+#endif
+	rc = qos_queue_set(priv->qdev, q_node, &queue_cfg);
+	if (rc) {
+		PR_ERR("failed to qos_queue_set\n");
+		qos_port_remove(priv->qdev, qos_port_node);
+		return DP_FAILURE;
+	}
+	return DP_SUCCESS;
+}
+
+/* Example: queue -> scheduler1- > scheduler 2 -> QOS/CQM dequeue port
+ * inst: dp instance
+ * dp_port: dp port id
+ * t_conf: for pon, it is used to get cqm dequeue port via t-cont index
+ *         for others, its value should be 0
+ * q_node: queueu node id
+ * sch_node1: schduler node which connected with queue
+ * sch_node2: schduler node which connected with sch_node1
+ */
+int ppv4_queue_scheduler(int inst, int dp_port, int t_cont, int q_node,
+			 int sch_node1, int sch_node2)
+{
+	struct pp_qos_port_conf  port_cfg;
+	struct pp_qos_queue_conf queue_cfg;
+	struct pp_qos_sched_conf sched_cfg;
+	int qos_port_node;
+	int rc;
+	int cqm_deq_port;
+	struct pmac_port_info *port_info;
+
+	port_info = &dp_port_info[inst][dp_port];
+	cqm_deq_port = port_info->deq_port_base + t_cont;
+
+	/* Allocate qos dequeue port's node id via cqm_deq_port */
+	rc = qos_port_allocate(priv->qdev, cqm_deq_port, &qos_port_node);
+	if (rc) {
+		PR_ERR("failed to qos_port_allocate\n");
+		return DP_FAILURE;
+	}
+
+	/* Configure QOS dequeue port */
+	qos_port_conf_set_default(&port_cfg);
+	port_cfg.ring_address = (void *)(port_info->tx_ring_addr +
+		port_info->tx_ring_offset * t_cont);
+	port_cfg.ring_size = port_info->tx_ring_size;
+	if (port_info->tx_pkt_credit) {
+		port_cfg.packet_credit_enable = 1;
+		port_cfg.credit = port_info->tx_pkt_credit;
+	}
+	if (port_info->tx_pkt_credit) {
+		port_cfg.byte_credit_enable = 1;
+		port_cfg.byte_credit = port_info->tx_pkt_credit;
+	}
+#ifdef EXT_BW
+	port_cfg.parent.arbitration = ARBITRATION_WRR;
+	port_cfg.common.bandwidth_limit = 1000;
+#endif
+	rc = qos_port_set(priv->qdev, qos_port_node, &port_cfg);
+	if (rc) {
+		PR_ERR("failed to qos_port_set\n");
+		qos_port_remove(priv->qdev, qos_port_node);
+		return DP_FAILURE;
+	}
+
+	/* Attach queue to sch_node1 */
+	qos_queue_conf_set_default(&queue_cfg);
+	queue_cfg.queue_child_prop.parent = sch_node1;
+#ifdef EXT_BW
+	queue_cfg.max_burst  = 64;
+	queue_cfg.child.bandwidth_share = 50;
+	queue_cfg.queue_wred_min_guaranteed = 1;
+	queue_cfg.queue_wred_max_allowed = 10;
+#endif
+	rc = qos_queue_set(priv->qdev, q_node, &queue_cfg);
+	if (rc) {
+		PR_ERR("failed to qos_queue_set\n");
+		qos_port_remove(priv->qdev, qos_port_node);
+		return DP_FAILURE;
+	}
+
+	/* Attach sch_node1 to sch_node2 */
+	qos_sched_conf_set_default(&sched_cfg);
+	sched_cfg.sched_child_prop.parent = sch_node2;
+	rc = qos_sched_set(priv->qdev, sch_node1, &sched_cfg);
+	if (rc) {
+		PR_ERR("failed to qos_sched_set\n");
+		qos_port_remove(priv->qdev, qos_port_node);
+		return DP_FAILURE;
+	}
+
+	/* Attach sch_node2 to qos/cqm dequeue port */
+	qos_sched_conf_set_default(&sched_cfg);
+	sched_cfg.sched_child_prop.parent = qos_port_node;
+	rc = qos_sched_set(priv->qdev, sch_node2, &sched_cfg);
+	if (rc) {
+		PR_ERR("failed to qos_sched_set\n");
+		qos_port_remove(priv->qdev, qos_port_node);
+		return DP_FAILURE;
+	}
+
+	return DP_SUCCESS;
+}
+
+#endif /* PP_QOS_LINK_EXAMPLE */
+
+static int get_children_list(int inst, struct dp_node *child, int node_id)
+{
+	int idx, num = 0, child_id;
+	struct hal_priv *priv = HAL(inst);
+
+	for (idx = 0; idx < DP_MAX_CHILD_PER_NODE; idx++) {
+		child_id =
+			get_qid_by_node(inst, CHILD(node_id, idx).node_id, 0);
+		if (priv->qos_sch_stat[node_id].child[idx].flag &
+		    PP_NODE_ACTIVE) {
+			child[idx].type = CHILD(node_id, idx).type;
+			if (child[idx].type == DP_NODE_SCH)
+				child[idx].id.q_id =
+						CHILD(node_id, idx).node_id;
+			else
+				child[idx].id.q_id = child_id;
+			num++;
+		}
+	}
+	return num;
+}
+
+/* dp_children_get_31 API
+ * Get direct chldren and type of given node and return DP_SUCCESS
+ * else return DP_FAILURE
+ */
+int dp_children_get_31(struct dp_node_child *cfg, int flag)
+{
+	int node_id, res = 0;
+	struct hal_priv *priv;
+
+	if (!cfg) {
+		PR_ERR("cfg cannot be NULL\n");
+		return DP_FAILURE;
+	}
+	priv = HAL(cfg->inst);
+	cfg->num = 0;
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	if (cfg->type == DP_NODE_SCH) {
+		if ((cfg->id.sch_id < 0) || (cfg->id.sch_id >= QOS_MAX_NODES)) {
+			PR_ERR("Invalid Sched ID:%d\n", cfg->id.sch_id);
+			return DP_FAILURE;
+		}
+		if (priv->qos_sch_stat[cfg->id.sch_id].c_flag == PP_NODE_FREE) {
+			PR_ERR("Invalid Sched flag:0x%x\n",
+			       priv->qos_sch_stat[cfg->id.sch_id].c_flag);
+			return DP_FAILURE;
+		}
+		node_id = cfg->id.sch_id;
+
+	} else if (cfg->type == DP_NODE_PORT) {
+		if ((cfg->id.cqm_deq_port < 0) ||
+		    (cfg->id.cqm_deq_port >= MAX_CQM_DEQ)) {
+			PR_ERR("Invalid Port ID:%d\n", cfg->id.cqm_deq_port);
+			return DP_FAILURE;
+		}
+		if (priv->deq_port_stat[cfg->id.cqm_deq_port].flag ==
+		    PP_NODE_FREE) {
+			PR_ERR("Invalid Port flag:0x%x\n",
+			       priv->deq_port_stat[cfg->id.cqm_deq_port].flag);
+			return DP_FAILURE;
+		}
+		node_id = priv->deq_port_stat[cfg->id.cqm_deq_port].node_id;
+
+	} else {
+		PR_ERR("Unkonwn type provided:0x%x\n", cfg->type);
+		return DP_FAILURE;
+	}
+	if (!priv->qos_sch_stat[node_id].child_num)
+		return DP_SUCCESS;
+
+	cfg->num = priv->qos_sch_stat[node_id].child_num;
+	res = get_children_list(cfg->inst, cfg->child, node_id);
+
+	if (cfg->num == res)
+		return DP_SUCCESS;
+
+	PR_ERR("child_num:[%d] not matched to res:[%d] for Node:%d\n",
+	       cfg->num, res, cfg->id.sch_id);
+	return DP_FAILURE;
+}
+
+/* #define DP_SUPPORT_RES_RESERVE */
+int dp_node_reserve(int inst, int ep, struct dp_port_data *data, int flags)
+{
+	int i;
+	unsigned int id;
+	struct pp_qos_queue_info info;
+	int len;
+	struct resv_q *resv_q;
+#ifdef WORKAROUND_DROP_PORT
+	struct pp_qos_queue_conf q_conf;
+#endif
+	struct hal_priv *priv = HAL(inst);
+	int res = DP_SUCCESS;
+
+#ifndef DP_SUPPORT_RES_RESERVE /* immediately return */
+	return res;
+#endif
+
+	if (!priv) {
+		PR_ERR("priv cannot be NULL\n");
+		return DP_FAILURE;
+	}
+
+	/* free resved queue/scheduler */
+	if (flags == DP_F_DEREGISTER)
+		goto FREE_EXIT;
+
+	/* Need reserve for queue/scheduler */
+/* #define DP_SUPPORT_RES_TEST */
+#ifdef DP_SUPPORT_RES_TEST
+	data->num_resv_q = 4;
+	data->num_resv_sched = 4;
+#endif
+	/* to reserve the queue */
+	if (data->num_resv_q <= 0)
+		goto RESERVE_SCHED;
+	len = sizeof(struct resv_q) * data->num_resv_q;
+	priv->resv[ep].resv_q = kzalloc(len, GFP_ATOMIC);
+	if (!priv->resv[ep].resv_q) {
+		res = DP_FAILURE;
+		goto FREE_EXIT;
+	}
+	PR_INFO("queue size =%d for ep=%d\n", len, ep);
+	resv_q = priv->resv[ep].resv_q;
+	for (i = 0; i < data->num_resv_q; i++) {
+		if (qos_queue_allocate(priv->qdev, &id)) {
+			res = DP_FAILURE;
+			PR_ERR("qos_queue_allocate failed\n");
+			goto FREE_EXIT;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "qos_queue_allocate: %d\n", id);
+#ifdef WORKAROUND_DROP_PORT /* use drop port */
+		qos_queue_conf_set_default(&q_conf);
+		q_conf.wred_enable = 0;
+		q_conf.queue_wred_max_allowed = DEF_QRED_MAX_ALLOW;
+		q_conf.queue_child_prop.parent = priv->ppv4_drop_p;
+		if (qos_queue_set(priv->qdev, id, &q_conf)) {
+			res = DP_FAILURE;
+			PR_ERR("qos_queue_set fail for queue=%d to parent=%d\n",
+			       id, q_conf.queue_child_prop.parent);
+			goto FREE_EXIT;
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "Workaround queue(/%d)-> tmp parent(/%d)\n",
+			 id, q_conf.queue_child_prop.parent);
+#endif
+		if (qos_queue_info_get(priv->qdev, id, &info)) {
+			qos_queue_remove(priv->qdev, id);
+			res = DP_FAILURE;
+			PR_ERR("qos_queue_info_get: %d\n", id);
+			goto FREE_EXIT;
+		}
+		resv_q[i].id = id;
+		resv_q[i].physical_id = info.physical_id;
+		priv->resv[ep].num_resv_q = i + 1;
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "reseve q[%d/%d]\n",
+			 resv_q[i].id, resv_q[i].physical_id);
+	}
+RESERVE_SCHED:
+	/* reserve scheduler */
+	if (data->num_resv_sched > 0) {
+		int len;
+		struct resv_sch *p_sch = priv->resv[ep].resv_sched;
+
+		len = sizeof(struct resv_sch) * data->num_resv_sched;
+		priv->resv[ep].resv_sched = kzalloc(len, GFP_ATOMIC);
+		PR_INFO("sched size =%d for ep=%d\n", len, ep);
+		if (!priv->resv[ep].resv_sched) {
+			res = DP_FAILURE;
+			goto FREE_EXIT;
+		}
+		p_sch = priv->resv[ep].resv_sched;
+		for (i = 0; i < data->num_resv_sched; i++) {
+			if (qos_sched_allocate(priv->qdev, &id)) {
+				res = DP_FAILURE;
+				PR_ERR("qos_queue_allocate failed\n");
+				goto FREE_EXIT;
+			}
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "qos_sched_allocate: %d\n", id);
+			p_sch[i].id = id;
+			priv->resv[ep].num_resv_sched = i + 1;
+			DP_DEBUG(DP_DBG_FLAG_QOS,
+				 "reseve sched[/%d]\n", resv_q[i].id);
+		}
+	}
+	return res;
+
+FREE_EXIT:
+	if (priv->resv[ep].resv_q) {
+		struct resv_q  *resv_q = priv->resv[ep].resv_q;
+
+		for (i = 0; i < priv->resv[ep].num_resv_q; i++)
+			qos_queue_remove(priv->qdev, resv_q[i].id);
+		kfree(resv_q);
+		priv->resv[ep].resv_q = NULL;
+		priv->resv[ep].num_resv_q = 0;
+	}
+	if (priv->resv[ep].resv_sched) {
+		struct resv_sch *resv_sch = priv->resv[ep].resv_sched;
+
+		for (i = 0; i < priv->resv[ep].num_resv_sched; i++)
+			qos_sched_remove(priv->qdev, resv_sch[i].id);
+		kfree(resv_sch);
+		priv->resv[ep].resv_sched = NULL;
+		priv->resv[ep].num_resv_sched = 0;
+	}
+	return res;
+}
+
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_proc.c b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_proc.c
new file mode 100644
index 000000000000..8e8467cd236e
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_proc.c
@@ -0,0 +1,2514 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include <linux/module.h>
+#include <net/datapath_proc_api.h>	/*for proc api */
+#include <net/datapath_api.h>
+#include <linux/list.h>
+
+#include "../datapath.h"
+#include "datapath_misc.h"
+
+#include "../datapath_swdev.h"
+
+#define DP_PROC_FILE_GSWIP_BP "bp"
+#define DP_PROC_FILE_SWDEV_BR "brctl"
+#define DP_PROC_SWDEV_FDB "fdb"
+#define DP_PROC_SWDEV_MAC "sw_mac"
+#define PROC_PARSER "parser"
+#define PROC_RMON_PORTS  "rmon"
+#define PROC_MIB_TIMER "mib_timer"
+#define PROC_MIB_INSIDE "mib_inside"
+#define PROC_MIBPORT "mib_port"
+#define PROC_MIBVAP "mib_vap"
+#define PROC_COMMON_CMD "cmd"
+#define PROC_COC "coc"
+#define PROC_PCE  "pce"
+#define PROC_PMAC  "pmac"
+#define PROC_EP "ep" /*EP/port ID info */
+#define DP_PROC_CBMLOOKUP "lookup"
+#define DP_MIB_Q "qos_mib"
+
+struct list_head fdb_tbl_list;
+
+static int proc_gsw_bp_dump(struct seq_file *s, int pos);
+static int proc_common_start(void);
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_SWITCHDEV)
+static int proc_swdev_brctl_dump(struct seq_file *s, int pos);
+static int proc_swdev_brctl_start(void);
+static ssize_t proc_swdev_brctl_write(struct file *,
+				      const char *, size_t, loff_t *);
+static int print_bridge(int fid, int inst);
+static ssize_t proc_swdev_fdb_write(struct file *,
+				    const char *, size_t, loff_t *);
+static void proc_swdev_fdb_read(struct seq_file *s);
+#endif
+static int proc_gsw_pce_dump(struct seq_file *s, int pos);
+static int proc_gsw_pce_start(void);
+
+#define MAX_GSW_L_PMAC_PORT  7
+#define MAX_GSW_R_PMAC_PORT  16
+static GSW_RMON_Port_cnt_t gsw_l_rmon_mib[MAX_GSW_L_PMAC_PORT];
+static GSW_RMON_Port_cnt_t gsw_r_rmon_mib[MAX_GSW_R_PMAC_PORT];
+static GSW_RMON_Redirect_cnt_t gswr_rmon_redirect;
+
+enum RMON_MIB_TYPE {
+	RX_GOOD_PKTS = 0,
+	RX_FILTER_PKTS,
+	RX_DROP_PKTS,
+	RX_OTHERS,
+
+	TX_GOOD_PKTS,
+	TX_ACM_PKTS,
+	TX_DROP_PKTS,
+	TX_OTHERS,
+
+	REDIRECT_MIB,
+	DP_DRV_MIB,
+
+	/*last entry */
+	RMON_MAX
+};
+
+static char f_q_mib_title_proc;
+static int tmp_inst;
+
+#define RMON_DASH_LINE 130
+void print_dash_line(struct seq_file *s)
+{
+	char buf[RMON_DASH_LINE + 4];
+
+	memset(buf, '-', RMON_DASH_LINE);
+	sprintf(buf + RMON_DASH_LINE, "\n");
+	seq_puts(s, buf);
+}
+
+#define GSW_PORT_RMON_PRINT(title, var)  do { \
+	seq_printf(s, \
+		   "%-14s%10s %12u %12u %12u %12u %12u %12u %12u\n", \
+		   title, "L(0-6)", \
+		   gsw_l_rmon_mib[0].var, gsw_l_rmon_mib[1].var, \
+		   gsw_l_rmon_mib[2].var, gsw_l_rmon_mib[3].var, \
+		   gsw_l_rmon_mib[4].var, gsw_l_rmon_mib[5].var, \
+		   gsw_l_rmon_mib[6].var); \
+	seq_printf(s, \
+		   "%-14s%10s %12u %12u %12u %12u %12u %12u %12u %12u\n", \
+		   title, "R(0-6,15)", \
+		   gsw_r_rmon_mib[0].var, gsw_r_rmon_mib[1].var, \
+		   gsw_r_rmon_mib[2].var, gsw_r_rmon_mib[3].var, \
+		   gsw_r_rmon_mib[4].var, gsw_r_rmon_mib[5].var, \
+		   gsw_r_rmon_mib[6].var, gsw_r_rmon_mib[15].var); \
+	seq_printf(s, \
+		   "%-14s%10s %12u %12u %12u %12u %12u %12u %12u %12u\n", \
+		   title, "R(7-14)", \
+		   gsw_r_rmon_mib[7].var, gsw_r_rmon_mib[8].var, \
+		   gsw_r_rmon_mib[9].var, gsw_r_rmon_mib[10].var, \
+		   gsw_r_rmon_mib[11].var, gsw_r_rmon_mib[12].var, \
+		   gsw_r_rmon_mib[13].var, gsw_r_rmon_mib[14].var); \
+	print_dash_line(s); \
+	} while (0)
+
+static void proc_parser_read(struct seq_file *s);
+static ssize_t proc_parser_write(struct file *, const char *, size_t,
+				 loff_t *);
+static int proc_gsw_pce_dump(struct seq_file *s, int pos);
+static int proc_gsw_pce_start(void);
+static ssize_t proc_gsw_pmac_write(struct file *file, const char *buf,
+				   size_t count, loff_t *ppos);
+static int rmon_display_port_full;
+
+static void proc_parser_read(struct seq_file *s)
+{
+	s8 cpu, mpe1, mpe2, mpe3;
+
+	dp_get_gsw_parser_31(&cpu, &mpe1, &mpe2, &mpe3);
+	seq_printf(s, "cpu : %s with parser size =%d bytes\n",
+		   parser_flag_str(cpu), parser_size_via_index(0));
+	seq_printf(s, "mpe1: %s with parser size =%d bytes\n",
+		   parser_flag_str(mpe1), parser_size_via_index(1));
+	seq_printf(s, "mpe2: %s with parser size =%d bytes\n",
+		   parser_flag_str(mpe2), parser_size_via_index(2));
+	seq_printf(s, "mpe3: %s with parser size =%d bytes\n",
+		   parser_flag_str(mpe3), parser_size_via_index(3));
+}
+
+ssize_t proc_parser_write(struct file *file, const char *buf,
+			  size_t count, loff_t *ppos)
+{
+	int len;
+	char str[64];
+	int num, i;
+	char *param_list[20];
+	s8 cpu = 0, mpe1 = 0, mpe2 = 0, mpe3 = 0, flag = 0;
+	int pce_rule_id;
+	static GSW_PCE_rule_t pce;
+	int inst = 0;
+	struct core_ops *gsw_handle;
+
+	memset(&pce, 0, sizeof(pce));
+	gsw_handle = dp_port_prop[inst].ops[GSWIP_R];
+	len = (sizeof(str) > count) ? count : sizeof(str) - 1;
+	len -= copy_from_user(str, buf, len);
+	str[len] = 0;
+	num = dp_split_buffer(str, param_list, ARRAY_SIZE(param_list));
+
+	if (dp_strncmpi(param_list[0], "enable", strlen("enable")) == 0) {
+		for (i = 1; i < num; i++) {
+			if (dp_strncmpi(param_list[i], "cpu", strlen("cpu")) == 0) {
+				flag |= 0x1;
+				cpu = 2;
+			}
+
+			if (dp_strncmpi(param_list[i], "mpe1", strlen("mpe1")) == 0) {
+				flag |= 0x2;
+				mpe1 = 2;
+			}
+
+			if (dp_strncmpi(param_list[i], "mpe2", strlen("mpe2")) == 0) {
+				flag |= 0x4;
+				mpe2 = 2;
+			}
+
+			if (dp_strncmpi(param_list[i], "mpe3", strlen("mpe3")) == 0) {
+				flag |= 0x8;
+				mpe3 = 2;
+			}
+		}
+
+		if (!flag) {
+			flag = 0x1 | 0x2 | 0x4 | 0x8;
+			cpu = 2;
+			mpe1 = 2;
+			mpe2 = 2;
+			mpe3 = 2;
+		}
+
+		DP_DEBUG(DP_DBG_FLAG_DBG,
+			 "flag=0x%x mpe3/2/1/cpu=%d/%d/%d/%d\n", flag, mpe3,
+			 mpe2, mpe1, cpu);
+		dp_set_gsw_parser_31(flag, cpu, mpe1, mpe2, mpe3);
+	} else if (dp_strncmpi(param_list[0], "disable", strlen("disable")) == 0) {
+		for (i = 1; i < num; i++) {
+			if (dp_strncmpi(param_list[i], "cpu", strlen("cpu")) == 0) {
+				flag |= 0x1;
+				cpu = 0;
+			}
+
+			if (dp_strncmpi(param_list[i], "mpe1", strlen("mpe1")) == 0) {
+				flag |= 0x2;
+				mpe1 = 0;
+			}
+
+			if (dp_strncmpi(param_list[i], "mpe2", strlen("mpe2")) == 0) {
+				flag |= 0x4;
+				mpe2 = 0;
+			}
+
+			if (dp_strncmpi(param_list[i], "mpe3", strlen("mpe3")) == 0) {
+				flag |= 0x8;
+				mpe3 = 0;
+			}
+		}
+
+		if (!flag) {
+			flag = 0x1 | 0x2 | 0x4 | 0x8;
+			cpu = 0;
+			mpe1 = 0;
+			mpe2 = 0;
+			mpe3 = 0;
+		}
+
+		DP_DEBUG(DP_DBG_FLAG_DBG,
+			 "flag=0x%x mpe3/2/1/cpu=%d/%d/%d/%d\n", flag, mpe3,
+			 mpe2, mpe1, cpu);
+		dp_set_gsw_parser_31(flag, cpu, mpe1, mpe2, mpe3);
+	} else if (dp_strncmpi(param_list[0], "refresh", strlen("refresh")) == 0) {
+		dp_get_gsw_parser_31(NULL, NULL, NULL, NULL);
+		PR_INFO("value:cpu=%d mpe1=%d mpe2=%d mpe3=%d\n", pinfo[0].v,
+			pinfo[1].v, pinfo[2].v, pinfo[3].v);
+		PR_INFO("size :cpu=%d mpe1=%d mpe2=%d mpe3=%d\n",
+			pinfo[0].size, pinfo[1].size, pinfo[2].size,
+			pinfo[3].size);
+		return count;
+	} else if (dp_strncmpi(param_list[0], "mark", strlen("mark")) == 0) {
+		int flag = dp_atoi(param_list[1]);
+
+		pce_rule_id = dp_atoi(param_list[2]);
+
+		if (flag < 0)
+			flag = 0;
+		else if (flag > 3)
+			flag = 3;
+		PR_INFO("eProcessPath_Action set to %d\n", flag);
+		/*: All packets set to same mpe flag as specified */
+		memset(&pce, 0, sizeof(pce));
+		pce.pattern.nIndex = pce_rule_id;
+		pce.pattern.bEnable = 1;
+
+		pce.pattern.bParserFlagMSB_Enable = 1;
+		/* rule.pce.pattern.nParserFlagMSB = 0x0021; */
+		pce.pattern.nParserFlagMSB_Mask = 0xffff;
+		pce.pattern.bParserFlagLSB_Enable = 1;
+		/* rule.pce.pattern.nParserFlagLSB = 0x0000; */
+		pce.pattern.nParserFlagLSB_Mask = 0xffff;
+		/* rule.pce.pattern.eDstIP_Select = 2; */
+
+		pce.pattern.nDstIP_Mask = 0xffffffff;
+		pce.pattern.bDstIP_Exclude = 0;
+
+		pce.action.bRtDstPortMaskCmp_Action = 1;
+		pce.action.bRtSrcPortMaskCmp_Action = 1;
+		pce.action.bRtDstIpMaskCmp_Action = 1;
+		pce.action.bRtSrcIpMaskCmp_Action = 1;
+
+		pce.action.bRoutExtId_Action = 1;
+		pce.action.nRoutExtId = 0; /*RT_EXTID_UDP; */
+		pce.action.bRtAccelEna_Action = 1;
+		pce.action.bRtCtrlEna_Action = 1;
+		pce.action.eProcessPath_Action = flag;
+		pce.action.bRMON_Action = 1;
+		pce.action.nRMON_Id = 0;	/*RMON_UDP_CNTR; */
+
+		if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_tflow_ops
+				 .TFLOW_PceRuleWrite, gsw_handle, &pce)) {
+			PR_ERR("PCE rule add fail: GSW_PCE_RULE_WRITE\n");
+			return count;
+		}
+
+	} else if (dp_strncmpi(param_list[0], "unmark", 6) == 0) {
+		/*: All packets set to same mpe flag as specified */
+		memset(&pce, 0, sizeof(pce));
+		pce_rule_id = dp_atoi(param_list[1]);
+		pce.pattern.nIndex = pce_rule_id;
+		pce.pattern.bEnable = 0;
+		if (gsw_core_api((dp_gsw_cb)gsw_handle->gsw_tflow_ops
+				 .TFLOW_PceRuleWrite, gsw_handle, &pce)) {
+			PR_ERR("PCE rule add fail:GSW_PCE_RULE_WRITE\n");
+			return count;
+		}
+	} else {
+		PR_INFO("Usage: echo %s [cpu] [mpe1] [mpe2] [mpe3] > parser\n",
+			"<enable/disable>");
+		PR_INFO("Usage: echo <refresh> parser\n");
+
+		PR_INFO("Usage: echo %s > parser\n",
+			"mark eProcessPath_Action_value(0~3) pce_rule_id");
+		PR_INFO("Usage: echo unmark pce_rule_id > parser\n");
+		return count;
+	}
+
+	return count;
+}
+
+#define GSW_PORT_RMON64_PRINT(title, var)  do { \
+	seq_printf(s, "%-14s%10s %12u %12u %12u %12u %12u %12u %12u\n", \
+		   title "(H)", "L(0-6)", \
+		   high_10dec(gsw_l_rmon_mib[0].var), \
+		   high_10dec(gsw_l_rmon_mib[1].var), \
+		   high_10dec(gsw_l_rmon_mib[2].var),  \
+		   high_10dec(gsw_l_rmon_mib[3].var), \
+		   high_10dec(gsw_l_rmon_mib[4].var),  \
+		   high_10dec(gsw_l_rmon_mib[5].var), \
+		   high_10dec(gsw_l_rmon_mib[6].var)); \
+	seq_printf(s, "%-14s%10s %12u %12u %12u %12u %12u %12u %12u\n", \
+		   title "(L)", "L(0-6)", \
+		   low_10dec(gsw_l_rmon_mib[0].var), \
+		   low_10dec(gsw_l_rmon_mib[1].var), \
+		   low_10dec(gsw_l_rmon_mib[2].var), \
+		   low_10dec(gsw_l_rmon_mib[3].var), \
+		   low_10dec(gsw_l_rmon_mib[4].var), \
+		   low_10dec(gsw_l_rmon_mib[5].var), \
+		   low_10dec(gsw_l_rmon_mib[6].var)); \
+	seq_printf(s, "%-14s%10s %12u %12u %12u %12u %12u %12u %12u %12u\n", \
+		   title "(H)", "R(0-6,15)", \
+		   high_10dec(gsw_r_rmon_mib[0].var), \
+		   high_10dec(gsw_r_rmon_mib[1].var), \
+		   high_10dec(gsw_r_rmon_mib[2].var), \
+		   high_10dec(gsw_r_rmon_mib[3].var), \
+		   high_10dec(gsw_r_rmon_mib[4].var), \
+		   high_10dec(gsw_r_rmon_mib[5].var), \
+		   high_10dec(gsw_r_rmon_mib[6].var), \
+		   high_10dec(gsw_r_rmon_mib[15].var)); \
+	seq_printf(s, "%-14s%10s %12u %12u %12u %12u %12u %12u %12u %12u\n", \
+		   title "(L)", "R(0-6,15)", \
+		   low_10dec(gsw_r_rmon_mib[0].var), \
+		   low_10dec(gsw_r_rmon_mib[1].var), \
+		   low_10dec(gsw_r_rmon_mib[2].var), \
+		   low_10dec(gsw_r_rmon_mib[3].var), \
+		   low_10dec(gsw_r_rmon_mib[4].var), \
+		   low_10dec(gsw_r_rmon_mib[5].var), \
+		   low_10dec(gsw_r_rmon_mib[6].var), \
+		   low_10dec(gsw_r_rmon_mib[15].var)); \
+	seq_printf(s, "%-14s%10s %12u %12u %12u %12u %12u %12u %12u %12u\n", \
+		   title "(H)", "R(7-14)", \
+		   high_10dec(gsw_r_rmon_mib[7].var), \
+		   high_10dec(gsw_r_rmon_mib[8].var), \
+		   high_10dec(gsw_r_rmon_mib[9].var),  \
+		   high_10dec(gsw_r_rmon_mib[10].var), \
+		   high_10dec(gsw_r_rmon_mib[11].var), \
+		   high_10dec(gsw_r_rmon_mib[12].var), \
+		   high_10dec(gsw_r_rmon_mib[13].var), \
+		   high_10dec(gsw_r_rmon_mib[14].var)); \
+	seq_printf(s, \
+		   "%-14s%10s %12u %12u %12u %12u %12u %12u %12u %12u\n", \
+		   title "(L)", "R(7-14)", \
+		   low_10dec(gsw_r_rmon_mib[7].var), \
+		   low_10dec(gsw_r_rmon_mib[8].var), \
+		   low_10dec(gsw_r_rmon_mib[9].var), \
+		   low_10dec(gsw_r_rmon_mib[10].var), \
+		   low_10dec(gsw_r_rmon_mib[11].var), \
+		   low_10dec(gsw_r_rmon_mib[12].var), \
+		   low_10dec(gsw_r_rmon_mib[13].var), \
+		   low_10dec(gsw_r_rmon_mib[14].var)); \
+	print_dash_line(s); \
+	} while (0)
+
+typedef int (*ingress_pmac_set_callback_t) (dp_pmac_cfg_t *pmac_cfg,
+					    u32 value);
+typedef int (*egress_pmac_set_callback_t) (dp_pmac_cfg_t *pmac_cfg,
+					   u32 value);
+struct ingress_pmac_entry {
+	char *name;
+	ingress_pmac_set_callback_t ingress_callback;
+};
+
+struct egress_pmac_entry {
+	char *name;
+	egress_pmac_set_callback_t egress_callback;
+};
+
+static int ingress_err_disc_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	pmac_cfg->ig_pmac.err_disc = value;
+	pmac_cfg->ig_pmac_flags = IG_PMAC_F_ERR_DISC;
+	return 0;
+}
+
+static int ingress_pmac_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	pmac_cfg->ig_pmac.pmac = value;
+	pmac_cfg->ig_pmac_flags = IG_PMAC_F_PRESENT;
+	return 0;
+}
+
+static int ingress_pmac_pmap_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	pmac_cfg->ig_pmac.def_pmac_pmap = value;
+	pmac_cfg->ig_pmac_flags = IG_PMAC_F_PMAP;
+	return 0;
+}
+
+static int ingress_pmac_en_pmap_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	pmac_cfg->ig_pmac.def_pmac_en_pmap = value;
+	pmac_cfg->ig_pmac_flags = IG_PMAC_F_PMAPENA;
+	return 0;
+}
+
+static int ingress_pmac_tc_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	pmac_cfg->ig_pmac.def_pmac_tc = value;
+	pmac_cfg->ig_pmac_flags = IG_PMAC_F_CLASS;
+	return 0;
+}
+
+static int ingress_pmac_en_tc_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	pmac_cfg->ig_pmac.def_pmac_en_tc = value;
+	pmac_cfg->ig_pmac_flags = IG_PMAC_F_CLASSENA;
+	return 0;
+}
+
+static int ingress_pmac_subifid_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	pmac_cfg->ig_pmac.def_pmac_subifid = value;
+	pmac_cfg->ig_pmac_flags = IG_PMAC_F_SUBIF;
+	return 0;
+}
+
+static int ingress_pmac_srcport_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	pmac_cfg->ig_pmac.def_pmac_src_port = value;
+	pmac_cfg->ig_pmac_flags = IG_PMAC_F_SPID;
+	return 0;
+}
+
+static int ingress_pmac_hdr1_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	u8 hdr;
+
+	hdr = (u8)value;
+	pmac_cfg->ig_pmac.def_pmac_hdr[0] = hdr;
+	pmac_cfg->ig_pmac_flags = IG_PMAC_F_PMACHDR1;
+	return 0;
+}
+
+static int ingress_pmac_hdr2_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	u8 hdr;
+
+	hdr = (u8)value;
+	pmac_cfg->ig_pmac.def_pmac_hdr[1] = hdr;
+	pmac_cfg->ig_pmac_flags = IG_PMAC_F_PMACHDR2;
+	return 0;
+}
+
+static int ingress_pmac_hdr3_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	u8 hdr;
+
+	hdr = (u8)value;
+	pmac_cfg->ig_pmac.def_pmac_hdr[2] = hdr;
+	pmac_cfg->ig_pmac_flags = IG_PMAC_F_PMACHDR3;
+	return 0;
+}
+
+static int ingress_pmac_hdr4_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	u8 hdr;
+
+	hdr = (u8)value;
+	pmac_cfg->ig_pmac.def_pmac_hdr[3] = hdr;
+	pmac_cfg->ig_pmac_flags = IG_PMAC_F_PMACHDR4;
+	return 0;
+}
+
+static int ingress_pmac_hdr5_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	u8 hdr;
+
+	hdr = (u8)value;
+	pmac_cfg->ig_pmac.def_pmac_hdr[4] = hdr;
+	pmac_cfg->ig_pmac_flags = IG_PMAC_F_PMACHDR5;
+	return 0;
+}
+
+static int ingress_pmac_hdr6_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	u8 hdr;
+
+	hdr = (u8)value;
+	pmac_cfg->ig_pmac.def_pmac_hdr[5] = hdr;
+	pmac_cfg->ig_pmac_flags = IG_PMAC_F_PMACHDR6;
+	return 0;
+}
+
+static int ingress_pmac_hdr7_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	u8 hdr;
+
+	hdr = (u8)value;
+	pmac_cfg->ig_pmac.def_pmac_hdr[6] = hdr;
+	pmac_cfg->ig_pmac_flags = IG_PMAC_F_PMACHDR7;
+	return 0;
+}
+
+static int ingress_pmac_hdr8_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	u8 hdr;
+
+	hdr = (u8)value;
+	pmac_cfg->ig_pmac.def_pmac_hdr[7] = hdr;
+	pmac_cfg->ig_pmac_flags = IG_PMAC_F_PMACHDR8;
+	return 0;
+}
+
+static int egress_fcs_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	pmac_cfg->eg_pmac.fcs = value;
+	pmac_cfg->eg_pmac_flags = EG_PMAC_F_FCS;
+	return 0;
+}
+
+static int egress_l2hdr_bytes_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	pmac_cfg->eg_pmac.num_l2hdr_bytes_rm = value;
+	pmac_cfg->eg_pmac_flags = EG_PMAC_F_L2HDR_RM;
+	return 0;
+}
+
+static int egress_rx_dma_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	pmac_cfg->eg_pmac.rx_dma_chan = value;
+	pmac_cfg->eg_pmac_flags = EG_PMAC_F_RXID;
+	return 0;
+}
+
+static int egress_pmac_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	pmac_cfg->eg_pmac.pmac = value;
+	pmac_cfg->eg_pmac_flags = EG_PMAC_F_PMAC;
+	return 0;
+}
+
+static int egress_res_dw_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	pmac_cfg->eg_pmac.res_dw1 = value;
+	pmac_cfg->eg_pmac_flags = EG_PMAC_F_RESDW1;
+	return 0;
+}
+
+static int egress_res1_dw_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	pmac_cfg->eg_pmac.res1_dw0 = value;
+	pmac_cfg->eg_pmac_flags = EG_PMAC_F_RES1DW0;
+	return 0;
+}
+
+static int egress_res2_dw_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	pmac_cfg->eg_pmac.res2_dw0 = value;
+	pmac_cfg->eg_pmac_flags = EG_PMAC_F_RES2DW0;
+	return 0;
+}
+
+static int egress_tc_ena_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	pmac_cfg->eg_pmac.tc_enable = value;
+	pmac_cfg->eg_pmac_flags = EG_PMAC_F_TCENA;
+	return 0;
+}
+
+static int egress_dec_flag_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	pmac_cfg->eg_pmac.dec_flag = value;
+	pmac_cfg->eg_pmac_flags = EG_PMAC_F_DECFLG;
+	return 0;
+}
+
+static int egress_enc_flag_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	pmac_cfg->eg_pmac.enc_flag = value;
+	pmac_cfg->eg_pmac_flags = EG_PMAC_F_ENCFLG;
+	return 0;
+}
+
+static int egress_mpe1_flag_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	pmac_cfg->eg_pmac.mpe1_flag = value;
+	pmac_cfg->eg_pmac_flags = EG_PMAC_F_MPE1FLG;
+	return 0;
+}
+
+static int egress_mpe2_flag_set(dp_pmac_cfg_t *pmac_cfg, u32 value)
+{
+	pmac_cfg->eg_pmac.mpe2_flag = value;
+	pmac_cfg->eg_pmac_flags = EG_PMAC_F_MPE2FLG;
+	return 0;
+}
+
+static struct ingress_pmac_entry ingress_entries[] = {
+	{"errdisc", ingress_err_disc_set},
+	{"pmac", ingress_pmac_set},
+	{"pmac_pmap", ingress_pmac_pmap_set},
+	{"pmac_en_pmap", ingress_pmac_en_pmap_set},
+	{"pmac_tc", ingress_pmac_tc_set},
+	{"pmac_en_tc", ingress_pmac_en_tc_set},
+	{"pmac_subifid", ingress_pmac_subifid_set},
+	{"pmac_srcport", ingress_pmac_srcport_set},
+	{"pmac_hdr1", ingress_pmac_hdr1_set},
+	{"pmac_hdr2", ingress_pmac_hdr2_set},
+	{"pmac_hdr3", ingress_pmac_hdr3_set},
+	{"pmac_hdr4", ingress_pmac_hdr4_set},
+	{"pmac_hdr5", ingress_pmac_hdr5_set},
+	{"pmac_hdr6", ingress_pmac_hdr6_set},
+	{"pmac_hdr7", ingress_pmac_hdr7_set},
+	{"pmac_hdr8", ingress_pmac_hdr8_set},
+	{NULL, NULL}
+};
+
+static struct egress_pmac_entry egress_entries[] = {
+	{"rx_dmachan", egress_rx_dma_set},
+	{"rm_l2hdr", egress_l2hdr_bytes_set},
+	{"fcs", egress_fcs_set},
+	{"pmac", egress_pmac_set},
+	{"res_dw1", egress_res_dw_set},
+	{"res1_dw0", egress_res1_dw_set},
+	{"res2_dw0", egress_res2_dw_set},
+	{"tc_enable", egress_tc_ena_set},
+	{"dec_flag", egress_dec_flag_set},
+	{"enc_flag", egress_enc_flag_set},
+	{"mpe1_flag", egress_mpe1_flag_set},
+	{"mpe2_flag", egress_mpe2_flag_set},
+	{NULL, NULL}
+};
+
+static int proc_gsw_port_rmon_dump(struct seq_file *s, int pos)
+{
+	int i;
+	GSW_return_t ret = 0;
+	struct core_ops *gsw_handle;
+	//char flag_buf[20];
+
+	if (pos == 0) {
+		memset(gsw_r_rmon_mib, 0, sizeof(gsw_r_rmon_mib));
+		memset(gsw_l_rmon_mib, 0, sizeof(gsw_l_rmon_mib));
+
+		/*read gswip-r rmon counter */
+		gsw_handle = dp_port_prop[0].ops[GSWIP_R];
+
+		for (i = 0; i < ARRAY_SIZE(gsw_r_rmon_mib); i++) {
+			gsw_r_rmon_mib[i].nPortId = i;
+			ret = gsw_core_api((dp_gsw_cb)gsw_handle
+					   ->gsw_rmon_ops.RMON_Port_Get,
+					   gsw_handle, &gsw_r_rmon_mib[i]);
+
+			if (ret != GSW_statusOk) {
+				PR_ERR("RMON_PORT_GET fail for Port %d\n", i);
+				return -1;
+			}
+		}
+
+		/*read pmac rmon redirect mib */
+		memset(&gswr_rmon_redirect, 0, sizeof(gswr_rmon_redirect));
+		ret = gsw_core_api((dp_gsw_cb)gsw_handle
+				   ->gsw_rmon_ops.RMON_Redirect_Get, gsw_handle,
+				   &gswr_rmon_redirect);
+
+		if (ret != GSW_statusOk) {
+			PR_ERR("GSW_RMON_REDIRECT_GET fail for Port %d\n", i);
+			return -1;
+		}
+
+		/*read gswip-l rmon counter */
+		gsw_handle = dp_port_prop[0].ops[GSWIP_L];
+		for (i = 0; i < ARRAY_SIZE(gsw_l_rmon_mib); i++) {
+			gsw_l_rmon_mib[i].nPortId = i;
+			ret = gsw_core_api((dp_gsw_cb)gsw_handle
+					   ->gsw_rmon_ops.RMON_Port_Get,
+					   gsw_handle,
+					   &gsw_l_rmon_mib[i]);
+
+			if (ret != GSW_statusOk) {
+				PR_ERR("RMON_PORT_GET fail for Port %d\n", i);
+				return -1;
+			}
+		}
+
+		seq_printf(s, "%-24s %12u %12u %12u %12u %12u %12u %12u\n",
+			   "GSWIP-L", 0, 1, 2, 3, 4, 5, 6);
+		seq_printf(s, "%-24s %12u %12u %12u %12u %12u %12u %12u %12u\n",
+			   "GSWIP-R(Fixed)", 0, 1, 2, 3, 4, 5, 6, 15);
+		seq_printf(s, "%-24s %12u %12u %12u %12u %12u %12u %12u %12u\n",
+			   "GSWIP-R(Dynamic)", 7, 8, 9, 10, 11, 12, 13, 14);
+		print_dash_line(s);
+	}
+
+	if (pos == RX_GOOD_PKTS) {
+		GSW_PORT_RMON_PRINT("RX_Good", nRxGoodPkts);
+	} else if (pos == RX_FILTER_PKTS) {
+		GSW_PORT_RMON_PRINT("RX_FILTER", nRxFilteredPkts);
+	} else if (pos == RX_DROP_PKTS) {
+		GSW_PORT_RMON_PRINT("RX_DROP", nRxDroppedPkts);
+	} else if (pos == RX_OTHERS) {
+		if (!rmon_display_port_full)
+			goto NEXT;
+
+		GSW_PORT_RMON_PRINT("RX_UNICAST", nRxUnicastPkts);
+		GSW_PORT_RMON_PRINT("RX_BROADCAST", nRxBroadcastPkts);
+		GSW_PORT_RMON_PRINT("RX_MULTICAST", nRxMulticastPkts);
+		GSW_PORT_RMON_PRINT("RX_FCS_ERR", nRxFCSErrorPkts);
+		GSW_PORT_RMON_PRINT("RX_UNDER_GOOD",
+				    nRxUnderSizeGoodPkts);
+		GSW_PORT_RMON_PRINT("RX_OVER_GOOD", nRxOversizeGoodPkts);
+		GSW_PORT_RMON_PRINT("RX_UNDER_ERR",
+				    nRxUnderSizeErrorPkts);
+		GSW_PORT_RMON_PRINT("RX_OVER_ERR", nRxOversizeErrorPkts);
+		GSW_PORT_RMON_PRINT("RX_ALIGN_ERR", nRxAlignErrorPkts);
+		GSW_PORT_RMON_PRINT("RX_64B", nRx64BytePkts);
+		GSW_PORT_RMON_PRINT("RX_127B", nRx127BytePkts);
+		GSW_PORT_RMON_PRINT("RX_255B", nRx255BytePkts);
+		GSW_PORT_RMON_PRINT("RX_511B", nRx511BytePkts);
+		GSW_PORT_RMON_PRINT("RX_1023B", nRx1023BytePkts);
+		GSW_PORT_RMON_PRINT("RX_MAXB", nRxMaxBytePkts);
+		GSW_PORT_RMON64_PRINT("RX_BAD_b", nRxBadBytes);
+	} else if (pos == TX_GOOD_PKTS) {
+		GSW_PORT_RMON_PRINT("TX_Good", nTxGoodPkts);
+	} else if (pos == TX_ACM_PKTS) {
+		GSW_PORT_RMON_PRINT("TX_ACM_DROP", nTxAcmDroppedPkts);
+	} else if (pos == TX_DROP_PKTS) {
+		GSW_PORT_RMON_PRINT("TX_Drop", nTxDroppedPkts);
+	} else if (pos == TX_OTHERS) {
+		if (!rmon_display_port_full)
+			goto NEXT;
+
+		GSW_PORT_RMON_PRINT("TX_UNICAST", nTxUnicastPkts);
+		GSW_PORT_RMON_PRINT("TX_BROADAST", nTxBroadcastPkts);
+		GSW_PORT_RMON_PRINT("TX_MULTICAST", nTxMulticastPkts);
+		GSW_PORT_RMON_PRINT("TX_SINGLE_COLL",
+				    nTxSingleCollCount);
+		GSW_PORT_RMON_PRINT("TX_MULT_COLL", nTxMultCollCount);
+		GSW_PORT_RMON_PRINT("TX_LATE_COLL", nTxLateCollCount);
+		GSW_PORT_RMON_PRINT("TX_EXCESS_COLL",
+				    nTxExcessCollCount);
+		GSW_PORT_RMON_PRINT("TX_COLL", nTxCollCount);
+		GSW_PORT_RMON_PRINT("TX_PAUSET", nTxPauseCount);
+		GSW_PORT_RMON_PRINT("TX_64B", nTx64BytePkts);
+		GSW_PORT_RMON_PRINT("TX_127B", nTx127BytePkts);
+		GSW_PORT_RMON_PRINT("TX_255B", nTx255BytePkts);
+		GSW_PORT_RMON_PRINT("TX_511B", nTx511BytePkts);
+		GSW_PORT_RMON_PRINT("TX_1023B", nTx1023BytePkts);
+		GSW_PORT_RMON_PRINT("TX_MAX_B", nTxMaxBytePkts);
+		GSW_PORT_RMON_PRINT("TX_UNICAST", nTxUnicastPkts);
+		GSW_PORT_RMON_PRINT("TX_UNICAST", nTxUnicastPkts);
+		GSW_PORT_RMON_PRINT("TX_UNICAST", nTxUnicastPkts);
+		GSW_PORT_RMON64_PRINT("TX_GOOD_b", nTxGoodBytes);
+
+	} else if (pos == REDIRECT_MIB) {
+		/*GSWIP-R PMAC Redirect conter */
+		seq_printf(s, "%-24s %12s %12s %12s %12s\n",
+			   "GSW-R Redirect", "Rx_Pkts", "Tx_Pkts",
+			   "Rx_DropsPkts", "Tx_DropsPkts");
+
+		seq_printf(s, "%-24s %12d %12d %12d %12d\n", "",
+			   gswr_rmon_redirect.nRxPktsCount,
+			   gswr_rmon_redirect.nTxPktsCount,
+			   gswr_rmon_redirect.nRxDiscPktsCount,
+			   gswr_rmon_redirect.nTxDiscPktsCount);
+		print_dash_line(s);
+	} else if (pos == DP_DRV_MIB) {
+		u64 eth0_rx = 0, eth0_tx = 0;
+		u64 eth1_rx = 0, eth1_tx = 0;
+		u64 dsl_rx = 0, dsl_tx = 0;
+		u64 other_rx = 0, other_tx = 0;
+		int i, j;
+		struct pmac_port_info *port;
+
+		for (i = 1; i < PMAC_MAX_NUM; i++) {
+			port = get_port_info(tmp_inst, i);
+
+			if (!port)
+				continue;
+
+			if (i < 6) {
+				for (j = 0; j < 16; j++) {
+					eth0_rx +=
+					    STATS_GET(port->subif_info[j].mib.
+					    rx_fn_rxif_pkt);
+					eth0_rx +=
+					    STATS_GET(port->subif_info[j].mib.
+					    rx_fn_txif_pkt);
+					eth0_tx +=
+					    STATS_GET(port->subif_info[j].mib.
+					    tx_cbm_pkt);
+					eth0_tx +=
+					    STATS_GET(port->subif_info[j].mib.
+					    tx_tso_pkt);
+				}
+			} else if (i == 15) {
+				for (j = 0; j < 16; j++) {
+					eth1_rx +=
+					    STATS_GET(port->subif_info[j].mib.
+					    rx_fn_rxif_pkt);
+					eth1_rx +=
+					    STATS_GET(port->subif_info[j].mib.
+					    rx_fn_txif_pkt);
+					eth1_tx +=
+					    STATS_GET(port->subif_info[j].mib.
+					    tx_cbm_pkt);
+					eth1_tx +=
+					    STATS_GET(port->subif_info[j].mib.
+					    tx_tso_pkt);
+				}
+			} else if (port->alloc_flags & DP_F_FAST_DSL) {
+				for (j = 0; j < 16; j++) {
+					dsl_rx +=
+					    STATS_GET(port->subif_info[j].mib.
+					    rx_fn_rxif_pkt);
+					dsl_rx +=
+					    STATS_GET(port->subif_info[j].mib.
+					    rx_fn_txif_pkt);
+					dsl_tx +=
+					    STATS_GET(port->subif_info[j].mib.
+					    tx_cbm_pkt);
+					dsl_tx +=
+					    STATS_GET(port->subif_info[j].mib.
+					    tx_tso_pkt);
+				}
+			} else {
+				for (j = 0; j < 16; j++) {
+					other_rx +=
+					    STATS_GET(port->subif_info[j].mib.
+					    rx_fn_rxif_pkt);
+					other_rx +=
+					    STATS_GET(port->subif_info[j].mib.
+					    rx_fn_txif_pkt);
+					other_tx +=
+					    STATS_GET(port->subif_info[j].mib.
+					    tx_cbm_pkt);
+					other_tx +=
+					    STATS_GET(port->subif_info[j].mib.
+					    tx_tso_pkt);
+				}
+			}
+		}
+
+		seq_printf(s, "%-15s %22s %22s %22s %22s\n", "DP Drv Mib",
+			   "ETH_LAN", "ETH_WAN", "DSL", "Others");
+
+		seq_printf(s, "%15s %22llu %22llu %22llu %22llu\n",
+			   "Rx_Pkts", eth0_rx, eth1_rx, dsl_rx, other_rx);
+		seq_printf(s, "%15s %22llu %22llu %22llu %22llu\n",
+			   "Tx_Pkts", eth0_tx, eth1_tx, dsl_tx, other_tx);
+		print_dash_line(s);
+	} else {
+		goto NEXT;
+	}
+ NEXT:
+	pos++;
+
+	if (pos >= RMON_MAX)
+		return -1;
+
+	return pos;
+}
+
+static int proc_gsw_rmon_port_start(void)
+{
+	f_q_mib_title_proc = 0;
+	tmp_inst = 0;
+	return 0;
+}
+
+static ssize_t proc_gsw_rmon_write(struct file *file, const char *buf,
+				   size_t count, loff_t *ppos)
+{
+	int len;
+	char str[64];
+	int num;
+	char *param_list[10];
+
+	len = (sizeof(str) > count) ? count : sizeof(str) - 1;
+	len -= copy_from_user(str, buf, len);
+	str[len] = 0;
+	num = dp_split_buffer(str, param_list, ARRAY_SIZE(param_list));
+
+	if (num < 1)
+		goto help;
+
+	if (dp_strncmpi(param_list[0], "clear", 5) == 0 ||
+	    dp_strncmpi(param_list[0], "c", 1) == 0 ||
+	    dp_strncmpi(param_list[0], "rest", 4) == 0 ||
+	    dp_strncmpi(param_list[0], "r", 1) == 0) {
+		dp_sys_mib_reset_31(0);
+		goto EXIT_OK;
+	}
+	if (dp_strncmpi(param_list[0], "RMON", 4) == 0) {
+		if (dp_strncmpi(param_list[1], "Full", 4) == 0) {
+			rmon_display_port_full = 1;
+			goto EXIT_OK;
+		} else if (dp_strncmpi(param_list[1], "Basic", 5) == 0) {
+			rmon_display_port_full = 0;
+			goto EXIT_OK;
+		}
+	}
+
+	/*unknown command */
+	goto help;
+
+EXIT_OK:
+	return count;
+
+help:
+	PR_INFO("usage: echo clear > /proc/dp/rmon\n");
+	PR_INFO("usage: echo RMON Full > /proc/dp/rmon\n");
+	PR_INFO("usage: echo RMON Basic > /proc/dp/rmon\n");
+	return count;
+}
+
+/*in bytes*/
+int get_q_qocc(int inst, int qid, u32 *c)
+{
+	if (c)
+		*c = readl((void *)(0xb8820E00 + qid * 4));
+	return 0;
+}
+
+/*in packet
+ *total accept(green/yellow ?)
+ */
+int get_q_mib(int inst, int qid,
+	      u32 *total_accept,
+	      u32 *total_drop,
+	      u32 *red_drop)
+{
+	/*indirect access */
+	while (readl((void *)(0xb88200c4)) & 1)/*busy and wait*/
+		;
+	writel(qid, (void *)0xb88200bc);
+
+	while (readl((void *)(0xb88200c4)) & 1) /*busy wait*/
+		;
+	writel(0, (void *)0xb88200c0);
+
+	while (readl((void *)(0xb88200c4)) & 1) /*busy wait*/
+		;
+	if (total_accept)
+		*total_accept = readl((void *)(0xb88200ec));
+	if (total_drop)
+		*total_drop = readl((void *)(0xb88200ec + 4));
+	if (red_drop)
+		*red_drop = readl((void *)(0xb88200ec + 8));
+	return 0;
+}
+
+/*in bytes*/
+int get_p_mib(int inst, int pid,
+	      u32 *green /*green bytes*/,
+	      u32 *yellow/*yellow bytes*/)
+{
+	if (green)
+		*green = readl((void *)0xb8820400 + pid * 4);
+	if (yellow)
+		*yellow = readl((void *)0xb8820200 + pid * 4);
+	return 0;
+}
+
+#define PRINT_Q_MIB(i, c, x, y, z)\
+	PR_INFO("Q[%03d]:0x%08x 0x%08x 0x%08x 0x%08x\n",\
+		 i, c, x, y, z)\
+
+ssize_t proc_qos_mib(struct file *file, const char *buf,
+		     size_t count, loff_t *ppos)
+{
+	int len = 0;
+	char data[100];
+	char *param_list[10];
+	int num;
+	int start, end, i;
+	int inst = 0;
+	u32 c;
+	u32 total_accept_pkt, total_drop_pkt, red_drop_ktp;
+	u32 gree_b, yellow_b;
+
+	len = (count >= sizeof(data)) ? (sizeof(data) - 1) : count;
+	DP_DEBUG(DP_DBG_FLAG_DBG, "len=%d\n", len);
+	if (len <= 0) {
+		PR_ERR("Wrong len value (%d)\n", len);
+		return count;
+	}
+	if (copy_from_user(data, buf, len)) {
+		PR_ERR("copy_from_user fail");
+		return count;
+	}
+	data[len - 1] = 0; /* Make string */
+	num = dp_split_buffer(data, param_list, ARRAY_SIZE(param_list));
+	if (num < 1)
+		goto help;
+	if (dp_strncmpi(param_list[0], "q", 1) == 0) {
+		start = 0;
+		end = MAX_QUEUE;
+		if (param_list[1])
+			start = dp_atoi(param_list[1]);
+		if (param_list[2])
+			end = dp_atoi(param_list[2]);
+		if (start < 0)
+			start = 0;
+		if (end <= start)
+			end = start + 1;
+		if (end > MAX_QUEUE)
+			end = MAX_QUEUE;
+		PR_INFO("%5s:%10s %10s  %10s  %10s (%d-%d)\n",
+			"QID", "qocc(b)", "accept(p)", "drop(p)", "red_drop(p)",
+			start, end);
+		for (i = start; i < end; i++) {
+			get_q_qocc(inst, i, &c);
+			get_q_mib(inst, i, &total_accept_pkt,
+				  &total_drop_pkt, &red_drop_ktp);
+			if (c || total_accept_pkt || total_drop_pkt ||
+			    red_drop_ktp)
+				PRINT_Q_MIB(i, c, total_accept_pkt,
+					    total_drop_pkt, red_drop_ktp);
+		}
+	} else if (dp_strncmpi(param_list[0], "p", 1) == 0) {
+		start = 0;
+		end = MAX_CQM_DEQ;
+		if (param_list[1])
+			start = dp_atoi(param_list[1]);
+		if (param_list[2])
+			end = dp_atoi(param_list[2]);
+		if (start < 0)
+			start = 0;
+		if (end <= start)
+			end = start + 1;
+		if (end > MAX_CQM_DEQ)
+			end = MAX_CQM_DEQ;
+		PR_INFO("Port Id :green(b)   yellow(b)  (%d_%d)\n",
+			start, end);
+		for (i = start; i < end; i++) {
+			get_p_mib(inst, i, &gree_b, &yellow_b);
+			if (gree_b || yellow_b)
+				PR_INFO("P[%03d]: 0x%08x 0x%08x\n",
+					i, gree_b, yellow_b);
+		}
+	} else
+		goto help;
+
+	return count;
+help:   /*                        [0]    [1]*/
+	PR_INFO("queue mib: echo <q> <start qid> <end qid> > %s\n",
+		"/proc/dp/" DP_MIB_Q);
+	PR_INFO("port  mib: echo <p> <start port_id> <end port_id> > %s\n",
+		"/proc/dp/" DP_MIB_Q);
+	return count;
+}
+
+static int proc_gsw_pce_dump(struct seq_file *s, int pos)
+{
+	struct core_ops *gsw_handle;
+	GSW_PCE_rule_t *rule;
+	int ret = 0, i;
+
+	rule = kmalloc(sizeof(GSW_PCE_rule_t) + 1,
+		       GFP_KERNEL);
+	if (!rule) {
+		pos = -1;
+		return pos;
+	}
+
+	/*read gswip-r rmon counter */
+	gsw_handle = dp_port_prop[0].ops[1];
+	rule->pattern.nIndex = pos;
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_tflow_ops
+			   .TFLOW_PceRuleRead, gsw_handle, rule);
+	if (ret != GSW_statusOk) {
+		pos = -1;
+		return pos;
+	}
+	if (!rule->pattern.bEnable)
+		goto EXIT;
+
+	seq_printf(s, "Pattern[%d]:-----\n", rule->pattern.nIndex);
+	if (rule->pattern.bPortIdEnable) {
+		seq_printf(s, "  bPortIdEnable           =   %d\n",
+			   rule->pattern.bPortIdEnable);
+		seq_printf(s, "  nPortId                 =   %d\n",
+			   rule->pattern.nPortId);
+		seq_printf(s, "  bPortId_Exclude         =   %d\n",
+			   rule->pattern.bPortId_Exclude);
+	}
+	if (rule->pattern.bSubIfIdEnable) {
+		seq_printf(s, "  bSubIfIdEnable          =   %d\n",
+			   rule->pattern.bSubIfIdEnable);
+		seq_printf(s, "  nSubIfId                =   %d\n",
+			   rule->pattern.nSubIfId);
+		seq_printf(s, "  bSubIfId_Exclude        =   %d\n",
+			   rule->pattern.bSubIfId_Exclude);
+	}
+	if (rule->pattern.bDSCP_Enable) {
+		seq_printf(s, "  bDSCP_Enable            =   %d\n",
+			   rule->pattern.bDSCP_Enable);
+		seq_printf(s, "  nDSCP                   =   %d\n",
+			   rule->pattern.nDSCP);
+		seq_printf(s, "  bDSCP_Exclude           =   %d\n",
+			   rule->pattern.bDSCP_Exclude);
+	}
+	if (rule->pattern.bInner_DSCP_Enable) {
+		seq_printf(s, "  bInner_DSCP_Enable      =   %d\n",
+			   rule->pattern.bInner_DSCP_Enable);
+		seq_printf(s, "  nInnerDSCP              =   %d\n",
+			   rule->pattern.nInnerDSCP);
+		seq_printf(s, "  bInnerDSCP_Exclude      =   %d\n",
+			   rule->pattern.bInnerDSCP_Exclude);
+	}
+	if (rule->pattern.bPCP_Enable) {
+		seq_printf(s, "  bPCP_Enable             =   %d\n",
+			   rule->pattern.bPCP_Enable);
+		seq_printf(s, "  nPCP                    =   %d\n",
+			   rule->pattern.nPCP);
+		seq_printf(s, "  bCTAG_PCP_DEI_Exclude   =   %d\n",
+			   rule->pattern.bCTAG_PCP_DEI_Exclude);
+	}
+	if (rule->pattern.bSTAG_PCP_DEI_Enable) {
+		seq_printf(s, "  bSTAG_PCP_DEI_Enable    =   %d\n",
+			   rule->pattern.bSTAG_PCP_DEI_Enable);
+		seq_printf(s, "  nSTAG_PCP_DEI           =   %d\n",
+			   rule->pattern.nSTAG_PCP_DEI);
+		seq_printf(s, "  bSTAG_PCP_DEI_Exclude   =   %d\n",
+			   rule->pattern.bSTAG_PCP_DEI_Exclude);
+	}
+	if (rule->pattern.bPktLngEnable) {
+		seq_printf(s, "  bPktLngEnable           =   %d\n",
+			   rule->pattern.bPktLngEnable);
+		seq_printf(s, "  nPktLng                 =   %d\n",
+			   rule->pattern.nPktLng);
+		seq_printf(s, "  nPktLngRange            =   %d\n",
+			   rule->pattern.nPktLngRange);
+		seq_printf(s, "  bPktLng_Exclude         =   %d\n",
+			   rule->pattern.bPktLng_Exclude);
+	}
+	if (rule->pattern.bMAC_DstEnable) {
+		seq_printf(s, "  bMAC_DstEnable          =   %d\n",
+			   rule->pattern.bMAC_DstEnable);
+		seq_printf(s,
+			   "  nMAC_Dst                =   %02x:%2x:%2x:%2x:%2x:%2x\n",
+			   rule->pattern.nMAC_Dst[0],
+			   rule->pattern.nMAC_Dst[1],
+			   rule->pattern.nMAC_Dst[2],
+			   rule->pattern.nMAC_Dst[3],
+			   rule->pattern.nMAC_Dst[4],
+			   rule->pattern.nMAC_Dst[5]);
+		seq_printf(s, "  nMAC_DstMask            =   %x\n",
+			   rule->pattern.nMAC_DstMask);
+		seq_printf(s, "  bDstMAC_Exclude         =   %d\n",
+			   rule->pattern.bDstMAC_Exclude);
+	}
+	if (rule->pattern.bMAC_SrcEnable) {
+		seq_printf(s, "  bMAC_SrcEnable          =   %d\n",
+			   rule->pattern.bMAC_SrcEnable);
+		seq_printf(s,
+			   "  nMAC_Src                =   %02x:%2x:%2x:%2x:%2x:%2x\n",
+			   rule->pattern.nMAC_Src[0],
+			   rule->pattern.nMAC_Src[1],
+			   rule->pattern.nMAC_Src[2],
+			   rule->pattern.nMAC_Src[3],
+			   rule->pattern.nMAC_Src[4],
+			   rule->pattern.nMAC_Src[5]);
+		seq_printf(s, "  nMAC_SrcMask            =   %x\n",
+			   rule->pattern.nMAC_SrcMask);
+		seq_printf(s, "  bSrcMAC_Exclude         =   %d\n",
+			   rule->pattern.bSrcMAC_Exclude);
+	}
+	if (rule->pattern.bAppDataMSB_Enable) {
+		seq_printf(s, "  bAppDataMSB_Enable      =   %d\n",
+			   rule->pattern.bAppDataMSB_Enable);
+		seq_printf(s, "  nAppDataMSB             =   %x\n",
+			   rule->pattern.nAppDataMSB);
+		seq_printf(s, "  bAppMaskRangeMSB_Select =   %d\n",
+			   rule->pattern.bAppMaskRangeMSB_Select);
+		seq_printf(s, "  nAppMaskRangeMSB        =   %x\n",
+			   rule->pattern.nAppMaskRangeMSB);
+		seq_printf(s, "  bAppMSB_Exclude         =   %d\n",
+			   rule->pattern.bAppMSB_Exclude);
+	}
+	if (rule->pattern.bAppDataLSB_Enable) {
+		seq_printf(s, "  bAppDataLSB_Enable      =   %d\n",
+			   rule->pattern.bAppDataLSB_Enable);
+		seq_printf(s, "  nAppDataLSB             =   %x\n",
+			   rule->pattern.nAppDataLSB);
+		seq_printf(s, "  bAppMaskRangeLSB_Select =   %d\n",
+			   rule->pattern.bAppMaskRangeLSB_Select);
+		seq_printf(s, "  nAppMaskRangeLSB        =   %x\n",
+			   rule->pattern.nAppMaskRangeLSB);
+		seq_printf(s, "  bAppLSB_Exclude         =   %d\n",
+			   rule->pattern.bAppLSB_Exclude);
+	}
+	if (rule->pattern.eDstIP_Select) {
+		seq_printf(s, "  eDstIP_Select           =   %d\n",
+			   rule->pattern.eDstIP_Select);
+		seq_printf(s, "  nDstIP                  =   %08x ",
+			   rule->pattern.nDstIP.nIPv4);
+		if (rule->pattern.eDstIP_Select == 2)
+			for (i = 2; i < 8; i++)
+				seq_printf(s, "%04x ",
+					   rule->pattern.nDstIP.nIPv6[i]);
+		seq_puts(s, "\n");
+		seq_printf(s, "  nDstIP_Mask             =   %x\n",
+			   rule->pattern.nDstIP_Mask);
+		seq_printf(s, "  bDstIP_Exclude          =   %d\n",
+			   rule->pattern.bDstIP_Exclude);
+	}
+	if (rule->pattern.eInnerDstIP_Select) {
+		seq_printf(s, "  eInnerDstIP_Select      =   %d\n",
+			   rule->pattern.eInnerDstIP_Select);
+		seq_printf(s, "  nInnerDstIP             =   %x\n",
+			   rule->pattern.nInnerDstIP.nIPv4);
+		seq_printf(s, "  nInnerDstIP_Mask        =   %x\n",
+			   rule->pattern.nInnerDstIP_Mask);
+		seq_printf(s, "  bInnerDstIP_Exclude     =   %d\n",
+			   rule->pattern.bInnerDstIP_Exclude);
+	}
+	if (rule->pattern.eSrcIP_Select) {
+		seq_printf(s, "  eSrcIP_Select           =   %d\n",
+			   rule->pattern.eSrcIP_Select);
+		seq_printf(s, "  nSrcIP                  =   %x\n",
+			   rule->pattern.nSrcIP.nIPv4);
+		seq_printf(s, "  nSrcIP_Mask             =   %x\n",
+			   rule->pattern.nSrcIP_Mask);
+		seq_printf(s, "  bSrcIP_Exclude          =   %d\n",
+			   rule->pattern.bSrcIP_Exclude);
+	}
+	if (rule->pattern.eInnerSrcIP_Select) {
+		seq_printf(s, "  eInnerSrcIP_Select      =   %d\n",
+			   rule->pattern.eInnerSrcIP_Select);
+		seq_printf(s, "  nInnerSrcIP             =   %x\n",
+			   rule->pattern.nInnerSrcIP.nIPv4);
+		seq_printf(s, "  nInnerSrcIP_Mask        =   %x\n",
+			   rule->pattern.nInnerSrcIP_Mask);
+		seq_printf(s, "  bInnerSrcIP_Exclude     =   %d\n",
+			   rule->pattern.bInnerSrcIP_Exclude);
+	}
+	if (rule->pattern.bEtherTypeEnable) {
+		seq_printf(s, "  bEtherTypeEnable        =   %d\n",
+			   rule->pattern.bEtherTypeEnable);
+		seq_printf(s, "  nEtherType              =   %x\n",
+			   rule->pattern.nEtherType);
+		seq_printf(s, "  nEtherTypeMask          =   %x\n",
+			   rule->pattern.nEtherTypeMask);
+		seq_printf(s, "  bEtherType_Exclude      =   %d\n",
+			   rule->pattern.bEtherType_Exclude);
+	}
+	if (rule->pattern.bProtocolEnable) {
+		seq_printf(s, "  bProtocolEnable         =   %d\n",
+			   rule->pattern.bProtocolEnable);
+		seq_printf(s, "  nProtocol               =   %x\n",
+			   rule->pattern.nProtocol);
+		seq_printf(s, "  nProtocolMask           =   %x\n",
+			   rule->pattern.nProtocolMask);
+		seq_printf(s, "  bProtocol_Exclude       =   %d\n",
+			   rule->pattern.bProtocol_Exclude);
+	}
+	if (rule->pattern.bInnerProtocolEnable) {
+		seq_printf(s, "  bInnerProtocolEnable    =   %d\n",
+			   rule->pattern.bInnerProtocolEnable);
+		seq_printf(s, "  nInnerProtocol          =   %x\n",
+			   rule->pattern.nInnerProtocol);
+		seq_printf(s, "  nInnerProtocolMask      =   %x\n",
+			   rule->pattern.nInnerProtocolMask);
+		seq_printf(s, "  bInnerProtocol_Exclude  =   %d\n",
+			   rule->pattern.bInnerProtocol_Exclude);
+	}
+	if (rule->pattern.bSessionIdEnable) {
+		seq_printf(s, "  bSessionIdEnable        =   %d\n",
+			   rule->pattern.bSessionIdEnable);
+		seq_printf(s, "  nSessionId              =   %x\n",
+			   rule->pattern.nSessionId);
+		seq_printf(s, "  bSessionId_Exclude      =   %d\n",
+			   rule->pattern.bSessionId_Exclude);
+	}
+	if (rule->pattern.bPPP_ProtocolEnable) {
+		seq_printf(s, "  bPPP_ProtocolEnable     =   %d\n",
+			   rule->pattern.bPPP_ProtocolEnable);
+		seq_printf(s, "  nPPP_Protocol           =   %x\n",
+			   rule->pattern.nPPP_Protocol);
+		seq_printf(s, "  nPPP_ProtocolMask       =   %x\n",
+			   rule->pattern.nPPP_ProtocolMask);
+		seq_printf(s, "  bPPP_Protocol_Exclude   =   %d\n",
+			   rule->pattern.bPPP_Protocol_Exclude);
+	}
+	if (rule->pattern.bVid) {
+		seq_printf(s, "  bVid                    =   %d\n",
+			   rule->pattern.bVid);
+		seq_printf(s, "  nVid                    =   %d\n",
+			   rule->pattern.nVid);
+		seq_printf(s, "  bVid_Exclude            =   %d\n",
+			   rule->pattern.bVid_Exclude);
+	}
+	if (rule->pattern.bSLAN_Vid) {
+		seq_printf(s, "  bSLAN_Vid               =    %d\n",
+			   rule->pattern.bSLAN_Vid);
+		seq_printf(s, "  nSLAN_Vid               =    %d\n",
+			   rule->pattern.nSLAN_Vid);
+		seq_printf(s, "  bSLANVid_Exclude        =    %d\n",
+			   rule->pattern.bSLANVid_Exclude);
+	}
+	if (rule->pattern.bPayload1_SrcEnable) {
+		seq_printf(s, "  bPayload1_SrcEnable     =   %d\n",
+			   rule->pattern.bPayload1_SrcEnable);
+		seq_printf(s, "  nPayload1               =   %x\n",
+			   rule->pattern.nPayload1);
+		seq_printf(s, "  nPayload1_Mask          =   %x\n",
+			   rule->pattern.nPayload1_Mask);
+		seq_printf(s, "  bPayload1_Exclude       =   %d\n",
+			   rule->pattern.bPayload1_Exclude);
+	}
+	if (rule->pattern.bPayload2_SrcEnable) {
+		seq_printf(s, "  bPayload2_SrcEnable     =   %d\n",
+			   rule->pattern.bPayload2_SrcEnable);
+		seq_printf(s, "  nPayload2               =   %x\n",
+			   rule->pattern.nPayload2);
+		seq_printf(s, "  nPayload2_Mask          =   %x\n",
+			   rule->pattern.nPayload2_Mask);
+		seq_printf(s, "  bPayload2_Exclude       =   %d\n",
+			   rule->pattern.bPayload2_Exclude);
+	}
+	if (rule->pattern.bParserFlagLSB_Enable) {
+		seq_printf(s, "  bParserFlagLSB_Enable   =   %d\n",
+			   rule->pattern.bParserFlagLSB_Enable);
+		seq_printf(s, "  nParserFlagLSB          =   %x\n",
+			   rule->pattern.nParserFlagLSB);
+		seq_printf(s, "  nParserFlagLSB_Mask     =   %x\n",
+			   rule->pattern.nParserFlagLSB_Mask);
+		seq_printf(s, "  bParserFlagLSB_Exclude  =   %d\n",
+			   rule->pattern.bParserFlagLSB_Exclude);
+	}
+	if (rule->pattern.bParserFlagMSB_Enable) {
+		seq_printf(s, "  bParserFlagMSB_Enable   =   %d\n",
+			   rule->pattern.bParserFlagMSB_Enable);
+		seq_printf(s, "  nParserFlagMSB          =   %x\n",
+			   rule->pattern.nParserFlagMSB);
+		seq_printf(s, "  nParserFlagMSB_Mask     =   %x\n",
+			   rule->pattern.nParserFlagMSB_Mask);
+		seq_printf(s, "  bParserFlagMSB_Exclude  =   %d\n",
+			   rule->pattern.bParserFlagMSB_Exclude);
+	}
+
+	seq_puts(s, "Action:\n");
+	if (rule->action.eTrafficClassAction) {
+		seq_printf(s, "  eTrafficClassAction      =   %d\n",
+			   rule->action.eTrafficClassAction);
+		seq_printf(s, "  nTrafficClassAlternate   =   %d\n",
+			   rule->action.nTrafficClassAlternate);
+	}
+	if (rule->action.eSnoopingTypeAction)
+		seq_printf(s, "  eSnoopingTypeAction      =   %d\n",
+			   rule->action.eSnoopingTypeAction);
+	if (rule->action.eLearningAction)
+		seq_printf(s, "  eLearningAction          =   %d\n",
+			   rule->action.eLearningAction);
+	if (rule->action.eIrqAction)
+		seq_printf(s, "  eIrqAction               =   %d\n",
+			   rule->action.eIrqAction);
+	if (rule->action.eCrossStateAction)
+		seq_printf(s, "  eCrossStateAction        =   %d\n",
+			   rule->action.eCrossStateAction);
+	if (rule->action.eCritFrameAction)
+		seq_printf(s, "  eCritFrameAction         =   %d\n",
+			   rule->action.eCritFrameAction);
+	if (rule->action.eTimestampAction) {
+		seq_printf(s, "  eTimestampAction         =   %d\n",
+			   rule->action.eTimestampAction);
+	}
+	if (rule->action.ePortMapAction) {
+		seq_printf(s, "  ePortMapAction           =   %d\n",
+			   rule->action.ePortMapAction);
+		seq_printf(s, "  nForwardSubIfId          =   %d\n",
+			   rule->action.nForwardSubIfId);
+	}
+	if (rule->action.bRemarkAction)
+		seq_printf(s, "  bRemarkAction            =   %d\n",
+			   rule->action.bRemarkAction);
+	if (rule->action.bRemarkPCP)
+		seq_printf(s, "  bRemarkPCP               =   %d\n",
+			   rule->action.bRemarkPCP);
+	if (rule->action.bRemarkSTAG_PCP)
+		seq_printf(s, "  bRemarkSTAG_PCP          =   %d\n",
+			   rule->action.bRemarkSTAG_PCP);
+	if (rule->action.bRemarkSTAG_DEI)
+		seq_printf(s, "  bRemarkSTAG_DEI          =   %d\n",
+			   rule->action.bRemarkSTAG_DEI);
+	if (rule->action.bRemarkDSCP)
+		seq_printf(s, "  bRemarkDSCP              =   %d\n",
+			   rule->action.bRemarkDSCP);
+	if (rule->action.bRemarkClass) {
+		seq_printf(s, "  bRemarkClass             =   %d\n",
+			   rule->action.bRemarkClass);
+	}
+	if (rule->action.eMeterAction) {
+		seq_printf(s, "  eMeterAction             =   %d\n",
+			   rule->action.eMeterAction);
+		seq_printf(s, "  nMeterId                 =   %d\n",
+			   rule->action.nMeterId);
+	}
+	if (rule->action.bRMON_Action) {
+		seq_printf(s, "  bRMON_Action             =   %d\n",
+			   rule->action.bRMON_Action);
+		seq_printf(s, "  nRMON_Id                 =   %d\n",
+			   rule->action.nRMON_Id);
+	}
+	if (rule->action.eVLAN_Action) {
+		seq_printf(s, "  eVLAN_Action             =   %d\n",
+			   rule->action.eVLAN_Action);
+		seq_printf(s, "  nVLAN_Id                 =   %d\n",
+			   rule->action.nVLAN_Id);
+	}
+	if (rule->action.eSVLAN_Action) {
+		seq_printf(s, "  eSVLAN_Action            =   %d\n",
+			   rule->action.eSVLAN_Action);
+		seq_printf(s, "  nSVLAN_Id                =   %d\n",
+			   rule->action.nSVLAN_Id);
+	}
+	if (rule->action.eVLAN_CrossAction)
+		seq_printf(s, "  eVLAN_CrossAction        =   %d\n",
+			   rule->action.eVLAN_CrossAction);
+	if (rule->action.nFId)
+		seq_printf(s, "  nFId                     =   %d\n",
+			   rule->action.nFId);
+	if (rule->action.bPortBitMapMuxControl)
+		seq_printf(s, "  bPortBitMapMuxControl    =   %d\n",
+			   rule->action.bPortBitMapMuxControl);
+	if (rule->action.bPortTrunkAction)
+		seq_printf(s, "  bPortTrunkAction         =   %d\n",
+			   rule->action.bPortTrunkAction);
+	if (rule->action.bPortLinkSelection)
+		seq_printf(s, "  bPortLinkSelection       =   %d\n",
+			   rule->action.bPortLinkSelection);
+	if (rule->action.bCVLAN_Ignore_Control)
+		seq_printf(s, "  bCVLAN_Ignore_Control    =   %d\n",
+			   rule->action.bCVLAN_Ignore_Control);
+	if (rule->action.bFlowID_Action) {
+		seq_printf(s, "  bFlowID_Action           =   %d\n",
+			   rule->action.bFlowID_Action);
+		seq_printf(s, "  nFlowID                  =   %d\n",
+			   rule->action.nFlowID);
+	}
+	if (rule->action.bRoutExtId_Action) {
+		seq_printf(s, "  bRoutExtId_Action        =   %d\n",
+			   rule->action.bRoutExtId_Action);
+		seq_printf(s, "  nRoutExtId               =   %d\n",
+			   rule->action.nRoutExtId);
+	}
+	if (rule->action.bRtDstPortMaskCmp_Action)
+		seq_printf(s, "  bRtDstPortMaskCmp_Action =   %d\n",
+			   rule->action.bRtDstPortMaskCmp_Action);
+	if (rule->action.bRtSrcPortMaskCmp_Action)
+		seq_printf(s, "  bRtSrcPortMaskCmp_Action =   %d\n",
+			   rule->action.bRtSrcPortMaskCmp_Action);
+	if (rule->action.bRtDstIpMaskCmp_Action)
+		seq_printf(s, "  bRtDstIpMaskCmp_Action   =   %d\n",
+			   rule->action.bRtDstIpMaskCmp_Action);
+	if (rule->action.bRtSrcIpMaskCmp_Action)
+		seq_printf(s, "  bRtSrcIpMaskCmp_Action   =   %d\n",
+			   rule->action.bRtSrcIpMaskCmp_Action);
+	if (rule->action.bRtInnerIPasKey_Action)
+		seq_printf(s, "  bRtInnerIPasKey_Action   =   %d\n",
+			   rule->action.bRtInnerIPasKey_Action);
+	if (rule->action.bRtAccelEna_Action)
+		seq_printf(s, "  bRtAccelEna_Action       =   %d\n",
+			   rule->action.bRtAccelEna_Action);
+	if (rule->action.bRtCtrlEna_Action)
+		seq_printf(s, "  bRtCtrlEna_Action        =   %d\n",
+			   rule->action.bRtCtrlEna_Action);
+	if (rule->action.eProcessPath_Action)
+		seq_printf(s, "  eProcessPath_Action      =   %d\n",
+			   rule->action.eProcessPath_Action);
+	if (rule->action.ePortFilterType_Action)
+		seq_printf(s, "  ePortFilterType_Action   =   %d\n",
+			   rule->action.ePortFilterType_Action);
+	seq_puts(s, "\n");
+ EXIT:
+	kfree(rule);
+	pos++;
+
+	return pos;
+}
+
+int proc_gsw_pce_start(void)
+{
+	return 0;
+}
+
+#define PMAC_EG_SET(x, y) (pmac.eg.x = dp_atoi(y))
+#define PMAC_IG_SET(x, y) (pmac.ig.x = dp_atoi(y))
+
+static int set_pmac_ig_v(char *p, char *tail, GSW_PMAC_Ig_Cfg_t *ig)
+{
+	char *tmp;
+	int k;
+
+	for (k = 0; k < 8; k++) {
+		if (k < (8 - 1)) {
+			tmp = strstr(p, ":");
+			if (!tmp ||
+			    ((u32)tmp >= (u32)tail)) {
+				PR_INFO("%s:%s %s\n",
+					"Wrong format",
+					"should be like",
+					"xx:xx:xx:xx:xx:xx:xx:xx");
+				return -1;
+			}
+			*tmp = 0; /*replace:with zero*/
+		}
+		ig->defPmacHdr[k] = dp_atoi(p);
+
+		p = tmp + 1; /* move to next value */
+	}
+	return 0;
+}
+
+static ssize_t proc_gsw_pmac_write(struct file *file, const char *buf,
+				   size_t count, loff_t *ppos)
+{
+	u16 len, i, k, start_param;
+	GSW_return_t ret = 0;
+	char *str = NULL;
+	char *param_list[20 * 2];
+	unsigned int num;
+	union {
+		GSW_PMAC_Eg_Cfg_t eg;
+		GSW_PMAC_Ig_Cfg_t ig;
+	} pmac;
+	#define MAX_GSWIP_CALSS 15
+	#define MAX_GSWIP_FLOW  3
+	struct core_ops *gsw_handle;
+	int class_s = 0, class_e = MAX_GSWIP_CALSS;
+	int flow_s = 0, flow_e = MAX_GSWIP_FLOW;
+	int id;
+
+	str = kmalloc(count + 1, GFP_KERNEL);
+	if (!str)
+		return count;
+	len = count;
+	len -= copy_from_user(str, buf, len);
+	str[len] = 0;
+
+	num = dp_split_buffer(str, param_list, ARRAY_SIZE(param_list));
+	if ((num < 2) || (num >= ARRAY_SIZE(param_list))) {
+		PR_INFO("parameter %d not enough/more. count=%d\n", num, count);
+		goto help;
+	}
+	if (dp_strncmpi(param_list[0], "help", strlen("help")) == 0)	/* help */
+		goto help;
+	/* set pmac */
+	if (dp_strncmpi(param_list[0], "set", strlen("set")) != 0) {
+		PR_INFO("wrong command: %s\n", param_list[0]);
+		goto help;
+	}
+	if (dp_strncmpi(param_list[1], "0", 1) == 0) {
+		id = 0;
+	} else if (dp_strncmpi(param_list[1], "1", 1) == 0) {
+		id = 1;
+	} else {
+		PR_INFO("wrong param:should provide L/R\n");
+		goto exit;
+	}
+	gsw_handle = dp_port_prop[0].ops[0];
+	memset(&pmac, 0, sizeof(pmac));
+	start_param = 3;
+
+	if (dp_strncmpi(param_list[start_param - 1], "EG", 2) == 0) {
+		/*egress pmac */
+		pmac.eg.nPmacId = id;
+		ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_pmac_ops
+				   .Pmac_Eg_CfgGet, gsw_handle, &pmac);
+		for (i = start_param; i < num; i += 2) {
+			if (dp_strncmpi(param_list[i], "Class", strlen("Class")) == 0) {
+				char *p = param_list[i + 1];
+				char *tail = p + strlen(p);
+				char *tmp;
+
+				tmp = strstr(p, ":");
+				if (!tmp || (tmp >= tail)) {
+					PR_INFO("%s:it should be like xx:xx\n",
+						"Wrong format for Class");
+					goto exit;
+				}
+				*tmp = 0;
+				class_s = dp_atoi(p);
+				class_e = dp_atoi(tmp + 1);
+			} else if (dp_strncmpi(param_list[i], "FlowID", strlen("FlowID")) == 0) {
+				char *p = param_list[i + 1];
+				char *tail = p + strlen(p);
+				char *tmp;
+
+				tmp = strstr(p, ":");
+				if (!tmp || (tmp >= tail)) {
+					PR_INFO("%s: it should be like xx:xx\n",
+						"Wrong format for FlowID");
+					goto exit;
+				}
+				*tmp = 0;
+				flow_s = dp_atoi(p);
+				flow_e = dp_atoi(tmp + 1);
+			} else if (dp_strncmpi(param_list[i], "DestPort", strlen("DestPort")) == 0) {
+				PMAC_EG_SET(nDestPortId, param_list[i + 1]);
+			} else if (dp_strncmpi(param_list[i], "RxDmaCH", strlen("RxDmaCH")) == 0) {
+				PMAC_EG_SET(nRxDmaChanId, param_list[i + 1]);
+			}
+#ifdef xxxxx
+			/*below global flag cannot be editted here*/
+			else if (dp_strncmpi(param_list[i], "MPE1", strlen("MPE1")) == 0)
+				PMAC_EG_SET(bMpe1Flag, param_list[i + 1]);
+			else if (dp_strncmpi(param_list[i], "MPE2", strlen("MPE2")) == 0)
+				PMAC_EG_SET(bMpe2Flag, param_list[i + 1]);
+			else if (dp_strncmpi(param_list[i], "DEC", strlen("DEC")) == 0)
+				PMAC_EG_SET(bDecFlag, param_list[i + 1]);
+			else if (dp_strncmpi(param_list[i], "ENC", strlen("ENC")) == 0)
+				PMAC_EG_SET(bEncFlag, param_list[i + 1]);
+			else if (dp_strncmpi(param_list[i], "ProcFlag", strlen("ProcFlag")) == 0)
+				PMAC_EG_SET(bProcFlagsSelect,
+					    param_list[i + 1]);
+#endif
+			else if (dp_strncmpi(param_list[i], "RemL2Hdr", strlen("RemL2Hdr")) == 0)
+				PMAC_EG_SET(bRemL2Hdr, param_list[i + 1]);
+			else if (dp_strncmpi(param_list[i], "RemNum", strlen("RemNum")) == 0)
+				PMAC_EG_SET(numBytesRem, param_list[i + 1]);
+			else if (dp_strncmpi(param_list[i], "FCS", strlen("FCS")) == 0)
+				PMAC_EG_SET(bFcsEna, param_list[i + 1]);
+			else if (dp_strncmpi(param_list[i], "PmacEna", strlen("PmacEna")) == 0)
+				PMAC_EG_SET(bPmacEna, param_list[i + 1]);
+			else if (dp_strncmpi(param_list[i], "TcEnable", strlen("TcEnable")) == 0)
+				PMAC_EG_SET(bTCEnable, param_list[i + 1]);
+			else {
+				PR_INFO("wrong parameter[%d]: %s\n",
+					i, param_list[i]);
+				goto exit;
+			}
+		}
+		if (class_e > MAX_GSWIP_CALSS)
+			class_e = MAX_GSWIP_CALSS;
+		if (flow_e > MAX_GSWIP_FLOW)
+			flow_e = MAX_GSWIP_FLOW;
+		if (class_s > class_e) {
+			PR_INFO("wrong param:class_s=%d should < class_e=%d\n",
+				class_s, class_e);
+			goto exit;
+		}
+		if (flow_s > flow_e) {
+			PR_INFO("wrong param:flow_s=%d should < flow_e=%d\n",
+				flow_s, flow_e);
+			goto exit;
+		}
+		PR_INFO("Set EG PMAC for class %d:%d flow %d:%d\n",
+			class_s, class_e, flow_s, flow_e);
+		for (i = class_s; i <= class_e; i++) {
+			for (k = flow_s; k <= flow_e; k++) {
+				pmac.eg.nTrafficClass = i;
+				/*Note: bProcFlagsSelect zero,
+				 *just nTrafficClass,
+				 *else use MPE1/2/ENC/DEC flag instead
+				 */
+				pmac.eg.bMpe1Flag = (pmac.eg.nTrafficClass >>
+					0) & 1;
+				pmac.eg.bMpe2Flag = (pmac.eg.nTrafficClass >>
+					1) & 1;
+				pmac.eg.bEncFlag = (pmac.eg.nTrafficClass >>
+					2) & 1;
+				pmac.eg.bDecFlag = (pmac.eg.nTrafficClass >>
+					3) & 1;
+				pmac.eg.nFlowIDMsb = k;
+				ret = gsw_core_api((dp_gsw_cb)gsw_handle->
+						   gsw_pmac_ops.Pmac_Eg_CfgSet,
+						   gsw_handle, &pmac);
+			}
+		}
+		if (ret < GSW_statusOk) {
+			PR_INFO("GSW_PMAC_EG_CFG_SET returned failure\n");
+			goto exit;
+		}
+	} else if (dp_strncmpi(param_list[start_param - 1], "IG", 2) == 0) {
+		/*ingress pmac1 */
+		pmac.ig.nPmacId = id;
+		ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_pmac_ops
+				   .Pmac_Ig_CfgGet, gsw_handle, &pmac);
+		for (i = start_param; i < num; i += 2) {
+			if (dp_strncmpi(param_list[i], "TxDmaCH", strlen("TxDmaCH")) == 0) {
+				PMAC_IG_SET(nTxDmaChanId, param_list[i + 1]);
+			} else if (dp_strncmpi(param_list[i], "ErrDrop", strlen("ErrDrop")) == 0) {
+				PMAC_IG_SET(bErrPktsDisc, param_list[i + 1]);
+			} else if (dp_strncmpi(param_list[i],
+					      "ClassEna", strlen("ClassEna")) == 0) {
+				PMAC_IG_SET(bClassEna, param_list[i + 1]);
+			} else if (dp_strncmpi(param_list[i],
+					      "ClassDefault", strlen("ClassDefault")) == 0) {
+				PMAC_IG_SET(bClassDefault, param_list[i + 1]);
+			} else if (dp_strncmpi(param_list[i], "PmacEna", strlen("PmacEna")) == 0) {
+				PMAC_IG_SET(bPmapEna, param_list[i + 1]);
+			} else if (dp_strncmpi(param_list[i],
+					      "PmacDefault", strlen("PmacEna")) == 0) {
+				PMAC_IG_SET(bPmapDefault, param_list[i + 1]);
+			} else if (dp_strncmpi(param_list[i],
+					      "SubIdDefault", strlen("SubIdDefault")) == 0) {
+				 /*changed from bSubIdDefault in GSWIP3.1 */
+				//PMAC_IG_SET(bSubIdDefault, param_list[i + 1]);
+				PMAC_IG_SET(eSubId, param_list[i + 1]);
+			} else if (dp_strncmpi(param_list[i],
+					      "SpIdDefault", strlen("SpIdDefault")) == 0) {
+				PMAC_IG_SET(bSpIdDefault, param_list[i + 1]);
+			} else if (dp_strncmpi(param_list[i],
+					      "PmacPresent", strlen("PmacPresent")) == 0) {
+				PMAC_IG_SET(bPmacPresent, param_list[i + 1]);
+			} else if (dp_strncmpi(param_list[i],
+					    "DefaultPmacHdr", strlen("DefaultPmacHdr")) == 0) {
+				char *p = param_list[i + 1];
+				char *tail = p + strlen(p);
+
+				if (set_pmac_ig_v(p, tail, &pmac.ig))
+					goto exit;
+			} else {
+				PR_INFO("wrong parameter[%d]: %s\n", i,
+					param_list[i]);
+				goto exit;
+			}
+		}
+		ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_pmac_ops
+				   .Pmac_Ig_CfgSet, gsw_handle, &pmac);
+		if (ret < GSW_statusOk) {
+			PR_ERR("GSW_PMAC_IG_CFG_SET returned failure\n");
+			goto exit;
+		}
+	} else if (dp_strncmpi(param_list[start_param - 1], "reset", strlen("reset")) == 0) {
+		GSW_reset_t reset;
+
+		gsw_core_api((dp_gsw_cb)gsw_handle->gsw_common_ops.Reset,
+			     gsw_handle, &reset);
+	} else {
+		PR_INFO("wrong parameter not supported: %s\n",
+			param_list[start_param - 1]);
+		goto exit;
+	}
+exit:
+	kfree(str);
+	return count;
+
+ help:
+	PR_INFO("usage:\n");
+	PR_INFO("  echo set <0/1> EG\n");
+	PR_INFO("    [DestPort] [Dst-PMAC-Port-value]\n");
+	PR_INFO("    [Class] [Class-start:end](0~15)\n");
+	PR_INFO("    [FlowID] [FlowID-start:end](0~3)\n");
+	PR_INFO("\n");
+	PR_INFO("    [PmacEna] [Enable PMAC HDR (1) or not(0)]\n");
+	PR_INFO("    [RxDmaCH] [RxDmaCH-value]\n");
+	PR_INFO("    [TcEnable] [TcEnable-value(0/1)] [FCS] [FCS-value(0/1]\n");
+	PR_INFO("    %s [RemL2Hdr-value(0/1)] [RemNum] [RemNum-value]\n",
+		"[RemL2Hdr]");
+	PR_INFO("     > /prooc/dp/%s\n", PROC_PMAC);
+	PR_INFO("  echo set <0/1> IG [TxDmaCH] [TX-DMA-CH-value]\n");
+	PR_INFO("\n");
+	PR_INFO("    [ErrDrop] [Error-Drop-value(0/1)]\n");
+	PR_INFO("    %s PMAC header(1) or incoming PMAC header(0)]\n",
+		"[ClassEna] [Class Enable info from default");
+	PR_INFO("    %s %s %s\n",
+		"[ClassDefault]",
+		"[Class Default info from default PMAC header(1)",
+		"or incoming PMAC header(0)]");
+	PR_INFO("    %s %s %s\n",
+		"[PmacEna]",
+		"[Port Map Enable info from default PMAC header(1)",
+		"or incoming PMAC header(0)]");
+	PR_INFO("    %s %s or incoming PMAC header(0)]\n",
+		"[PmacDefault]",
+		"[Port Map info from default PMAC header(1)");
+	PR_INFO("    %s %s or in packet descriptor (0)]\n",
+		"[SubIdDefault]",
+		"[Sub_Interface Id Info from default PMAC header(1)");
+	PR_INFO("    %s %s or incoming PMAC header (False)]\n",
+		"[SpIdDefault]",
+		"[Source port id from default PMAC header(1)");
+	PR_INFO("    %s %s or not (0)]\n",
+		"[PmacPresent]",
+		"[Packet PMAC header is present (1)");
+	PR_INFO("    %s [Default PMAC HDR(8 bytes: xx:xx:xx:xx:xx:xx:xx:xx]\n",
+		"[DefaultPmacHdr]");
+	PR_INFO("     > /prooc/dp/%s\n", PROC_PMAC);
+	PR_INFO("  echo set <0/1> reset\n");
+	PR_INFO("  ext1: echo %s %s %s %s > /proc/dp/pmac\n",
+		"set R IG TxDmaCH 1 ErrDrop 0 PmacDefault 0 PmacEna 0",
+		"ClassEna 1 ClassDefault 1 SubIdDefault 1 SpIdDefault 1",
+		"PmacPresent 0",
+		"DefaultPmacHdr 0x11:0x22:0x33:0x44:0x55:0x66:0x77:0x88");
+	PR_INFO("  ext2: %s %s > /proc/dp/pmac\n",
+		"echo set R EG DestPort 15 class 0:15 FlowID 0:3",
+		"RxDmaCH 4 TcEnable 1 RemL2Hdr 1 RemNum 8 PmacEna 0 FCS 1");
+	goto exit;
+}
+
+static int proc_gsw_pmac_start(void)
+{
+	return 0;
+}
+
+static int proc_gsw_pmac_dump(struct seq_file *s, int pos)
+{
+	GSW_PMAC_Ig_Cfg_t igCfg;
+	GSW_PMAC_Eg_Cfg_t egCfg;
+	struct core_ops *gsw_handle;
+	u8 i = 0, j = 0;
+
+	/* Do the GSW-L configuration */
+	gsw_handle = dp_port_prop[0].ops[GSWIP_L];
+	seq_puts(s, "\nGSWIP-L Ingress PMAC Configure\n");
+	seq_printf(s, "%15s %15s %15s %15s %15s %15s %15s %15s %15s %15s\n",
+		   "nTxDmaChanId", "bErrPktsDisc", "bPmapDefault", "bPmapEna",
+		   "bClassDefault", "bClassEna", "bSubIdDefault",
+		   "bSpIdDefault", "bPmacPresent", "defPmacHdr");
+	for (i = 0; i <= 15; i++)  {
+		memset(&igCfg, 0x00, sizeof(igCfg));
+		igCfg.nTxDmaChanId = i;
+		gsw_core_api((dp_gsw_cb)gsw_handle->gsw_pmac_ops.Pmac_Ig_CfgGet,
+			     gsw_handle, &igCfg);
+		seq_printf(s, "%15d %15d %15d %15d %15d %15d %15d %15d %15d ",
+			   igCfg.nTxDmaChanId, igCfg.bErrPktsDisc,
+			   igCfg.bPmapDefault, igCfg.bPmapEna,
+			   igCfg.bClassDefault, igCfg.bClassEna,
+			   igCfg.eSubId, igCfg.bSpIdDefault,
+			   igCfg.bPmacPresent);
+		for (j = 0; j < 8; j++)
+			seq_printf(s, "%02x\n", igCfg.defPmacHdr[j]);
+		seq_puts(s, "\n");
+	}
+
+	seq_puts(s, "GSWIP-L Egress PMAC Configure\n");
+	for (i = 0; i <= 3; i++) {
+		memset(&egCfg, 0x00, sizeof(egCfg));
+
+		egCfg.nDestPortId	= i;
+		egCfg.nTrafficClass	= 0;
+		egCfg.nFlowIDMsb	= 0;
+		egCfg.bDecFlag		= 0;
+		egCfg.bEncFlag		= 0;
+		egCfg.bMpe1Flag		= 0;
+		egCfg.bMpe2Flag		= 0;
+		gsw_core_api((dp_gsw_cb)gsw_handle->gsw_pmac_ops.Pmac_Eg_CfgGet,
+			     gsw_handle, &egCfg);
+		seq_printf(s, " nRxDmaChanId  = %d\n", egCfg.nRxDmaChanId);
+		seq_printf(s, " bRemL2Hdr     = %d\n", egCfg.bRemL2Hdr);
+		seq_printf(s, " numBytesRem   = %d\n", egCfg.numBytesRem);
+		seq_printf(s, " bFcsEna       = %d\n", egCfg.bFcsEna);
+		seq_printf(s, " bPmacEna      = %d\n", egCfg.bPmacEna);
+		seq_printf(s, " nResDW1       = %d\n", egCfg.nResDW1);
+		seq_printf(s, " nRes1DW0      = %d\n", egCfg.nRes1DW0);
+		seq_printf(s, " nRes2DW0      = %d\n", egCfg.nRes2DW0);
+		seq_printf(s, " nDestPortId   = %d\n", egCfg.nDestPortId);
+		seq_printf(s, " nTrafficClass = %d\n", egCfg.nTrafficClass);
+		seq_printf(s, " nFlowIDMsb    = %d\n", egCfg.nFlowIDMsb);
+		seq_printf(s, " bDecFlag      = %d\n", egCfg.bDecFlag);
+		seq_printf(s, " bEncFlag      = %d\n", egCfg.bEncFlag);
+		seq_printf(s, " bMpe1Flag     = %d\n", egCfg.bMpe1Flag);
+		seq_printf(s, " bMpe2Flag     = %d\n", egCfg.bMpe2Flag);
+		seq_puts(s, "\n");
+	}
+
+	gsw_handle = dp_port_prop[0].ops[GSWIP_R];
+	seq_puts(s, "\n\n\nGSWIP-R Ingress PMAC Configure\n");
+		seq_printf(s, "\n%15s %15s %15s %15s %15s %15s %15s %15s %15s %15s\n",
+			   "nTxDmaChanId", "bErrPktsDisc", "bPmapDefault",
+			   "bPmapEna", "bClassDefault", "bClassEna",
+			   "bSubIdDefault", "bSpIdDefault", "bPmacPresent",
+			   "defPmacHdr");
+	for (i = 0; i <= 15; i++) {
+		memset(&igCfg, 0x00, sizeof(igCfg));
+		igCfg.nTxDmaChanId = i;
+		gsw_core_api((dp_gsw_cb)gsw_handle->gsw_pmac_ops.Pmac_Ig_CfgGet,
+			     gsw_handle, &igCfg);
+		seq_printf(s, "%15d %15d %15d %15d %15d %15d %15d %15d %15d ",
+			   igCfg.nTxDmaChanId, igCfg.bErrPktsDisc,
+			   igCfg.bPmapDefault, igCfg.bPmapEna,
+			   igCfg.bClassDefault, igCfg.bClassEna,
+			   igCfg.eSubId, igCfg.bSpIdDefault,
+			   igCfg.bPmacPresent);
+		for (j = 0; j < 8; j++)
+			seq_printf(s, "%02x", igCfg.defPmacHdr[j]);
+		seq_puts(s, "\n");
+	}
+
+	seq_puts(s, "\n\n\nGSWIP-R Egress PMAC Configure\n");
+	for (i = 0; i <= 15; i++) {
+		memset(&egCfg, 0x00, sizeof(egCfg));
+
+		egCfg.nDestPortId	= i;
+		egCfg.nTrafficClass	= 0;
+		egCfg.nFlowIDMsb	= 0;
+		egCfg.bDecFlag		= 0;
+		egCfg.bEncFlag		= 0;
+		egCfg.bMpe1Flag		= 0;
+		egCfg.bMpe2Flag		= 0;
+
+		gsw_core_api((dp_gsw_cb)gsw_handle->gsw_pmac_ops.Pmac_Eg_CfgGet,
+			     gsw_handle, &egCfg);
+
+		seq_printf(s, " nRxDmaChanId  = %d\n", egCfg.nRxDmaChanId);
+		seq_printf(s, " bRemL2Hdr     = %d\n", egCfg.bRemL2Hdr);
+		seq_printf(s, " numBytesRem   = %d\n", egCfg.numBytesRem);
+		seq_printf(s, " bFcsEna       = %d\n", egCfg.bFcsEna);
+		seq_printf(s, " bPmacEna      = %d\n", egCfg.bPmacEna);
+		seq_printf(s, " nResDW1       = %d\n", egCfg.nResDW1);
+		seq_printf(s, " nRes1DW0      = %d\n", egCfg.nRes1DW0);
+		seq_printf(s, " nRes2DW0      = %d\n", egCfg.nRes2DW0);
+		seq_printf(s, " nDestPortId   = %d\n", egCfg.nDestPortId);
+		seq_printf(s, " nTrafficClass = %d\n", egCfg.nTrafficClass);
+		seq_printf(s, " nFlowIDMsb    = %d\n", egCfg.nFlowIDMsb);
+		seq_printf(s, " bDecFlag      = %d\n", egCfg.bDecFlag);
+		seq_printf(s, " bEncFlag      = %d\n", egCfg.bEncFlag);
+		seq_printf(s, " bMpe1Flag     = %d\n", egCfg.bMpe1Flag);
+		seq_printf(s, " bMpe2Flag     = %d\n", egCfg.bMpe2Flag);
+		seq_puts(s, "\n");
+	}
+	if (!seq_has_overflowed(s))
+		pos++;
+	if (pos == 1)
+		return -1;
+
+	return pos;
+}
+
+int proc_common_start(void)
+{
+	return 0;
+}
+
+char *get_bp_member_string(int inst, u16 bp, char *buf)
+{
+	GSW_BRIDGE_portConfig_t bp_cfg;
+	int i, ret;
+	struct core_ops *gsw_handle;
+
+	gsw_handle = dp_port_prop[inst].ops[GSWIP_L];
+	if (!buf)
+		return NULL;
+	buf[0] = 0;
+	bp_cfg.nBridgePortId = bp;
+	bp_cfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP |
+		GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID;
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			   .BridgePort_ConfigGet, gsw_handle, &bp_cfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("Failed to get bridge port's member for bridgeport=%d\n",
+		       bp_cfg.nBridgePortId);
+		return buf;
+	}
+	for (i = 0; i < MAX_BP_NUM; i++)
+		if (GET_BP_MAP(bp_cfg.nBridgePortMap, i))
+			sprintf(buf + strlen(buf), "%d ", i);
+	sprintf(buf + strlen(buf), " Fid=%d ", bp_cfg.nBridgeId);
+	return buf;
+}
+
+static int proc_gsw_bp_dump(struct seq_file *s, int pos)
+{
+	pos = -1;
+	return pos;
+}
+
+/* proc_print_ctp_bp_info is an callback API, not a standalone proc API */
+int proc_print_ctp_bp_info(struct seq_file *s, int inst,
+			   struct pmac_port_info *port,
+			   int subif_index, u32 flag)
+{
+	struct logic_dev *tmp;
+	int bp = port->subif_info[subif_index].bp;
+	unsigned char *buf = kmalloc(MAX_BP_NUM * 5 + 1, GFP_KERNEL);
+
+	seq_printf(s, "          : bp=%d(member:%s)\n", bp,
+		   get_bp_member_string(inst, bp, buf));
+	list_for_each_entry(tmp, &port->subif_info[subif_index].logic_dev,
+			    list) {
+		seq_printf(s, "             %s: bp=%d(member:%s\n",
+			   tmp->dev->name, tmp->bp,
+			   get_bp_member_string(inst, tmp->bp, buf));
+	}
+
+	kfree(buf);
+	return 0;
+}
+
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_SWITCHDEV)
+static u32 br_hash_index;
+static struct br_info *brdev_info;
+static int proc_swdev_brctl_dump(struct seq_file *s, int pos)
+{
+	struct bridge_member_port *tmp = NULL;
+
+	while (!brdev_info) {
+		br_hash_index++;
+		pos = 0;
+		if (br_hash_index == BR_ID_ENTRY_HASH_TABLE_SIZE) {
+			seq_puts(s, "end\n");
+			return -1;
+		}
+		brdev_info = hlist_entry_safe
+			     ((&g_bridge_id_entry_hash_table
+			      [0][br_hash_index])->first,
+			      struct br_info, br_hlist);
+	}
+	seq_printf(s, "Hash=%u pos=%d dev=%s inst= fid=%d\n",
+		   br_hash_index,
+		   pos,
+		   brdev_info->br_device_name ? brdev_info->br_device_name
+		   : NULL,
+		   brdev_info->fid);
+	seq_puts(s, "  bp_list=");
+	list_for_each_entry(tmp, &brdev_info->bp_list, list) {
+		seq_printf(s, "%u ", tmp->portid);
+	}
+	seq_puts(s, "\n");
+	brdev_info = hlist_entry_safe((brdev_info)->br_hlist.next,
+				      struct br_info, br_hlist);
+	if (!seq_has_overflowed(s))
+		pos++;
+	return pos;
+}
+
+static int proc_swdev_brctl_start(void)
+{
+	br_hash_index = 0;
+
+	brdev_info = hlist_entry_safe((
+			&g_bridge_id_entry_hash_table[0][br_hash_index])->first,
+				struct br_info, br_hlist);
+	return 0;
+}
+
+static int print_bridge(int fid, int inst)
+{
+	int ret;
+	GSW_BRIDGE_config_t brcfg;
+	struct core_ops *gsw_handle;
+
+	gsw_handle = dp_port_prop[inst].ops[0];
+	memset(&brcfg, 0, sizeof(brcfg));
+	brcfg.nBridgeId = fid;
+	brcfg.eMask = GSW_BRIDGE_CONFIG_MASK_FORWARDING_MODE;
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdg_ops
+			   .Bridge_ConfigGet, gsw_handle, &brcfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("Failed to get bridge id(%d)\n", brcfg.nBridgeId);
+		return -1;
+	}
+	PR_INFO("eForwardBroadcast=:%d\r\n", brcfg.eForwardBroadcast);
+	PR_INFO("eForwardUnknownMulticastNonIp=:%d\r\n",
+		brcfg.eForwardUnknownMulticastNonIp);
+	PR_INFO("eForwardUnknownUnicast=:%d\r\n", brcfg.eForwardUnknownUnicast);
+	return 0;
+}
+
+static ssize_t proc_swdev_brctl_write(struct file *file,
+				      const char *buf,
+				      size_t count,
+				      loff_t *ppos)
+{
+	int len;
+	char str[64];
+	int num, ret, k;
+	char *param_list[10];
+	struct br_info *br_info = NULL;
+	struct bridge_member_port *temp_list = NULL;
+	GSW_BRIDGE_portConfig_t brportcfg;
+	unsigned char *buf1;
+	struct core_ops *gsw_handle;
+
+	gsw_handle = dp_port_prop[0].ops[0];
+	memset(&brportcfg, 0, sizeof(GSW_BRIDGE_portConfig_t));
+	len = (sizeof(str) > count) ? count : sizeof(str) - 1;
+	len -= copy_from_user(str, buf, len);
+	str[len] = 0;
+	num = dp_split_buffer(str, param_list, ARRAY_SIZE(param_list));
+
+	if ((num != 2) || (dp_strncmpi(param_list[0], "help", strlen("help")) == 0))
+		goto HELP;
+
+	buf1 = kmalloc(MAX_BP_NUM + 1, GFP_KERNEL);
+	if (!buf1)
+		return -1;
+	buf1[0] = 0;
+
+	if (dp_strncmpi(param_list[0], "brctl", strlen("brctl")) == 0) {
+		br_info = dp_swdev_bridge_entry_lookup(param_list[1],
+						       0);
+		if (br_info) {
+			print_bridge(br_info->fid, br_info->inst);
+			list_for_each_entry(temp_list,  &br_info->bp_list,
+					    list) {
+				PR_INFO("stored BP=:%d\r\n", temp_list->portid);
+				brportcfg.nBridgePortId = temp_list->portid;
+				brportcfg.eMask =
+				GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID |
+				GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
+				ret = gsw_core_api((dp_gsw_cb)gsw_handle
+						   ->gsw_brdgport_ops
+						   .BridgePort_ConfigGet,
+						   gsw_handle, &brportcfg);
+				if (ret != GSW_statusOk) {
+					PR_ERR("fail to get bport configed\n");
+					goto EXIT;
+				}
+				/*for (i = 0; i < MAX_BP_NUM; i++) {
+				 *	if (GET_BP_MAP(brportcfg.nBridgePortMap,
+				 *	    i)) {
+				 *		sprintf(buf1 + strlen(buf1),
+				 *			"%d ", i);
+				 *	}
+				 *}
+				 */
+				for (k = 0; k < 16; k++) {
+					PR_INFO("  nBridgePortMap[%d] = %x\n",
+						k, brportcfg.nBridgePortMap[k]);
+				}
+				sprintf(buf1 + strlen(buf1), " Fid=%d ",
+					brportcfg.nBridgeId);
+				PR_INFO("          : bp=%d(%s)\n",
+					brportcfg.nBridgePortId, buf1);
+				buf1[0] = 0;
+				}
+			}
+		}
+EXIT:
+	kfree(buf1);
+	return count;
+HELP:
+	PR_INFO("Provide brname: echo brctl brdev_name > /proc/dp/brctl\n");
+	return count;
+}
+
+static int fdb_cnt;
+static void proc_swdev_fdb_read(struct seq_file *s)
+{
+	struct fdb_tbl *tmp = NULL;
+
+	seq_puts(s, "dev_name		MAC\n");
+	seq_puts(s, "---------------------------------\n");
+	list_for_each_entry(tmp, &fdb_tbl_list, fdb_list) {
+		if (tmp) {
+			seq_printf
+			(s, "%s		%02x:%02x:%02x:%02x:%02x:%02x\n",
+			 tmp->port_dev->name, tmp->addr[0],
+			 tmp->addr[1], tmp->addr[2], tmp->addr[3],
+			 tmp->addr[4], tmp->addr[5]);
+		} else {
+			break;
+		}
+	}
+}
+
+static ssize_t proc_swdev_fdb_write(struct file *file, const char *buf,
+				    size_t count,
+				    loff_t *ppos)
+{
+	int len;
+	char str[64];
+	int num, flag = 0;
+	char *param_list[10];
+	struct fdb_tbl *tmp = NULL;
+	struct net_device *dev;
+	u8 b[6];
+
+	len = (sizeof(str) > count) ? count : sizeof(str) - 1;
+	len -= copy_from_user(str, buf, len);
+	str[len] = 0;
+	num = dp_split_buffer(str, param_list, ARRAY_SIZE(param_list));
+	if ((num != 5) || (dp_strncmpi(param_list[0], "help", strlen("help")) == 0))
+		goto HELP;
+
+	if (dp_strncmpi(param_list[0], "fdb", strlen("fdb")) == 0) {
+		if (dp_strncmpi(param_list[1], "add", strlen("add")) == 0) {
+			if (dp_strncmpi(param_list[2], "dev", strlen("dev")) == 0) {
+				/*fdb add <mac> dev <port>*/
+				tmp =
+				kmalloc(sizeof(struct fdb_tbl *), GFP_KERNEL);
+				if (!tmp)
+					goto exit;
+				INIT_LIST_HEAD(&tmp->fdb_list);
+				tmp->port_dev =
+				dev_get_by_name(&init_net, param_list[3]);
+				mac_stob(param_list[4], tmp->addr);
+				flag = 1;
+			}
+		}
+		if (dp_strncmpi(param_list[1], "del", strlen("del")) == 0) {
+			if (dp_strncmpi(param_list[2], "dev", strlen("dev")) == 0) {
+				/*fdb add dev <port> <mac>*/
+				dev = dev_get_by_name(&init_net, param_list[3]);
+				mac_stob(param_list[4], b);
+				flag = 2;
+			}
+		}
+	}
+	if (!fdb_cnt)
+		INIT_LIST_HEAD(&fdb_tbl_list);
+	if (list_empty(&fdb_tbl_list) && (fdb_cnt != 0))
+		goto exit;
+	if (fdb_cnt < 0)
+		goto exit;
+	if (flag == 1) {
+		list_add_tail(&tmp->fdb_list, &fdb_tbl_list);
+		fdb_cnt++;
+	}
+	if (flag == 2) {
+		list_for_each_entry(tmp, &fdb_tbl_list, fdb_list) {
+			if (memcmp(&tmp->addr, &b, 6) == 0) {
+				list_del(&tmp->fdb_list);
+				kfree(tmp);
+				fdb_cnt--;
+				break;
+			}
+		}
+	}
+	return count;
+exit:
+	PR_ERR("list add / count error %d\n", fdb_cnt);
+	return count;
+HELP:
+	PR_INFO("echo fdb add dev <dev_name> <mac_addr> > /proc/dp/fdb\n");
+	PR_INFO("echo fdb del dev <dev_name> <mac_addr> > /proc/dp/fdb\n");
+	return count;
+}
+
+static void proc_swdev_mac_read(struct seq_file *s)
+{
+	int ret, i;
+	//GSW_MAC_tableRead_t macread;
+	GSW_MAC_tableQuery_t macread;
+	struct core_ops *gsw_handle;
+	static int bp = 1;
+
+	gsw_handle = dp_port_prop[0].ops[0];
+	seq_puts(s, "FID    BP    static	MAC\n");
+	seq_puts(s, "---------------------------------\n");
+	for (i = 0; i < 10; i++) {
+	memset(&macread, 0, sizeof(macread));
+	macread.nFId = 1;
+	macread.nPortId = bp;
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_swmac_ops.
+			   //MAC_TableEntryRead, gsw_handle, &macread);
+			   MAC_TableEntryQuery, gsw_handle, &macread);
+	if (ret != GSW_statusOk) {
+		PR_ERR("Failed to get MAC entry\n");
+		//return -1;
+	}
+
+	seq_printf(s, "%d    %d    %d	%02x:%02x:%02x:%02x:%02x:%02x\n",
+		   macread.nFId, macread.nPortId, macread.bStaticEntry,
+		   macread.nMAC[0],
+		   macread.nMAC[1], macread.nMAC[2], macread.nMAC[3],
+		   macread.nMAC[4], macread.nMAC[5]);
+	bp += 1;
+	}
+}
+#endif
+
+static void pmac_eg_cfg(char *param_list[], int num, dp_pmac_cfg_t *pmac_cfg)
+{
+	int i, j;
+	u32 value;
+
+	for (i = 2; i < num; i += 2) {
+		for (j = 0; j < ARRAY_SIZE(egress_entries); j++) {
+			if (dp_strncmpi(param_list[i],
+				       egress_entries[j].name, strlen(egress_entries[j].name)))
+				continue;
+			if (dp_strncmpi(egress_entries[j].name,
+				       "rm_l2hdr", strlen("rm_l2hdr")) == 0) {
+				if (dp_atoi(param_list[i + 1]) > 0) {
+					pmac_cfg->eg_pmac.rm_l2hdr = 1;
+					value = dp_atoi(param_list[i + 1]);
+					egress_entries[j].
+					   egress_callback(pmac_cfg,
+							   value);
+					PR_INFO("egress ep %s configed ok\n",
+						egress_entries
+					     [j].name);
+					break;
+				}
+				pmac_cfg->eg_pmac.rm_l2hdr =
+				    dp_atoi(param_list[i + 1]);
+			} else {
+				value = dp_atoi(param_list[i + 1]);
+				egress_entries[j].
+				    egress_callback(pmac_cfg,
+						    value);
+				PR_INFO("egress pmac ep %s configured ok\n",
+					egress_entries[j].name);
+				break;
+			}
+		}
+	}
+}
+
+static ssize_t ep_port_write(struct file *file, const char *buf, size_t count,
+			     loff_t *ppos)
+{
+	int len;
+	char str[64];
+	int num, i, j, ret;
+	u32 value;
+	u32 port;
+	char *param_list[10];
+	dp_pmac_cfg_t pmac_cfg;
+	int inst = 0;
+
+	memset(&pmac_cfg, 0, sizeof(dp_pmac_cfg_t));
+	len = (sizeof(str) > count) ? count : sizeof(str) - 1;
+	len -= copy_from_user(str, buf, len);
+	str[len] = 0;
+	num = dp_split_buffer(str, param_list, ARRAY_SIZE(param_list));
+
+	if (dp_strncmpi(param_list[0], "ingress", strlen("ingress")) == 0) {
+		port = dp_atoi(param_list[1]);
+
+		for (i = 2; i < num; i += 2) {
+			for (j = 0; j < ARRAY_SIZE(ingress_entries); j++) {
+				if (dp_strncmpi
+				    (param_list[i],
+				     ingress_entries[j].name, strlen(ingress_entries[j].name)) == 0) {
+					value = dp_atoi(param_list[i + 1]);
+					ingress_entries[j].
+					    ingress_callback(&pmac_cfg,
+							     value);
+					PR_INFO("ingress pmac ep %s configed\n",
+						ingress_entries[j].name);
+					break;
+				}
+			}
+		}
+
+		ret = dp_pmac_set_31(inst, port, &pmac_cfg);
+
+		if (ret != 0) {
+			PR_INFO("pmac set configuration failed\n");
+			return -1;
+		}
+	} else if (dp_strncmpi(param_list[0], "egress", strlen("egress")) == 0) {
+		port = dp_atoi(param_list[1]);
+
+		pmac_eg_cfg(param_list, num, &pmac_cfg);
+		ret = dp_pmac_set_31(inst, port, &pmac_cfg);
+
+		if (ret != 0) {
+			PR_INFO("pmac set configuration failed\n");
+			return -1;
+		}
+	} else {
+		PR_INFO("wrong command\n");
+		goto help;
+	}
+
+	return count;
+ help:
+	PR_INFO("echo %s > /proc/dp/ep\n",
+		"ingress/egress [ep_port] ['ingress/egress fields'] [value]");
+	PR_INFO("(eg) echo ingress 1 pmac 1 > /proc/dp/ep\n");
+	PR_INFO("(eg) echo egress 1 rm_l2hdr 2 > /proc/dp/ep\n");
+	PR_INFO("echo %s %s",
+		"ingress [ep_port]",
+		"['errdisc/pmac/pmac_pmap/pmac_en_pmap/pmac_tc");
+	PR_INFO("                         %s [value] > /proc/dp/ep\n",
+		"/pmac_en_tc/pmac_subifid/pmac_srcport']");
+	PR_INFO("echo %s %s > /proc/dp/ep\n",
+		"egress [ep_port]",
+		"['rx_dmachan/fcs/pmac/res_dw1/res1_dw0/res2_dw0] [value]");
+	PR_INFO("echo egress [ep_port] ['rm_l2hdr'] [value] > /proc/dp/ep\n");
+	return count;
+}
+
+static struct dp_proc_entry dp_proc_entries[] = {
+	/*name single_callback_t multi_callback_t/_start write_callback_t */
+	{PROC_PARSER, proc_parser_read, NULL, NULL, proc_parser_write},
+	{PROC_RMON_PORTS, NULL, proc_gsw_port_rmon_dump,
+	 proc_gsw_rmon_port_start, proc_gsw_rmon_write},
+#ifdef CONFIG_LTQ_DATAPATH_MIB
+	{PROC_MIB_TIMER, proc_mib_timer_read, NULL, NULL,
+		proc_mib_timer_write},
+	{PROC_MIB_INSIDE, NULL, proc_mib_inside_dump,
+		proc_mib_inside_start, proc_mib_inside_write},
+	{PROC_MIBPORT, NULL, proc_mib_port_dump,
+		proc_mib_port_start, proc_mib_port_write},
+	{PROC_MIBVAP, NULL, proc_mib_vap_dump, proc_mib_vap_start,
+		proc_mib_vap_write},
+#endif
+#ifdef CONFIG_LTQ_DATAPATH_CPUFREQ
+	{PROC_COC, proc_coc_read, NULL, NULL, proc_coc_write},
+#endif
+	{PROC_EP, NULL, NULL, NULL, ep_port_write},
+	{PROC_PCE, NULL, proc_gsw_pce_dump, proc_gsw_pce_start, NULL},
+	{PROC_PMAC, NULL, proc_gsw_pmac_dump, proc_gsw_pmac_start,
+	 proc_gsw_pmac_write},
+	{DP_PROC_FILE_GSWIP_BP, NULL, proc_gsw_bp_dump,
+		proc_common_start, NULL},
+#if IS_ENABLED(CONFIG_LTQ_DATAPATH_SWITCHDEV)
+	{DP_PROC_FILE_SWDEV_BR, NULL, proc_swdev_brctl_dump,
+		proc_swdev_brctl_start, proc_swdev_brctl_write},
+	{DP_PROC_SWDEV_FDB, proc_swdev_fdb_read, NULL, NULL,
+		proc_swdev_fdb_write},
+	{DP_PROC_SWDEV_MAC, proc_swdev_mac_read, NULL, NULL,
+		NULL},
+#endif
+	{DP_PROC_CBMLOOKUP, NULL, lookup_dump31, lookup_start31,
+		proc_get_qid_via_index31},
+	{DP_MIB_Q, NULL, NULL, NULL, proc_qos_mib},
+
+	/*the last place holder */
+	{NULL, NULL, NULL, NULL, NULL}
+};
+
+int dp_sub_proc_install_31(void)
+{
+	int i;
+
+	if (!dp_proc_node) {
+		PR_ERR("dp_sub_proc_install failed\n");
+		return 0;
+	}
+
+	for (i = 0; i < ARRAY_SIZE(dp_proc_entries); i++)
+		dp_proc_entry_create(dp_proc_node, &dp_proc_entries[i]);
+	PR_INFO("dp_sub_proc_install ok\n");
+	return 0;
+}
+
+#define INVALID_DMA_CH 255
+char *get_dma_flags_str31(u32 epn, char *buf, int buf_len)
+{
+	char tmp[30]; /*must be static */
+	u32 flags;
+	u32 tx_ch, k;
+	u8 f_found;
+	int i;
+	int inst = 0;
+
+	if (!buf || (buf_len < 1))
+		return NULL;
+	tx_ch = 0;
+	flags = 0;
+	tmp[0] = '\0';
+	f_found = 0;
+	for (i = 0; i < ARRAY_SIZE(dp_port_info); i++) {
+		if ((dp_port_info[inst][i].flag_other &
+		    CBM_PORT_DMA_CHAN_SET) &&
+		    (dp_port_info[inst][i].deq_port_base == epn)) {
+			tx_ch = dp_port_info[inst][i].dma_chan;
+			break;
+		}
+	}
+	if (i >= ARRAY_SIZE(dp_port_info))
+		goto EXIT;
+	sprintf(tmp, "--");
+	if (tx_ch != INVALID_DMA_CH) {
+		if (!(flags & DP_F_FAST_DSL))/*DSLupstrem no DMA1/2 TX CHannel*/
+			sprintf(tmp + strlen(tmp), "CH%02d ", tx_ch);
+		else
+			sprintf(tmp + strlen(tmp), "CHXX ");
+	} else {
+		sprintf(tmp + strlen(tmp), "CHXX ");
+	}
+	if (flags == 0) {
+		if (epn < 4)
+			sprintf(tmp + strlen(tmp), "CPU");
+		else
+			sprintf(tmp + strlen(tmp), "Flag0");
+
+		goto EXIT;
+	}
+	for (k = 0; k < get_dp_port_type_str_size(); k++) {
+		if (flags & dp_port_flag[k]) {
+			sprintf(tmp + strlen(tmp), "%s ",
+				dp_port_type_str[k]);
+			f_found = 1;
+		}
+	}
+	if ((f_found == 1) &&
+	    (flags == DP_F_FAST_ETH_LAN)) { /*try to find its ep */
+		u32 i, num, j;
+		cbm_tmu_res_t *res;
+		struct pmac_port_info *port;
+
+		for (i = 3; i <= 4; i++) {	/*2 LAN port */
+			num = 0;
+			port = get_port_info(inst, i);
+			if (!port)
+				continue;
+			if (cbm_dp_port_resources_get
+			    (&i, &num, &res, port->alloc_flags))
+				continue;
+			for (j = 0; j < num; j++) {
+				if (res[j].tmu_port != epn)/* not match */
+					continue;
+				sprintf(tmp + strlen(tmp), "%d", i);
+			}
+			kfree(res);
+		}
+	}
+	if (!f_found)
+		sprintf(tmp + strlen(tmp), "Unknown[0x%x]\n", flags);
+
+ EXIT:
+	strncpy(buf, tmp, buf_len);
+	return buf;
+}
+EXPORT_SYMBOL(get_dma_flags_str31);
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_proc.h b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_proc.h
new file mode 100644
index 000000000000..9a8f011b115a
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_proc.h
@@ -0,0 +1,9 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_switchdev.c b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_switchdev.c
new file mode 100644
index 000000000000..a2073ba20b3e
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_switchdev.c
@@ -0,0 +1,389 @@
+/*
+ * Copyright (C) Intel Corporation
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/etherdevice.h>
+#include <net/datapath_api.h>
+#include "../datapath_swdev.h"
+#include "../datapath.h"
+#include "datapath_misc.h"
+
+int dp_swdev_alloc_bridge_id(int inst)
+{
+	GSW_return_t ret;
+	GSW_BRIDGE_alloc_t br;
+	struct core_ops *gsw_handle;
+
+	gsw_handle = dp_port_prop[inst].ops[0];
+	memset(&br, 0, sizeof(br));
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdg_ops.Bridge_Alloc,
+			   gsw_handle, &br);
+	if ((ret != GSW_statusOk) ||
+	    (br.nBridgeId < 0)) {
+		PR_ERR("Failed to get a FID\n");
+		return -1;
+	}
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "FID=%d\n", br.nBridgeId);
+	return br.nBridgeId;
+}
+
+int dp_set_gswip_mac_learning(GSW_BRIDGE_portConfig_t *bport)
+{
+	int i, cnt = 0;
+	/*check if there is atleast one non-cpu bridge port as member to
+	 *enable MAC learning.Ignore if CPU port is a member
+	 */
+	for (i = 0; i < MAX_BP_NUM; i++) {
+		if (i == CPU_BP)
+			continue;
+		if (GET_BP_MAP(bport->nBridgePortMap, i))
+			cnt++;
+	}
+	if (cnt < 1) {
+		bport->eMask |= GSW_BRIDGE_PORT_CONFIG_MASK_MC_SRC_MAC_LEARNING;
+		bport->bSrcMacLearningDisable = 1;
+	} else {
+		/*Enable src mac learning*/
+		bport->eMask |= GSW_BRIDGE_PORT_CONFIG_MASK_MC_SRC_MAC_LEARNING;
+		bport->bSrcMacLearningDisable = 0;
+	}
+	if (bport->bSrcMacLearningDisable)
+		DP_DEBUG(DP_DBG_FLAG_SWDEV,
+			 "MAC learning disable cnt:%d\n", cnt);
+	else
+		DP_DEBUG(DP_DBG_FLAG_SWDEV,
+			 "MAC learning enable cnt:%d\n", cnt);
+	return 0;
+}
+
+int dp_swdev_bridge_port_cfg_set(struct br_info *br_item,
+				 int inst, int bport)
+{
+	GSW_return_t ret;
+	struct bridge_member_port *bport_list = NULL;
+	GSW_BRIDGE_portConfig_t brportcfg;
+	struct core_ops *gsw_handle;
+
+	gsw_handle = dp_port_prop[inst].ops[0];
+	/*To set other members to the current bport*/
+	memset(&brportcfg, 0, sizeof(GSW_BRIDGE_portConfig_t));
+	brportcfg.nBridgePortId = bport;
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "Set current BP=%d inst:%d\n",
+		 brportcfg.nBridgePortId, inst);
+	brportcfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops.
+			   BridgePort_ConfigGet, gsw_handle, &brportcfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("fail in getting bridge port config\r\n");
+		return -1;
+	}
+	list_for_each_entry(bport_list, &br_item->bp_list, list) {
+		if (bport_list->portid != bport) {
+			SET_BP_MAP(brportcfg.nBridgePortMap,
+				   bport_list->portid);
+		}
+	}
+	brportcfg.nBridgeId = br_item->fid;
+	brportcfg.nBridgePortId = bport;
+	brportcfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID |
+		GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
+	dp_set_gswip_mac_learning(&brportcfg);
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops.
+			   BridgePort_ConfigSet, gsw_handle, &brportcfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("Fail in allocating/configuring bridge port\n");
+		return -1;
+	}
+	/* To set other member portmap with current bridge port map */
+	list_for_each_entry(bport_list, &br_item->bp_list, list) {
+		if (bport_list->portid != bport) {
+			memset(&brportcfg, 0,
+			       sizeof(GSW_BRIDGE_portConfig_t));
+			brportcfg.nBridgePortId = bport_list->portid;
+			DP_DEBUG(DP_DBG_FLAG_SWDEV,
+				 "Set other BP=%d inst:%d\n",
+				 brportcfg.nBridgePortId, inst);
+			brportcfg.eMask =
+				GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
+			ret = gsw_core_api((dp_gsw_cb)gsw_handle
+					->gsw_brdgport_ops
+					.BridgePort_ConfigGet,
+					gsw_handle, &brportcfg);
+			if (ret != GSW_statusOk) {
+				PR_ERR
+					("fail in getting br port config\r\n");
+				return -1;
+			}
+			SET_BP_MAP(brportcfg.nBridgePortMap, bport);
+			brportcfg.nBridgeId = br_item->fid;
+			brportcfg.nBridgePortId = bport_list->portid;
+			brportcfg.eMask =
+				GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID |
+				GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
+			dp_set_gswip_mac_learning(&brportcfg);
+			ret = gsw_core_api((dp_gsw_cb)gsw_handle
+					->gsw_brdgport_ops
+					.BridgePort_ConfigSet,
+					gsw_handle, &brportcfg);
+			if (ret != GSW_statusOk) {
+				PR_ERR("Fail alloc/cfg bridge port\n");
+				return -1;
+			}
+		}
+	}
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "%s successfully set\n", __func__);
+	return 0;
+}
+
+int dp_swdev_bridge_port_cfg_reset(struct br_info *br_item,
+				   int inst, int bport)
+{
+	GSW_BRIDGE_portConfig_t brportcfg;
+	struct bridge_member_port *bport_list = NULL;
+	int i, cnt = 0, bp = 0;
+	GSW_return_t ret;
+	struct core_ops *gsw_handle;
+
+	gsw_handle = dp_port_prop[inst].ops[0];
+	memset(&brportcfg, 0, sizeof(GSW_BRIDGE_portConfig_t));
+	brportcfg.nBridgePortId = bport;
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "Reset BP=%d inst:%d\n",
+		 brportcfg.nBridgePortId, inst);
+	brportcfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
+	/*Reset other members from current bport map*/
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			   .BridgePort_ConfigGet, gsw_handle, &brportcfg);
+	if (ret != GSW_statusOk) {
+		/* Note: here may fail if this device is not removed from
+		 * linux bridge via brctl delif but user try to un-regiser
+		 * from DP. The correct flow to unregister is like below:
+		 *  1) brctl delif xxx xxxx: remove this device from bridge
+		 *  2) dp_register_subif_ext: to un-register device from DP
+		 * Anyway, it will also work if not follow this propsal.
+		 * The only side effect is this API call fail since GSWIP
+		 * bridge port is already freed during subif_hw_reset before
+		 * this API call
+		 */
+		DP_DEBUG(DP_DBG_FLAG_SWDEV,
+			 "GSW_BRIDGE_portConfig_t fail:bp=%d\n", bport);
+		return -1;
+	}
+	for (i = 0; i < MAX_BP_NUM; i++) {
+		if (GET_BP_MAP(brportcfg.nBridgePortMap, i)) {
+			bp = i;
+			cnt++;
+		}
+	}
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "cnt:%d last bp:%d\n", cnt, bp);
+	list_for_each_entry(bport_list, &br_item->bp_list, list) {
+		if (bport_list->portid != bport) {
+			DP_DEBUG(DP_DBG_FLAG_SWDEV,
+				 "reset other from current BP=%d inst:%d\n",
+				 brportcfg.nBridgePortId, inst);
+			UNSET_BP_MAP(brportcfg.nBridgePortMap,
+				     bport_list->portid);
+		}
+	}
+	brportcfg.nBridgeId = CPU_FID; /*reset of FID*/
+	brportcfg.nBridgePortId = bport;
+	brportcfg.eMask = GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID |
+			  GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
+	dp_set_gswip_mac_learning(&brportcfg);
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdgport_ops
+			   .BridgePort_ConfigSet, gsw_handle, &brportcfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("Fail in configuring GSW_BRIDGE_portConfig_t in %s\r\n",
+		       __func__);
+		return -1;
+	}
+	/*Reset current bp from all other bridge port's port map*/
+	list_for_each_entry(bport_list, &br_item->bp_list, list) {
+		if (bport_list->portid != bport) {
+			memset(&brportcfg, 0,
+			       sizeof(GSW_BRIDGE_portConfig_t));
+			brportcfg.nBridgePortId = bport_list->portid;
+			DP_DEBUG(DP_DBG_FLAG_SWDEV,
+				 "reset current BP from other BP=%d inst:%d\n",
+				 brportcfg.nBridgePortId, inst);
+			brportcfg.eMask =
+				 GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP |
+				 GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID;
+			ret = gsw_core_api((dp_gsw_cb)gsw_handle
+					 ->gsw_brdgport_ops
+					 .BridgePort_ConfigGet,
+					 gsw_handle, &brportcfg);
+			if (ret != GSW_statusOk) {
+				PR_ERR("failed getting br port cfg\r\n");
+				return -1;
+			}
+			UNSET_BP_MAP(brportcfg.nBridgePortMap, bport);
+			//brportcfg.nBridgeId = br_item->fid;
+			brportcfg.nBridgePortId = bport_list->portid;
+			brportcfg.eMask =
+				 GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_ID |
+				 GSW_BRIDGE_PORT_CONFIG_MASK_BRIDGE_PORT_MAP;
+			dp_set_gswip_mac_learning(&brportcfg);
+			ret = gsw_core_api((dp_gsw_cb)gsw_handle
+					 ->gsw_brdgport_ops
+					 .BridgePort_ConfigSet,
+					 gsw_handle, &brportcfg);
+			if (ret != GSW_statusOk) {
+				PR_ERR("Fail alloc/cfg br port\n");
+				return -1;
+			}
+		}
+	}
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "%s success\n", __func__);
+	/*Remove bridge entry if no member in port map of
+	 * current bport except CPU port
+	 */
+	if ((bp == 0 && cnt == 1) || (cnt == 0))
+		return DEL_BRENTRY;
+
+	return 0;
+}
+
+int dp_swdev_bridge_cfg_set(int inst, u16 fid)
+{
+	GSW_return_t ret;
+	GSW_BRIDGE_config_t brcfg;
+	GSW_BRIDGE_alloc_t br;
+	struct core_ops *gsw_handle;
+
+	gsw_handle = dp_port_prop[inst].ops[0];
+	memset(&br, 0, sizeof(br));
+	memset(&brcfg, 0, sizeof(brcfg));
+	brcfg.nBridgeId = fid;
+	brcfg.eMask = GSW_BRIDGE_CONFIG_MASK_FORWARDING_MODE;
+	brcfg.eForwardBroadcast = GSW_BRIDGE_FORWARD_FLOOD;
+	brcfg.eForwardUnknownMulticastNonIp = GSW_BRIDGE_FORWARD_FLOOD;
+	brcfg.eForwardUnknownUnicast = GSW_BRIDGE_FORWARD_FLOOD;
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdg_ops
+			   .Bridge_ConfigSet, gsw_handle, &brcfg);
+	if (ret != GSW_statusOk) {
+		PR_ERR("Failed to set bridge id(%d)\n", brcfg.nBridgeId);
+		br.nBridgeId = fid;
+		gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdg_ops.Bridge_Free,
+			     gsw_handle, &br);
+		return -1;
+	}
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "FID(%d) cfg success for inst %d\n",
+		 fid, inst);
+	return 0;
+}
+
+int dp_swdev_free_brcfg(int inst, u16 fid)
+{
+	GSW_return_t ret;
+	GSW_BRIDGE_alloc_t br;
+	struct core_ops *gsw_handle;
+
+	gsw_handle = dp_port_prop[inst].ops[0];
+	memset(&br, 0, sizeof(br));
+	br.nBridgeId = fid;
+	ret = gsw_core_api((dp_gsw_cb)gsw_handle->gsw_brdg_ops.Bridge_Free,
+			   gsw_handle, &br);
+	if (ret != GSW_statusOk) {
+		PR_ERR("Failed to free bridge id(%d)\n", br.nBridgeId);
+		return -1;
+	}
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "FID(%d) freed for inst:%d\n", fid, inst);
+	return 0;
+}
+
+int dp_gswip_ext_vlan(int inst, int vap, int ep)
+{
+	struct core_ops *gsw_handle;
+	struct ext_vlan_info *vlan;
+	struct vlan_prop vlan_prop = {0};
+	struct pmac_port_info *port;
+	struct logic_dev *tmp = NULL;
+	int flag = 0, ret, i = 0;
+	int v1 = 0, v2 = 0;
+
+	gsw_handle = dp_port_prop[inst].ops[0];
+	port = &dp_port_info[inst][ep];
+	vlan = kzalloc(sizeof(*vlan), GFP_KERNEL);
+	if (!vlan) {
+		PR_ERR("failed to alloc ext_vlan of %d bytes\n", sizeof(*vlan));
+		return 0;
+	}
+	vlan->vlan2_list = kzalloc(sizeof(*vlan->vlan2_list), GFP_KERNEL);
+	if (!vlan->vlan2_list) {
+		PR_ERR("failed to alloc ext_vlan of %d bytes\n",
+		       sizeof(*vlan->vlan2_list));
+		goto EXIT;
+	}
+	vlan->vlan1_list = kzalloc(sizeof(*vlan->vlan1_list), GFP_KERNEL);
+	if (!vlan->vlan1_list) {
+		PR_ERR("failed to alloc ext_vlan of %d bytes\n",
+		       sizeof(*vlan->vlan1_list));
+		goto EXIT;
+	}
+	list_for_each_entry(tmp, &dp_port_info[inst][ep].
+			    subif_info[vap].logic_dev, list) {
+		DP_DEBUG(DP_DBG_FLAG_SWDEV, "tmp dev name:%s\n",
+			 tmp->dev ? tmp->dev->name : "NULL");
+		if (!tmp->dev) {
+			PR_ERR("tmp->dev is NULL\n");
+			goto EXIT;
+		}
+		get_vlan_via_dev(tmp->dev, &vlan_prop);
+		if (vlan_prop.num == 2) {
+			DP_DEBUG(DP_DBG_FLAG_SWDEV,
+				 "VLAN Inner proto=%x, vid=%d\n",
+				 vlan_prop.in_proto, vlan_prop.in_vid);
+			DP_DEBUG(DP_DBG_FLAG_SWDEV,
+				 "VLAN out proto=%x, vid=%d\n",
+				 vlan_prop.out_proto, vlan_prop.out_vid);
+			vlan->vlan2_list[v2].outer_vlan.vid = vlan_prop.out_vid;
+			vlan->vlan2_list[v2].outer_vlan.tpid =
+							vlan_prop.out_proto;
+			vlan->vlan2_list[v2].ether_type = 0;
+			vlan->vlan2_list[v2].inner_vlan.vid = vlan_prop.in_vid;
+			vlan->vlan2_list[v2].inner_vlan.tpid =
+							vlan_prop.in_proto;
+			vlan->vlan2_list[v2].bp = tmp->bp;
+			v2 += 1;
+		} else if (vlan_prop.num == 1) {
+			DP_DEBUG(DP_DBG_FLAG_SWDEV,
+				 "outer VLAN proto=%x, vid=%d\n",
+				 vlan_prop.out_proto, vlan_prop.out_vid);
+			vlan->vlan1_list[v1].outer_vlan.vid = vlan_prop.out_vid;
+			vlan->vlan1_list[v1].outer_vlan.tpid =
+							vlan_prop.out_proto;
+			vlan->vlan1_list[v1].bp = tmp->bp;
+			v1 += 1;
+		}
+		i += 1;
+	}
+	DP_DEBUG(DP_DBG_FLAG_SWDEV, "vlan1=%d vlan2=%d total vlan int=%d\n",
+		 v1, v2, i);
+	vlan->n_vlan1 = v1;
+	vlan->n_vlan2 = v2;
+	vlan->bp = port->subif_info[vap].bp;
+	vlan->logic_port = port->port_id;
+	vlan->subif_grp = port->subif_info[vap].subif;/*subif value*/
+
+	if (port->subif_info[vap].swdev_priv)
+		vlan->priv = port->subif_info[vap].swdev_priv;
+	else
+		vlan->priv = NULL;
+	ret = set_gswip_ext_vlan(gsw_handle, vlan, flag);
+	if (ret == 0)
+		port->subif_info[vap].swdev_priv = vlan->priv;
+	else
+		PR_ERR("set gswip ext vlan return error\n");
+
+EXIT:
+	kfree(vlan->vlan2_list);
+	kfree(vlan->vlan1_list);
+	kfree(vlan);
+	return 0; /*return -EIO from GSWIP but later cannot fail swdev*/
+}
diff --git a/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_switchdev.h b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_switchdev.h
new file mode 100644
index 000000000000..d93e823e34d3
--- /dev/null
+++ b/drivers/net/ethernet/lantiq/datapath/gswip31/datapath_switchdev.h
@@ -0,0 +1,20 @@
+/*
+ * Copyright (C) Intel Corporation
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+
+#ifndef DATAPATH_SWITCHDEV_H_
+#define DATAPATH_SWITCHDEV_H_
+
+int dp_swdev_alloc_bridge_id(int inst);
+int dp_swdev_bridge_port_cfg_set(struct br_info *br_item,
+				 int inst, int bport);
+int dp_swdev_bridge_port_cfg_reset(struct br_info *br_item,
+				   int inst, int bport);
+int dp_swdev_bridge_cfg_set(int inst, u16 fid);
+int dp_swdev_free_brcfg(int inst, u16 fid);
+int dp_gswip_ext_vlan(int inst, int vap, int ep);
+#endif
diff --git a/include/net/datapath_api_gswip31.h b/include/net/datapath_api_gswip31.h
new file mode 100644
index 000000000000..efb4b7ba9355
--- /dev/null
+++ b/include/net/datapath_api_gswip31.h
@@ -0,0 +1,319 @@
+/*
+ * Copyright (C) Intel Corporation
+ * Author: Shao Guohua <guohua.shao@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ */
+#ifndef DATAPATH_API_FALCONMX_H
+#define DATAPATH_API_FALCONMX_H
+
+#ifdef CONFIG_LITTLE_ENDIAN
+struct dma_rx_desc_0 {
+	/* DWORD 0 */
+	union {
+		struct {
+			u32 dest_sub_if_id:15;
+			u32 eth_type:2;
+			u32 flow_id:8;
+			u32 tunnel_id:4;
+			u32 resv:2;
+			u32 redir:1;
+		} __packed field;
+		u32 all;
+	};
+} __packed;
+#else /*big endian */
+struct dma_rx_desc_0 {
+	/* DWORD 0 */
+	union {
+		struct {
+			u32 redir:1;
+			u32 resv:2;
+			u32 tunnel_id:4;
+			u32 flow_id:8;
+			u32 eth_type:2;
+			u32 dest_sub_if_id:15;
+		} __packed field;
+		u32 all;
+	};
+} __packed;
+#endif
+
+#ifdef CONFIG_LITTLE_ENDIAN
+struct dma_rx_desc_1 {
+	/* DWORD 1 */
+	union {
+		struct {
+			u32 classid:4;
+			u32 ip:4;
+			u32 ep:4;
+			u32 color:2;
+			u32 mpe1:1;
+			u32 mpe2:1;
+			u32 enc:1;
+			u32 dec:1;
+			u32 nat:1;
+			u32 tcp_err:1;
+			u32 session_id:12;
+		} __packed field;
+		u32 all;
+	};
+} __packed;
+#else /*big endian */
+struct dma_rx_desc_1 {
+	/* DWORD 1 */
+	union {
+		struct {
+			u32 session_id:12;
+			u32 tcp_err:1;
+			u32 nat:1;
+			u32 dec:1;
+			u32 enc:1;
+			u32 mpe2:1;
+			u32 mpe1:1;
+			u32 color:2;
+			u32 ep:4;
+			u32 ip:4;
+			u32 classid:4;
+		} __packed field;
+		u32 all;
+	};
+} __packed;
+#endif
+
+#ifdef CONFIG_LITTLE_ENDIAN
+struct dma_rx_desc_2 {
+	/*DWORD 2 */
+	union {
+		struct {
+			u32 data_ptr;
+		} __packed field;
+		u32 all;
+	};
+
+} __packed;
+#else /*big endian */
+struct dma_rx_desc_2 {
+	/*DWORD 2 */
+	union {
+		struct {
+			u32 data_ptr;
+		} __packed field;
+		u32 all;
+	};
+} __packed;
+#endif
+
+#ifdef CONFIG_LITTLE_ENDIAN
+struct dma_rx_desc_3 {
+	/*DWORD 3 */
+	union {
+		struct {
+			u32 data_len:16;
+			u32 pool:3;
+			u32 res:1;
+			u32 policy:3;
+			u32 byte_offset:3;
+			u32 pdu_type:1;
+			u32 dic:1;
+			u32 eop:1;
+			u32 sop:1;
+			u32 c:1;
+			u32 own:1;
+		} __packed field;
+		u32 all;
+	};
+} __packed;
+#else /*big endian */
+struct dma_rx_desc_3 {
+	/*DWORD 3 */
+	union {
+		struct {
+			u32 own:1;
+			u32 c:1;
+			u32 sop:1;
+			u32 eop:1;
+			u32 dic:1;
+			u32 pdu_type:1;
+			u32 byte_offset:3;
+			u32 policy:3;
+			u32 res:1;
+			u32 pool:3;
+			u32 data_len:16;
+		} __packed field;
+		u32 all;
+	};
+} __packed;
+#endif
+
+#define dma_tx_desc_0 dma_rx_desc_0
+#define dma_tx_desc_1 dma_rx_desc_1
+#define dma_tx_desc_2 dma_rx_desc_2
+#define dma_tx_desc_3 dma_rx_desc_3
+
+#ifdef CONFIG_CPU_BIG_ENDIAN
+/*Note:pmac normally not DWORD aligned. Most time 2 bytes aligment */
+struct pmac_rx_hdr { /*Egress PMAC header*/
+	/*byte 0 */
+	u8 res0:1;
+	u8 ver_done:1;
+	u8 ip_offset:6;
+
+	/*byte 1 */
+	u8 tcp_h_offset:5;
+	u8 tcp_type:3;
+
+	/*byte 2 */
+	u8 res2:4;
+	u8 class:4;
+
+	/*byte 3 */
+	u8 res31:1;
+	u8 oam:1;
+	u8 res32:2;
+	u8 ins:1;
+	u8 ext:1;
+	u8 pkt_type:2;
+
+	/*byte 4 */
+	u8 res4:1;
+	u8 one_step:1;
+	u8 ptp:1;
+	u8 src_dst_subif_id_msb:5;
+
+	/*byte 5 */
+	u8 src_dst_subif_id_lsb:8;
+
+	/*byte 6 */
+	u8 record_id_msb:8;
+
+	/*byte 7 */
+	u8 record_id_lsb:4;
+	u8 igp_egp:4;
+} __packed;
+#else /*little endian */
+struct pmac_rx_hdr { /*Egress PMAC header*/
+	/*byte 0 */
+	u8 ip_offset:6;
+	u8 ver_done:1;
+	u8 res0:1;
+
+	/*byte 1 */
+	u8 tcp_type:3;
+	u8 tcp_h_offset:5;
+
+	/*byte 2 */
+	u8 class:4;
+	u8 res2:4;
+
+	/*byte 3 */
+	u8 pkt_type:2;
+	u8 ext:1;
+	u8 ins:1;
+	u8 res32:2;
+	u8 oam:1;
+	u8 res31:1;
+
+	/*byte 4 */
+	u8 src_dst_subif_id_msb:5;
+	u8 ptp:1;
+	u8 one_step:1;
+	u8 res4:1;
+
+	/*byte 5 */
+	u8 src_dst_subif_id_lsb:8;
+
+	/*byte 6 */
+	u8 record_id_msb:8;
+
+	/*byte 7 */
+	u8 igp_egp:4;
+	u8 record_id_lsb:4;
+} __packed;
+#endif
+
+#ifdef CONFIG_LITTLE_ENDIAN
+struct pmac_tx_hdr { /*Ingress PMAC header*/
+	/*byte 0 */
+	u8 tcp_chksum:1;
+	u8 res1:1;
+	u8 ip_offset:6;
+
+	/*byte 1 */
+	u8 tcp_h_offset:5;
+	u8 tcp_type:3;
+
+	/*byte 2 */
+	u8 igp_msb:4; /*gswip30: sppid */
+	u8 res2:4;
+
+	/*byte 3 */
+	u8 res3:1;
+	u8 oam:1;
+	u8 lrnmd:1;
+	u8 class_en:1;/*TC Enable*/
+	u8 ins:1;
+	u8 ext:1;
+	u8 pkt_type:2;
+
+	/*byte 4 */
+	u8 fcs_ins_dis:1;
+	u8 one_step:1;
+	u8 ptp:1;
+	u8 src_dst_subif_id_msb:5;
+
+	/*byte 5 */
+	u8 src_dst_subif_id_lsb:8;
+
+	/*byte 6 */
+	u8 record_id_msb:8;
+
+	/*byte 7 */
+	u8 record_id_lsb:4;
+	u8 igp_egp:4;
+} __packed;
+#else /*big endian */
+struct pmac_tx_hdr { /*Ingress PMAC header*/
+	/*byte 0 */
+	u8 ip_offset:6;
+	u8 res1:1;
+	u8 tcp_chksum:1;
+
+	/*byte 1 */
+	u8 tcp_type:3;
+	u8 tcp_h_offset:5;
+
+	/*byte 2 */
+	u8 res2:4;
+	u8 igp_msb:4;
+
+	/*byte 3 */
+	u8 pkt_type:2; /* refer to PMAC_TCP_TYPE */
+	u8 ext:1;
+	u8 ins:1;
+	u8 class_en:1; /*TC Enable*/
+	u8 lrnmd:1;
+	u8 oam:1;
+	u8 res3:1;
+
+	/*byte 4 */
+	u8 src_dst_subif_id_msb:5;
+	u8 ptp:1;
+	u8 one_step:1;
+	u8 fcs_ins_dis:1;
+
+	/*byte 5 */
+	u8 src_dst_subif_id_lsb:8;
+
+	/*byte 6 */
+	u8 record_id_msb:8;
+
+	/*byte 7 */
+	u8 igp_egp:4;
+	u8 record_id_lsb:4;
+} __packed;
+#endif
+
+#endif /*DATAPATH_API_FALCONMX_H*/
