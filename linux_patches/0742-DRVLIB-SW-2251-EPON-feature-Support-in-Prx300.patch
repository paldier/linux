From 27525f3bf3d9d846aacc06cb3fcd804a2671f5cd Mon Sep 17 00:00:00 2001
From: Rekha Eswaran <rekha.eswaran@intel.com>
Date: Fri, 17 May 2019 16:07:50 +0800
Subject: [PATCH] DRVLIB_SW-2251: EPON feature Support in Prx300

    - EPON continuous Q api,DP register, de-register subif changes in DPM
    - DRV_DPM_SW-1: EPON workaround to support QID for upstream traffic via pmapper in DPM
    - Fix for PONIP base addr and num QID index mapping in CQM Driver
    - EPON Pmac Subifid setting corrected
---
 drivers/net/datapath/dpm/datapath.h                |  21 +-
 drivers/net/datapath/dpm/datapath_api.c            | 163 ++++--
 drivers/net/datapath/dpm/datapath_proc.c           |  50 +-
 drivers/net/datapath/dpm/gswip30/datapath_misc.c   |   2 +-
 drivers/net/datapath/dpm/gswip31/datapath_gswip.c  |   2 +-
 drivers/net/datapath/dpm/gswip31/datapath_misc.c   | 632 +++++++++++----------
 drivers/net/datapath/dpm/gswip31/datapath_misc.h   |   4 +-
 drivers/net/datapath/dpm/gswip31/datapath_ppv4.c   |   2 +-
 .../net/datapath/dpm/gswip31/datapath_ppv4_api.c   | 148 ++++-
 drivers/net/datapath/dpm/gswip31/datapath_tx.c     |  14 +-
 drivers/net/ethernet/lantiq/cqm/prx300/cqm.c       |   4 +-
 include/net/datapath_api.h                         |  83 ++-
 12 files changed, 701 insertions(+), 424 deletions(-)

diff --git a/drivers/net/datapath/dpm/datapath.h b/drivers/net/datapath/dpm/datapath.h
index 07c222016433..b742a5ee64fb 100644
--- a/drivers/net/datapath/dpm/datapath.h
+++ b/drivers/net/datapath/dpm/datapath.h
@@ -170,6 +170,7 @@
 #define DP_MAX_DMA_CHAN 64
 /* maximum dma controller*/
 #define DP_DMAMAX 7
+#define DP_DEQ(p, q) (dp_deq_port_tbl[p][q])
 
 enum dp_xmit_errors {
 	DP_XMIT_ERR_DEFAULT = 0,
@@ -410,11 +411,21 @@ struct dp_subif_info {
 #if IS_ENABLED(CONFIG_NET_SWITCHDEV)
 	void *swdev_priv; /*to store ext vlan info*/
 #endif
-	s16 qid;    /* physical queue id */
+	u8 num_qid; /*!< number of queue id*/
+	u8 deq_port_idx; /* To store deq port index from register_subif */
+	union {
+		s16 qid;    /* physical queue id Still keep it to be
+			     * back-compatible for legacy platform and legacy
+			     * integration
+			     */
+		s16 qid_list[DP_MAX_DEQ_PER_SUBIF];/* physical queue id */
+	};
 	s16 sched_id; /* can be physical scheduler id or logical node id */
-	s16 q_node; /* logical queue node Id if applicable */
-	s16 qos_deq_port; /* qos port id */
-	s16 cqm_deq_port; /* CQM physical dequeue port ID (absolute) */
+	s16 q_node[DP_MAX_DEQ_PER_SUBIF]; /* logical Q node Id if applicable */
+	s16 qos_deq_port[DP_MAX_DEQ_PER_SUBIF]; /* qos port id */
+	s16 cqm_deq_port[DP_MAX_DEQ_PER_SUBIF]; /* CQM physical dequeue port ID
+						 * (absolute)
+						 */
 	s16 cqm_port_idx; /* CQM relative dequeue port index, like tconf id */
 	u32 subif_flag; /* To store original flag from caller during
 			 * dp_register_subif
@@ -522,6 +533,8 @@ struct pmac_port_info {
 	u32 num_dma_chan; /*For G.INT it's 8 or 16, for other 1*/
 	u32 lct_idx; /* LCT subif register flag */
 	u32 dma_ch_base; /*! Base entry index of dp_dma_chan_tbl */
+	u32 res_qid_base; /* Base entry for the device's reserved Q */
+	u32 num_resv_q; /* Num of reserved Q per device */
 #if IS_ENABLED(CONFIG_INTEL_DATAPATH_PTP1588)
 	u32 f_ptp:1; /* PTP1588 support enablement */
 #endif
diff --git a/drivers/net/datapath/dpm/datapath_api.c b/drivers/net/datapath/dpm/datapath_api.c
index 804affbee1e9..43cf9edc2126 100644
--- a/drivers/net/datapath/dpm/datapath_api.c
+++ b/drivers/net/datapath/dpm/datapath_api.c
@@ -530,11 +530,14 @@ int32_t dp_register_subif_private(int inst, struct module *owner,
 {
 	int res = DP_FAILURE;
 
-	int i, port_id, start, end;
+	int i, port_id, start, end, j;
 	struct pmac_port_info *port_info;
 	struct cbm_dp_en_data cbm_data = {0};
 	struct subif_platform_data platfrm_data = {0};
 	struct dp_subif_info *sif;
+	u32 cqm_deq_port;
+	u32 dma_ch_offset;
+	u32 dma_ch_ref, dma_ch_ref_curr;
 
 	port_id = subif_id->port_id;
 	port_info = get_dp_port_info(inst, port_id);
@@ -582,13 +585,16 @@ int32_t dp_register_subif_private(int inst, struct module *owner,
 	/*PR_INFO("search range: start=%d end=%d\n",start, end);*/
     /*allocate a free subif */
 	for (i = start; i < end; i++) {
-		u32 cqm_deq_port;
-		u32 dma_ch_offset;
-
 		sif = get_dp_port_subif(port_info, i);
 		if (sif->flags) /*used already & not free*/
 			continue;
 
+		if (data->num_deq_port == 0)
+			data->num_deq_port = 1;
+		cqm_deq_port = port_info->deq_port_base + data->deq_port_idx;
+		dma_ch_offset =	DP_DEQ(inst, cqm_deq_port).dma_ch_offset;
+		dma_ch_ref_curr = atomic_read(&(dp_dma_chan_tbl[inst] +
+					      dma_ch_offset)->ref_cnt);
 		/*now find a free subif or valid subif
 		 *need to do configuration HW
 		 */
@@ -605,13 +611,14 @@ int32_t dp_register_subif_private(int inst, struct module *owner,
 			PR_ERR("port info status fail for 0\n");
 			return res;
 		}
-		cqm_deq_port = sif->cqm_deq_port;
-		dma_ch_offset =
-			dp_deq_port_tbl[inst][cqm_deq_port].dma_ch_offset;
-
 		sif->flags = 1;
 		sif->netif = dev;
 		port_info->port_id = port_id;
+		/*currently this field is used for EPON case. Later can further
+		 * enhance
+		 */
+		sif->num_qid = data->num_deq_port;
+		sif->deq_port_idx = data->deq_port_idx;
 
 		if (subif_id->subif < 0) /*dynamic:shift bits as HW defined*/
 			sif->subif =
@@ -634,8 +641,6 @@ int32_t dp_register_subif_private(int inst, struct module *owner,
 		port_info->num_subif++;
 		if ((port_info->num_subif == 1) ||
 		    (platfrm_data.act & TRIGGER_CQE_DP_ENABLE)) {
-			u32 dma_ch_ref;
-
 			cbm_data.dp_inst = inst;
 			cbm_data.num_dma_chan = port_info->num_dma_chan;
 			cbm_data.cbm_inst = dp_port_prop[inst].cbm_inst;
@@ -651,24 +656,39 @@ int32_t dp_register_subif_private(int inst, struct module *owner,
 				PR_ERR("dp_dma_chan_tbl[%d] NULL\n", inst);
 				return res;
 			}
-			dma_ch_ref = atomic_read(&(dp_dma_chan_tbl[inst] +
+			for (j = 0; j < data->num_deq_port; j++) {
+				cqm_deq_port = sif->cqm_deq_port[j];
+				dma_ch_offset =
+					DP_DEQ(inst, cqm_deq_port).dma_ch_offset;
+				dma_ch_ref =
+					atomic_read(&(dp_dma_chan_tbl[inst] +
 						 dma_ch_offset)->ref_cnt);
-			/* PPA Directpath/LitePath don't have DMA CH */
-			if (dma_ch_ref == 1 &&
-			    !(port_info->alloc_flags & DP_F_DIRECT))
-				cbm_data.dma_chnl_init = 1; /*to enable DMA*/
-			DP_DEBUG(DP_DBG_FLAG_REG, "%s:%s%d %s%d %s%d  %s%d\n",
-				 "cbm_dp_enable",
-				 "dp_port=", port_id,
-				 "deq_port=", cbm_data.deq_port,
-				 "dma_chnl_init=", cbm_data.dma_chnl_init,
-				 "tx_dma_chan ref=%d\n",
-				 dma_ch_ref);
-			if (cbm_dp_enable(owner, port_id, &cbm_data, 0,
-					  port_info->alloc_flags)) {
-				DP_DEBUG(DP_DBG_FLAG_REG,
-					 "cbm_dp_enable fail\n");
-				return res;
+				cbm_data.deq_port = port_info->deq_port_base +
+							data->deq_port_idx + j;
+				/* No need to enable DMA if multiple DEQ->single
+				 * DMA
+				 */
+				if (j != 0)
+					cbm_data.dma_chnl_init = 0;
+				else
+				/* PPA Directpath/LitePath don't have DMA CH */
+					if (dma_ch_ref_curr == 0 &&
+					    !(port_info->alloc_flags &
+					    DP_F_DIRECT))
+						/*to enable DMA*/
+						cbm_data.dma_chnl_init = 1;
+				DP_DEBUG(DP_DBG_FLAG_REG, "%s%d%s%d%s%d%s%d%d\n",
+					 "cbm_dp_enable: dp_port=", port_id,
+					 " deq_port=", cbm_data.deq_port,
+					 " dma_chnl_init=",
+					 cbm_data.dma_chnl_init,
+					 " tx_dma_chan ref=", dma_ch_ref, dma_ch_ref_curr);
+				if (cbm_dp_enable(owner, port_id, &cbm_data, 0,
+						  port_info->alloc_flags)) {
+					DP_DEBUG(DP_DBG_FLAG_REG,
+						 "cbm_dp_enable fail\n");
+					return res;
+				}
 			}
 			DP_DEBUG(DP_DBG_FLAG_REG, "cbm_dp_enable ok\n");
 		} else {
@@ -702,12 +722,13 @@ int32_t dp_deregister_subif_private(int inst, struct module *owner,
 				    uint32_t flags)
 {
 	int res = DP_FAILURE;
-	int i, port_id, cqm_port, bp;
+	int i, j, port_id, cqm_port, bp;
 	u8 find = 0;
 	struct pmac_port_info *port_info;
 	struct cbm_dp_en_data cbm_data = {0};
 	struct subif_platform_data platfrm_data = {0};
 	struct dp_subif_info *sif;
+	u32 dma_ch_offset, dma_ch_ref;
 
 	port_id = subif_id->port_id;
 	port_info = get_dp_port_info(inst, port_id);
@@ -757,7 +778,6 @@ int32_t dp_deregister_subif_private(int inst, struct module *owner,
 			 subif_name);
 		return res;
 	}
-	cqm_port = sif->cqm_deq_port;
 	bp = sif->bp;
 	/* reset mib, flag, and others */
 	memset(&sif->mib, 0, sizeof(sif->mib));
@@ -771,35 +791,43 @@ int32_t dp_deregister_subif_private(int inst, struct module *owner,
 	}
 	if (!port_info->num_subif)
 		port_info->status = PORT_DEV_REGISTERED;
-
-	if (!dp_deq_port_tbl[inst][cqm_port].ref_cnt) {
-		/*delete all queues which may created by PPA or other apps*/
-		struct dp_node_alloc port_node;
-
-		port_node.inst = inst;
-		port_node.dp_port = port_id;
-		port_node.type = DP_NODE_PORT;
-		port_node.id.cqm_deq_port = cqm_port;
-		dp_node_children_free(&port_node, 0);
-		/*disable cqm port */
-		cbm_data.dp_inst = inst;
-		cbm_data.cbm_inst = dp_port_prop[inst].cbm_inst;
-		cbm_data.deq_port = cqm_port;
-		/* PPA Directpath/LitePath don't have DMA CH */
-		if (!(port_info->alloc_flags & DP_F_DIRECT))
-			cbm_data.dma_chnl_init = 1; /*to disable DMA */
-		if (cbm_dp_enable(owner, port_id, &cbm_data,
-				  CBM_PORT_F_DISABLE, port_info->alloc_flags)) {
+	for (j = 0; j < sif->num_qid; j++) {
+		cqm_port = sif->cqm_deq_port[j];
+		if (!dp_deq_port_tbl[inst][cqm_port].ref_cnt) {
+			/*delete all queues which may created
+			 * by PPA or other apps
+			 */
+			struct dp_node_alloc port_node;
+
+			port_node.inst = inst;
+			port_node.dp_port = port_id;
+			port_node.type = DP_NODE_PORT;
+			port_node.id.cqm_deq_port = cqm_port;
+			dp_node_children_free(&port_node, 0);
+			/*disable cqm port */
+			cbm_data.dp_inst = inst;
+			cbm_data.cbm_inst = dp_port_prop[inst].cbm_inst;
+			cbm_data.deq_port = cqm_port;
+			dma_ch_offset =	DP_DEQ(inst, cqm_port).dma_ch_offset;
+			dma_ch_ref = atomic_read(&(dp_dma_chan_tbl[inst] +
+						 dma_ch_offset)->ref_cnt);
+			/* PPA Directpath/LitePath don't have DMA CH */
+			if ((!(port_info->alloc_flags & DP_F_DIRECT)) &&
+			    (!dma_ch_ref))
+				cbm_data.dma_chnl_init = 1; /*to disable DMA */
+			if (cbm_dp_enable(owner, port_id, &cbm_data,
+					  CBM_PORT_F_DISABLE,
+					  port_info->alloc_flags)) {
+				DP_DEBUG(DP_DBG_FLAG_REG,
+					 "cbm_dp_disable fail:port=%d subix=%d %s=%d\n",
+					 port_id, i, "dma_chnl_init",
+					 cbm_data.dma_chnl_init);
+				return res;
+			}
 			DP_DEBUG(DP_DBG_FLAG_REG,
-				 "cbm_dp_disable fail:port=%d subix=%d %s=%d\n",
-				 port_id, i,
-				 "dma_chnl_init", cbm_data.dma_chnl_init);
-
-			return res;
+				 "cbm_dp_disable ok:port=%d subix=%d cqm_port=%d\n",
+				 port_id, i, cqm_port);
 		}
-		DP_DEBUG(DP_DBG_FLAG_REG,
-			 "cbm_dp_disable ok:port=%d subix=%d cqm_port=%d\n",
-			 port_id, i, cqm_port);
 	}
 	/* for pmapper and non-pmapper both
 	 *  1)for PRX300, dev is managed at its HAL level
@@ -977,7 +1005,6 @@ int32_t dp_register_dev_ext(int inst, struct module *owner, uint32_t port_id,
 		return res;
 	}
 
-	/*register a device */
 	if (port_info->status != PORT_ALLOCATED) {
 		DP_DEBUG(DP_DBG_FLAG_REG,
 			 "No de-register for %s for unknown status:%d\n",
@@ -994,6 +1021,8 @@ int32_t dp_register_dev_ext(int inst, struct module *owner, uint32_t port_id,
 	}
 
 	DP_CB(inst, dev_platform_set)(inst, port_id, data, flags);
+	port_info->res_qid_base = data->qos_resv_q_base;
+	port_info->num_resv_q = data->num_resv_q;
 
 	cbm_data = kzalloc(sizeof(*cbm_data), GFP_ATOMIC);
 	if (!cbm_data) {
@@ -1036,7 +1065,6 @@ int32_t dp_register_dev_ext(int inst, struct module *owner, uint32_t port_id,
 	port_info->status = PORT_DEV_REGISTERED;
 	if (dp_cb)
 		port_info->cb = *dp_cb;
-
 	DP_LIB_UNLOCK(&dp_lock);
 	kfree(cbm_data);
 
@@ -1319,7 +1347,9 @@ int32_t dp_get_netif_subifid_priv(struct net_device *netif, struct sk_buff *skb,
 				bport = sif->bp;
 				subif->flag_bp = 0;
 				gpid = sif->gpid;
-				subif->def_qid = sif->qid;
+				subif->num_q = sif->num_qid;
+				memcpy(subif->def_qlist, sif->qid_list,
+				       sizeof(sif->qid_list));
 				num++;
 				break;
 			}
@@ -1345,7 +1375,9 @@ int32_t dp_get_netif_subifid_priv(struct net_device *netif, struct sk_buff *skb,
 					 */
 					subifs[num] = sif->subif;
 					gpid = sif->gpid;
-					subif->def_qid = sif->qid;
+					subif->num_q = sif->num_qid;
+					memcpy(subif->def_qlist, sif->qid_list,
+					       sizeof(sif->qid_list));
 					subif_flag[num] = sif->subif_flag;
 					if (sif->ctp_dev)
 						subif->flag_pmapper = 1;
@@ -1368,7 +1400,9 @@ int32_t dp_get_netif_subifid_priv(struct net_device *netif, struct sk_buff *skb,
 					subif->port_id = k;
 					subif->bport = tmp->bp;
 					subif->gpid = sif->gpid;
-					subif->def_qid = sif->qid;
+					subif->num_q = sif->num_qid;
+					memcpy(subif->def_qlist, sif->qid_list,
+					       sizeof(sif->qid_list));
 					res = 0;
 					/*note: logical device no callback */
 					goto EXIT;
@@ -1444,6 +1478,9 @@ static int dp_build_cqm_data(int inst, uint32_t port_id,
 		       sizeof(struct dp_rx_ring));
 	}
 
+	cbm_data->num_qid = data->num_resv_q;
+	cbm_data->qid_base = data->qos_resv_q_base;
+
 	return 0;
 }
 
@@ -1882,7 +1919,7 @@ int dp_set_pmapper(struct net_device *dev, struct dp_pmapper *mapper, u32 flag)
 	}
 	/* get the subif from the dev */
 	ret = dp_get_netif_subifid(dev, NULL, NULL, NULL, &subif, 0);
-	if ((ret == DP_FAILURE) || (subif.flag_pmapper == 0)) {
+	if (ret == DP_FAILURE) {
 		PR_ERR("Fail to get subif:dev=%s ret=%d flag_pmap=%d bp=%d\n",
 		       dev->name, ret, subif.flag_pmapper, subif.bport);
 		return DP_FAILURE;
@@ -1968,7 +2005,7 @@ int dp_get_pmapper(struct net_device *dev, struct dp_pmapper *mapper, u32 flag)
 
 	/*get the subif from the dev*/
 	ret = dp_get_netif_subifid(dev, NULL, NULL, NULL, &subif, 0);
-	if (ret == DP_FAILURE || subif.flag_pmapper == 0) {
+	if (ret == DP_FAILURE) {
 		PR_ERR("Can not get the subif from the dev\n");
 		return DP_FAILURE;
 	}
diff --git a/drivers/net/datapath/dpm/datapath_proc.c b/drivers/net/datapath/dpm/datapath_proc.c
index 6c0dd30e6ffb..9fc05c1177b5 100644
--- a/drivers/net/datapath/dpm/datapath_proc.c
+++ b/drivers/net/datapath/dpm/datapath_proc.c
@@ -156,10 +156,10 @@ int proc_port_dump(struct seq_file *s, int pos)
 				print_ctp_bp(s, tmp_inst, port, 0, 0);
 			seq_printf(s, "           qid/node:       %d/%d\n",
 				   sif->qid,
-				   sif->q_node);
-			seq_printf(s, "           port/node:      %d/%d\n",
-				   sif->cqm_deq_port,
-				   sif->qos_deq_port);
+				   sif->q_node[0]);
+			seq_printf(s, "           port/node:    %d/%d\n",
+				   sif->cqm_deq_port[0],
+				   sif->qos_deq_port[0]);
 		} else
 			seq_printf(s,
 				   "%02d: rx_err_drop=0x%08x  tx_err_drop=0x%08x\n",
@@ -209,6 +209,8 @@ int proc_port_dump(struct seq_file *s, int pos)
 	seq_printf(s, "    vap_offset/mask:   %d/0x%x\n", port->vap_offset,
 		   port->vap_mask);
 	seq_printf(s, "    flag_other:        0x%x\n", port->flag_other);
+	seq_printf(s, "    resv_queue:        %d\n", port->num_resv_q);
+	seq_printf(s, "    resv_queue_base:   %d\n", port->res_qid_base);
 	seq_printf(s, "    deq_port_base:     %d\n", port->deq_port_base);
 	seq_printf(s, "    deq_port_num:      %d\n", port->deq_port_num);
 	seq_printf(s, "    num_dma_chan:      %d\n", port->num_dma_chan);
@@ -270,25 +272,33 @@ int proc_port_dump(struct seq_file *s, int pos)
 			   STATS_GET(mib->tx_hdr_room_pkt));
 		if (print_ctp_bp)
 			print_ctp_bp(s, tmp_inst, port, i, 0);
-		seq_printf(s, "           qid/node:       %d/%d\n",
-			   sif->qid, sif->q_node);
-		cqm_p = sif->cqm_deq_port;
-		seq_printf(s, "           port/node:      %d/%d(ref=%d)\n",
-			   cqm_p, sif->qos_deq_port,
-			   dp_deq_port_tbl[tmp_inst][cqm_p].ref_cnt);
-		seq_printf(s, "           mac_learn_dis:  %d\n",
-			   sif->mac_learn_dis);
-		cid = _DMA_CONTROLLER(
-				dp_deq_port_tbl[tmp_inst][cqm_p].dma_chan);
-		pid = _DMA_PORT(dp_deq_port_tbl[tmp_inst][cqm_p].dma_chan);
-		nid = _DMA_CHANNEL(dp_deq_port_tbl[tmp_inst][cqm_p].dma_chan);
-		dma_ch_offset = dp_deq_port_tbl[tmp_inst][cqm_p].dma_ch_offset;
-		if (port->num_dma_chan && dp_dma_chan_tbl[tmp_inst]) {
-			dma = dp_dma_chan_tbl[tmp_inst] + dma_ch_offset;
-			seq_printf(s, "           tx_dma_ch:      0x%x(ref=%d,dma-ctrl=%d,port=%d,channel=%d)\n",
+		seq_printf(s, "           subif_qid=%d\n", sif->num_qid);
+		seq_printf(s, "           dqport_idx=%d\n", sif->cqm_port_idx);
+		for (j = 0; j < sif->num_qid; j++) {
+			seq_printf(s, "           [%02d]qid/node:    %d/%d\n",
+				   j, sif->qid_list[j],
+				   sif->q_node[j]);
+			cqm_p = sif->cqm_deq_port[j];
+			seq_printf(s, "           port/node:    %d/%d(ref=%d)\n",
+				   cqm_p, sif->qos_deq_port[j],
+				   dp_deq_port_tbl[tmp_inst][cqm_p].ref_cnt);
+
+		        cid = _DMA_CONTROLLER(
+				    dp_deq_port_tbl[tmp_inst][cqm_p].dma_chan);
+		        pid = _DMA_PORT(dp_deq_port_tbl[tmp_inst][cqm_p].dma_chan);
+		        nid = _DMA_CHANNEL(dp_deq_port_tbl[tmp_inst][cqm_p].dma_chan);
+		        dma_ch_offset = dp_deq_port_tbl[tmp_inst][cqm_p].dma_ch_offset;
+		        if (port->num_dma_chan && dp_dma_chan_tbl[tmp_inst]) {
+			        dma = dp_dma_chan_tbl[tmp_inst] + dma_ch_offset;
+			        seq_printf(s, "           tx_dma_ch:      0x%x(ref=%d,dma-ctrl=%d,port=%d,channel=%d)\n",
 				   dp_deq_port_tbl[tmp_inst][cqm_p].dma_chan,
 				   atomic_read(&dma->ref_cnt), cid, pid, nid);
+		        }
+
 		}
+		seq_printf(s, "           mac_learn_dis:    %d\n",
+			   sif->mac_learn_dis);
+
 		seq_printf(s, "           gpid:           %d\n", sif->gpid);
 		seq_puts(s, "           ctp_dev:        ");
 		if (sif->ctp_dev && sif->ctp_dev->name)
diff --git a/drivers/net/datapath/dpm/gswip30/datapath_misc.c b/drivers/net/datapath/dpm/gswip30/datapath_misc.c
index 3a1693c74169..4a964382dc68 100644
--- a/drivers/net/datapath/dpm/gswip30/datapath_misc.c
+++ b/drivers/net/datapath/dpm/gswip30/datapath_misc.c
@@ -533,7 +533,7 @@ static int subif_hw_set(int inst, int portid, int subif_ix,
 	}
 	cqe_deq = port_info->deq_port_base + deq_port_idx;
 	dma_ch_offset = dp_deq_port_tbl[inst][cqe_deq].dma_ch_offset;
-	get_dp_port_subif(port_info, subif_ix)->cqm_deq_port = cqe_deq;
+	get_dp_port_subif(port_info, subif_ix)->cqm_deq_port[0] = cqe_deq;
 	dp_deq_port_tbl[inst][cqe_deq].ref_cnt++;
 	if (port_info->num_dma_chan)
 		atomic_inc(&(dp_dma_chan_tbl[inst] + dma_ch_offset)->ref_cnt);
diff --git a/drivers/net/datapath/dpm/gswip31/datapath_gswip.c b/drivers/net/datapath/dpm/gswip31/datapath_gswip.c
index a09f9ece2102..8d5958a0aac9 100644
--- a/drivers/net/datapath/dpm/gswip31/datapath_gswip.c
+++ b/drivers/net/datapath/dpm/gswip31/datapath_gswip.c
@@ -46,7 +46,7 @@ static struct ctp_assign ctp_assign_info[] = {
 	/*note: multiple flags must put first */
 	{DP_F_CPU, GSW_LOGICAL_PORT_8BIT_WLAN, 16, 16, 8, 0xF, CQE_LU_MODE0, 8},
 	{DP_F_GPON, GSW_LOGICAL_PORT_GPON, 256, 256, 0, 0xFF, CQE_LU_MODE1, 1},
-	{DP_F_EPON, GSW_LOGICAL_PORT_EPON, 256, 256, 0, 0xFF, CQE_LU_MODE1, 1},
+	{DP_F_EPON, GSW_LOGICAL_PORT_EPON, 64, 64, 0, 0xFF, CQE_LU_MODE1, 1},
 	{DP_F_GINT, GSW_LOGICAL_PORT_GINT, 16, 16, 0, 0xFF, CQE_LU_MODE1, 1},
 	{DP_F_FAST_ETH_WAN, GSW_LOGICAL_PORT_8BIT_WLAN, 8, 8, 8, 0xF,
 		CQE_LU_MODE2, 1},
diff --git a/drivers/net/datapath/dpm/gswip31/datapath_misc.c b/drivers/net/datapath/dpm/gswip31/datapath_misc.c
index cdbfcb8cb814..a304b5650224 100644
--- a/drivers/net/datapath/dpm/gswip31/datapath_misc.c
+++ b/drivers/net/datapath/dpm/gswip31/datapath_misc.c
@@ -1127,9 +1127,9 @@ int dp_platform_queue_set(int inst, u32 flag)
 		port_info->deq_port_num++;
 		sif = get_dp_port_subif(port_info, i);
 		sif->qid = q_port.qid;
-		sif->q_node = q_port.q_node;
-		sif->qos_deq_port = q_port.port_node;
-		sif->cqm_deq_port = q_port.cqe_deq;
+		sif->q_node[0] = q_port.q_node;
+		sif->qos_deq_port[0] = q_port.port_node;
+		sif->cqm_deq_port[0] = q_port.cqe_deq;
 		if (!f_cpu_q) {
 			f_cpu_q = 1;
 			/*Map all CPU port's lookup to its 1st queue only */
@@ -1337,6 +1337,7 @@ static int dev_platform_set(int inst, u8 ep, struct dp_dev_data *data,
 	itf = ctp_port_assign(inst, ep, priv->bp_def, flags, data);
 	/*reset_gsw_itf(ep); */
 	get_dp_port_info(inst, ep)->itf_info = itf;
+	dp_node_reserve(inst, ep, data, flags);
 	return 0;
 }
 
@@ -1355,11 +1356,6 @@ static int port_platform_set(int inst, u8 ep, struct dp_port_data *data,
 		return DP_FAILURE;
 	}
 	set_port_lookup_mode_31(inst, ep, flags);
-	if (flags & DP_F_DEREGISTER) {
-		dp_node_reserve(inst, ep, NULL, flags);
-		return 0;
-	}
-
 	DP_DEBUG(DP_DBG_FLAG_QOS, "priv=%p deq_port_stat=%p qdev=%p\n",
 		 priv,
 		 priv ? priv->deq_port_stat : NULL,
@@ -1402,7 +1398,6 @@ static int port_platform_set(int inst, u8 ep, struct dp_port_data *data,
 			   CBM_QUEUE_MAP_F_MPE1_DONTCARE |
 			   CBM_QUEUE_MAP_F_MPE2_DONTCARE);
 
-	dp_node_reserve(inst, ep, data, flags);
 	dp_port_spl_cfg(inst, ep, data, flags);
 #if IS_ENABLED(CONFIG_INTEL_DATAPATH_DBG)
 	if (DP_DBG_FLAG_QOS & dp_dbg_flag) {
@@ -1419,11 +1414,14 @@ static int port_platform_set(int inst, u8 ep, struct dp_port_data *data,
 	return 0;
 }
 
-static int set_ctp_bp(int inst, int ctp, int portid, int bp)
+static int set_ctp_bp(int inst, int ctp, int portid, int bp,
+		      struct subif_platform_data *data)
 {
 	GSW_CTP_portConfig_t tmp;
 	struct core_ops *gsw_handle;
+	struct pmac_port_info *port_info;
 
+	port_info = get_dp_port_info(inst, portid);
 	gsw_handle = dp_port_prop[inst].ops[GSWIP_L];
 	memset(&tmp, 0, sizeof(tmp));
 	tmp.nLogicalPortId = portid;
@@ -1444,7 +1442,11 @@ static int reset_ctp_bp(int inst, int ctp, int portid, int bp)
 {
 	GSW_CTP_portConfig_t tmp;
 	struct core_ops *gsw_handle;
+	struct pmac_port_info *port_info;
+	struct dp_subif_info *sif;
 
+	port_info = get_dp_port_info(inst, portid);
+	sif = get_dp_port_subif(port_info, ctp);
 	gsw_handle = dp_port_prop[inst].ops[GSWIP_L];
 	memset(&tmp, 0, sizeof(tmp));
 	tmp.nLogicalPortId = portid;
@@ -1489,14 +1491,8 @@ static int subif_hw_set(int inst, int portid, int subif_ix,
 {
 	struct ppv4_q_sch_port q_port = {0};
 	static cbm_queue_map_entry_t lookup = {0};
-	u32 lookup_f = CBM_QUEUE_MAP_F_FLOWID_L_DONTCARE |
-		CBM_QUEUE_MAP_F_FLOWID_H_DONTCARE |
-		CBM_QUEUE_MAP_F_EN_DONTCARE |
-		CBM_QUEUE_MAP_F_DE_DONTCARE |
-		CBM_QUEUE_MAP_F_MPE1_DONTCARE |
-		CBM_QUEUE_MAP_F_MPE2_DONTCARE |
-		CBM_QUEUE_MAP_F_TC_DONTCARE;
-	int subif, deq_port_idx = 0, bp = -1;
+	u32 lookup_f;
+	int subif, deq_port_idx = 0, bp = -1, i = 0;
 	int dma_ch_offset = 0;
 	struct pmac_port_info *port_info;
 	struct dp_subif_info *sif;
@@ -1543,9 +1539,9 @@ static int subif_hw_set(int inst, int portid, int subif_ix,
 	sif->bp = bp;
 	if (port_info->ctp_max == 1) {
 		if (port_info->num_subif == 0)
-			set_ctp_bp(inst, 0, portid, sif->bp);
+			set_ctp_bp(inst, 0, portid, sif->bp, data);
 	} else {
-		set_ctp_bp(inst, subif_ix, portid, sif->bp);
+		set_ctp_bp(inst, subif_ix, portid, sif->bp, data);
 	}
 	data->act = 0;
 	if (flags & DP_F_SUBIF_LOGICAL) {
@@ -1576,6 +1572,12 @@ static int subif_hw_set(int inst, int portid, int subif_ix,
 	}
 	/*QUEUE_CFG if needed */
 	q_port.cqe_deq = port_info->deq_port_base + deq_port_idx;
+	if ((data->subif_data->flag_ops & DP_SUBIF_DEQPORT_NUM) &&
+	    (data->subif_data->num_deq_port > DP_MAX_DEQ_PER_SUBIF)) {
+		DP_ERR("deq_port(%d), cannot be more than max Q per subif %d\n",
+		       data->subif_data->num_deq_port, DP_MAX_DEQ_PER_SUBIF);
+		return -1;
+	}
 	if (!priv) {
 		PR_ERR("priv NULL\n");
 		return -1;
@@ -1591,199 +1593,227 @@ static int subif_hw_set(int inst, int portid, int subif_ix,
 			 q_port.cqe_deq, inst);
 	}
 #endif
-	q_port.tx_pkt_credit =
-		dp_deq_port_tbl[inst][q_port.cqe_deq].tx_pkt_credit;
-	q_port.tx_ring_addr =
-		(u32)dp_deq_port_tbl[inst][q_port.cqe_deq].txpush_addr_qos;
-	q_port.tx_ring_size =
-		dp_deq_port_tbl[inst][q_port.cqe_deq].tx_ring_size;
-	q_port.inst = inst;
-	q_port.dp_port = portid;
-	q_port.ctp = subif_ix;
-
-	dma_ch_offset = dp_deq_port_tbl[inst][q_port.cqe_deq].dma_ch_offset;
-	if (data->subif_data->flag_ops & DP_SUBIF_SPECIFIC_Q) {
-		q_flag = DP_SUBIF_SPECIFIC_Q;
-	} else if (data->subif_data->flag_ops & DP_SUBIF_AUTO_NEW_Q) {
-		q_flag = DP_SUBIF_AUTO_NEW_Q;
-	}  else { /*sharing mode (default)*/
-		if (!dp_deq_port_tbl[inst][q_port.cqe_deq].f_first_qid)
-			q_flag = DP_SUBIF_AUTO_NEW_Q; /*no queue created yet*/
-	}
-	DP_DEBUG(DP_DBG_FLAG_QOS, "Queue decision:%s\n", q_flag_str(q_flag));
-	if (q_flag == DP_SUBIF_AUTO_NEW_Q) {
+	for (i = 0; i < data->subif_data->num_deq_port; i++) {
 		int cqe_deq;
 
-		if (alloc_q_to_port(&q_port, 0)) {
-			PR_ERR("alloc_q_to_port fail for dp_port=%d\n",
-			       q_port.dp_port);
-			return -1;
-		}
-		if (dp_q_tbl[inst][q_port.qid].flag) {
-			PR_ERR("Why dp_q_tbl[%d][%d].flag =%d:expect 0?\n",
-			       inst, q_port.qid,
-			       dp_q_tbl[inst][q_port.qid].flag);
-			return -1;
-		}
-		if (dp_q_tbl[inst][q_port.qid].ref_cnt) {
-			PR_ERR("Why dp_q_tbl[%d][%d].ref_cnt =%d:expect 0?\n",
-			       inst, q_port.qid,
-			       dp_q_tbl[inst][q_port.qid].ref_cnt);
-			return -1;
-		}
-		/*update queue table */
-		dp_q_tbl[inst][q_port.qid].flag = 1;
-		dp_q_tbl[inst][q_port.qid].need_free = 1;
-		dp_q_tbl[inst][q_port.qid].ref_cnt = 1;
-		dp_q_tbl[inst][q_port.qid].q_node_id = q_port.q_node;
-		dp_q_tbl[inst][q_port.qid].cqm_dequeue_port = q_port.cqe_deq;
-
-		/*update port table */
+		lookup_f = CBM_QUEUE_MAP_F_FLOWID_L_DONTCARE |
+			CBM_QUEUE_MAP_F_FLOWID_H_DONTCARE |
+			CBM_QUEUE_MAP_F_EN_DONTCARE |
+			CBM_QUEUE_MAP_F_DE_DONTCARE |
+			CBM_QUEUE_MAP_F_MPE1_DONTCARE |
+			CBM_QUEUE_MAP_F_MPE2_DONTCARE |
+			CBM_QUEUE_MAP_F_TC_DONTCARE;
+		q_port.cqe_deq = port_info->deq_port_base + deq_port_idx + i;
 		cqe_deq = q_port.cqe_deq;
-		dp_deq_port_tbl[inst][cqe_deq].ref_cnt++;
-		if (port_info->num_dma_chan)
-			atomic_inc(&(dp_dma_chan_tbl[inst] +
-				   dma_ch_offset)->ref_cnt);
-		dp_deq_port_tbl[inst][cqe_deq].qos_port = q_port.port_node;
-		if (!dp_deq_port_tbl[inst][cqe_deq].f_first_qid) {
-			dp_deq_port_tbl[inst][cqe_deq].first_qid = q_port.qid;
-			dp_deq_port_tbl[inst][cqe_deq].f_first_qid = 1;
-			DP_DEBUG(DP_DBG_FLAG_QOS,
-				 "dp_deq_port_tbl[%d][%d].first_qid=%d\n",
-				 inst, q_port.cqe_deq,
-				 dp_deq_port_tbl[inst][cqe_deq].first_qid);
-		}
-		/*update scheduler table later */
 
-	} else if (q_flag == DP_SUBIF_SPECIFIC_Q) { /*specified queue */
-		if (!dp_q_tbl[inst][q_port.qid].flag) {
-			/*1st time to use it
-			 *In this case, normally this queue is created by caller
-			 */
+		q_port.tx_pkt_credit =
+			dp_deq_port_tbl[inst][q_port.cqe_deq].tx_pkt_credit;
+		q_port.tx_ring_addr =
+		(u32)dp_deq_port_tbl[inst][q_port.cqe_deq].txpush_addr_qos;
+		q_port.tx_ring_size =
+			dp_deq_port_tbl[inst][q_port.cqe_deq].tx_ring_size;
+		q_port.inst = inst;
+		q_port.dp_port = portid;
+		q_port.ctp = subif_ix;
+
+		dma_ch_offset = dp_deq_port_tbl[inst][q_port.cqe_deq].
+								dma_ch_offset;
+		if (data->subif_data->flag_ops & DP_SUBIF_SPECIFIC_Q) {
+			q_flag = DP_SUBIF_SPECIFIC_Q;
+		} else if (data->subif_data->flag_ops & DP_SUBIF_AUTO_NEW_Q) {
+			q_flag = DP_SUBIF_AUTO_NEW_Q;
+		}  else { /*sharing mode (default)*/
+			if (!dp_deq_port_tbl[inst][q_port.cqe_deq].f_first_qid)
+				q_flag = DP_SUBIF_AUTO_NEW_Q;/*no Q create yet*/
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS, "Queue decision:%s\n",
+			 q_flag_str(q_flag));
+		if (q_flag == DP_SUBIF_AUTO_NEW_Q) {
+			if (alloc_q_to_port(&q_port, 0)) {
+				PR_ERR("alloc_q_to_port fail for dp_port=%d\n",
+				       q_port.dp_port);
+				return -1;
+			}
+			if (dp_q_tbl[inst][q_port.qid].flag) {
+				PR_ERR("Why dp_q_tbl[%d][%d].flag =%d%s?\n",
+				       inst, q_port.qid,
+				       dp_q_tbl[inst][q_port.qid].flag,
+				       ":expect 0");
+				return -1;
+			}
+			if (dp_q_tbl[inst][q_port.qid].ref_cnt) {
+				PR_ERR("Why dp_q_tbl[%d][%d].ref_cnt =%d%s?\n",
+				       inst, q_port.qid,
+				       dp_q_tbl[inst][q_port.qid].ref_cnt,
+				       ":expect 0");
+				return -1;
+			}
+			/*update queue table */
 			dp_q_tbl[inst][q_port.qid].flag = 1;
-			dp_q_tbl[inst][q_port.qid].need_free = 0; /*caller q*/
+			dp_q_tbl[inst][q_port.qid].need_free = 1;
 			dp_q_tbl[inst][q_port.qid].ref_cnt = 1;
-
-			/*update port table
-			 *Note: since this queue is created by caller itself
-			 *      we need find way to get cqm_dequeue_port
-			 *      and qos_port later
-			 */
-			/* need set cqm_dequeue_port/qos_port since not fully
-			 * tested
-			 */
+			dp_q_tbl[inst][q_port.qid].q_node_id = q_port.q_node;
 			dp_q_tbl[inst][q_port.qid].cqm_dequeue_port =
 				q_port.cqe_deq;
-			dp_deq_port_tbl[inst][q_port.cqe_deq].qos_port = -1;
-			dp_deq_port_tbl[inst][q_port.cqe_deq].ref_cnt++;
-			if (port_info->num_dma_chan)
-				atomic_inc(&(dp_dma_chan_tbl[inst] +
-					   dma_ch_offset)->ref_cnt);
-		} else {
-			/*note: don't change need_free in this case */
-			dp_q_tbl[inst][q_port.cqe_deq].ref_cnt++;
-			dp_deq_port_tbl[inst][q_port.cqe_deq].ref_cnt++;
+
+			/*update port table */
+			dp_deq_port_tbl[inst][cqe_deq].ref_cnt++;
 			if (port_info->num_dma_chan)
 				atomic_inc(&(dp_dma_chan_tbl[inst] +
-					   dma_ch_offset)->ref_cnt);
-		}
+						dma_ch_offset)->ref_cnt);
+			dp_deq_port_tbl[inst][cqe_deq].qos_port =
+				q_port.port_node;
+			if (!dp_deq_port_tbl[inst][cqe_deq].f_first_qid) {
+				dp_deq_port_tbl[inst][cqe_deq].first_qid =
+					q_port.qid;
+				dp_deq_port_tbl[inst][cqe_deq].f_first_qid = 1;
+				DP_DEBUG(DP_DBG_FLAG_QOS,
+					 "dp_deq_port_tbl[%d][%d].first_qid=%d\n",
+					 inst, q_port.cqe_deq,
+					 dp_deq_port_tbl[inst][cqe_deq].
+					 first_qid);
+			}
+			/*update scheduler table later */
+
+		} else if (q_flag == DP_SUBIF_SPECIFIC_Q) { /*specified queue */
+			if (!dp_q_tbl[inst][q_port.qid].flag) {
+				/*1st time to use it
+				 *In this case, normally this queue
+				 *is created by caller
+				 */
+				dp_q_tbl[inst][q_port.qid].flag = 1;
+				/*caller q*/
+				dp_q_tbl[inst][q_port.qid].need_free = 0;
+				dp_q_tbl[inst][q_port.qid].ref_cnt = 1;
+
+				/*update port table
+				 *Note: since this queue is created
+				 *by caller itself
+				 *      we need find way to get cqm_dequeue_port
+				 *      and qos_port later
+				 */
+				/* need set cqm_dequeue_port/qos_port
+				 *since not fully tested
+				 */
+				dp_q_tbl[inst][q_port.qid].cqm_dequeue_port =
+					q_port.cqe_deq;
+				dp_deq_port_tbl[inst][q_port.cqe_deq].qos_port = -1;
+				dp_deq_port_tbl[inst][q_port.cqe_deq].ref_cnt++;
+				if (port_info->num_dma_chan)
+					atomic_inc(&(dp_dma_chan_tbl[inst] +
+						   dma_ch_offset)->ref_cnt);
+			} else {
+				/*note: don't change need_free in this case */
+				dp_q_tbl[inst][q_port.cqe_deq].ref_cnt++;
+				dp_deq_port_tbl[inst][q_port.cqe_deq].ref_cnt++;
+				if (port_info->num_dma_chan)
+					atomic_inc(&(dp_dma_chan_tbl[inst] +
+						   dma_ch_offset)->ref_cnt);
+			}
 
-		/*get already stored q_node_id/qos_port id to q_port
-		 */
-		q_port.q_node = dp_q_tbl[inst][q_port.qid].q_node_id;
-		q_port.port_node =
-			dp_deq_port_tbl[inst][q_port.cqe_deq].qos_port;
+			/*get already stored q_node_id/qos_port id to q_port
+			 */
+			q_port.q_node = dp_q_tbl[inst][q_port.qid].q_node_id;
+			q_port.port_node =
+				dp_deq_port_tbl[inst][q_port.cqe_deq].qos_port;
 
-		/* need to further set q_port.q_node/port_node
-		 * via special internal QOS HAL API to get it
-		 * since it is created by caller itself\n");
-		 */
+			/* need to further set q_port.q_node/port_node
+			 * via special internal QOS HAL API to get it
+			 * since it is created by caller itself\n");
+			 */
 
-	} else { /*auto sharing queue: if go to here, it means sharing queue
-		  *is ready and it is created by previous dp_register_subif_ext
-		  */
+		} else { /*auto sharing queue: if go to here,
+			  *it means sharing queue
+			  *is ready and it is created by previous
+			  *dp_register_subif_ext
+			  */
 
-		/*get already stored q_node_id/qos_port id to q_port
-		 */
-		q_port.qid = dp_deq_port_tbl[inst][q_port.cqe_deq].first_qid;
-		q_port.q_node = dp_deq_port_tbl[inst][q_port.cqe_deq].q_node;
-		q_port.port_node =
-			dp_deq_port_tbl[inst][q_port.cqe_deq].qos_port;
-		dp_q_tbl[inst][q_port.qid].ref_cnt++;
-		dp_deq_port_tbl[inst][q_port.cqe_deq].ref_cnt++;
-		if (port_info->num_dma_chan)
-			atomic_inc(&(dp_dma_chan_tbl[inst] +
-				   dma_ch_offset)->ref_cnt);
-	}
-	DP_DEBUG(DP_DBG_FLAG_QOS,
-		 "%s:%s=%d %s=%d q[%d].cnt=%d cqm_p[%d].cnt=%d tx_dma_chan ref=%d\n",
-		 "subif_hw_set",
-		 "dp_port", portid,
-		 "vap", subif_ix,
-		 q_port.qid, dp_q_tbl[inst][q_port.qid].ref_cnt,
-		 q_port.cqe_deq, dp_deq_port_tbl[inst][q_port.cqe_deq].ref_cnt,
-		 atomic_read(&(dp_dma_chan_tbl[inst] +
-			     dma_ch_offset)->ref_cnt));
+			/*get already stored q_node_id/qos_port id to q_port
+			 */
+			q_port.qid =
+				dp_deq_port_tbl[inst][q_port.cqe_deq].first_qid;
+			q_port.q_node =
+				dp_deq_port_tbl[inst][q_port.cqe_deq].q_node;
+			q_port.port_node =
+				dp_deq_port_tbl[inst][q_port.cqe_deq].qos_port;
+			dp_q_tbl[inst][q_port.qid].ref_cnt++;
+			dp_deq_port_tbl[inst][q_port.cqe_deq].ref_cnt++;
+			if (port_info->num_dma_chan)
+				atomic_inc(&(dp_dma_chan_tbl[inst] +
+						dma_ch_offset)->ref_cnt);
+		}
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "%s:%s=%d %s=%d q[%d].cnt=%d cqm_p[%d].cnt=%d %s=%d\n",
+			 "subif_hw_set",
+			 "dp_port", portid,
+			 "vap", subif_ix,
+			 q_port.qid, dp_q_tbl[inst][q_port.qid].ref_cnt,
+			 q_port.cqe_deq,
+			 dp_deq_port_tbl[inst][q_port.cqe_deq].ref_cnt,
+			 "tx_dma_chan ref",
+			 atomic_read(&(dp_dma_chan_tbl[inst] +
+						dma_ch_offset)->ref_cnt));
 #if IS_ENABLED(CONFIG_INTEL_DATAPATH_QOS_HAL)
-	if (dp_deq_port_tbl[inst][q_port.cqe_deq].ref_cnt == 1) /*first CTP*/
-		data->act = TRIGGER_CQE_DP_ENABLE;
+		if (dp_deq_port_tbl[inst][q_port.cqe_deq].ref_cnt == 1)
+			data->act = TRIGGER_CQE_DP_ENABLE; /*first CTP*/
 #else
-	if (q_port.f_deq_port_en)
-		data->act = TRIGGER_CQE_DP_ENABLE;
+		if (q_port.f_deq_port_en)
+			data->act = TRIGGER_CQE_DP_ENABLE;
 #endif
-	/* update caller dp_subif_data.q_id with allocated queue number */
-	data->subif_data->q_id = q_port.qid;
-	/*update subif table */
-	sif->qid = q_port.qid;
-	sif->q_node = q_port.q_node;
-	sif->qos_deq_port = q_port.port_node;
-	sif->cqm_deq_port = q_port.cqe_deq;
-	sif->cqm_port_idx = deq_port_idx;
-
-	qid = q_port.qid;
-	if (data->subif_data->flag_ops & DP_SUBIF_VANI) {
-		lookup_f &= ~CBM_QUEUE_MAP_F_MPE2_DONTCARE;
-		lookup_f |= CBM_QUEUE_MAP_F_SUBIF_DONTCARE;
-		lookup.mpe2 = 0;
-		/* Map to CPU Q */
-		qid = get_dp_port_info(inst, 0)->subif_info[0].qid;
-	} else if (port_info->alloc_flags & DP_F_VUNI) {
-		/* Map to GSWIP */
-		lookup_f &= ~CBM_QUEUE_MAP_F_MPE2_DONTCARE;
-		lookup_f &= ~CBM_QUEUE_MAP_F_MPE1_DONTCARE;
-		lookup_f |= CBM_QUEUE_MAP_F_SUBIF_DONTCARE;
-		lookup.mpe2 = 1;
-		lookup.mpe1 = 0;
+		/* update caller dp_subif_data.q_id with allocated queue num */
+		data->subif_data->q_id = q_port.qid;
+		/*update subif table */
+		sif->qid_list[i] = q_port.qid;
+		sif->q_node[i] = q_port.q_node;
+		sif->qos_deq_port[i] = q_port.port_node;
+		sif->cqm_deq_port[i] = q_port.cqe_deq;
+		sif->cqm_port_idx = deq_port_idx;
+		port_info->subif_info[subif_ix].cqm_port_idx = deq_port_idx;
+		qid = q_port.qid;
+		if (data->subif_data->flag_ops & DP_SUBIF_VANI) {
+			lookup_f &= ~CBM_QUEUE_MAP_F_MPE2_DONTCARE;
+			lookup_f |= CBM_QUEUE_MAP_F_SUBIF_DONTCARE;
+			lookup.mpe2 = 0;
+			/* Map to CPU Q */
+			qid = get_dp_port_info(inst, 0)->subif_info[0].qid;
+		} else if (port_info->alloc_flags & DP_F_VUNI) {
+			/* Map to GSWIP */
+			lookup_f &= ~CBM_QUEUE_MAP_F_MPE2_DONTCARE;
+			lookup_f &= ~CBM_QUEUE_MAP_F_MPE1_DONTCARE;
+			lookup_f |= CBM_QUEUE_MAP_F_SUBIF_DONTCARE;
+			lookup.mpe2 = 1;
+			lookup.mpe1 = 0;
+		}
+		/* Map this port's lookup to its 1st queue only */
+		lookup.ep = portid;
+		if (port_info->alloc_flags & DP_F_EPON) {
+			lookup.sub_if_id = deq_port_idx + i;
+		} else {
+			lookup.sub_if_id = subif; /*Note:CQM need full subif(15bits)*/
+		}
+			/* For 1st subif and mode 0, use CBM_QUEUE_MAP_F_SUBIF_DONTCARE,
+			 * otherwise, don't use this flag
+			 */
+			if (!port_info->num_subif &&
+			    (port_info->cqe_lu_mode == CQE_LU_MODE0))
+				lookup_f |= CBM_QUEUE_MAP_F_SUBIF_DONTCARE;
+		cbm_queue_map_set(dp_port_prop[inst].cbm_inst, qid,
+				  &lookup, lookup_f);
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "%s %s=%d %s=%d %s=%d %s=%d %s=0x%x %s=0x%x(%d)\n",
+			 "cbm_queue_map_set",
+			 "qid", qid, "for dp_port", lookup.ep,
+			 "num_subif", port_info->num_subif,
+			 "lu_mode", port_info->cqe_lu_mode, "flag", lookup_f,
+			 "subif", subif, subif_ix);
 	}
-	/* Map this port's lookup to its 1st queue only */
-	lookup.ep = portid;
-	lookup.sub_if_id = subif; /* Note:CQM API need full subif(15bits) */
-	/* For 1st subif and mode 0, use CBM_QUEUE_MAP_F_SUBIF_DONTCARE,
-	 * otherwise, don't use this flag
-	 */
-	if (!port_info->num_subif &&
-	    (port_info->cqe_lu_mode == CQE_LU_MODE0))
-		lookup_f |= CBM_QUEUE_MAP_F_SUBIF_DONTCARE;
-	cbm_queue_map_set(dp_port_prop[inst].cbm_inst,
-			  qid,
-			  &lookup,
-			  lookup_f);
-	DP_DEBUG(DP_DBG_FLAG_QOS,
-		 "%s %s=%d %s=%d %s=%d %s=%d %s=0x%x %s=0x%x(%d)\n",
-		 "cbm_queue_map_set",
-		 "qid", qid,
-		 "for dp_port", lookup.ep,
-		 "num_subif", port_info->num_subif,
-		 "lu_mode", port_info->cqe_lu_mode,
-		 "flag", lookup_f,
-		 "subif", subif, subif_ix);
 	return 0;
 }
 
 static int subif_hw_reset(int inst, int portid, int subif_ix,
 			  struct subif_platform_data *data, u32 flags)
 {
-	int qid;
+	int qid, i;
 	int cqm_deq_port;
 	int dma_ch_offset;
 	struct pmac_port_info *port_info = get_dp_port_info(inst, portid);
@@ -1791,123 +1821,131 @@ static int subif_hw_reset(int inst, int portid, int subif_ix,
 	struct dp_subif_info *sif = get_dp_port_subif(port_info, subif_ix);
 	int bp = sif->bp;
 
-	qid = sif->qid;
-	cqm_deq_port = sif->cqm_deq_port;
-	dma_ch_offset = dp_deq_port_tbl[inst][cqm_deq_port].dma_ch_offset;
-	bp = sif->bp;
+	for (i = 0; i < sif->num_qid; i++) {
+		qid = sif->qid_list[i];
+		cqm_deq_port = sif->cqm_deq_port[i];
+		dma_ch_offset =
+			dp_deq_port_tbl[inst][cqm_deq_port].dma_ch_offset;
+		bp = sif->bp;
 
-	if (!dp_dma_chan_tbl[inst]) {
-		PR_ERR("dp_dma_chan_tbl[%d] NULL\n", inst);
-		return DP_FAILURE;
-	}
-	/* santity check table */
-	if (!dp_q_tbl[inst][qid].ref_cnt) {
-		PR_ERR("Why dp_q_tbl[%d][%d].ref_cnt Zero: expect > 0\n",
-		       inst, qid);
-		return DP_FAILURE;
-	}
-	if (!dp_deq_port_tbl[inst][cqm_deq_port].ref_cnt) {
-		PR_ERR("Why dp_deq_port_tbl[%d][%d].ref_cnt Zero\n",
-		       inst, cqm_deq_port);
-		return DP_FAILURE;
-	}
-	if ((sif->ctp_dev) &&
-	    !dp_bp_dev_tbl[inst][bp].ref_cnt) {
-		PR_ERR("Why dp_bp_dev_tbl[%d][%d].ref_cnt =%d\n",
-		       inst, bp, dp_bp_dev_tbl[inst][bp].ref_cnt);
-		return DP_FAILURE;
-	}
-	/* update queue/port/sched/bp_pmapper table's ref_cnt */
-	dp_q_tbl[inst][qid].ref_cnt--;
-	dp_deq_port_tbl[inst][cqm_deq_port].ref_cnt--;
-	if (port_info->num_dma_chan)
-		atomic_dec(&(dp_dma_chan_tbl[inst] + dma_ch_offset)->ref_cnt);
-	if (sif->ctp_dev) { /* pmapper */
-		sif->ctp_dev = NULL;
-		dp_bp_dev_tbl[inst][bp].ref_cnt--;
-		if (!dp_bp_dev_tbl[inst][bp].ref_cnt) {
-			dp_bp_dev_tbl[inst][bp].dev = NULL;
-			dp_bp_dev_tbl[inst][bp].flag = 0;
-			DP_DEBUG(DP_DBG_FLAG_REG,
-				 "ctp ref_cnt becomes zero:%s\n",
-				 sif->device_name);
+		if (!dp_dma_chan_tbl[inst]) {
+			PR_ERR("dp_dma_chan_tbl[%d] NULL\n", inst);
+			return DP_FAILURE;
 		}
-	}
-	/* Check for max_ctp since CTP,BP is shared,
-	 * for all subif in case of DSL
-	 */
-	if (port_info->ctp_max == 1) {
-		if (port_info->num_subif == 0) {
-			reset_ctp_bp(inst, 0, portid, bp);
-			if (!dp_bp_dev_tbl[inst][bp].dev) {
-				/*NULL already, then free it */
-				DP_DEBUG(DP_DBG_FLAG_PAE,
-					 "Free BP[%d] vap=%d\n", bp, subif_ix);
-				free_bridge_port(inst, bp);
-			}
+		/* santity check table */
+		if (!dp_q_tbl[inst][qid].ref_cnt) {
+			PR_ERR("Why dp_q_tbl[%d][%d].ref_cnt Zero:expect > 0\n",
+			       inst, qid);
+			return DP_FAILURE;
 		}
-	} else {
-		reset_ctp_bp(inst, subif_ix, portid, bp);
-		if (!dp_bp_dev_tbl[inst][bp].dev) {
-			/*NULL already, then free it */
-			DP_DEBUG(DP_DBG_FLAG_PAE, "Free BP[%d] vap=%d\n",
-				 bp, subif_ix);
-			free_bridge_port(inst, bp);
+		if (!dp_deq_port_tbl[inst][cqm_deq_port].ref_cnt) {
+			PR_ERR("Why dp_deq_port_tbl[%d][%d].ref_cnt Zero\n",
+			       inst, cqm_deq_port);
+			return DP_FAILURE;
+		}
+		if ((sif->ctp_dev) &&
+		    !dp_bp_dev_tbl[inst][bp].ref_cnt) {
+			PR_ERR("Why dp_bp_dev_tbl[%d][%d].ref_cnt =%d\n",
+			       inst, bp, dp_bp_dev_tbl[inst][bp].ref_cnt);
+			return DP_FAILURE;
+		}
+		/* update queue/port/sched/bp_pmapper table's ref_cnt */
+		dp_q_tbl[inst][qid].ref_cnt--;
+		dp_deq_port_tbl[inst][cqm_deq_port].ref_cnt--;
+		if (port_info->num_dma_chan)
+			atomic_dec(&(dp_dma_chan_tbl[inst] +
+					dma_ch_offset)->ref_cnt);
+		if (i == 0) { /* Cannot reset BP for all qid */
+			if (sif->ctp_dev) { /* pmapper */
+				sif->ctp_dev = NULL;
+				dp_bp_dev_tbl[inst][bp].ref_cnt--;
+				if (!dp_bp_dev_tbl[inst][bp].ref_cnt) {
+					dp_bp_dev_tbl[inst][bp].dev = NULL;
+					dp_bp_dev_tbl[inst][bp].flag = 0;
+					DP_DEBUG(DP_DBG_FLAG_REG,
+						 "ctp ref_cnt become zero:%s\n",
+						 sif->device_name);
+				}
+			}
+			/* Check for max_ctp since CTP,BP is shared,
+			 * for all subif in case of DSL
+			 */
+			if (port_info->ctp_max == 1) {
+				if (port_info->num_subif == 0) {
+					reset_ctp_bp(inst, 0, portid, bp);
+					if (!dp_bp_dev_tbl[inst][bp].dev) {
+						/*NULL already, then free it */
+						DP_DEBUG(DP_DBG_FLAG_PAE,
+							 "Free BP[%d] vap=%d\n",
+							 bp, subif_ix);
+						free_bridge_port(inst, bp);
+					}
+				}
+			} else {
+				reset_ctp_bp(inst, subif_ix, portid, bp);
+				if (!dp_bp_dev_tbl[inst][bp].dev) {
+					/*NULL already, then free it */
+					DP_DEBUG(DP_DBG_FLAG_PAE, "Free BP[%d] vap=%d\n",
+							bp, subif_ix);
+					free_bridge_port(inst, bp);
+				}
+			}
 		}
-	}
 #ifdef CONFIG_INTEL_DATAPATH_QOS_HAL
-	qid = sif->qid;
-	cqm_deq_port = dp_q_tbl[inst][qid].cqm_dequeue_port;
-
-	if (dp_q_tbl[inst][qid].flag &&
-	    !dp_q_tbl[inst][qid].ref_cnt &&/*no one is using */
-	    dp_q_tbl[inst][qid].need_free) {
-		DP_DEBUG(DP_DBG_FLAG_QOS, "Free qid %d\n", qid);
-		node.id.q_id = qid;
-		/*if no subif using this queue, need to delete it*/
-		node.inst = inst;
-		node.dp_port = portid;
-		node.type = DP_NODE_QUEUE;
-		dp_node_free(&node, 0);
-
-		/*update dp_q_tbl*/
-		dp_q_tbl[inst][qid].flag = 0;
-		dp_q_tbl[inst][qid].need_free = 0;
-		if (dp_deq_port_tbl[inst][cqm_deq_port].f_first_qid &&
-		    (dp_deq_port_tbl[inst][cqm_deq_port].first_qid
-		     == qid)) {
-			dp_deq_port_tbl[inst][cqm_deq_port].f_first_qid = 0;
-			dp_deq_port_tbl[inst][cqm_deq_port].first_qid = 0;
-			DP_DEBUG(DP_DBG_FLAG_QOS, "q_id[%d] is freed\n", qid);
-			DP_DEBUG(DP_DBG_FLAG_QOS,
-				 "dp_deq_port_tbl[%d][%d].f_first_qid reset\n",
-				 inst, cqm_deq_port);
+		cqm_deq_port = dp_q_tbl[inst][qid].cqm_dequeue_port;
+
+		if (dp_q_tbl[inst][qid].flag &&
+		    !dp_q_tbl[inst][qid].ref_cnt &&/*no one is using */
+		    dp_q_tbl[inst][qid].need_free) {
+			DP_DEBUG(DP_DBG_FLAG_QOS, "Free qid %d\n", qid);
+			node.id.q_id = qid;
+			/*if no subif using this queue, need to delete it*/
+			node.inst = inst;
+			node.dp_port = portid;
+			node.type = DP_NODE_QUEUE;
+			dp_node_free(&node, 0);
+
+			/*update dp_q_tbl*/
+			dp_q_tbl[inst][qid].flag = 0;
+			dp_q_tbl[inst][qid].need_free = 0;
+			if (dp_deq_port_tbl[inst][cqm_deq_port].f_first_qid &&
+			    (dp_deq_port_tbl[inst][cqm_deq_port].first_qid
+								== qid)) {
+				dp_deq_port_tbl[inst][cqm_deq_port].f_first_qid = 0;
+				dp_deq_port_tbl[inst][cqm_deq_port].first_qid = 0;
+				DP_DEBUG(DP_DBG_FLAG_QOS, "q_id[%d] is freed\n",
+					 qid);
+				DP_DEBUG(DP_DBG_FLAG_QOS,
+					 "dp_deq_port_tbl[%d][%d].f_first_qid reset\n",
+					 inst, cqm_deq_port);
+			}
+		} else {
+			DP_DEBUG(DP_DBG_FLAG_QOS, "q_id[%d] dont need freed\n",
+				 qid);
 		}
-	} else {
-		DP_DEBUG(DP_DBG_FLAG_QOS, "q_id[%d] dont need freed\n", qid);
-	}
-	DP_DEBUG(DP_DBG_FLAG_QOS,
-		 "%s:%s=%d %s=%d q[%d].cnt=%d cqm_p[%d].cnt=%d tx_dma_chan ref=%d\n",
-		 "subif_hw_reset",
-		 "dp_port", portid,
-		 "vap", subif_ix,
-		 qid, dp_q_tbl[inst][qid].ref_cnt,
-		 cqm_deq_port, dp_deq_port_tbl[inst][cqm_deq_port].ref_cnt,
-		 atomic_read(&(dp_dma_chan_tbl[inst] +
-			     dma_ch_offset)->ref_cnt));
+		DP_DEBUG(DP_DBG_FLAG_QOS,
+			 "%s:%s=%d %s=%d q[%d].cnt=%d cqm_p[%d].cnt=%d %s=%d\n",
+			 "subif_hw_reset", "dp_port", portid,
+			 "vap", subif_ix, qid, dp_q_tbl[inst][qid].ref_cnt,
+			 cqm_deq_port,
+			 dp_deq_port_tbl[inst][cqm_deq_port].ref_cnt,
+			 "tx_dma_chan_ref",
+			 atomic_read(&(dp_dma_chan_tbl[inst] +
+						dma_ch_offset)->ref_cnt));
 #else
-	qos_queue_flush(priv->qdev, sif->q_node);
-	qos_queue_remove(priv->qdev, sif->q_node);
-	qos_port_remove(priv->qdev, sif->qos_deq_port);
-	priv->deq_port_stat[sif->cqm_deq_port].flag = PP_NODE_FREE;
+		qos_queue_flush(priv->qdev, sif->q_node);
+		qos_queue_remove(priv->qdev, sif->q_node);
+		qos_port_remove(priv->qdev, sif->qos_deq_port);
+		priv->deq_port_stat[sif->cqm_deq_port].flag = PP_NODE_FREE;
 #endif /* CONFIG_INTEL_DATAPATH_QOS_HAL */
 
-	if (!port_info->num_subif &&
-	    dp_deq_port_tbl[inst][cqm_deq_port].ref_cnt) {
-		PR_ERR("num_subif(%d) not match dp_deq_port[%d][%d].ref_cnt\n",
-		       port_info->num_subif,
-		       inst, cqm_deq_port);
-		return DP_FAILURE;
+		if (!port_info->num_subif &&
+		    dp_deq_port_tbl[inst][cqm_deq_port].ref_cnt) {
+			PR_ERR("num_subif(%d) not match %s[%d][%d].ref_cnt\n",
+			       port_info->num_subif, "dp_deq_port", inst,
+			       cqm_deq_port);
+			return DP_FAILURE;
+		}
 	}
 
 	return DP_SUCCESS;
diff --git a/drivers/net/datapath/dpm/gswip31/datapath_misc.h b/drivers/net/datapath/dpm/gswip31/datapath_misc.h
index d828dbce306f..9d24d2560121 100644
--- a/drivers/net/datapath/dpm/gswip31/datapath_misc.h
+++ b/drivers/net/datapath/dpm/gswip31/datapath_misc.h
@@ -162,7 +162,9 @@ int dp_get_gsw_pmapper_31(int inst, int bport, int lport,
 			  struct dp_pmapper *mapper, u32 flag);
 int dp_children_get_31(struct dp_node_child *cfg, int flag);
 int dp_free_children_via_parent_31(struct dp_node_alloc *node, int flag);
-int dp_node_reserve(int inst, int ep, struct dp_port_data *data, int flags);
+int dp_node_reserve(int inst, int ep, struct dp_dev_data *data, int flags);
+int dp_q_reserve_continuos(int inst, int ep, struct dp_dev_data *data,
+			   int flags);
 int dp_qos_level_get_31(struct dp_qos_level *dp, int flag);
 int dp_meter_alloc_31(int inst, int *meterid, int flag);
 int dp_meter_add_31(struct net_device *dev,
diff --git a/drivers/net/datapath/dpm/gswip31/datapath_ppv4.c b/drivers/net/datapath/dpm/gswip31/datapath_ppv4.c
index 7a4ef2caf49c..0bba3535aad5 100644
--- a/drivers/net/datapath/dpm/gswip31/datapath_ppv4.c
+++ b/drivers/net/datapath/dpm/gswip31/datapath_ppv4.c
@@ -262,7 +262,7 @@ int init_ppv4_qos(int inst, int flag)
 		goto EXIT;
 	}
 	qos_port_conf_set_default(&t->p_conf);
-	t->p_conf.port_parent_prop.arbitration = PP_QOS_ARBITRATION_WSP;
+	t->p_conf.port_parent_prop.arbitration = PP_QOS_ARBITRATION_WRR;
 	t->p_conf.ring_address =
 	(void *)dp_deq_port_tbl[inst][idx].txpush_addr_qos;
 	t->p_conf.ring_size = dp_deq_port_tbl[inst][idx].tx_ring_size;
diff --git a/drivers/net/datapath/dpm/gswip31/datapath_ppv4_api.c b/drivers/net/datapath/dpm/gswip31/datapath_ppv4_api.c
index 1ee3e0f5b127..9f791343933f 100644
--- a/drivers/net/datapath/dpm/gswip31/datapath_ppv4_api.c
+++ b/drivers/net/datapath/dpm/gswip31/datapath_ppv4_api.c
@@ -4498,8 +4498,137 @@ int dp_children_get_31(struct dp_node_child *cfg, int flag)
 	return DP_FAILURE;
 }
 
-/* #define DP_SUPPORT_RES_RESERVE */
-int dp_node_reserve(int inst, int ep, struct dp_port_data *data, int flags)
+static int dp_queue_alloc_conf(int inst, int *logical_id, int *qid)
+{
+	unsigned int id = 0;
+	struct hal_priv *priv = HAL(inst);
+	struct pp_qos_queue_conf q_conf;
+	struct pp_qos_queue_info info;
+	int res = DP_SUCCESS;
+
+	if (qos_queue_allocate(priv->qdev, &id)) {
+		res = DP_FAILURE;
+		PR_ERR("qos_queue_allocate failed\n");
+		return res;
+	}
+	qos_queue_conf_set_default(&q_conf);
+	q_conf.wred_enable = 0;
+	q_conf.queue_wred_max_allowed = DEF_QRED_MAX_ALLOW;
+	q_conf.queue_child_prop.parent = priv->ppv4_drop_p;
+	if (qos_queue_set(priv->qdev, id, &q_conf)) {
+		res = DP_FAILURE;
+		PR_ERR("qos_queue_set fail for queue=%d to parent=%d\n",
+		       id, q_conf.queue_child_prop.parent);
+		goto EXIT;
+	}
+	DP_DEBUG(DP_DBG_FLAG_QOS, "Workaround queue(/%d)-> tmp parent(/%d)\n",
+		 id, q_conf.queue_child_prop.parent);
+	if (qos_queue_info_get(priv->qdev, id, &info)) {
+		res = DP_FAILURE;
+		PR_ERR("qos_queue_info_get: %d\n", id);
+		goto EXIT;
+	}
+	*qid = info.physical_id;
+	*logical_id = id;
+	return res;
+EXIT:
+	qos_queue_remove(priv->qdev, id);
+	return res;
+}
+
+int dp_q_reserve_continuous(int inst, int ep, struct dp_dev_data *data,
+			    int flags)
+{
+	int i;
+	unsigned int id = 0, qid = 0;
+	int len, curr_num = 0, curr_off = 0;
+	struct resv_q *resv_q, *tmp_resv_q;
+	struct hal_priv *priv = HAL(inst);
+	int res = DP_SUCCESS;
+
+	/* free resved queue */
+	if (flags == DP_F_DEREGISTER)
+		goto FREE_EXIT;
+	/* to reserve the queue */
+	if (data->num_resv_q <= 0) {
+		DP_ERR("Provide valid Q no for continuous Q reserve\n");
+		return DP_FAILURE;
+	}
+
+	len = sizeof(struct resv_q) * data->num_resv_q;
+	priv->resv[ep].resv_q = kzalloc(len, GFP_ATOMIC);
+	if (!priv->resv[ep].resv_q)
+		return DP_FAILURE;
+	tmp_resv_q = kzalloc(sizeof(*tmp_resv_q) * MAX_QUEUE, GFP_ATOMIC);
+	if (!tmp_resv_q) {
+		kfree(priv->resv[ep].resv_q);
+		return DP_FAILURE;
+	}
+	if (dp_queue_alloc_conf(inst, &id, &qid)) {
+		PR_ERR("qos_queue_allocate failed\n");
+		kfree(priv->resv[ep].resv_q);
+		kfree(tmp_resv_q);
+		return DP_FAILURE;
+	}
+	tmp_resv_q[curr_num].id = id;
+	tmp_resv_q[curr_num].physical_id = qid;
+	curr_num++;
+	while (((curr_num) - curr_off) < data->num_resv_q) {
+		if (dp_queue_alloc_conf(inst, &id, &qid)) {
+			res = DP_FAILURE;
+			PR_ERR("qos_queue_allocate failed\n");
+			kfree(priv->resv[ep].resv_q);
+			goto clear_q;
+		}
+		tmp_resv_q[curr_num].id = id;
+		tmp_resv_q[curr_num].physical_id = qid;
+		curr_num++;
+		if ((tmp_resv_q[curr_num - 1].physical_id -
+		     tmp_resv_q[curr_num - 2].physical_id == 1) ||
+		     (tmp_resv_q[curr_num - 2].physical_id -
+		     tmp_resv_q[curr_num - 1].physical_id == 1)) {
+			DP_DEBUG(DP_DBG_FLAG_QOS, "%d %d\n",
+				 curr_num, curr_off);
+			continue;
+		} else {
+			curr_off = curr_num - 1;
+		}
+	}
+	if (curr_num - curr_off == data->num_resv_q) {
+		/* Copy information to priv structure */
+		resv_q = priv->resv[ep].resv_q;
+		for (i = 0; i < data->num_resv_q; i++) {
+			resv_q[i].id = tmp_resv_q[curr_off + i].id;
+			resv_q[i].physical_id =
+				tmp_resv_q[curr_off + i].physical_id;
+			priv->resv[ep].num_resv_q = i + 1;
+			DP_DEBUG(DP_DBG_FLAG_QOS, "reseve q[%d/%d]\n",
+				 resv_q[i].id, resv_q[i].physical_id);
+		}
+	} else {
+		goto clear_q;
+	}
+clear_q:
+	/* clear other non-continuously allocated Q */
+	for (i = 0; i < curr_off; i++)
+		qos_queue_remove(priv->qdev, tmp_resv_q[i].id);
+	kfree(tmp_resv_q);
+	data->qos_resv_q_base = resv_q[curr_off].physical_id;
+	return res;
+FREE_EXIT:
+	if (priv->resv[ep].resv_q) {
+		struct resv_q  *resv_q = priv->resv[ep].resv_q;
+
+		for (i = 0; i < priv->resv[ep].num_resv_q; i++)
+			qos_queue_remove(priv->qdev, resv_q[i].id);
+		kfree(resv_q);
+		priv->resv[ep].resv_q = NULL;
+		priv->resv[ep].num_resv_q = 0;
+	}
+	return res;
+}
+
+int dp_node_reserve(int inst, int ep, struct dp_dev_data *data, int flags)
 {
 	int i;
 	unsigned int id;
@@ -4512,10 +4641,6 @@ int dp_node_reserve(int inst, int ep, struct dp_port_data *data, int flags)
 	struct hal_priv *priv = HAL(inst);
 	int res = DP_SUCCESS;
 
-#ifndef DP_SUPPORT_RES_RESERVE /* immediately return */
-	return res;
-#endif
-
 	if (!priv) {
 		PR_ERR("priv cannot be NULL\n");
 		return DP_FAILURE;
@@ -4525,12 +4650,11 @@ int dp_node_reserve(int inst, int ep, struct dp_port_data *data, int flags)
 	if (flags == DP_F_DEREGISTER)
 		goto FREE_EXIT;
 
-	/* Need reserve for queue/scheduler */
-/* #define DP_SUPPORT_RES_TEST */
-#ifdef DP_SUPPORT_RES_TEST
-	data->num_resv_q = 4;
-	data->num_resv_sched = 4;
-#endif
+	if (data->flag_ops & (DP_F_DEV_RESV_Q | DP_F_DEV_CONTINUOUS_Q)) {
+		dp_q_reserve_continuous(inst, ep, data, flags);
+		return res;
+	}
+
 	/* to reserve the queue */
 	if (data->num_resv_q <= 0)
 		goto RESERVE_SCHED;
diff --git a/drivers/net/datapath/dpm/gswip31/datapath_tx.c b/drivers/net/datapath/dpm/gswip31/datapath_tx.c
index 0b220c6ae35a..21653cd372fb 100644
--- a/drivers/net/datapath/dpm/gswip31/datapath_tx.c
+++ b/drivers/net/datapath/dpm/gswip31/datapath_tx.c
@@ -113,7 +113,7 @@ int32_t dp_xmit_31(struct net_device *rx_if, dp_subif_t *rx_subif,
 	char tx_chksum_flag = 0; /*check csum cal can be supported or not */
 	char insert_pmac_f = 1; /*flag to insert one pmac */
 	int res = DP_SUCCESS;
-	int ep, vap;
+	int ep, vap, num_deq_port, deq_port_idx, class;
 	enum dp_xmit_errors err_ret = 0;
 	int inst = 0;
 	struct cbm_tx_data data;
@@ -328,6 +328,18 @@ int32_t dp_xmit_31(struct net_device *rx_if, dp_subif_t *rx_subif,
 	if (insert_pmac_f)
 		DP_CB(inst, set_pmac_subif)(&pmac, rx_subif->subif);
 
+	/* For EPON subifid set as Deq Port Idx + Class */
+	if (dp_info->alloc_flags & DP_F_EPON) {
+		num_deq_port = get_dp_port_subif(dp_info, vap)->num_qid;
+		deq_port_idx = get_dp_port_subif(dp_info, vap)->cqm_port_idx;
+		if (skb->priority <= num_deq_port)
+			class = skb->priority;
+		else
+			class = num_deq_port - 1;
+		desc_0->field.dest_sub_if_id = deq_port_idx + class;
+		DP_CB(inst, set_pmac_subif)(&pmac, deq_port_idx + class);
+	}
+
 	if (unlikely(dp_dbg_flag)) {
 		if (insert_pmac_f)
 			dp_xmit_dbg("After", skb, ep, len, flags, &pmac,
diff --git a/drivers/net/ethernet/lantiq/cqm/prx300/cqm.c b/drivers/net/ethernet/lantiq/cqm/prx300/cqm.c
index 6470bfa62152..62a5c49c4849 100644
--- a/drivers/net/ethernet/lantiq/cqm/prx300/cqm.c
+++ b/drivers/net/ethernet/lantiq/cqm/prx300/cqm.c
@@ -13,7 +13,7 @@
 #define MAX_PORT(FLAG) (((FLAG) & FLAG_LAN) ? LAN_PORT_NUM : 1)
 #define OWN_BIT  BIT(31)
 #define COMPLETE_BIT  BIT(30)
-#define CQM_PON_IP_BASE_ADDR 0x1000
+#define CQM_PON_IP_BASE_ADDR 0x181003FC
 #define CQM_PON_IP_PKT_LEN_ADJ_BYTES 9
 #define PRX300_CQM_DROP_INIT ((PRX300_CQM_DROP_Q << 24) | \
 			   (PRX300_CQM_DROP_Q << 16) | \
@@ -2412,7 +2412,7 @@ static s32 dp_port_alloc_complete(struct module *owner, struct net_device *dev,
 		reg = (EPON_EPON_MODE_REG_EPONCHKEN_MASK |
 		       EPON_EPON_MODE_REG_EPONPKTSIZADJ_MASK |
 		       (data->deq_port << EPON_EPON_MODE_REG_EPONBASEPORT_POS) |
-		       ((data->qid_base + data->num_qid) <<
+		       ((data->qid_base + (data->num_qid - 1)) <<
 			 EPON_EPON_MODE_REG_ENDQ_POS)
 		       | data->qid_base);
 		cbm_w32(cqm_ctrl->enq + EPON_EPON_MODE_REG, reg);
diff --git a/include/net/datapath_api.h b/include/net/datapath_api.h
index 6dbb4f9c94a7..a76a8f23831b 100644
--- a/include/net/datapath_api.h
+++ b/include/net/datapath_api.h
@@ -310,6 +310,13 @@ enum DP_SPL_TYPE {
 				 *     pmapper althogh GSIWP support 64
 				 *     in pcp mode
 				 */
+#define DP_MAX_DEQ_PER_SUBIF 8 /*!<@brief the maximum number of dequeue port per
+				* subif Note : one subif, it can create more
+				* than 8 queues.
+				* for example of epon, one subif can use up to 8
+				* dequeue port, each dequeue port will use 1 Q
+				* only
+				*/
 #define DP_PMAPPER_DISCARD_CTP 0xFFFF  /*!<@brief Discard ctp flag for pmapper*/
 /*! @brief structure for pmapper */
 struct dp_pmapper {
@@ -354,7 +361,13 @@ typedef struct dp_subif {
 		   *   normally one GPID per subif for non PON device.
 		   *   For PON case, one GPID per bridge port
 		   */
-	u16 def_qid; /*!< [out] default physical queue id assigned by DP */
+	u8  num_q; /*!< [out] num of default number of queues allocated by DP */
+	union {
+		u16 def_qid; /*!< [out] def physical queue id assigned by DP */
+		u16 def_qlist[DP_MAX_DEQ_PER_SUBIF]; /*!< [in/out] default
+						      * physical qid list 
+						      * assigned by DP */
+	};
 	int lookup_mode; /*!< [out] CQM lookup mode for this device
 			  *   (dp_port based)
 			  *   valid for dp_get_netif_subifid only
@@ -745,6 +758,7 @@ enum DP_SUBIF_DATA_FLAG {
 				       */
 	DP_SUBIF_LCT = BIT(2), /*!< Register as LCT port */
 	DP_SUBIF_VANI = BIT(3), /*!< Register as vANI Subif */
+	DP_SUBIF_DEQPORT_NUM = BIT(4), /*!< Specify num of deq port per subif */
 };
 
 /*! @brief dp_subif_id struct for get_netif_subif */
@@ -850,6 +864,7 @@ struct dp_subif_data {
 	u16 mac_learn_disable; /*!< [in] To enable or disable
 				* mac learning for subif
 				*/
+	u16 num_deq_port; /*!< [in] To specify number of DEQ_PORT one subif */
 };
 
 /*! @brief enum DP_F_DATA_RESV_CQM_PORT */
@@ -881,14 +896,14 @@ struct dp_port_data {
 			    */
 	u32 start_port_no; /*!< valid only if DP_F_DATA_RESV_CQM_PORT is set */
 
-	int num_resv_q; /*!< input:reserve the required number of queues. Valid
+	int num_resv_q; /*!< [in] reserve the required number of queues. Valid
 			 *   only if DP_F_DATA_RESV_Q bit valid in \ref flag_ops
 			 */
-	int num_resv_sched; /*!< input:reserve required number of schedulers.
+	int num_resv_sched; /*!< [in] reserve required number of schedulers.
 			     *   Valid only if DP_F_DATA_RESV_SCH bit valid in
 			     *   \ref flag_ops
 			     */
-	int deq_port_base; /*!< output: the CQM dequeue port base for the
+	int deq_port_base; /*!< [out] the CQM dequeue port base for the
 			    *   traffic to this device.
 			    */
 	int deq_num;  /*!< [out] the number of dequeue port allocated for the
@@ -976,6 +991,20 @@ struct dp_buf_info {
  */
 int dp_free_dc_buf(struct dp_buf_info *buf, int flag);
 
+/*! @brief enum DP_SUBIF_DATA_FLAG */
+enum DP_DEV_DATA_FLAG {
+	DP_F_DEV_RESV_Q = BIT(0), /*!< Reserve queues for this dev
+				   * based on num_resv_q
+				   */
+	DP_F_DEV_RESV_SCH = BIT(1), /*!< Reserve scheduler for this dev
+				     * based on num_resv_sched
+				     */
+	DP_F_DEV_CONTINUOUS_Q = BIT(2), /*!< Reserve continuous Q for this dev
+					 * DP_F_DEV_RESV_Q bit should also be
+					 * set for this continuous Q alloc
+					 */
+};
+
 /**
  * @brief dp_gpid_tx_info
  */
@@ -1227,23 +1256,27 @@ struct dp_umt {
  *  applications
  */
 struct dp_dev_data {
-	u8 num_rx_ring; /*!< [in] number of rx ring from DC device to Host.
-			 *   num_rx_ring requirement:
-			 *   @num_rings <= @DP_RX_RING_NUM
-			 *   GRX350/PRX300:1 rx ring
-			 *   LGM: up to 2 rx ring, like Docsis can use 2 rings
-			 *   For two ring case:
-			 *    1st rxout ring without qos
-			 *    2nd rxout ring with qos
-			 */
-	u8 num_tx_ring; /*!< [in] number of tx ring from Host to DC device
-			 *   num_rx_ring requirement:
-			 *   @num_rings <= @DP_TX_RING_NUM
-			 *   Normally it is 1 TX ring only.
-			 *   But for 5G, it can support up to 8 TX ring
-			 *   For docsis, alhtough it is 16 dequeue port to WIB.
-			 *   But the final ring only 1, ie, WIB to Dcosis
-			 */
+	enum DP_DEV_DATA_FLAG flag_ops; /*!< flag operation, for subif
+					 * registration
+					 * refer to enum DP_DEV_DATA_FLAG 
+					 */
+	u8 num_rx_ring;   /*!< [in] number of rx ring from DC device to Host.
+			    *   num_rx_ring requirement:
+			    *   @num_rings <= @DP_RX_RING_NUM
+			    *   GRX350/PRX300:1 rx ring
+			    *   LGM: up to 2 rx ring, like Docsis can use 2 rings
+			    *   For two ring case:
+			    *    1st rxout ring without qos
+			    *    2nd rxout ring with qos
+			    */
+	u8 num_tx_ring;   /*!< [in] number of tx ring from Host to DC device
+			    *   num_rx_ring requirement:
+			    *   @num_rings <= @DP_TX_RING_NUM
+			    *   Normally it is 1 TX ring only.
+			    *   But for 5G, it can support up to 8 TX ring
+			    *   For docsis, alhtough it is 16 dequeue port to WIB.
+			    *   But the final ring only 1, ie, WIB to Dcosis
+			    */
 	u8 num_umt_port;   /*!< [in] number of UMT port.
 			    *    Normally is 1 only. But Docsis can use up to 2
 			    */
@@ -1283,6 +1316,14 @@ struct dp_dev_data {
 			* DP need to add this parameter to fully use of shared
 			* HW resource.
 			*/
+	int num_resv_q; /*!< [in] reserve the required number of queues. Valid
+			 *   only if DP_F_DATA_RESV_Q bit valid in \ref flag_ops
+			 */
+	int num_resv_sched; /*!< [in] reserve required number of schedulers.
+			     *   Valid only if DP_F_DATA_RESV_SCH bit valid in
+			     *   \ref flag_ops
+			     */
+	u16 qos_resv_q_base; /*!< [out] PPv4 QoS reserved Q base */
 };
 
 /*! @addtogroup Datapath_Driver_API */
